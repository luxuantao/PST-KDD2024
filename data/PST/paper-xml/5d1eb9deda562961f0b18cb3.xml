<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Leakage from Gradients</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
							<email>ligeng@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<email>zhijian@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Leakage from Gradients</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradients exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. Thereby we want to raise people's awareness to rethink the gradient's safety. We also discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed training becomes necessary to speedup training on large-scale datasets. In a distributed learning system, the computation is executed parallely on each worker and synchronized via exchanging gradients (both parameter server <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> and all-reduce <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>). The distribution of computation naturally leads to the splitting of data: Each client has its own the training data and only communicates gradient during training (says the training set never leaves local machine). It allows to train a model using data from multiple sources without centralizing them. This scheme is named as collaborative learning and widely used when the training set contains private information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. For example, multiple hospitals train a model jointly without sharing their patients' medical data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Distributed training and collaborative learning have been widely used in large scale machine learning tasks. However, does the "gradient sharing" scheme protect the privacy of the training datasets of each participant? In most scenarios, people assume that gradients are safe to share and will not expose the training data. Some recent studies show that gradients reveal some properties of the training data, for example, property classifier <ref type="bibr" target="#b25">[26]</ref> (whether a sample with certain property is in the batch) and using generative adversarial networks to generate pictures that look similar to the training images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. Here we consider a more challenging case: can we completely steal the training data from gradients? Formally, given a machine learning model F () and its weights W , if we have the gradients ∇w w.r.t a pair of input and label, can we obtain the training data reversely? Conventional wisdom suggests that the answer is no, but we show that this is actually possible.</p><p>In this work, we demonstrate Deep Leakage from Gradients (DLG): sharing the gradients can leak private training data. We present an optimization algorithm that can obtain both the training inputs and the labels in just few iterations. To perform the attack, we first randomly generate a pair of "dummy" inputs and labels and then perform the usual forward and backward. After deriving the dummy gradients from the dummy data, instead of optimizing model weights as in typical training, we optimize the dummy inputs and labels to minimize the distance between dummy gradients and real gradients (illustrated in Fig. <ref type="figure">2</ref>). Matching the gradients makes the dummy data close to the &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V 1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V 1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V 1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h                         original ones (Fig. <ref type="figure">5</ref>). When the optimization finishes, the private training data (both inputs and labels) will be fully revealed.</p><formula xml:id="formula_0">l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_1">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_2">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_3">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_4">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_5">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_6">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_7">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_8">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_9">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_10">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_11">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_12">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_13">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_14">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_15">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_16">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_17">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_18">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_19">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_20">Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_21">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_22">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_23">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><formula xml:id="formula_24">G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s Z K v Y H E Q C D p D q s 1 t + 7 m I O v E K 0 g N C r S G 1 a / B K K Z p x K S h A r X u e 2 5 i / A y V 4 V S w e W W Q a p Y g n e K Y 9 S 2 V G D H t Z / m 9 c 3 J h l R E J Y 2 V L G p K r v y c y j L S e R Y H t j N B M 9 K q 3 E P / z + q k J G 3 7 G Z Z I a J u l y U Z g K Y m K y e J 6 M u G L U i J k l S B W 3 t x I 6 Q Y X U 2 I g q N g R v 9 e V</formula><p>Conventional "shallow" leakages (property inference <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> and generative model <ref type="bibr" target="#b11">[12]</ref> using class labels) requires extra label information and can only generate similar synthetic images. Our "deep" leakage is an optimization process and does not depend on any generative models; therefore, DLG does not require any other extra prior about the training set, instead, it can infer the label from shared gradients and the results produced by DLG (both images and texts) are the exact original training samples instead of synthetic look-alike alternatives. We evaluate the effectiveness of our algorithm on both vision (image classification) and language tasks (masked language model). On various datasets and tasks, DLG fully recovers the training data in just a few gradient steps. Such a deep leakage from gradients is first discovered and we want to raise people's awareness of rethinking the safety of gradients.</p><p>The deep leakage puts a severe challenge to the multi-node machine learning system. The fundamental gradient sharing scheme, as shown in our work, is not always reliable to protect the privacy of the training data. In centralized distributed training (Fig. <ref type="figure" target="#fig_25">1a</ref>), the parameter server, which usually does not store any training data, is able to steal local training data of all participants. For decentralized distributed training (Fig. <ref type="figure" target="#fig_25">1b</ref>), it becomes even worse since any participant can steal its neighbors' private training data. To prevent the deep leakage, we demonstrate three defense strategies: gradient perturbation, low precision, and gradient compression. For gradient perturbation, we find both Gaussian and Laplacian noise with a scale higher than 10 −2 would be a good defense. While half precision fails to protect, gradient compression successfully defends the attack with the pruned gradient is more than 20%.</p><p>Our contributions include:</p><p>• We demonstrate that it is possible to obtain the private training data from the publicly shared gradients. To our best knowledge, DLG is the first algorithm achieving it.</p><p>• DLG only requires the gradients and can reveal pixel-wise accurate images and token-wise matching texts. While conventional approaches usually need extra information to attack and only produce partial properties or synthetic alternatives.</p><p>• To prevent potential leakage of important data, we analyze the attack difficulties in various settings and discuss several defense strategies against the attack.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributed Training</head><p>Training large machine learning models (e.g., deep neural networks) is computationally intensive. In order to finish the training process in a reasonable time, many studies worked on distributed training to speedup. There are many works that aim to improve the scalability of distributed training, both at the algorithm level <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> and at the framework level <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. Most of them adapt synchronous SGD as the backbone because the stable performance while scaling up.</p><p>In general, distributed training can be classified into two categories: with a parameter server (centralized) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> and without a parameter server (decentralized) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>). In both schemes, each node first performs the computation to update its local weights, and then sends gradients to other nodes. For the centralized mode, the gradients first get aggregated and then delivered back to each node. For decentralized mode, gradients are exchanged between neighboring nodes.</p><p>In many application scenarios, the training data is privacy-sensitive. For example, a patient's medical condition can not to be shared across hospitals. To avoid the sensitive information being leaked, collaborative learning has recently emerged <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref> where two or more participants can jointly train a model while the training dataset never leave each participants' local server. Only the gradients are shared across the network. This technique has been used to train models for medical treatments across multiple hospitals <ref type="bibr" target="#b16">[17]</ref>, analyze patient survival situations from various countries <ref type="bibr" target="#b15">[16]</ref> and build predictive keyboards to improve typing experience <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">"Shallow" Leakage from Gradients</head><p>Previous works have made some explorations on how to infer the information of training data from gradients. For some layers, the gradients already leak certain level of information. For example, the embedding layer in language tasks only produces gradients for words occurred in training data, which reveals what words have been used in other participant's training set <ref type="bibr" target="#b25">[26]</ref>. But such leakage is "shallow": The leaked information is unordered and hard to infer the original sentence due to ambiguity. Another case is fully connected layers, where observations of gradient updates can be used to infer output feature values. However, this cannot extend to convolutional layers because the size of the features is far larger than the size of weights.</p><p>Some recent works develop learning-based methods to infer properties of the batch. They show that a binary classifier trained on gradients is able to determine whether an exact data record (membership inference <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>) or a data record with certain properties (property inference <ref type="bibr" target="#b25">[26]</ref>) is included in the other participant's' batch. Furthermore, they train GAN models <ref type="bibr" target="#b8">[9]</ref> to synthesis images look similar to training data from the gradient <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>, but the attack is limited and only works when all class members look alike (e.g., face recognition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We show that it's possible to steal an image pixel-wise and steal a sentence token-wise from the gradients. We focus on the standard synchronous distributed training: at each step t, every node i samples a minibatch (x t,i , y t,i ) from its own dataset to compute the gradients:</p><formula xml:id="formula_25">∇W t,i = ∂ (F (x t,i , W t ), y t,i ) ∂W t<label>(1)</label></formula><p>The gradients are averaged across the N servers and then used to update the weights:</p><formula xml:id="formula_26">∇W t = 1 N N j ∇W t,j ; W t+1 = W t − η∇W t<label>(2)</label></formula><p>Given gradients ∇W t,k received from other participant k, we aim to steal participant k's training data (x t,k , y t,k ). Note F () and W t are shared by default for distributed optimization.</p><p>[0.2, 0.          x 1 ← N (0, 1) , y 1 ← N (0, 1)</p><formula xml:id="formula_27">t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o M v f b 9 1 Q q F o t b P U 6 o H + G B Y C E j W B u p Z y M v w V I z z J E X Y T 0 M g u x q g k 5 / 1 L u e X X G q z h R o k b g F q U C B R s / + 9 P o x S S M q N O F Y q a 7 r J N r P 8 n m E 0 0 n Z S x V N M B n h A e 0 a K n B E l Z 9 N L 5 m g Y 6 P 0 U R h L 8 4 R G U / V 3 R 4 Y j p c Z R Y C r z d d W 8 l 4 v / e d 1 U h z U / Y y J J N R V k 9 l G Y c q R j l M e C + k x S o v n Y E E w k M 7 s i M s Q S E 2 3 C K 5 s Q 3 P m T F 0 n r r O o 6 V f f m v F K v F X G U 4 B C O 4 A R c u I A 6 X E M D m k D g A Z 7 g B V 6 t R + v Z e r P e Z 6 V L V t F z A H 9 g f X w D f G K Z g Q = = &lt;</formula><formula xml:id="formula_28">t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o M v f b 9 1 Q q F o t b P U 6 o H + G B Y C E j W B u p Z y M v w V I z z J E X Y T 0 M g u x q g k 5 / 1 L u e X X G q z h R o k b g F q U C B R s / + 9 P o x S S M q N O F Y q a 7 r J N r P 8 n m E 0 0 n Z S x V N M B n h A e 0 a K n B E l Z 9 N L 5 m g Y 6 P 0 U R h L 8 4 R G U / V 3 R 4 Y j p c Z R Y C r z d d W 8 l 4 v / e d 1 U h z U / Y y J J N R V k 9 l G Y c q R j l M e C + k x S o v n Y E E w k M 7 s i M s Q S E 2 3 C K 5 s Q 3 P m T F 0 n r r O o 6 V f f m v F K v F X G U 4 B C O 4 A R c u I A 6 X E M D m k D g A Z 7 g B V 6 t R + v Z e r P e Z 6 V L V t F z A H 9 g f X w D f G K Z g Q = = &lt;</formula><formula xml:id="formula_29">t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o M v f b 9 1 Q q F o t b P U 6 o H + G B Y C E j W B u p Z y M v w V I z z J E X Y T 0 M g u x q g k 5 / 1 L u e X X G q z h R o k b g F q U C B R s / + 9 P o x S S M q N O F Y q a 7 r J N r P 8 n m E 0 0 n Z S x V N M B n h A e 0 a K n B E l Z 9 N L 5 m g Y 6 P 0 U R h L 8 4 R G U / V 3 R 4 Y j p c Z R Y C r z d d W 8 l 4 v / e d 1 U h z U / Y y J J N R V k 9 l G Y c q R j l M e C + k x S o v n Y E E w k M 7 s i M s Q S E 2 3 C K 5 s Q 3 P m T F 0 n r r O o 6 V f f m v F K v F X G U 4 B C O 4 A R c u I A 6 X E M D m k D g A Z 7 g B V 6 t R + v Z e r P e Z 6 V L V t F z A H 9 g f X w D f G K Z g Q = = &lt;</formula><formula xml:id="formula_30">t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o M v f b 9 1 Q q F o t b P U 6 o H + G B Y C E j W B u p Z y M v w V I z z J E X Y T 0 M g u x q g k 5 / 1 L u e X X G q z h R o k b g F q U C B R s / + 9 P o x S S M q N O F Y q a 7 r J N r P 8 n m E 0 0 n Z S x V N M B n h A e 0 a K n B E l Z 9 N L 5 m g Y 6 P 0 U R h L 8 4 R G U / V 3 R 4 Y j p c Z R Y C r z d d W 8 l 4 v / e d 1 U h z U / Y y J J N R V k 9 l G Y c q R j l M e C + k x S o v n Y E E w k M 7 s i M s Q S E 2 3 C K 5 s Q 3 P m T F 0 n r r O o 6 V f f m v F K v F X G U 4 B C O 4 A R c u I A 6 X E M D m k D g A Z 7 g B V 6 t R + v Z e r P e Z 6 V L V t F z A H 9 g f X w D f G K Z g Q = = &lt; / l a t e x i t &gt; D = ||rW 0 rW || 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z H c r S D 9 G J X 5 i x 0 3 Z h 0 P i G A M 2 n Z o = " &gt; A A A C E n i c b V D L S s N A F J 3 4 r P U V d e l m s I i 6 s C R F s B u h o A u X F e w D m l g m 0 0 k 7 d D I J M x O h p P k G N / 6 K G x e K u H X l z r 9 x 0 k b Q 1 g M D h 3 P u 5 c 4 5 X s S o V J b 1 Z S w s L i 2 v r B b W i u s b m 1 v b 5 s 5 u U 4 a x w K S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 8 D L z W / d E S B r y W z W K i B u g P q c + x U h p q W u e O A F S A 8 9 L r l J 4 A c d j h y O P I d g 6 g q f w h 4 / H d 0 k l 7 Z o l q 2 x N A O e J n Z M S y F H v m p 9 O L 8 R x Q L j C D E n Z s a 1 I u Q k S i m J G 0 q I T S x I h P E R 9 0 t G U o 4 B I N 5 l E S u G h V n r Q D 4 V + X M G J + n s j Q Y G U o 8 D T k 1 k A O e t l 4 n 9 e J 1 Z + 1 U 0 o j 2 J F O J 4 e 8 m M G V Q i z f m C P C o I V G 2 m C s K D 6 r x A P k E B Y 6 R a L u g R 7 N v I 8 a V b K t l W 2 b 8 5 K t W p e R</formula><formula xml:id="formula_31">S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 8 D L z W / d E S B r y W z W K i B u g P q c + x U h p q W u e O A F S A 8 9 L r l J 4 A c d j h y O P I d g 6 g q f w h 4 / H d 0 k l 7 Z o l q 2 x N A O e J n Z M S y F H v m p 9 O L 8 R x Q L j C D E n Z s a 1 I u Q k S i m J G 0 q I T S x I h P E R 9 0 t G U o 4 B I N 5 l E S u G h V n r Q D 4 V + X M G J + n s j Q Y G U o 8 D T k 1 k A O e t l 4 n 9 e J 1 Z + 1 U 0 o j 2 J F O J 4 e 8 m M G V Q i z f m C P C o I V G 2 m C s K D 6 r x A P k E B Y 6 R a L u g R 7 N v I 8 a V b K t l W 2 b 8 5 K t W p e R</formula><formula xml:id="formula_32">S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 8 D L z W / d E S B r y W z W K i B u g P q c + x U h p q W u e O A F S A 8 9 L r l J 4 A c d j h y O P I d g 6 g q f w h 4 / H d 0 k l 7 Z o l q 2 x N A O e J n Z M S y F H v m p 9 O L 8 R x Q L j C D E n Z s a 1 I u Q k S i m J G 0 q I T S x I h P E R 9 0 t G U o 4 B I N 5 l E S u G h V n r Q D 4 V + X M G J + n s j Q Y G U o 8 D T k 1 k A O e t l 4 n 9 e J 1 Z + 1 U 0 o j 2 J F O J 4 e 8 m M G V Q i z f m C P C o I V G 2 m C s K D 6 r x A P k E B Y 6 R a L u g R 7 N v I 8 a V b K t l W 2 b 8 5 K t W p e R</formula><formula xml:id="formula_33">S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 8 D L z W / d E S B r y W z W K i B u g P q c + x U h p q W u e O A F S A 8 9 L r l J 4 A c d j h y O P I d g 6 g q f w h 4 / H d 0 k l 7 Z o l q 2 x N A O e J n Z M S y F H v m p 9 O L 8 R x Q L j C D E n Z s a 1 I u Q k S i m J G 0 q I T S x I h P E R 9 0 t G U o 4 B I N 5 l E S u G h V n r Q D 4 V + X M G J + n s j Q Y G U o 8 D T k 1 k A O e t l 4 n 9 e J 1 Z + 1 U 0 o j 2 J F O J 4 e 8 m M G V Q i z f m C P C o I V G 2 m C s K D 6 r x A P k E B Y 6 R a L u g R 7 N v I 8 a V b K t l W 2 b 8 5 K t W p e R w H s g w N w D G x w D m r g G t R B A 2 D w A J 7 A C 3 g 1 H o 1 n 4 8 1 4 n 4 4 u G P n O H v g D 4 + M b F O m c b A = = &lt; / l a t e x i t &gt; rW 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I m 5 G d X 2 Q S q f 9 b s u G M t 3 K J 1 E K 1 G w = " &gt; A A A B 8 H i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 6 G j K t j 7 o r u H F Z w T 6 k H U o m z b S h S W Z I M k I Z + h V u X C j i 1 s 9 x 5 9 + Y a U d Q 0 Q M X D u f c</formula><formula xml:id="formula_34">W i O j J z V q t U a 9 H K l A n I 0 B + X 3 / j A i i a D S E I 6 1 7 n k o N n 6 K l W G E 0 1 m p n 2 g a Y z L B I 9 q z V G J B t Z / O D 5 7 B I 6 s M Y R g p W 9 L A u f p 9 I s V C 6 6 k I b K f A Z q x / e 5 n 4 l 9 d L T F j 3 U y b j x F B J F o v C h E M T w e x 7 O G S K E s O n l m C i m L 0 V k j F W m B i b U c m G 8 P U p / J + 0 q 6 6 H X O / m t N K o 5 3 E U w Q E 4 B C f A A x e g A a 5 B E 7 Q A A Q I 8 g C f w 7 C j n 0 X l x X h e t B S e f 2 Q c / 4 L x 9 A h y m j + o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I m 5 G d X 2 Q S q f 9 b s u G M t 3 K J 1 E K 1 G w = " &gt; A A A B 8 H i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 6 G j K t j 7 o r u H F Z w T 6 k H U o m z b S h S W Z I M k I Z + h V u X C j i 1 s 9 x 5 9 + Y a U d Q 0 Q M X D u f c</formula><p>Initialize dummy inputs and labels.</p><p>3:</p><p>for i ← 1 to n do 4:</p><p>∇W i ← ∂ (F (x i , W t ), y i )/∂W t Compute dummy gradients.</p><p>5:</p><formula xml:id="formula_35">D i ← ||∇W i − ∇W || 2 6: x i+1 ← x i − η∇ x i D i , y i+1 ← y i − η∇ y i D i</formula><p>Update data to match gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>end for 8:</p><p>return x n+1 , y n+1 9: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data Leakage through Gradients Matching</head><p>To recover the data from gradients, we first randomly initialize a dummy input x and label input y . We then feed these "dummy data" into models and get "dummy gradients".</p><formula xml:id="formula_36">∇W = (F (x , W ), y ) W<label>(3)</label></formula><p>Optimizing the dummy gradients close as to original also makes the dummy data close to the real training data (the trends shown in Fig. <ref type="figure">5</ref>). Given gradients at a certain step, we obtain the training data by minimizing the following objective</p><formula xml:id="formula_37">x * , y * = arg min x ,y ||∇W − ∇W || 2 = arg min x ,y || ∂ (F (x , W ), y ) ∂W ) − ∇W || 2<label>(4)</label></formula><p>The distance ||∇W − ∇W || 2 is differentiable w.r.t dummy inputs x and labels y can thus can be optimized using standard gradient-based methods. Note that this optimization requires 2 nd order derivatives. We make a mild assumption that F is twice differentiable, which holds for the majority of modern machine learning models (e.g., most neural networks) and tasks.</p><p>Iters=0 Iters=10 Iters=50 Iters=100 Iters=500 Melis <ref type="bibr" target="#b25">[26]</ref> Ground Truth Figure <ref type="figure">3</ref>: The visualization showing the deep leakage on images from MNIST <ref type="bibr" target="#b20">[21]</ref>, CIFAR-100 <ref type="bibr" target="#b19">[20]</ref>, SVHN <ref type="bibr" target="#b26">[27]</ref> and LFW <ref type="bibr" target="#b12">[13]</ref> respectively. Our algorithm fully recovers the four images while previous work only succeeds on simple images with clean backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Leakage for Batched Data</head><p>The algo. 1 works well when there is only a single pair of input and label in the batch. However when we naively apply it to the case where batch size N ≥ 1, the algorithm would be too slow to converge. We think the reason is that batched data can have N ! different permutations and thus make optimizer hard to choose gradient directions. To force the optimization closer to a solution, instead of updating the whole batch, we update a single training sample instead. We modify the line 6 in algo. 1 to :</p><formula xml:id="formula_38">x i mod N t+1 ← x i mod N t − ∇ x i mod N t+1 D y i mod N t+1 ← y i mod N t − ∇ y i mod N t+1 D<label>(5)</label></formula><p>Then we can observe fast and stable convergence. We list the iterations required for convergence for different batch sizes in Tab. 1 and provide visualized results in Fig. <ref type="figure" target="#fig_35">4</ref>. The larger the batch size is, the more iterations DLG requires to attack.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Setup. Implementing algorithm. 1 requires to calculate the high order gradients and we choose PyTorch <ref type="bibr" target="#b27">[28]</ref> as our experiment platform. We use L-BFGS <ref type="bibr" target="#b23">[24]</ref> with learning rate 1, history size 100 and max iterations 20 and optimize for 1200 iterations and 100 iterations for image and text task respectively. We aim to match gradients from all trainable parameters. Notably, DLG has no requirements on model's convergence status, in another word, the attack can happen anytime during the training. To be more general, all our experiments are using randomly initialized weights. More task-specific details can be found in following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Leakage on Image Classification</head><p>Given an image containing objects, images classification aims to determine the class of the item. We experiment our algorithm on modern CNN architectures ResNet-56 <ref type="bibr" target="#b10">[11]</ref> and pictures from MNIST <ref type="bibr" target="#b20">[21]</ref>, CIFAR-100 <ref type="bibr" target="#b19">[20]</ref>, SVHN <ref type="bibr" target="#b26">[27]</ref> and LFW <ref type="bibr" target="#b12">[13]</ref>. Two changes we have made to the models are replacing activation ReLU to Sigmoid and removing strides, as our algorithm requires the model to be twice-differentiable. For image labels, instead of directly optimizing the discrete categorical values, we random initialize a vector with shape N × C where N is the batch size and C is the number of classes, and then take its softmax output as the one-hot label for optimization.</p><p>The leaking process is visualized in Fig. <ref type="figure">3</ref>. We start with random Gaussian noise (first column) and try to match the gradients produced by the dummy data and real ones. As shown in <ref type="bibr">Fig 5,</ref><ref type="bibr">minimizing</ref> the distance between gradients also reduces the gap between data. We observe that monochrome images with a clean background (MNIST) are easiest to recover, while complex images like face take more iterations to recover (Fig. <ref type="figure">3</ref>). When the optimization finishes, the recover results are almost identical to ground truth images, despite few negligible artifact pixels.</p><p>We visually compare the results from other method <ref type="bibr" target="#b25">[26]</ref> and ours in Fig. <ref type="figure">3</ref>. The previous method uses GAN models when the class label is given and only works well on MNIST. The result on SVHN, though is still visually recognizable as digit "9", this is no longer the original training image. The cases are even worse on LFW and collapse on CIFAR. We also make a numerical comparison by performing leaking and measuring the MSE on all dataset images in Fig. <ref type="figure" target="#fig_36">6</ref>. Images are normalized to the range [0, 1] and our algorithm appears much better results (ours &lt; 0.03 v.s. previous &gt; 0.2) on all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Leakage on Masked Language Model</head><p>For language task, we verify our algorithm on Masked Language Model (MLM) task. In each sequence, 15% of the words are replaced with a [MASK] token and MLM model attempts to predict  we welcome proposals for tutor **ials on either core machine learning topics or topics of emerging importance for machine learning .</p><p>we invite submissions for the thirty -third annual conference on neural information processing systems .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Text</head><p>Registration, volunteer applications, and student travel application open the first week of September. Child care will be available.</p><p>We welcome proposals for tutorials on either core machine learning topics or topics of emerging importance for machine learning.</p><p>We invite submissions for the Thirty-Third Annual Conference on Neural Information Processing Systems.</p><p>Table <ref type="table">2</ref>: The progress of deep leakage on language tasks. the original value of the masked words from a given context. We choose BERT <ref type="bibr" target="#b6">[7]</ref> as our backbone and adapt hyperparameters from the official implementation * . Different from vision tasks where RGB inputs are continuous values, language models need to preprocess discrete words into embeddings. We apply DLG on embedding space and minimize the gradients distance between dummy embeddings and real ones. After optimization finishes, we derive original words by finding the closest entry in the embedding matrix reversely.</p><p>In Tab. 2, we exhibit the leaking history on three sentences selected from NeurIPS conference page. Similar to the vision task, we start with randomly initialized embedding: the reverse query results at iteration 0 is meaningless. During the optimization, the gradients produced by dummy embedding gradually match the original ones and so the embeddings. In later iterations, part of sequence gradually appears. In example 3, at iteration 20, 'annual conference' appeared and at iteration 30 and the leaked sentence is already close to the original one. When DLG finishes, though there are few mismatches caused by the ambiguity in tokenizing, the main content is already fully leaked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Defense Strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Noisy Gradients</head><p>One straightforward attempt to defense DLG is to add noise on gradients before sharing. To evaluate, we experiment Gaussian and Laplacian noise (widely used in differential privacy studies) distributions with variance range from 10 −1 to 10 −4 and central 0. From Fig. <ref type="figure">7a</ref> and 7b, we observe that the defense effect mainly depends on the magnitude of distribution variance and less related to the noise types. When variance is at the scale of 10 −4 , the noisy gradients do not prevent the leak. For noise with variance 10 −3 , though with artifacts, the leakage can still be performed. Only when the variance is larger than 10 −2 and the noise is starting affect the accuracy, DLG will fail to execute and Laplacian * https://github.com/google-research/bert maximum tolerance of sparsity is around 20%. When pruning ratio is larger, the recovered images are no longer visually recognizable and thus gradient compression successfully prevents the leakage.</p><p>Previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> show that gradients can be compressed by more than 300× without losing accuracy by error compensation techniques. In this case, the sparsity is above 99% and already exceeds the maximum tolerance of DLG (which is around 20%). It suggests that compressing the gradients is a practical approach to avoid the deep leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we introduce the Deep Leakage from Gradients (DLG): an algorithm that can obtain the local training data from public shared gradients. DLG does not rely on any generative model or extra prior about the data. Our experiments on vision and language tasks both demonstrate the critical risks of this deep leakage and show that such deep leakage can be only prevented when defense strategies starts to degrade the accuracy. This sets a challenge to modern multi-node learning system (e.g., distributed training, federated learning). We hope this work would raise people's awareness about the security of gradients and bring the community to rethink the safety of existing gradient sharing scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 )</head><label>1</label><figDesc>Distributed training with a centralized server (2) Distributed training without a centralized server</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; rW &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; Flower Cat Leak rW &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; rW &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; Flower Cat (a) Distributed training with a centralized server (1) Distributed training with a centralized server (2) Distributed training without a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; rW &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q K G v T W M S q F 6 B m g k v W N t w I 1 k s U w y g Q r B t M b x d + 9 4 k p z W P 5 Y G Y J 8 y M c S x 5 y i s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; Flower Cat Leak rW &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>1 0 r m q e 2 7 d u 7 + u N R t F H G U 4 g 3 O 4 B A 9 u o A l 3 0 I I 2 U B D w D K / w 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 8 g f P 5 A 3 9 3 j 5 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I z z V U c y 5 d 0 X J 5 b D l c r R T g L M U C A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j N s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The deep leakage scenarios in two categories of classical multi-node training. The little red demon appears in the location where the deep leakage might happen. When performing centralized training, the parameter server is capable to steal all training data from gradients received from worker nodes. While training in a decentralized manner (e.g., ring all reduce [29]), any participant can be malicious and steal the training data from its neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " e x Z P u H J P r Y b R 0 p / + A e z m 6 f f h G N M = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Y i 2 G V B F y 4 r 2 A c 0 o U y m k 3 b o Z B J m J k I J 3 b r x V 9 y 4 U M S t f + D O v 3 H S B t T W A w O H c + 6 9 c + 8 J E s 6 U d p w v q 7 S y u r a + U d 6 s b G 3 v 7 O 7 Z + w d t F a e S 0 B a J e S y 7 AV a U M 0 F b m m l O u 4 m k O A o 4 7 Q T j q 9 z v 3 F O p W C z u 9 C S h f o S H g o W M Y G 2 k v o 2 8 B E v N M E d e h P U o C L L r K T r 7 U b t 9 u + r U n B n Q M n E L U o U C z b 7 9 6 Q 1 i k k Z U a M K x U j 3 X S b S f 5 f M I p 9 O K l y q a Y D L G Q 9 o z V O C I K j + b X T J F J 0 Y Z o D C W 5 g m N Z u r v j g x H S k 2 i w F T m 6 6 p F L x f / 8 3 q p D u t + x k S S a i r I / K M w 5 U j H K I 8 F D Z i k R P O J I Z h I Z n Z F Z I Q l J t q E V z E h u I s n L 5 P 2 e c 1 1 a u 7 t R b V R L + I o w x E c w y m 4 c A k N u I E m t I D A A z z B C 7x a j 9 a z 9 W a 9 z 0 t L V t F z C H 9 g f X w D e t 6 Z g A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e x Z P uH J P r Y b R 0 p / + A e z m 6 f f h G N M = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Y i 2 G V B F y 4 r 2 A c 0 o U y m k 3 b o Z B J m J k I J 3 b r x V 9 y 4 U M S t f + D O v 3 H S B t T W A w O H c + 6 9 c + 8 J E s 6 U d p w v q 7 S y u r a + U d 6 s b G 3 v 7 O 7 Z + w d t F a e S 0 B a J e S y 7 A V a U M 0 F b m m l O u 4 m k O A o 4 7 Q T j q 9 z v 3 F O p W C z u 9 C S h f o S H g o W M Y G 2 k v o 2 8 B E v N M E d e h P U o C L L r K T r 7 U b t 9 u + r U n B n Q M n E L U o U C z b 7 9 6 Q 1 i k k Z U a M K x U j 3 X S b S f 5 f M I p 9 O K l y q a Y D L G Q 9 o z V O C I K j + b X T J F J 0 Y Z o D C W 5 g m N Z u r v j g x H S k 2 i w F T m 6 6 p F L x f / 8 3 q p D u t + x k S S a i r I / K M w 5 U j H K I 8 F D Z i k R P O J I Z h I Z n Z F Z I Q l J t q E V z Eh u I s n L 5 P 2 e c 1 1 a u 7 t R b V R L + I o w x E c w y m 4 c A k N u I E m t I D A A z z B C 7 x a j 9 a z 9 W a 9 z 0 t L V t F z C H 9 g f X w D e t 6 Z g A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e x Z P u H J P r Y b R 0 p / + A e z m 6 f f h G N M = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Y i 2 G V B F y 4 r 2 A c 0 o U y m k 3 b o Z B J m J k I J 3 b r x V 9 y 4 U M S t f + D O v 3 H S B t T W A w O H c + 6 9 c + 8 J E s 6 U d p w v q 7 S y u r a + U d 6 s b G 3 v 7 O 7 Z + w d t F a e S 0 B a J e S y 7 A V a U M 0 F b m m l O u 4 m k O A o 4 7 Q Tj q 9 z v 3 F O p W C z u 9 C S h f o S H g o W M Y G 2 k v o 2 8 B E v N M E d e h P U o C L L r K T r 7 U b t 9 u + r U n B n Q M n E L U o U C z b 7 9 6 Q 1 i k k Z U a M K x U j 3 X S b S f 5 f M I p 9 O K l y q a Y D L G Q 9 o z V O C I K j + b X T J F J 0 Y Z o D C W 5 g m N Z u r v j g x H S k 2 i w F T m 6 6 p F L x f / 8 3 q p D u t + x k S S a i r I / K M w 5 U j H K I 8 F D Z i k R P O J I Z h I Z n Z F Z I Q l J t q E V z E h u I s n L 5 P 2 e c 1 1 a u 7 t R b V R L + I o w x E c w y m 4 c A k N u I E m t I D A A z z B C 7x a j 9 a z 9 W a 9 z 0 t L V t F z C H 9 g f X w D e t 6 Z g A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e x Z P uH J P r Y b R 0 p / + A e z m 6 f f h G N M = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Y i 2 G V B Fy 4 r 2 A c 0 o U y m k 3 b o Z B J m J k I J 3 b r x V 9 y 4 U M S t f + D O v 3 H S B t T W A w O H c + 6 9 c + 8 J E s 6 U d p w v q 7 S y u r a + U d 6 s b G 3 v 7 O 7 Z + w d t F a e S 0 B a J e S y 7 A V a U M 0 F b m m l O u 4 m k O A o 4 7 Q T j q 9 z v 3 F O p W C z u 9 C S h f o S H g o W M Y G 2 k v o 2 8 B E v N M E d e h P U o C L L r K T r 7 U b t 9 u + r U n B n Q M n E L U o U C z b 7 9 6 Q 1 i k k Z U a M K x U j 3 X S b S f 5 f M I p 9 O K l y q a Y D L G Q 9 o z V O C I K j + b X T J F J 0 Y Z o D C W 5 g m N Z u r v j g x H S k 2 i w F T m 6 6 p F L x f / 8 3 q p D u t + x k S S a i r I / K M w 5 U j H K I 8 F D Z i k R P O J I Z h I Z n Z F Z I Q l J t q E V z E h u I s n L 5 P 2 e c 1 1 a u 7 t R b V R L + I o w x E c w y m 4 c A k N u I E m t I D A A z z B C 7 x a j 9 a z 9 W a 9 z 0 t L V t F z C H 9 g f X w D e t 6 Z g A = = &lt; / l a t e x i t &gt; @D/@Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 0 X 4 b t E 7 e g L o 8 5 x y c G d 8 z g 1 c 7 + I = " &gt; A A A C C X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B F c 1 U Q E u y z o w m U F + 5 A m l M l 0 0 g 6 d T M L M R C i h W z f + i h s X i r j 1 D 9 z 5 N 0 7 a g N p 6 Y O B w z r 1 3 7 j 1 B w p n S j v N l L S 2 v r K 6 t l z b K m 1 v b O 7 v 2 3 n 5 L x a k k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 0 X 4 b t E 7 e g L o 8 5 x y c G d 8 z g 1 c 7 + I = " &gt; A A A C C X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B F c 1 U Q E u y z o w m U F + 5 A m l M l 0 0 g 6 d T M L M R C i h W z f + i h s X i r j 1 D 9 z 5 N 0 7 a g N p 6 Y O B w z r 1 3 7 j 1 B w p n S j v N l L S 2 v r K 6 t l z b K m 1 v b O 7 v 2 3 n 5 L x a k k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 0 X 4 b t E 7 e g L o 8 5 x y c G d 8 z g 1 c 7 + I = " &gt; A A A C C X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B F c 1 U Q E u y z o w m U F + 5 A m l M l 0 0 g 6 d T M L M R C i h W z f + i h s X i r j 1 D 9 z 5 N 0 7 a g N p 6 Y O B w z r 1 3 7 j 1 B w p n S j v N l L S 2 v r K 6 t l z b K m 1 v b O 7 v 2 3 n 5 L x a k k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 0 X 4 b t E 7 e g L o 8 5 x y c G d 8 z g 1 c 7 + I = " &gt; A A A C C X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B F c 1 U Q E u y z o w m U F + 5 A m l M l 0 0 g 6 d T M L M R C i h W z f + i h s X i r j 1 D 9 z 5 N 0 7 a g N p 6 Y O B w z r 1 3 7 j 1 B w p n S j v N l L S 2 v r K 6 t l z b K m 1 v b O 7 v 2 3 n 5 L x a k k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>w H s g w N w D G x w D m r g G t R B A 2 D w A J 7 A C 3 g 1 H o 1 n 4 8 1 4 n 4 4 u G P n O H v g D 4 + M b F O m c b A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z H c r S D 9 G J X 5 i x 0 3 Z h 0 P i G A M 2 n Z o = " &gt; A A A C E n i c b V D L S s N A F J 3 4 r P U V d e l m s I i 6 s C R F s B u h o A u X F e w D m l g m 0 0 k 7 d D I J M x O h p P k G N / 6 K G x e K u H X l z r 9 x 0 k b Q 1 g M D h 3 P u 5 c 4 5 X s S o V J b 1 Z S w s L i 2 v r B b W i u s b m 1 v b 5 s 5 u U 4 a x w K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>w H s g w N w D G x w D m r g G t R B A 2 D w A J 7 A C 3 g 1 H o 1 n 4 8 1 4 n 4 4 u G P n O H v g D 4 + M b F O m c b A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z H c r S D 9 G J X 5 i x 0 3 Z h 0 P i G A M 2 n Z o = " &gt; A A A C E n i c b V D L S s N A F J 3 4 r P U V d e l m s I i 6 s C R F s B u h o A u X F e w D m l g m 0 0 k 7 d D I J M x O h p P k G N / 6 K G x e K u H X l z r 9 x 0 k b Q 1 g M D h 3 P u 5 c 4 5 X s S o V J b 1 Z S w s L i 2 v r B b W i u s b m 1 v b 5 s 5 u U 4 a x w K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>w H s g w N w D G x w D m r g G t R B A 2 D w A J 7 A C 3 g 1 H o 1 n 4 8 1 4 n 4 4 u G P n O H v g D 4 + M b F O m c b A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z H c r S D 9 G J X 5 i x 0 3 Z h 0 P i G A M 2 n Z o = " &gt; A A A C E n i c b V D L S s N A F J 3 4 r P U V d e l m s I i 6 s C R F s B u h o A u X F e w D m l g m 0 0 k 7 d D I J M x O h p P k G N / 6 K G x e K u H X l z r 9 x 0 k b Q 1 g M D h 3 P u 5 c 4 5 X s S o V J b 1 Z S w s L i 2 v r B b W i u s b m 1 v b 5 s 5 u U 4 a x w K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>y 7 3 3 B D F n 2 i D 0 4 R S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 5 e W 0 e J I r R F I h 6 p b o A 1 5 U z S l m G G 0 2 6 s K B Y B p 5 1 g c p X 5 n X u q N I v k r Z n G 1 B d 4 J F n I C D Z W u u t L H H A M O 8 e D c g W 5 l 8 g 7 r 3 s Q u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 2 :Algorithm 1</head><label>21</label><figDesc>Figure2: The overview of our DLG algorithm. Variables to be updated are marked with a bold border. While normal participants calculate ∇W to update parameter using its private training data, the malicious attacker updates its dummy inputs and labels to minimize the gradients distance. When the optimization finishes, the evil user is able to obtain the training set from honest participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of deep leakage of batched data. Though the order may not be the same and there are more artifact pixels, DLG still produces images very close to the original ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Compassion of the MSE of images leaked by different algorithms and the ground truth. Our method consistently outperforms previous approach by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>fill given **less word **itude fine **nton overheard living vegas **vac **vation *f forte **dis cerambycidae ellison **don yards marne **kali toni **enting asbestos cutler km nail **oof **dation **ori righteous **xie lucan **hot **ery at **tle ordered pa **eit smashing proto [MASK] **ry toppled **wled major relief dive displaced **lice [CLS] us apps _ **face **bet Iters = 10 tilting fill given *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>7, 0.1]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The iterations required for restore batched data on CIFAR<ref type="bibr" target="#b19">[20]</ref> dataset.</figDesc><table><row><cell></cell><cell cols="4">BS=1 BS=2 BS=4 BS=8</cell></row><row><cell>ResNet-20</cell><cell>270</cell><cell>602</cell><cell>1173</cell><cell>2711</cell></row><row><cell>Initial</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Middle Stage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully Leaked</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Figure 5: Layer-i means MSE between real and dummy gradients of i th layer. When the gradients' distance gets smaller, the MSE between leaked image and the original image also gets smaller.</figDesc><table><row><cell>Melis</cell><cell>Ours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>MNIST</cell><cell>CIFAR</cell><cell>SVHN</cell><cell>LFW</cell></row><row><cell>Ours</cell><cell>0.0038</cell><cell>0.0069</cell><cell>0.0051</cell><cell>0.0055</cell></row><row><cell>Melis</cell><cell>0.2275</cell><cell>0.2578</cell><cell>0.2771</cell><cell>0.2951</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank MIT-IBM Watson AI lab, Intel, Facebook and AWS for supporting this work. We sincerely thank John Cohn for the discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">3</ref>: The trade-off between accuracy and defendability. G: Gaussian noise, L: Laplacian noise, FP: Floating number, Int: Integer quantization. means it successfully defends against DLG while means fails to defend (whether the results are visually recognizable). The accuracy is evaluated on CIFAR-100. tends to slight better at scale 10 −3 . However, noise with variance larger than 10 −2 will degrade the accuracy significantly (Tab. 3).</p><p>Another common perturbation on gradients is half precision, which was initially designed to save GPU memory footprints and also widely used to reduce communication bandwidth. We test two popular half precision implementations IEEE float16 (Single-precision floating-point format) and bfloat16 (Brain Floating Point <ref type="bibr" target="#b32">[33]</ref>, a truncated version of 32 bit float). Shown in Fig. <ref type="figure">7c</ref>, both half precision formats fail to protect the training data. We also test popular low-bit representation Int-8. Though it successfully prevents the leakage, the performance of model drops a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gradient Compression and Sparsification</head><p>We next experimented to defend by gradient compression <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>: Gradients with small magnitudes are pruned to zero. It's more difficult for DLG to match the gradients as the optimization targets are pruned. We evaluate how different level of sparsities (range from 1% to 70%) defense the leakage. When sparsity is 1% to 10%, it has almost no effects against DLG. When prune ratio increases to 20%, as shown in Fig. <ref type="figure">7d</ref>, there are obvious artifact pixels on the recover images. We notice that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 3</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ChainerMN: Scalable Distributed Deep Learning Framework</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on ML Systems in The Thirty-first Annual Conference on Neural Information Processing Systems</title>
				<meeting>Workshop on ML Systems in The Thirty-first Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introduction to parallel computing</title>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Barney</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konecny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01046</idno>
		<title level="m">Towards federated learning at scale: System design</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc V Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 22nd ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep models under the GAN: information leakage from collaborative deep learning</title>
		<author>
			<persName><forename type="first">Briland</forename><surname>Hitaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pérez-Cruz</surname></persName>
		</author>
		<idno>CoRR, abs/1702.07464</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Firecaffe: near-linear acceleration of deep neural network training on compute clusters</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes</title>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Developing and validating a survival prediction model for nsclc patients through distributed learning across 3 countries</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jochems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issam</forename><forename type="middle">El</forename><surname>Deist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Naqa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Matuszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ten Haken</surname></persName>
		</author>
		<author>
			<persName><surname>Van Soest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Radiation Oncology* Biology* Physics</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed learning: developing a predictive model based on data from multiple hospitals without data leaving the hospital-a real life proof of concept</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jochems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Deist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Van Soest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Eble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Bulens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Dries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Lambin</surname></persName>
		</author>
		<author>
			<persName><surname>Dekker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiotherapy and Oncology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepspark: Spark-based deep learning supporting asynchronous updates and caffe compatibility</title>
		<author>
			<persName><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehee</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08191</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Jakub Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><surname>Bacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Private Multi-Party Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/.5,6" />
		<title level="m">The mnist database of handwritten digits</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Yiing</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th {USENIX} Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>{OSDI} 14</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep gradient compression: Reducing the communication bandwidth for distributed training</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01887</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">Eider</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><surname>Hampson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05629</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploiting unintended feature leakage in collaborative learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno>CoRR, abs/1805.04049</idno>
		<imprint>
			<date type="published" when="2006">2018. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bandwidth optimal all-reduce algorithms for clusters of workstations</title>
		<author>
			<persName><forename type="first">Pitch</forename><surname>Patarasuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A transprecision floating-point platform for ultra-low power computing</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Tagliavini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Mach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Marongiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<idno>CoRR, abs/1711.10374</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Variance-based gradient compression for efficient distributed deep learning</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroto</forename><surname>Imachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06058</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
