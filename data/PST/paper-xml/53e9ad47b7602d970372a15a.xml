<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Unsupervised Outlier Detection in High-Dimensional Numerical Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<postCode>T6G 2E8</postCode>
									<settlement>Edmonton</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erich</forename><surname>Schubert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Informatics</orgName>
								<orgName type="institution">Ludwig-Maximilians Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Informatics</orgName>
								<orgName type="institution">Ludwig-Maximilians Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>0. 1 0 . 2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100</addrLine>
									<postCode>1000</postCode>
									<settlement>Normalized vector length Dimensionality</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on Unsupervised Outlier Detection in High-Dimensional Numerical Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4EC3F5A08F6786CA798821AF1A9B08ED</idno>
					<idno type="DOI">10.1002/sam.11161</idno>
					<note type="submission">Received 30 January 2012; revised 22 June 2012; accepted 2 August 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>curse of dimensionality</term>
					<term>anomalies in high-dimensional data</term>
					<term>outlier detection in high-dimensional data</term>
					<term>approximate outlier detection</term>
					<term>subspace outlier detection</term>
					<term>correlation outlier detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>High-dimensional data poses special challenges for data mining in general and outlier detection in particular. Though in recent years, several surveys on outlier detection have been published (see refs. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, to name a few), the difficulties in high-dimensional data and specialized approaches in this area have not been sketched in any of those (though, notably, a recent textbook edition sketches three example approaches <ref type="bibr" target="#b8">[9]</ref>). In fact, most approaches to this problem have been proposed just recently in the past two or three years. Since the development of unsupervised methods for outlier detection in high-dimensional data in Euclidean space appears to be an emerging topic, this survey is specialized on this topic. We hope to help researchers working in this area to become aware of other Correspondence to: Arthur Zimek (zimek@ualberta.ca) approaches, to understand the advancements and the lasting problems, and to better identify the true achievements of the various approaches. Though a lot of variants for outlier detection are around, such as supervised versus unsupervised or specialized approaches for specific data types (such as item sets, time series, sequences, categorical data, see refs. 7,10 for an overview), here we focus on unsupervised approaches for numerical data in Euclidean space.</p><p>Albeit the infamous 'curse of dimensionality' has been credited for many problems and has indiscriminately been used as a motivation for many new approaches, we should try to understand the problems occurring in highdimensional data in more detail. For example, there is a widespread mistaken belief that every point in highdimensional space is an outlier. This-misleading, to say the least-statement has been suggested as a motivation for the first approach specialized to outlier detection in subspaces of high dimensional data <ref type="bibr" target="#b10">[11]</ref>, recurring superficially to a fundamental paper on the 'curse of dimensionality' by Beyer et al. <ref type="bibr" target="#b11">[12]</ref>. Alas, as we will discuss in the following section, it is not as simple as that.</p><p>Indeed, this often cited but less often well-understood study <ref type="bibr" target="#b11">[12]</ref>, has been reconsidered recently by several researchers independently in different research areas. We will, therefore, begin our survey by inspecting truths and myths associated with the 'curse of dimensionality', study a couple of its effects that may be relevant for outlier detection, and discuss the findings of the renewed interest in this more than 10-year-old study (Section 2). Afterwards, we will discuss different families of outlier detection approaches concerned with high-dimensional data: first, approaches that treat the issues of efficiency and effectiveness in high-dimensional data without specific interest in a definition of outliers with respect to subspaces of the data (Section 3), and second, those that search for outliers specifically in subspaces of the data space (Section 4). In Section 5, we will comment on some opensource tools providing implementations of outlier detection algorithms, and remark on the difficulties of understanding and evaluating the results of outlier detection algorithms. Finally, in Section 6, we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE CURSE OF DIMENSIONALITY</head><p>The 'curse of dimensionality' has motivated a lot of research in the area of databases due to its impacts on similarity search <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Yet still there are open questions, unresolved issues, and even the known results have found less attention in research than appropriate. Here we discuss some effects and influential characteristics of high-dimensional data and relate these to issues for outlier detection in high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concentration of Distances-Concentration of Outlier Scores</head><p>Let us recall the basic statements of the fundamental study of Beyer et al. <ref type="bibr" target="#b11">[12]</ref>. Their key result states the following:</p><p>Assumption The ratio of the variance of the length of any point vector (denoted by X d ) with the length of the mean point vector (denoted by E[ X d ]) converges to zero with increasing data dimensionality.</p><p>This assumption covers a broad range of data distributions and distance measures (generally: all L p -norms with p ≥ 1).</p><p>Consequence The proportional difference between the farthest-point distance D max and the closest-point distance D min (the relative contrast) vanishes.</p><p>Formally:</p><formula xml:id="formula_0">If lim d→∞ var X d E[ X d ] = 0, then D max -D min D min → 0.</formula><p>Intuitively, under the given assumption the relative contrast between near and far neighbors does diminish as the dimensionality increases. This concentration effect of the distance measure hence reduces the usability of the measure for discrimination between near and far neighbors <ref type="bibr" target="#b21">[22]</ref>. The influence of different values for p in L p -norms on the concentration effect has been studied in refs. <ref type="bibr">23,24.</ref> In ref. <ref type="bibr" target="#b22">23</ref>, the authors showed by means of an analytic argument that L 1 and L 2 are the only integer norms useful for higher dimensions. In addition, they studied the use of projections for discrimination, the effectiveness of which depended on localized dissimilarity measures that did not satisfy the symmetry and triangle inequality conditions of distance metrics. In ref. <ref type="bibr" target="#b23">24</ref>, fractional L p distance measures (with 0 &lt; p &lt; 1) have been studied in a similar context. The authors provide evidence supporting the contention that smaller values of p offer better results in higher dimensional settings. Their result, however, is valid only for uniformly distributed data <ref type="bibr" target="#b24">[25]</ref>. The concentration of the cosine similarity has been studied in ref. <ref type="bibr">26.</ref> This does, however, only tell part of the story. A change in the value range for the distances does not come unexpected. The maximum distance from the origin in the unit cube is p √ d for L p -norms, the average distance converges to p √ d • 1 √ 3 with increasing d. For a normalized maximum distance, for example, in the unit-hypercube, this comes down to 1 √ 3 ≈ 0.577. So at first sight, it might be feasible to counter these effects by rescaling the distances appropriately.</p><p>The resulting effects can be seen in Fig. <ref type="figure">1</ref> (compare to ref. <ref type="bibr" target="#b24">25</ref>, which shows similar results without normalization). In this figure, we plot some characteristics of the distribution of vector length values for vectors uniformly distributed in the unit cube over increasing dimensionality d. The observed length of each vector is normalized by 1   √ d . Plotted are mean and standard deviation of the observed lengths as well as the actually observed minimal (maximal) length in each dimensionality. While the observed distances now have a comparable average value, there is still a loss in relative contrast. On this data set of a sample of 10 5 instances, the curse (more explicitly, the effect of distance concentration) is now clearly visible: the standard deviation disappears already at 10 dimensions, and the observed actual minimum and maximum shrink rapidly with increasing dimensionality. This is actually not very Statistical Analysis and Data Mining DOI:10.1002/sam surprising-in this i.i.d. setting, we are observing the central limit theorem with a shrinking variance. If we do not assume a uniform distribution but instead use standard normal (Gaussian) distributions, the result looks virtually the same (Fig. <ref type="figure">2</ref>), except that the average now converges to the standard deviation of 1. The distance from the origin (the length of the vector) is a certain simplification; however, the same effects occur when looking at pairwise distances for any two points in the data set, as we can observe in Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref>, except with different average values. For many distance-based data mining algorithms, this loss of numerical contrast is a major problem. To study this effect, we generate two series of very basic distributions. All points are drawn from the same distribution, so in this sense, there are no outliers in either data set series. The first series is uniformly distributed on the interval [0 . . . 1], the second series is standard normal distributed with mean μ = 0 and standard deviation σ = 1. Since the second distribution contains objects on the long tail of the distribution, some of these can be considered as outliers in the common sense of objects with a low probability density function (pdf). For example, the kNN distances as used for kNN outlier detection <ref type="bibr" target="#b26">[27]</ref> for uniformly distributed data (Fig. <ref type="figure">5</ref>) and normally distributed data (Fig. <ref type="figure">6</ref>) become virtually the same in high dimensionality, while in low dimensionality, the maximum distance was clearly different for the two distributions. But already at just 10 dimensions, even the uniform distribution seems to produce outliers for this method that are clearly larger than the mean distance (0.28740 μ = 0.20512 ± σ = 0.00027). The more advanced method LOF <ref type="bibr" target="#b27">[28]</ref> is less likely to detect false outliers at this medium dimensionality, but also quickly fails to recognize objects as atypical by their distance (Figs. <ref type="figure">7</ref> and<ref type="figure">8</ref>).</p><p>Up to now, the complete data set was generated by a single distribution. As such, it does not contain outliers in the sense of objects that actually are generated by a different generating mechanism. We now modify the datasets by manually placing an outlier at a constant position.</p><p>For the uniform distribution, we position the manual outlier at 0.9 in all dimensions. While this is within the domain of the uniform distribution, the point stands out clearly in high dimensions, as it can be seen in Fig. <ref type="figure">9</ref>. The reason for this outstanding behavior is that, for this object, all attributes are, by construction, strongly correlated. The point has an average normalized distance to the i.i.d. objects of the uniform distribution that is approximately 0.1 larger than the corresponding values of the remainder of the data. At around 700 dimensions however, the closest neighbor of this outlier is farther away from the outlier than from any other object in the uniform distribution! We see here that this kind of outlierness becomes not less but ever more prominent when increasing the dimensionality (as long as the dimensions add information).</p><p>A similar effect can be seen in the data generated by (i.i.d.) Gaussian distributions in all dimensions. For Fig. <ref type="figure">10</ref>, we placed the manual outlier at the fixed value of 2 in every dimension, resulting in a point that is at 2σ in every dimension and thus well inside the one-dimensional distributions. At one dimension, there exist larger distances within the Gaussian distribution than to this outlier. With increasing dimensionality, however, it remains at a constant distance to the cluster mean, while the other distances concentrate. At around 100 dimensions, again the nearest neighbor of the manual outlier is farther away from the outlier than the maximum observed distance inside the Gaussian distribution.</p><p>Applying outlier detection on these two data sets produces the expected results, as we exemplify for LOF in Figs. <ref type="figure">11</ref> and<ref type="figure" target="#fig_0">12</ref>. At low dimensionality, the outlier remains well hidden inside the distribution of LOF values for the clustered points. However, with increasing dimensionality the outlier achieves a score well distinguishable from the scores of the cluster objects (the standard deviations are Statistical Analysis and Data Mining DOI:10.1002/sam All this demonstrates that the concentration effect per se -although often seen this way-is not the main problem for outlier detection in high-dimensional data. On the contrary, for points that deviate in every attribute from the usual data distribution, the outlier characteristics just become even stronger and more pronounced with increasing dimensionality. And this observation is all but unexpected since adding another dimension where the behavior of the outlier and the remainder of usual data objects is different means adding more information that helps discriminating the different characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Relevant and Irrelevant Attributes</head><p>All the studies on the concentration effect we sketched above generally assumed that the full data set followed a single data distribution, subject to certain restrictions. In fact, when the data follows a mixture of distributions, the concentration effect is not always observed. Instead, in such cases, distances between members of different distributions may not necessarily tend to the global mean as the dimensionality increases. As briefly noted in refs. 12,29, if a data set is composed of many natural groupings or clusters, each following their own distribution, then the concentration effect will typically be less severe for queries based on points within a cluster of similar points generated according to the same mechanism, especially when the clusters are well separated.</p><p>The fundamental differences between singly distributed data and multiply distributed data are already discussed in detail in ref. <ref type="bibr" target="#b29">30</ref>. The authors demonstrate that nearestneighbor queries are both theoretically and practically meaningful if the search is limited to objects from the same cluster (distribution) as the query point (i.e., any point belonging to the same cluster is considered a valid answer to a nearest-neighbor query), and other clusters are well separated from the cluster in question. The key concept is that of pairwise stability of clusters, which is said to hold whenever the mean distance between points of different clusters dominates the mean distance between points belonging to the same cluster. When the clusters are pairwise stable, for any point belonging to a given cluster, its nearest neighbors tend to belong to the same cluster. Here, a nearest-neighbor query of size on the order of the cluster size can be considered meaningful, whereas differentiation between nearest and farthest neighbors within the same cluster may still be meaningless. Note that for many common distributions these considerations may remain valid even as the dimension d tends to infinity: for example, two Gaussian distributions with widely separated means may find that their separability improves as the data dimension increases. However, it should also be noted that these arguments are based on the assumption that all dimensions bear information relevant to the different clusters, classes, or distributions. Depending on the ratio of relevant versus irrelevant attributes, and on the actual separation of sets of points belonging to different distributions, irrelevant attributes in a data set may impede Statistical Analysis and Data Mining DOI:10.1002/sam the separability of different distributions and thus have the potential for eventually rendering nearest neighbor query results meaningless.</p><p>The observations of ref. <ref type="bibr" target="#b29">30</ref>, and the important distinction between the effects of relevant and irrelevant attributes, both seem to have received little if any attention in the research literature. Despite the demonstrated deficiency of conventional L p norms for high-dimensional data, a plethora of work based on the Euclidean distance has been dedicated to clustering strategies, which appear to be effective in practice to varying degrees for highdimensional data <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. Many heuristics have been proposed or evaluated for clustering <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>, outlier detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>, and indexing or similarity search <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref> that seek to mitigate the effects of the curse of dimensionality. While some of these strategies, such as projected or subspace clustering or subspace outlier detection, do recognize implicitly the effect of relevant versus irrelevant attributes for a cluster, all these papers (as well as others) abstain from discussing these effects, let alone studying them in detail. In particular, the concept of pairwise stability of clusters as introduced in ref. <ref type="bibr" target="#b29">30</ref> has not been taken into account in any of these papers. Although their underlying data models do generally assume (explicitly or otherwise) different underlying mechanisms for the formation of data groupings, they motivate their new approaches with only a passing reference to the curse of dimensionality. Also in a recent study on the 'distance compression effect' <ref type="bibr" target="#b51">[52]</ref>, although they discuss other interesting aspects, the importance of a cluster structure for the curse to apply or to not apply has been missed. Hence they fail to convincingly explain their (allegedly surprising) finding that L p metrics seem to perform better (in their experiments) in high-dimensional data than in lowdimensional data. Considering that a cluster structure will become stronger with an increasing number of relevant dimensions (i.e., attributes actually contributing information for the cluster separation), this observation is far from surprising-see the extensive experiments in ref. <ref type="bibr" target="#b52">53</ref> and the related reasoning in refs. <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref>.</p><p>A more general picture has been drawn by Durrant and Kabán <ref type="bibr" target="#b56">[57]</ref>. They show that the correlation between attributes is an important effect for avoiding the concentration of distances. Correlated attributes will also result in an intrinsic dimensionality that is considerably lower than the representational dimensionality, an effect that also led to opposing the curse of dimensionality with the 'selfsimilarity blessing' <ref type="bibr" target="#b57">[58]</ref>. Contrary to the conjecture in refs. 12, 25; however, Durrant and Kabán <ref type="bibr" target="#b56">[57]</ref> show that a low intrinsic dimensionality alone does not suffice to avoid the concentration, but essentially the correlation between attributes needs to be strong enough. Let us note that, actually, different aspects like a strong cluster-structure of data <ref type="bibr" target="#b29">[30]</ref> and low intrinsic dimensionality <ref type="bibr" target="#b57">[58]</ref> may be merely symptoms for a strong, though possibly latent, correlation between attributes. Durrant and Kabán also identify irrelevant dimensions as the core of the problem of the curse. In essence, the ratio between noise (e.g., irrelevant attributes or additive noise masking information in relevant attributes) and (latent) correlation in the attributes of a dataset will determine whether asking for the 'nearest neighbor' actually is meaningful.</p><p>Let us modify the experimental setup used before slightly in order to study the effect of relevant versus irrelevant attributes on outlier detection: we fix the dimensionality to d = 100. Instead, we vary the number of attributes d where the outlier is set to its fixed value, and draw the remaining attributes from the random distribution that is also used for the usual data. At d = 0, the outlier will be distributed identically as the remainder of the data, while at d = d all attributes will be set as before. Figure <ref type="figure">13</ref> shows the uniformly distributed setting, while Fig. <ref type="figure">14</ref> shows the normally distributed setting. With increasing d (along the x-axis), the score of the outlier becomes more and more prominent. With a low portion of relevant attributes, the outlier is well hidden in the data set, but once d is about This is the key motivation for subspace outlier detection: if we have too many irrelevant (noise) dimensions in the data set, the outlier can easily be masked. Once we, however, choose mostly relevant attributes (or projections), it becomes much more detectable. The challenge is to choose the right subspace.</p><p>That the relevance of attributes may be related to certain subgroups of the data has been recognized in applications like gene expression analysis <ref type="bibr" target="#b58">[59]</ref>, a problem setting that also motivated research on specialized methods for subspace clustering <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b59">60]</ref>. Similar traits can be found in areas like multiview clustering or, sometimes, alternative clustering or ensemble clustering <ref type="bibr" target="#b60">[61]</ref>. For subspace outlier detection, a similar reasoning motivates the identification of objects that are outlying with respect to certain attributes, and this set of relevant attributes may differ from object to object. On the basis of such considerations, only few methods have been proposed so far. However, the interest in this topic seems to be increasing lately, which is a major motivation for this survey. We will discuss all methods known in the literature for subspace outlier detection in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Discrimination Versus Ranking of Values</head><p>The distinction between relevant and irrelevant attributes in sight of increasing dimensionality has been studied recently in ref. <ref type="bibr" target="#b52">53</ref> for vector data and cosine distance along with L p norms as well as in ref. <ref type="bibr" target="#b61">62</ref> for time series data and related specialized distance measures. The focus of these studies was to experimentally evaluate (and in effect to mostly confirm) the advantage of shared-neighbor distance measures in high-dimensional data, that was only assumed before. However, they can also serve as an investigation of the effects of relevant versus irrelevant attributes in data separation, based on distance rankings discerning between objects belonging to the same distribution versus objects belonging to some other distribution.</p><p>So, still, the differentiation between near and far points (the latter possibly being outliers) seems feasible by means of a ranking of distance values. Yet judging similarity or dissimilarity (as well as outlierness) by means of absolute distance values becomes more and more difficult in higher dimensional space. Let us illustrate this with another aspect of high-dimensional data exemplified for Euclidean distances (L 2 -norms), the counter-intuitive behavior of the volume of a hypersphere with increasing dimensionality. This effect does not even depend on specific data distributions but on the mere dimensionality of the data space. In <ref type="bibr">Fig 15,</ref><ref type="bibr"></ref> we depict the fundamental effect of this counter-intuitive behavior, the volume of a hypersphere over a range of dimensionalities. For the unit-hypersphere (radius equals 1), we see the volume increasing from dimension 1 (a segment of length 2), dimension 2 (the volume equals π ) to dimension 5. After that, on first sight surprisingly, the volume decreases again and soon approaches 0. The volume of a 20-dimensional hypersphere with a radius of 1 is almost 0. The same behavior is given with spheres of slightly varied radii. A small change of the radius moves the peak in altitude and position, but the general behavior remains the same. It would be rather naïve, though, to just interpret this as 'decreasing volume' since it is meaningless to compare a volume unit x with a volume unit y , where x = y, just as we cannot say a square meter was more or less than half a cubic meter.</p><p>A meaningful interpretation of this plot is, however, easily given, if we interpret the values as the ratio of the volume of a hypersphere versus the volume of the unit hypercube (side length of 1). This way, also the behavior becomes understandable: for any radius, with increasing dimensionality the corners of the unit hypercube will eventually emerge toward the space outside the sphere since the diameter between the farthest opposite corners increases by √ d, that is, unbounded, albeit slowly, while the radius of the sphere, how large it ever may be, remains constant. A more informal way of putting it is: high-dimensional spaces consist mostly of corners.</p><p>The second plot (Fig. <ref type="figure" target="#fig_5">16</ref>) shows such ratios<ref type="foot" target="#foot_0">1</ref> for hyperspheres with radius ≤ 0.5 vs. the volume of the unit cube (side length of 1), that is, all hyperspheres here are completely inside the hypercube. The cube is in fact the minimal bounding box of the sphere with radius 0.5. Consequently, the corners of the unit hypercube are beyond the boundaries of the sphere in two dimensional space already, the maximum of the volume ratio is with dimensionality 1. In the one-dimensional case, for radius = 0.5 both volumes are identical. In two dimensions, the ratio is obviously π 4 . We see the ratio rapidly decreasing again, this time allowing meaningful conclusions. In 10 dimensions already, the volume of the sphere is almost negligible compared to the volume of the bounding box. As the figure shows, the general behavior is the same for different radii as well (shown are hyperspheres with slightly smaller radii). The absolute values of the ratio are varying, though. These effects (and some others) are discussed nicely in ref. <ref type="bibr" target="#b62">63</ref>. In our context, this observation suggests that the choice of a certain radius ε to select a neighborhood (a fundamental step in many outlier detection methods) is notoriously rather sensitive to dimensionality. Small changes in the radius may decide whether everything or nothing is selected in a given dimensionality (e.g., a certain subspace), but the same amount of change may have no effect whatsoever if we have another data set (or another subspace) with some dimensions more. In order to describe a neighborhood containing at least some point, in high dimensions we need a radius that, if it were used in a lower dimensional subspace, would engulf the complete data space multiple times (see Fig. <ref type="figure" target="#fig_6">17</ref> for a visualization, note here the log-scale of the volume). It is more stable to use k-nearest neighbors instead of some ε-neighborhood since this guarantees to always have k objects for further computations. However, the effect on a refined modeling of data characteristics (as, e.g., deriving a density model) remains, as a certain density level (that may be a threshold discerning between inlierness and outlierness) is not comparable over subspaces of different dimensionalities.</p><p>There are two obvious consequences for outlier detection. First, while outlier rankings may be still good, the underlying outlier scores do not allow to separate between outliers and inliers, that is, considering the outlier scores, there is no obvious gap between outliers and inliers. Aside from a viable ranking, however, such a gap would be highly desirable, as stated already by Hawkins <ref type="bibr" target="#b63">[64]</ref>: 'a sample containing outliers would show up such characteristics as large gaps between 'outlying' and 'inlying' observations and the deviation between outliers and the group of inliers, as measured on some suitably standardized scale'.</p><p>Second, if outlier scores are influenced by distance values, and distance values vary substantially over different dimensionality, how can outlier scores derived from subspaces of different dimensionality be compared? Outlier scores based on more attributes may be more prominent numerically while the outlier characteristic may actually be less prominent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Combinatorial Issues and Subspace Selection</head><p>Another aspect also known as the 'curse of dimensionality' in other domains is the combinatorial explosion. For example, combinatorial effects strike at the level of distance functions. For a normal distribution, an object is farther than three standard deviations away from the mean with a probability of approximately 0.27%-nota bene, in a single dimension. Assuming that each dimension is independently normally distributed, an object appears to be normal with a likelihood of approximately 0.9973 d ; at d = 10 dimensions, Statistical Analysis and Data Mining DOI:10.1002/sam an object is within the 3σ range in every single dimension with 97.33%, at d = 100 with 76.31%, and at d = 1000 at just 6.696%. So looking at a single attribute of highdimensional distributions, virtually every object is extreme in at least one dimension. This can to some extend be controlled with extreme significance thresholds-in order to get 99.73% correct at 1000 dimensions, we could use a significance threshold of 99.99973% in every single dimension, or about 5σ -but at the cost of missing outliers that are less significant. In addition, we might need to increase these thresholds additionally to account for data set sizes: in a normally distributed, single dimensional data set of just 10000 objects, around 27 objects will appear to be outliers at 3σ just by chance. Therefore, it may be necessary to increase the threshold even further.</p><p>The statistical bias introduced by testing such a large quantity of models is also known as data-snooping bias and is closely related to model overfitting. The proper statistical practice of developing a hypothesis and then validating it on independent data is violated by an exhaustive search of subspaces reusing the same data that is to be evaluated. In order to obtain statistically valid results, we must therefore not base the choice of the evaluation subspace on the object we want to test. But even at choosing the subspace, we already see a combinatorial explosion once we go beyond looking at single attributes as in the example above.</p><p>A method for generating models that does not incur a data-snooping bias is to partition the data set with a grid. This works quite well in low-dimensional data, but while a grid with 10 bins in each dimension has 100 cells in two dimensions it already has 10 100 (i.e., one googol) cells in 100 dimensions. In order for these cells not to be already empty on average, we need at least as many objects as cells. In order to draw significant conclusions from the number of objects in a cell, we need a data set at least an order of magnitude larger than the number of cells. Therefore, a naïve grid-based approach is also not feasible in high dimensions.</p><p>When selecting a subspace out of d dimensions, there are already 2 d -1 unordered possible subspaces to choose from. Looking at all projected subspaces is therefore also not feasible in high-dimensional data. Instead, these decisions will need to be taken on the basis of single dimensions and aggregated to a higher order subspace. The number of combinations further increases, when we instead of selecting (or not selecting) dimensions select an interval in each dimension (not selecting a subspace can be seen as restricting it to its full value range). If we had just 10 interval borders to choose from, we then have 45 d affine, axis-parallel subspaces to choose from. Allowing arbitrarily oriented linear manifolds removes any bounds from the number of models to choose from.</p><p>In this more general setting, a subspace outlier can be thought of as an object that is outlying in some subspace or some combination of attributes, whereas it could be perfectly normal in other attributes and even in the majority of attributes. The normality in many attributes might also outweigh the abnormality in some subspace. This could render the subspace outlier undetectable (masked) in the full dimensional space. A simple model (in fact, the first model proposed for subspace outliers <ref type="bibr" target="#b10">[11]</ref>) would partition the data space into a grid of cells. For a single cell, an expected number of contained points can be computed assuming a uniform distribution. As (subspace) clusters can be seen as (groups of) dense grid cells <ref type="bibr" target="#b30">[31]</ref>, all points contained in unexpectedly sparse grid cells could be seen as being outliers.</p><p>As we have seen, combinatorial issues of highdimensional data can be problematic on various facets: on one hand, the model search space can explode, rendering many search methods unusable. On the other hand, evaluating an object against many possible subspaces-or even the broad search for a subspace in which the object is unusual-may introduce a statistical bias called the data-snooping bias and may cost us statistical validity and significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Hubness</head><p>Recently, a couple of papers studied the effect of so called 'hubness' in high-dimensional data on different data mining and machine learning tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref>. Hubness is a phenomenon well known in graph data analysis <ref type="bibr" target="#b68">[69]</ref>. In vector spaces, 'hubs' are points relatively close to many other points, that is, they occur very often in k-neighborhoods of other points while other points may occur rarely if ever in k-neighborhoods. 'Hubness' (or, more specifically, k-hubness of an object o: N k (o)) is thus the number of times a point o is counted as one of the k nearest neighbors of any other point in a data set. It turns out that, with increasing dimensionality, many points show a small or intermediate hubness while some points exhibit a very high hubness. As they put it in ref. <ref type="bibr" target="#b65">66</ref>, intuitively, we could interpret these 'hubs' as very popular neighbors. More formally, this behavior of k-neighborhoods can also be seen as a distribution of the distances to the kth nearest neighbor which is increasingly skewed with increasing (intrinsic) dimensionality, as studied in refs. 65,66. As they also notice, setting a fixed ε-neighborhood, hubs do not emerge. The phenomenon seems, thus, closely related to the diminishing contrast and discriminability of distances: while the absolute distance values are uninformative, the rankings differ considerably and result in high hubness of some points, usually those that are more central with respect to the corresponding distribution.</p><p>Why this special aspect of the concentration effect is worth making a separate point here was pointed out by Radovanović et al. <ref type="bibr" target="#b65">[66]</ref>: the other side of the coin of hubness is probably rather relevant for outlier detection in high-dimensional data. There will also be antihubs that are unusually far away from most other points and that, specifically, exhibit a large distance from their kth nearest neighbor which would qualify them as outliers according to an outlier model based on the kth nearest neighbor distances <ref type="bibr" target="#b26">[27]</ref>, despite the fact that they are generated by the usual distribution. Probabilistically, however, as argued in ref. <ref type="bibr" target="#b65">66</ref>, hubs are also outlierish since they are rare. However, Radovanović et al. only touched upon this question. Overall, the relation of hubness and outlier degree appears to be remaining an open issue for high-dimensional outlier detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Consequences</head><p>What did we learn about the impact of the curse of dimensionality on outlier detection in high-dimensional data, what are open issues and questions? Let us recollect the important points as a couple of problems or challenges for outlier detection in high-dimensional data:</p><p>Problem 1 (Concentration of Scores) Due to the central limit theorem, the distances of attributewise i.i.d. distributed objects converge to an approximately normal distribution with low variance, giving way to numerical and parametrization issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 2 (Noise attributes)</head><p>A high portion of irrelevant (approximately i.i.d. distributed) attributes can mask the relevant distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 3 (Definition of Reference-Sets) Common notions of locality (for local outlier detection) rely on distance-based neighborhoods, which often leads to the vicious circle of needing to know the neighbors to choose the right subspace, and needing to know the right subspace to find appropriate neighbors.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 4 (Bias of Scores)</head><p>Scores based on L p norms are biased toward highdimensional subspaces, if they are not normalized appropriately. In particular, distances in different dimensionality (and thus distances measured in different subspaces) are not directly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 5 (Interpretation &amp; Contrast of Scores)</head><p>Distances and distance-derived scores may still provide a reasonable ranking, while (due to concentration) the scores appear to be virtually identical. Choosing a threshold boundary between inliers and outliers based on the distance or score may be virtually impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 6 (Exponential Search Space)</head><p>The number of potential subspaces grows exponentially with the dimensionality, making it increasingly hard to systematically scan through the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 7 (Data-Snooping Bias)</head><p>This is the correct version of the misconception that every point in high-dimensional data is an outlier: Given enough subspaces, we can find at least one subspace such that the point appears to be an outlier. Statistical principles of testing the hypothesis on a different set of objects need be employed. This problem is probably the most subtle and persistent one. We could see it also as problem of overfitting, since the outlier model is overly adapted to the data point given. This point of view makes it obvious that this problem is the most fundamental one to avoid in any learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 8 (Hubness)</head><p>What is the relationship of hubness and outlier degree? While antihubs may exhibit a certain affinity to also being recognized as distance-based outliers, hubs are also rare and unusual and, thus, possibly are outliers in a probabilistic sense.</p><p>Aside from these problems and challenges for correctness and statistical validity, high-dimensional data pose special problems for the feasibility, for example, for efficient neighborhood search which is usually a fundamental step to derive a reference-set for each point to judge on its deviation in characteristics from the reference-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EFFICIENCY AND EFFECTIVENESS FOR OUTLIER DETECTION IN HIGH-DIMENSIONAL DATA</head><p>The outlier detection methods we are discussing in this survey (unsupervised methods for high-dimensional numeric data in Euclidean space), usually rely on the assessment of neighborhoods (e.g., counting objects in εneighborhoods <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>, using kth-nearest-neighbor-distances or aggregates thereof as outlier score <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b71">72]</ref>, or even comparing the object's neighbors with the neighbors of the object's neighbors <ref type="bibr" target="#b27">[28]</ref>).</p><p>Various efficient variants have been proposed for these basic approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref> usually based on pruning, on sampling, or on ranking strategies (see ref. <ref type="bibr" target="#b79">80</ref> for an overview and unifying discussion). Using efficient indexing-structures, such as R-trees <ref type="bibr" target="#b80">[81]</ref>, R*-trees <ref type="bibr" target="#b81">[82]</ref>, or X-trees <ref type="bibr" target="#b14">[15]</ref>, can also yield O(n log n) performance for many approaches. However, these techniques usually deteriorate in efficiency with increasing dimensionality <ref type="bibr" target="#b19">[20]</ref> due to the same issues described in Section 2: rectangular Statistical Analysis and Data Mining DOI:10.1002/sam pages offer an increasingly bad approximation for the data, the options for splitting increase, and the distances between closest and nearest objects concentrate.</p><p>Hence, some recent outlier detection methods tuned for efficiency in high-dimensional data use approximate neighborhood computation (see Section 3.1).</p><p>Regardless of the efficiency, the neighborhood in highdimensional data is also less expressive (however depending on the given data distribution as discussed in Section 2). Hence, other adaptations to high-dimensional data address the issue of effectiveness and stability (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Background: approximate neighborhoods</head><p>Locality sensitive hashing (LSH) was first proposed in ref. <ref type="bibr" target="#b18">19</ref> as a cure to the curse of dimensionality for closest-pairs search and later refined, for example, in refs. <ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref>. The key ingredient for this is the Johnson-Lindenstrauss Lemma <ref type="bibr" target="#b85">[86]</ref>, which proved the existence and bounds of projecting n objects into a lower dimensional space of dimensionality O(log n/ 2 ), such that the distances are preserved within a factor of 1 + . Matoušek <ref type="bibr" target="#b86">[87]</ref> further improves these error bounds. The most interesting and surprising property is that the reduced dimensionality depends only logarithmically on the number of objects and on the error bound, but not on the original dimensionality d. Different ways of obtaining such a projection have been proposed for common norms such as Manhattan and Euclidean distance. A popular choice are the 'databasefriendly' random projections <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref>, where 2  3 of the factors are 0 and the others ±1, which can be computed more efficiently than the previously used matrices. See ref. <ref type="bibr" target="#b89">90</ref> for an overview and empirical study on different variations of the Johnson-Lindenstrauss transform, indicating that a reduced dimensionality of k = 2 • log n/ 2 will usually maintain the pairwise distances within the expected quality. The known error bounds give a controlled way of reducing the dimensionality of the data set, conveniently based on random projections that are independent of the actual data and often rather cheap to compute as opposed to using, for example, principal component analysis. The error bounds allow for choosing a controlled trade-off between efficiency and precision. It should however be noted, as pointed out in ref. <ref type="bibr" target="#b90">91</ref>, that random projection methods are not suitable to defy the concentration of distances (see Section 2.1). They do, however, preserve strong structure that may be present in the data (see also discussion in Section 2.2).</p><p>Space-filling curves, also known as Peano curves <ref type="bibr" target="#b91">[92]</ref>, are another method of reducing the dimensionality while approximately preserving neighborhoods. In contrast to random projections, the projection to a space-filling curve does not directly preserve distances, but neighborhoods (to a certain extend). However they can still be used to compute neighbor candidates efficiently. Then only for those candidates the full-dimensional distance needs to be computed.</p><p>The key idea of a space-filling curve is to draw a onedimensional curve through the data space that-by being recursively defined, which results in a fractal curve-gets arbitrary close to every point without intersecting itself. The most popular variants are the Z-curve <ref type="bibr" target="#b92">[93]</ref> and the Hilbert curve <ref type="bibr" target="#b93">[94]</ref> that recursively split the space into halves. The original Peano curve <ref type="bibr" target="#b91">[92]</ref>, which divides into three parts, is less often used. Originally, these curves were developed for two-dimensional spaces. They can be generalized to higher dimensional spaces, though. Morton <ref type="bibr" target="#b92">[93]</ref> was the first to apply them in database systems for indexing geographic data. Since then, they have been used for example in Hilbert R-Trees <ref type="bibr" target="#b94">[95]</ref>.</p><p>As the original motivation of space-filling curves was to provide a complete order of (two-dimensional) vectors, space-filling curves are an extreme dimensionality reduction technique: they always reduce to a one-dimensional space, a linear order of all points. Intuitively, they can be interpreted as repeatedly cutting and opening the data space along some edge in the process of linearization. With an increasing number of dimensions, the number of such cuts-where neighborhoods are not preserved-increases. Figure <ref type="figure" target="#fig_7">18</ref> is a visualization of the 3rd order Hilbert curve, with the actual curve in black while the red lines indicate neighborhoods that are not preserved well. When the curve is unfolded, the red edges are no longer adjacent along the curve. It can be seen that space filling curves rely on an intricate (and just as infinite and fractal) pattern of cutting the data space into fragments that are then ordered by the curve. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Outlier detection using approximate neighborhoods</head><p>The concept of pruning based on approximate nearest neighbor search has been introduced to outlier detection by ref. <ref type="bibr" target="#b26">27</ref>. This concept is sometimes supported by data partitioning techniques as, for example, in refs. <ref type="bibr">27,96.</ref> For high-dimensional data, a variant has been proposed in ref. <ref type="bibr" target="#b96">97</ref>. Their approach RBRP (recursive binning and re-projection) is a combination of binning and projecting the data. In a first phase, they bin the data, recursively, into k clusters (using a k-means like clustering <ref type="bibr" target="#b97">[98]</ref> at each level). This results in k bins, and again, in each bin, k bins and so forth, unless a bin does not contain a sufficient number of points. For each resulting bin, in a second phase, approximate neighbors are listed following their linear order as projected onto the principal component of the bin. Finally, within each bin (as long as necessary), they use a variant of the nested loop algorithm <ref type="bibr" target="#b73">[74]</ref>, to derive the top-n outliers. The resulting outliers, though based on approximate neighborhood, are reported to be the same as delivered by ORCA <ref type="bibr" target="#b73">[74]</ref>, yet they are retrieved by RBRP far more efficiently in high-dimensional data than they are by ORCA. RBRP could thus be seen as an adaptation of ORCA to high-dimensional data.</p><p>An outlier detection method, LSOD (locality sensitive outlier detection), based on LSH has been proposed in ref. <ref type="bibr" target="#b98">99</ref>. In LSOD, also, the approximate neighborhood search (here based on LSH) is combined with a data partitioning step using a k-means type clustering. The idea of outlierness is intuitively that points in sparse buckets will probably have fewer neighbors and are therefore more likely to be (distance-based) outliers. The pruning is based on a ranking of this outlier likelihood, based on statistics on the partitions. As this intuition is closely connected to the distance-based notion of outliers, their conjecture that their approach 'can be used in conjunction with any outlier detection algorithm' may be overly optimistic. Nevertheless, as a ranking approach for distance-based outliers LSOD constitutes an interesting contribution to the research area.</p><p>PINN <ref type="bibr" target="#b99">[100]</ref> (projection-indexed nearest-neighbors), a variant of LOF <ref type="bibr" target="#b27">[28]</ref>, uses random projections to perform outlier detection in high-dimensional data. PINN facilitates an approximate neighborhood search that is used as a replacement of the neighborhood and distances computation of LOF. The distances required by LOF are estimated based on the random projections. They prove that the projection also preserves the outlier scores within the known error bound of random projection which is a theoretically very interesting result. In order to further improve the result quality, they retrieve the c • k nearest neighbors in the projected space, and then refine these candidates to the intended number k, which for low values of c = 2 or 3 offered considerable benefits. We conjecture that the combination of PINN with outlier detection methods other than LOF will prove a very interesting topic for further research. <ref type="foot" target="#foot_2">2</ref>Let us also note that there is an approximate method for the well-known LOCI (local correlation integral) algorithm <ref type="bibr" target="#b101">[102]</ref>. The basic version is estimating a multi-granularity deviation factor (MDEF) for each point and its local neighborhood. The approximate method aLOCI approximates the neighborhood by means of a space partitioning grid, leading to practically linear performance. Both variants, LOCI and aLOCI, are however not especially suitable for high-dimensional data. Since the approximation in aLOCI is grid-based, its quality and efficiency deteriorates with increasing dimensionality.</p><p>In refs. 72,103, space-filling curves are used for an approximation step. The main achievements of this work <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b102">103]</ref> are (i) the proposal of a new variant of the distance-based notion of outlierness, using the aggregated distances to the k nearest neighbors instead of using solely the distance to the kth nearest neighbor; and (ii) an efficient approximation of the nearest neighbors, based on Hilbert space filling curves <ref type="bibr" target="#b93">[94]</ref>. Using the aggregated distances (i) has a certain smoothing effect and is usually more stable than using solely the distance to the kth nearest neighbor. The effectiveness of Hilbert curves (ii) has been widely studied <ref type="bibr" target="#b103">[104]</ref>. The problem with space filling curves for approximations is that, though neighbors along the space filling curve are also close in the data space, the opposite relationship does not necessarily hold. In order to limit this loss for approximate nearest neighbor search, in <ref type="bibr" target="#b104">[105]</ref>, the authors proposed shifting the objects d times (where d is the data space dimensionality), resulting in d + 1 copies of the data. They hereby yield a guaranteed approximation quality. This idea is also used in <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b102">103]</ref> as a filter step before the exact computation of the top-n outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effectiveness and Stability</head><p>Some statistical methods for outlier detection meet the requirements of high-dimensional data by feature selection or dimensionality reduction methods (see ref. <ref type="bibr" target="#b105">106</ref> for a discussion of dimensionality reduction and e.g., refs. <ref type="bibr" target="#b106">[107]</ref><ref type="bibr" target="#b107">[108]</ref><ref type="bibr" target="#b108">[109]</ref> for the use of PCA for high-dimensional outlier detection; a methodology for quality assessment of dimensionality reduction is discussed in ref. <ref type="figure">110</ref>). An example for a more data mining-oriented method is ref. <ref type="bibr" target="#b110">111</ref>, where a feature extraction method is proposed specifically to enhance outlier detection. Actually, this method retrieves not a subset of the features but an optimized combination of features, based on separation of outliers versus inliers in training data. In these methods, the dimensionality is reduced and all the outliers are sought in the remaining or transformed feature space where they are expected to be more prominent.</p><p>In such approaches to outlier detection based on dimensionality reduction, essentially a single subspace for the complete data set is assumed to be sufficient to derive all outliers. As a side effect of the evaluation of their method, it was demonstrated in ref. <ref type="bibr" target="#b111">112</ref> that a global dimensionality reduction by PCA and outlier detection as a second step in the reduced feature space is likely to fail in the typical subspace setting. As opposed to global feature reduction methods, in this survey, we are interested in methods that still define outliers in the full space (present section) or identify potentially different subspaces for different outliers (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Combination of feature subsets</head><p>Combining different feature subsets (i.e., subspaces) for outlier detection has been proposed first in the 'feature bagging' approach <ref type="bibr" target="#b112">[113]</ref>. There, using different feature subsets was motivated by improving the quality of the overall prediction of an ensemble outlier detector, built from the single outlier detectors on the different (randomly selected) feature subsets. Though not aiming at high-dimensional data, this approach could be seen as an approach to deal with high dimensionality. This has been made more explicit in ref.</p><p>114, combining randomly selected subspaces for an (unsupervised) ensemble. Motivated by the 'curse of dimensionality', computing distances in randomly selected subspaces and combining the findings is expected to be more stable and effective than computing the distances (as required by most outlier detection methods) in the high-dimensional complete feature space.</p><p>Let us note that, aside from the mentioned approaches, there exist only a few outlier detection ensemble methods, namely <ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b116">[117]</ref>. These ensemble approaches are not specialized on feature subsets although they straightforwardly could use a similar setup as well.</p><p>Common to all these approaches is that, though they are using subspaces in the process of identification of outliers, they do not actually define outlierness with respect to specific subspaces but, as a result of the combination of different subspaces, they define outlierness with respect to the full, potentially high-dimensional, space. Using subspaces explicitly for modeling outlierness will be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Angle-based method</head><p>Since the cosine distance is often successfully used in high-dimensional data, it is not surprising that an outlier detection method tailored for high-dimensional data was based on the use of angles instead of plain distances: The outlier detection method ABOD (angle-based outlier detection) <ref type="bibr" target="#b43">[44]</ref> assesses the variance in angles between an outlier candidate and all other pairs of points. For outliers, most other data objects will be concentrated in some directions, while for inliers, data objects are distributed in all directions. Hence the variance of angles is lower for outliers than for inliers. The lower variance therefore signals the higher outlierness (as opposed to many other approaches where the higher score signals stronger outlierness).</p><p>ABOD has been shown to remain stable, where plainly distance-based methods deteriorate. However, the method scales cubic with respect to the number of data objects, since for each object, all pairs of other objects are checked. A more efficient variant is based on samples but still relatively stable <ref type="bibr" target="#b43">[44]</ref>.</p><p>A near-linear time efficient approximation variant of ABOD is proposed in ref. <ref type="bibr" target="#b100">101</ref>, based on random projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SUBSPACE OUTLIER DETECTION</head><p>The idea of defining outliers with respect to subspaces of the original feature space can be traced back to ref. <ref type="bibr" target="#b117">118</ref>. There, the explanation of outlierness is based on the features of major deviation from the majority of the usual data. The nature of anomaly of an already identified outlier is explained by referring to those attributes where this outlier shows a high deviation from the remainder of the data set, that is, the 'intensional knowledge of distance-based outliers' takes the form of explanations using a subset of attributes.</p><p>Yet, in the approach of Knorr and Ng, the outliers were defined in the full dimensional space and only in retrospect explained by subspace properties. The question arises, if outliers could also be defined in terms of subspace properties in the first place. Methods to do so came up mostly very recently and focus mainly on two aspects of the problem: (i) how to identify the subspace where an object is an outlier (tackling, e.g., Problems 2, 3, 6), and (ii) how to make scores based on different characteristics (such as the differing dimensionality of some subspaces) actually comparable in a meaningful way (tackling, e.g., <ref type="bibr">Problems 4,</ref><ref type="bibr" target="#b4">5)</ref>.</p><p>We discuss the existing methods with respect to these problems (i) and (ii) in the following Sections 4.1 and 4.2, respectively. Finally, we will have a short comment on the possible types of subspaces identified by the methods (such as: axis-parallel, or arbitrarily-oriented, that is, based on correlations among attributes), in Section 4.3.</p><p>Statistical Analysis and Data Mining DOI:10.1002/sam</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Identification of Subspaces</head><p>One possible fundamental approach to outlier detection is to identify clusters, for example, using the densitybased paradigm <ref type="bibr" target="#b118">[119]</ref>, where objects need not belong to any cluster but could remain as 'noise'. Those noise objects could be interpreted as being outliers. For lowdimensional outlier detection, this is usually seen as a rather crude approach. Many subspace outlier detection approaches, however, can be seen that way. They define clusters (implicitly or explicitly), and identify those objects not belonging to any cluster as outliers.</p><p>What makes this approach suitable for subspace outlier detection is the fact that many subspace clustering approaches are density-based or grid-based (i.e., the data space is partitioned by some grid, and those grid cells containing more than some expected number of objects constitute cluster elements <ref type="bibr" target="#b30">[31]</ref>). To avoid searching a number of grid-cells that explodes exponentially with the number of dimensions, these approaches usually start with partitioning each dimension individually and combining only those dimensions for two-dimensional (three-dimensional, and so on) grids that at least contain one cluster (usually adopting an Apriori-like <ref type="bibr" target="#b119">[120]</ref> search heuristic).</p><p>What makes this approach questionable for subspace outlier detection, however, is, first, that an Apriori-like strategy is not possible if we are searching for sparse regions, instead of dense regions, since the sparsity is not anti-monotonic over increasing dimensionality. Besides, even in subspace clustering the monotonicity is based on restrictions that render the usage of results limited <ref type="bibr" target="#b32">[33]</ref>. Second, it should be kept in mind that the subspace clustering problem is still not sufficiently well defined and does not have satisfying solutions. It is not even clear what a satisfying solution in terms of retrieved subspace clusters would be as the evaluation of these approaches is an open and challenging problem (some aspects are discussed in refs. <ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b121">122)</ref>.</p><p>Accordingly, the first approach claimed to be suitable for high-dimensional data <ref type="bibr" target="#b10">[11]</ref> (with the same content also published in ref. <ref type="bibr" target="#b122">123</ref>) resembles a grid-based subspace clustering approach where not dense but sparse grid cells are sought to report objects that are contained within these sparse grid cells as outliers. The grid is defined to partition the data space dimension-wise using equidepth ranges, that is, each attribute is divided into φ ranges such that each range contains a fraction of</p><formula xml:id="formula_1">f = 1 φ</formula><p>of the data objects. Note that, although this way each onedimensional range contains the same number of objects, the intersection of two one-dimensional partitions can contain more or less than the fraction f . The algorithm now shall assess whether an intersection of k one-dimensional partitions (i.e., a k-dimensional hyper-cuboid in the d dimensional space, k ≤ d) contains unexpectedly few data points. Assuming the data consisting of N points being uniformly distributed in d dimensions, the expected value is given by</p><formula xml:id="formula_2">N • f k ,</formula><p>and the standard deviation is given by</p><formula xml:id="formula_3">N • f k • (1 -f k ).</formula><p>The outlier model therefore designates any point an outlier that is contained in a cuboid containing (significantly) less than expected data points. This approach has various problems. As discussed in Problem 6, with increasing dimensionality, the expected value of a grid cell quickly becomes too low to find significantly sparse grid cells. The authors are aware of this problem <ref type="bibr">[123, p. 217]</ref>, and argue that k must be chosen small enough. Secondly, the parameter k must be fixed, as the scores are not comparable across different values of k (Problem 4). The search space is too large even for a fixed k only (Problem 6), which is why the authors propose a genetic search that ensures the value of k is preserved across mutations. With a restricted computation time, their method will inspect just a tiny subset of the n k projections (not yet to speak of individual subspaces), and their randomized search strategy does neither encourage fast enough convergence nor diversity. The method can thus not give any guarantees about the outliers detected or missed.</p><p>Using this randomized model optimization without a statistical control also raises the question of the statistical bias (Problem 7) and how meaningful the detected outliers actually are. By assuming that the quantile grid will have approximately equally filled bins, the statistical test employed essentially assumes that the joint probability always equals the product of the marginal probabilities. Hence the presence of clusters in the data set will skew the results considerably. Additionally, the proposed equidepth binning is likely to include outliers in the grid cell of a nearby cluster and thus hide them from detection entirely; in fact a non-equidepth grid approach is much more likely to have outliers within sparse grid cells.</p><p>In general, when using a grid-based search, the identified dense areas also need to be refined to detect outliers that happen to fall into a cluster bin. In the presence of correlations, deviations from such a trend can otherwise not be recognized. Nevertheless, the authors of ref. 11 are credited for bringing the problem of subspace outliers to the attention of the data mining community.</p><p>Historically, the second approach dedicated to this problem was HOS-Miner <ref type="bibr" target="#b123">[124]</ref>. They propose not to identify Statistical Analysis and Data Mining DOI:10.1002/sam subspace outliers but rather to identify the subspaces in which a given point is an outlier. However, for that purpose, they also define the outlying degree of a point with respect to a certain space (or possibly a subspace) s in terms of the sum of distances to the k nearest neighbors in this (sub-)space s. For a fixed subspace s, this is the outlier model of ref. <ref type="bibr" target="#b71">72</ref>.</p><p>Defining subspace outlierness this way has the advantageous property of a monotonic behavior over subspaces and superspaces of s, since the outlying degree OD is directly related to the distance-values. At least for L p -norms (this restriction is not discussed in ref. <ref type="bibr" target="#b123">124</ref>), the following property holds for any object o and subspaces s 1 , s 2 :</p><formula xml:id="formula_4">OD s 1 (o) ≥ OD s 2 (o) ⇐⇒ s 1 ⊇ s 2 .</formula><p>(</p><p>This is trivially true and is used to facilitate an Apriori-like search strategy (down-ward as well as up-ward) for outlying subspaces for any query point by setting a threshold T discriminating outliers (where OD s (o) ≥ T ) from inliers (where OD s (o) &lt; T ) in any subspace s. Alas, setting the same fixed threshold to discern outliers with respect to their score OD in subspaces of different dimensionality ignores the problem that these scores are rather incomparable (see <ref type="bibr">Problem 4)</ref>. Later approaches specifically addressed this comparability-problem, see Section 4.2. The authors of ref. <ref type="bibr" target="#b124">125</ref> (see below) even assert that the monotonicity of Eq. (1) must not be fulfilled for true subspace outliers (since it implies the outlier can be found trivially in the full-dimensional space then). Furthermore, the systematic search for the subspace with the highest score raises the question of a data-snooping bias (see <ref type="bibr">Problem 7)</ref>.</p><p>OutRank <ref type="bibr" target="#b44">[45]</ref> (or the variant described shortly in ref. <ref type="formula">126</ref>) extends a grid-based clustering approach with an analysis of the non-clustered objects that is much more feasible for high dimensionality. Clusters as opposed to outliers are not rare objects, and are likely to be recognizable in both lower and higher dimensionality (although a meaningful definition of subspace clusters is still challenging, as noted above). Since OutRank is based on finding a cluster model first (with some gridbased subspace clustering such as DUSC (dimensionality unbiased subspace clustering) <ref type="bibr" target="#b126">[127]</ref> or some suitable density-based subspace clustering such as EDSC (efficient density-based subspace clustering) <ref type="bibr" target="#b127">[128]</ref>), it avoids the aforementioned statistical bias (Problem 7) that comes along with approaches searching some subspace for each object where the object is an outlier. Instead, the outlierness is essentially asserted based on how often the object is recognized as part of a cluster and on the dimensionality and size of the subspace clusters it is a part of. In order for this to work, a strong redundancy in the clustering is implicitly assumed. The result then may be biased towards hubs (Problem 8). It is, however, not totally satisfying to see outliers as just a side-product of densitybased clustering. Depending on the parametrization of the density-based clustering algorithm, this can result in a large set of outliers: On a data set where no clusters are recognized, even all objects become outliers. Furthermore, outlier detection based on subspace clustering relies on the subspace clusters being well separated which links back to the issues we discussed in Section 2.2 on distributions being well-separated or not.</p><p>A method proposed for tackling the specialized problem of finding outliers in subspaces without a previous explicit clustering-step is SOD (subspace outlier detection) <ref type="bibr" target="#b128">[129]</ref>. Based on a reference set for a point, the outlierness of the point is assessed. The reference set is seen as possibly defining (implicitly) a subspace cluster (or a part of such a cluster), that is, the points may scatter along some dimensions while they may concentrate within others. If the query point deviates considerably from the reference set in the latter attributes, it is a subspace outlier with respect to the corresponding subspace. SOD provides actually not a decision (outlier vs. inlier) but an outlier score which could be interpreted as a probability-score of being an outlier according to the deviation from the reference set in the corresponding subspace by assuming a distribution of the distances in the relevant attributes, as visualized in Fig. <ref type="figure">19</ref>.</p><p>For a single object, there is no comparison of multiple subspaces in SOD, but the subspace decision is based on the whole reference set, this way avoiding the statistical bias (Problem 7). It is crucial, though, to select Irrelevant Attribute Relevant Attribute Fig. <ref type="figure">19</ref> SOD projects to locally relevant attributes, then assumes a normal distribution to judge outlierness. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.] a meaningful reference set for each query point. In ref.</p><p>129, the authors proposed to use shared nearest-neighbors in order to stabilize the assessment of neighborhoods in high-dimensional spaces to attenuate Problem 3. Shared nearest-neighborhood was indeed demonstrated to enhance a meaningful selection of neighbors in high-dimensional space <ref type="bibr" target="#b52">[53]</ref>. The normalization of scores and the interpretation as 'probability estimates' aims at improving with respect to Problem 5, which will be discussed further in Section 4.2.</p><p>OUTRES <ref type="bibr" target="#b129">[130]</ref> specifically addresses the problem of bias of outlier scores due to differing value range of distance values in subspaces of different dimensionality (Problem 4). For their approach, this is actually even more important since they assess deviations of each object in several subspaces simultaneously and are interested in a general (combined) ranking of the objects according to their outlier scores in all 'relevant subspaces'. They restrict the assessment to this set of 'relevant subspaces', excluding subspaces with highly scattered objects and, thus, low contrast between outliers and inliers. OUTRES needs comparable neighborhoods (see Problem 3) for each point to estimate densities. In order to adjust for different subspace dimensionalities (see Problem 4), they adjust the ε radius for the given subspaces dimensionality. The score of a single subspace is then obtained by comparing the object's density to the average density of its neighborhood. The total score of an object is the product of all its scores in all relevant subspaces. Assuming a score in [0, 1], where the smaller score denotes the stronger outlier, this should provide a good contrast for those outliers with very small scores in many relevant subspaces. Details of the selection of relevant subspaces for OUTRES are revealed in the follow-up paper <ref type="bibr" target="#b130">[131]</ref>. Any attribute that exhibits uniformly distributed values in the neighborhood of the currently considered point o is rejected, based on a statistical significance test. This excludes, for this o, also any superspaces of uniformly distributed attributes (and, hence, an Apriori-like search strategy can be applied).</p><p>The assumption is, that subspaces where the objects are not uniformly distributed (in the neighborhood of o), exhibit a clustering structure where outliers can possibly meaningfully deviate. The remaining attributes (and their combinations) are seen as relevant. Note that the relevance is assessed for each data object individually, that is, a subspace can be relevant for one object and irrelevant for another object which renders this approach relatively expensive. This approach alleviates the problem of noise attributes (Problem 2) while it is based on a statistic on the neighborhood of the point and thus not likely susceptible to a statistical bias (Problem 7).</p><p>For HighDOD <ref type="bibr" target="#b124">[125]</ref>, the authors motivate their method with an interesting observation on the sum of distances to the k nearest neighbors as the outlier score (as defined by Angiulli and Pizzuti <ref type="bibr" target="#b102">[103]</ref>) and the monotonicity property of Equation 1: while this enables an Apriori-like subspace search, it also implies that the outlier score must be maximal in the full-dimensional space, and the method will thus not find true subspace outliers (and a full-dimensional algorithm could have been used in the first place). Instead, they modify the outlier score of ref. <ref type="bibr" target="#b102">103</ref> to use the L p norm normalization we also used in Section 2. They conclude that the pruning of subspaces is impossible, and therefore examine all subspaces up to a user-defined maximum dimensionality m, similar to ref. <ref type="bibr" target="#b10">11</ref>. This however suggests that they possibly also fall prey to the data-snooping bias. To lessen the exponential number of subspaces, they only explore them up to a maximum dimensionality of m = log 10 n , and use a linear-time (O(n • m)) density estimation to generate outlier candidates they compute the nearest neighbors for.</p><p>At first sight, the observation that neither an upwardnor a downward-pruning is possible may appear to contrast to most of the previous efforts. However, this only holds when applied to the actual outlier score. Methods that use a pruning strategy for example for clustering will still work. It does however indicate that Problem 4 is fatal for HOS-Miner <ref type="bibr" target="#b123">[124]</ref>, and the maximum outlier score of HOS-Miner will always be in the full-dimensional space.</p><p>The latest paper on the topic of subspace outlier detection is dedicated to identifying high contrast subspaces (HiCS) <ref type="bibr" target="#b111">[112]</ref>, using Monte Carlo sampling to obtain a notion of contrast of a subspace. In these subspaces, they use LOF and aggregate the LOF scores for a single object over all interesting subspaces (although not using score normalization, as we discuss below in Section 4.2). Their philosophy of decoupling subspace search and outlier ranking may raise some questions since a certain measure of contrast to identify interesting subspaces will relate quite differently to different outlier ranking measures. It is doubtful that, as they state, instead of LOF, any other outlier score could be used interchangeably without a considerable impact on the results. As their measure of interestingness is based on an implicit notion of density, it may only be appropriate for density-based outlier scores. Nevertheless, this decoupling allows them to discuss the issue of subspace selection with great diligence as this is the focus of their study. The core concept for these subspaces with high contrast is a measure of correlation among the attributes of a subspace, based on the deviation of the observed probability density function from the expected probability density function, assuming independence of the attributes. These deviations are aggregated over several Monte Carlo samples to assign a value of contrast to a subspace. This measure is closely related to the notion of mutual information. Actually, it can be seen as a Monte Carlo estimation of the mutual information. The intuition is that, Statistical Analysis and Data Mining DOI:10.1002/sam in these subspaces, outliers are not trivial (e.g., identifiable already in one-dimensional subspaces) but deviate from the (although probably nonlinear and complex) correlation trend exhibited by the majority of data in this subspace.</p><p>The idea of comparing the joint probability with the marginal probabilities can be traced back to implicit assumptions in ref. <ref type="bibr" target="#b10">11</ref>. Their approach considered objects in areas less dense than expected immediately and indiscriminately as outliers, while HiCS refines them using LOF in the corresponding subspace.</p><p>Let us note that, while the focus of research was the identification of meaningful subspaces for outlier detection, an open and interesting future issue for finding outliers in different subspaces efficiently may be the appropriate use of suitable index structures for subspace similarity search, which is, however, a rather immature research area itself. To the best of our knowledge, all approaches to subspace similarity search potentially suitable are refs. <ref type="bibr" target="#b131">[132]</ref><ref type="bibr" target="#b132">[133]</ref><ref type="bibr" target="#b133">[134]</ref><ref type="bibr" target="#b134">[135]</ref><ref type="bibr" target="#b135">[136]</ref><ref type="bibr" target="#b136">[137]</ref>. However, unless the effectiveness of subspace selection for outlier detection is sufficiently resolved (and it is our impression that there is potential for improvement), it is not yet the turn of tackling efficiency issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparability of Outlier Scores</head><p>An outlier score provided by some outlier model should help the user to decide whether an object actually is an outlier. For many approaches even in low-dimensional data the outlier score is not readily interpretable. The scores provided by varying methods differ widely in their scale, their range, and their meaning. For many methods, the scaling of occurring values of the outlier score even differs within the same method from data set to data set, that is, outlier score o in one data set means, we have an outlier, while in another data set it may not be really exceptional. Even within one data set, the identical outlier score o for two different database objects can denote actually substantially different degrees of outlierness, depending on different local data distributions. Obviously, this makes the interpretation and comparison of different outlier detection models a very difficult task.</p><p>For low dimensional data, this problem has been recognized. LOF <ref type="bibr" target="#b27">[28]</ref>, for example, intends to level out different density values in different regions of the data, as it assesses the local outlier factor. Yet still, the same LOF value can designate very different degrees of outlierness in particular across different data sets and dimensionalities. LoOP <ref type="bibr" target="#b137">[138]</ref> is a recent LOF variant that addresses this problem more specifically, providing a statistical interpretation of the outlier score by translating it into a probability estimate. This includes a normalization to become independent from the specific data distribution in a given data set. In ref. <ref type="bibr" target="#b115">116</ref>, generalized scaling methods for a range of different outlier models have been proposed. The unified scaling and better comparability of different methods could also facilitate a combined use to get the best of different worlds, for example, by means of setting up an ensemble of outlier detection methods. This is experimentally also shown for different projections, a common situation in subspace outlier detection, when scores of different subspaces need to be combined.</p><p>Considering subspaces for outlier detection sets up additional problems of comparability and bias (cf. Problems 4 and 5). Most outlier scorings are based on assessment of distances, usually L p distances, which can be expected to grow with additional dimensions, while the relative variance decreases. Hence a numerically higher outlier score, based on a subspace of more dimensions, does not necessarily mean the corresponding object is a stronger outlier than an object with a numerically lower outlier score, based, however, on a subspace with less dimensions. Many methods that combine multiple scores into a single score neglect to normalize the scores before the combination. Another effect considerably affecting L p distances is the normalization and scaling of the data set.</p><p>This specific problem has been accounted for by some of the subspace outlier detection methods. The model of refs. 11,123 circumvents the problem since they restrict the search for outliers to subspaces of a fixed dimensionality (given by the user as input parameter). OutRank <ref type="bibr" target="#b44">[45]</ref> weights the outlier scores by size and dimensionality of the corresponding reference cluster. SOD <ref type="bibr" target="#b128">[129]</ref> may be of special interest here since it also provides a scoring interpretable as probability, including a normalization over the dimensionality (cf. Fig. <ref type="figure">19</ref>).</p><p>For OUTRES <ref type="bibr" target="#b129">[130,</ref><ref type="bibr" target="#b130">131]</ref>, this problem of bias is the core motivation. The score definition of OUTRES uses density estimates that are based on the number of objects within an ε-range in a given subspace (or, enhanced: on the sum of their weighted distances, using an Epanechnikov Kernel <ref type="bibr" target="#b138">[139]</ref> with a bandwidth adaptive to the dimensionality). To account for the bias of distance-based scores towards high-dimensional subspaces, an adaptive neighborhood (ε is increasing with dimensionality) is proposed as well as an adaptive density by scaling the distance values accordingly. The score is also adapted to locally varying densities as the score of a point o is based on a comparison of the density around o vs. the average density among the neighbors of o (i.e., they seek local outliers in the sense of LOF <ref type="bibr" target="#b27">[28]</ref>). This renders, however, the time complexity O(n 3 ) for a database of n objects unless suitable data structures (such as, e.g., precomputed neighborhoods) are used. Because of the adaptation to different dimensionality of subspaces, where different neighborhoods are relevant for any point in different subspaces, this is not trivial here, yet it is not discussed in the paper.</p><p>The bias of distance-based outlier scores toward higher dimensions is also the main motivation for HighDOD <ref type="bibr" target="#b124">[125]</ref>. Their approach is based on the outlier notion of ref. <ref type="bibr" target="#b71">72</ref> (sum of distances to the k nearest neighbors), adapting the distances to the dimensionality d of the corresponding subspace by scaling the sum with 1 √ d (or, if using another L p distance than Euclidean, with 1 p √ d ). Furthermore assuming normalized attributes (with a value range in [0, 1]), this results in restricting each summand to ≤ 1 and the sum therefore to ≤ k, irrespective of the considered dimensionality. A normalization to adjust variances is, however, not done, and as such the curse of dimensionality is not completely handled.</p><p>For HiCS <ref type="bibr" target="#b111">[112]</ref>, it seems questionable whether the scores are comparable in a meaningful way since LOF scores retrieved in subspaces of different dimensionality are aggregated for a single object without normalization. As we have seen on the basis of Figs. <ref type="figure">7</ref> and<ref type="figure">8</ref>, the LOF scores differ widely with varying dimensionality. An improvement could be based on LoOP <ref type="bibr" target="#b137">[138]</ref> or the general scaling and normalization methods discussed in ref. <ref type="bibr">116.</ref> In the experiments carried out in ref. <ref type="bibr" target="#b111">112</ref>, however, this is not a major issue since the relevant subspaces vary only between 2 and 5 dimensions. Yet in general, while much diligence has been spent in ref. <ref type="bibr" target="#b111">112</ref> on the selection of subspaces, the issue of comparability of scores has been ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Types of Subspaces: Axis-parallel or Arbitrarily Oriented</head><p>Most of the algorithms for subspace outlier detection so far are restricted to outliers in axis-parallel subspaces. This restriction is, for example, due to grid-based approaches or to the required first step of subspace or projected clustering. The type of subspace sought by HiCS <ref type="bibr" target="#b111">[112]</ref> is not restricted in this sense or even biased to find notaxis-aligned subspaces due to the analysis of correlation between attributes. Another example of definition of outliers in arbitrarily-oriented subspaces due to correlation among some attributes and the deviation of outliers from this correlation pattern has been proposed earlier as COP (correlation outlier probability) in ref. <ref type="bibr">140, ch. 18</ref> (see also ref. <ref type="bibr" target="#b140">141)</ref>, as an application of the correlation clustering concepts discussed in ref. <ref type="bibr" target="#b141">142</ref>. According to this model, points have a high probability of being a 'correlation outlier' if their neighbors show strong linear dependencies among attributes and the points in question deviate substantially from the corresponding linear model. Identifying so called 'correlation outliers' may, thus, be another interesting way to define outliers in highdimensional data. But for both, axis-parallel and arbitrarily oriented outlier models, many (and to a good part, identical) issues remain open and wait for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tools and Implementations</head><p>We would also like to point to some open-source implementation frameworks related to subspace outlier detection.</p><p>Subspace Outlier Ranking Exploration Toolkit (SOREX)<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b142">[143]</ref> is a collection of some outlier detection algorithms for high-dimensional data, mainly providing variants of OutRank that are based on using different subspace clustering algorithms. SOREX is part of the OpenSubspace project <ref type="bibr" target="#b143">[144]</ref>, which again is an extension of WEKA <ref type="bibr" target="#b144">[145]</ref> for subspace clustering. WEKA does however not include index structures, which can provide substantial speedups for many data mining applications. Let us remark, though, that SOREX does unfortunately not come along with the source code of the available methods.</p><p>ELKI <ref type="foot" target="#foot_5">4</ref> is an actively developed and maintained 'Environment for deveLoping KDD-applications supported by Index-structures'. Two recent releases <ref type="bibr" target="#b145">[146,</ref><ref type="bibr" target="#b146">147]</ref> were especially dedicated to outlier detection, release 0.3 <ref type="bibr" target="#b145">[146]</ref> focusing on visual evaluation of outlier scores, release 0.4 <ref type="bibr" target="#b146">[147]</ref> specialized on geographical or spatial outlier detection, comprising-additionally to a selection of fundamental outlier detection methods such as those in refs. <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b137">138,</ref><ref type="bibr" target="#b147">148</ref> and LOCI including the approximate aLOCI <ref type="bibr" target="#b101">[102]</ref>-also some spatial outlier detection methods such as those in refs. <ref type="bibr" target="#b148">[149]</ref><ref type="bibr" target="#b149">[150]</ref><ref type="bibr" target="#b150">[151]</ref>, among others. With the latest release 0.5 <ref type="bibr" target="#b151">[152]</ref>, several implementations of algorithms for outlier detection in high-dimensional data are provided, such as refs. <ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b152">153</ref> and the implementation of refs. 72,103 based on Hilbert curves, more are going to be included. Some of the figures in this article were generated using ELKI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data Preparation, Normalization, and Bias</head><p>Data preprocessing affects the outcome considerably. Some methods require the data to be normalized to the interval [0 : 1], others expect standardized attributes (μ = 0, σ = 1) or work on the ranks. Any such transformation implies a slight information loss, but often improves the results considerably. Some algorithms can only be used with normalized attributes, while others are just biased by non-standardized attributes. Normalization can be done as a preprocessing step, or by adjusting the distance function accordingly. Last but not least, the distance function itself is an implicit parameter to many algorithms. Frameworks such as WEKA and ELKI offer a wide choice of such normalizations, filters, preprocessors, and distance functions beyond the classic L p norms.</p><p>To discern between high and low variance in order to distinguish irrelevant from relevant attributes, perhaps even in some local neighborhood, is also far from trivial. In presence of a single highly correlated attribute pair (for example a duplicate attribute or the sensor readings of two redundant sensors), all other correlations in the data set may appear insignificant in contrast to this. Yet, few of the established methods consider this situation of extreme (but trivial) correlations masking weaker but more interesting trends in the data set. HiCS uses a variant of mutual information to skip attribute combinations. Only the candidates with the least mutual information are kept, but there is no treshold on what amount of mutual information is significant. COP <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141]</ref> uses PCA and a local subspace filter to compute arbitrarily oriented subspaces, and needs to decide which axes are of high or low importance. In ref. <ref type="bibr" target="#b39">40</ref>, improvements for PCA in presence of outliers and the interaction with such filters are discussed. The ELKI framework includes a large collection of those filters to discern relevant and irrelevant subspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Outlier Detection Methods</head><p>Unsupervised data mining on high-dimensional data is in general hard to evaluate. Two studies on subspace clustering, as a related field, already demonstrate the difficulties associated with evaluation <ref type="bibr" target="#b143">[144,</ref><ref type="bibr" target="#b153">154]</ref>. Qualitatively evaluating outlier detection is even harder. If you evaluate a new algorithm, one of the most delicate tasks is probably to choose a fair setup for the competitors. It is our hope that this survey will also help to understand the different aspects that influence the results of different algorithms. For example, to parametrize SOD <ref type="bibr" target="#b128">[129]</ref> requires to understand the impact and appropriate choice of the neighborhood for computation of shared neighbor distances <ref type="bibr" target="#b52">[53]</ref>.</p><p>Not only for high-dimensional data, but generally for outlier detection, how to properly evaluate in an informative way the derived outlier rankings and outlier scores is an open and notorious problem.</p><p>Existing evaluation procedures exhibit, though often used, a couple of problems that should also be acknowledged when using such evaluation methods. For example, precision@k, which is the true positive rate for the top k results in a data set that contains k outliers, is a rather naïve way of evaluating the result. What renders this approach problematic is the imbalanced nature of the outlier problem. Consider a data set containing 100 objects. The typical expectation would be that 2 of those are outliers. A method that ranks the true outliers on rank #3 and #4 will have a precision@2 of 0. Thus, occasionally, the precision values are shown for a larger range of k.</p><p>For an area like information retrieval, for example, in search engines, this evaluation method is sensible. There, usually only the top, say, 10 results of a search query are displayed, and the classification of the data set provides complete information on the relevancy.</p><p>Contrariwise, for the task of evaluating unsupervised outlier detection, using some labeled data set, it is important to acknowledge that the 'ground truth' may be incomplete and that real world data may include sensible outliers that are just not yet known or were considered uninteresting during labeling. This could happen in particular when algorithms are evaluated using classification data sets, assuming that some rare (or downsampled) class contains the outliers. Such a setup is neglecting the possibility that the rare class may be clustered, whereas there are probably additional true outliers within the frequent classes. If a method is detecting such outliers, that should actually be rated as a good performance of the method. Instead, in this setup, detecting such outliers is overly punished due to class imbalance.</p><p>A more advanced method of evaluation, widely used in evaluation of outlier detection methods, is based on receiver operating characteristic (ROC) curves. ROC curves are a plot of the true positive rate against the false positive rate. Since a plot is hard to evaluate, this is turned into a measure by computing the area under this curve (AUC). For a random result, both rates will grow simultaneously, resulting in an area approximately filling half of the space. For a perfect result returning all outliers first and only then returning the inliers (i.e., we have 100% true positives before we even get the first false positive), the area under the corresponding curve will cover all available space. ROC curves and ROC AUC analysis inherently treat the class imbalance problem by using the relative frequencies which makes them popular for evaluation of outlier detection. However, the best thing we have is not necessarily good. A problem associated with ROC curves is that they lose the score information. The ROC AUC analysis does not assess whether the outlier score offers a reasonable contrast between outliers and inliers, though yielding such high contrast would be a plus for any outlier detection algorithm (see <ref type="bibr">Problem 5)</ref>.</p><p>Motivated by these findings, lately a correlation measure to compare rankings has been developed <ref type="bibr" target="#b116">[117]</ref> that is taking the scores into account. This correlation not primarily to a given ground truth but between different outlier score rankings provided by different outlier detection algorithms, with different parameterization, and on different data (e.g., using feature subsets), allows for a finer judgement of similarities and disagreements between different outlier Statistical Analysis and Data Mining DOI:10.1002/sam detectors. Where all methods would have a similar performance as judged by ROC AUC, supplementing a ROC analysis with this ranking correlation measure allows for deeper insights in similarities and differences between different methods, parametrization, distance measures, and data.</p><p>Along with a score normalization (as, e.g., in ref. <ref type="bibr" target="#b115">116</ref>) and randomization methods such as feature bagging <ref type="bibr" target="#b112">[113]</ref>, these advances in evaluation methodology are important ingredients for improving ensemble approaches to outlier detection, a field of research that is rather unexplored but is promising to have also big potential for advancements of outlier detection in high-dimensional data. For example, the combination of rankings in different subspaces to a somehow aggregated ranking (the approach pursued in HiCS <ref type="bibr" target="#b111">[112]</ref>), could be straightforwardly interpreted as an ensemble approach and, hence, could be possibly explored and understood more detailed on the basis of the theoretical background of ensemble techniques. The normalization <ref type="bibr" target="#b115">[116]</ref> and the diversity analysis demonstrated in ref. <ref type="bibr" target="#b116">117</ref> are obvious extensions that should further improve the results of methods such as HiCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this survey, we have inspected some typical problems associated with high-dimensional data and discussed the challenges that outlier detection is presented with by the so-called 'curse of dimensionality' and related issues. On the basis of this discussion, we named some problems that should be acknowledged by research on new methods for outlier detection in high-dimensional data.</p><p>Furthermore, we discussed existing work, where we distinguish between approaches, on the one hand, not especially interested in subspace-definitions of outliers but just in efficiency and effectiveness issues for highdimensional data. On the other hand, we discussed specialized methods for subspace outlier detection.</p><p>Finally, we gave some pointers to tools and implementations, highlighted the importance of understanding the impact of data preparation, and remarked on an important meta-problem for any unsupervised outlier detection approach, the issue of a proper evaluation procedure.</p><p>Overall, we would like to conclude this survey not without emphasizing again that the area of outlier detection specialized for high-dimensional data offers lots of opportunities for improvement. There are just a few approaches around in the literature so far, yet there are many directions to go and problems still to solve. The researcher should, though, be aware of the existing attempts of solution and the associated pitfalls. To support further research on specialized high-dimensional outlier detection methods was our motivation for writing this survey and we are looking forward to seeing new ideas emerging in the research community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 Fig. 2 Normalized</head><label>12</label><figDesc>Fig. 1 Normalized Euclidean length, uniform [0,1], logarithmic scale for dimensionality d. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 NormalizedFig. 4 NormalizedFig. 5</head><label>345</label><figDesc>Fig. 3 Normalized Euclidean pairwise distance, uniform [0,1], logarithmic scale for dimensionality d. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 Fig. 8 Fig. 9 Normalized</head><label>789</label><figDesc>Fig. 7 LOF with k = 50, uniform [0,1], logarithmic scale for dimensionality d. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 NormalizedFig. 11 LOFFig. 12 LOF</head><label>101112</label><figDesc>Fig. 10 Normalized Euclidean pairwise distance, Gaussian [0,1], 1 outlier, logarithmic scale for d. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>10 Fig. 15</head><label>1015</label><figDesc>Fig. 15 Volumes of hyper-spheres. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>StatisticalFig. 16</head><label>16</label><figDesc>Fig. 16 Ratio: volume of a sphere vs. volume of the unit hypercube. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17 Small change of radius, big change of volume. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 18</head><label>18</label><figDesc>Fig. 18 Visualization of Hilbert curve unfolding. Red edges are cut at curve iteration 3. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.]</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that, as the ratio in comparison to the unit hypercube is obtained dividing the volume by 1, the values of the y-axis in this plot are just interpreted differently, but are not changed numerically and, hence, are directly comparable to the previous plot. We can interpret the first plot as it is exactly the same way. It is just convenient to have two different plots as the shape of the curves is different for smaller radii versus larger radii.Statistical Analysis and Data Mining DOI:10.1002/sam</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Statistical Analysis and Data Mining DOI:10.1002/sam</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Lately<ref type="bibr" target="#b100">[101]</ref>, random projections have been used to speed up ABOD<ref type="bibr" target="#b43">[44]</ref>, which we discuss in Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>3.2.2. Statistical Analysis and Data Mining DOI:10.1002/sam</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>http://dme.rwth-aachen.de/de/OpenSubspace/SOREX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>http://elki.dbs.ifi.lmu.de/ Statistical Analysis and Data Mining DOI:10.1002/sam</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>A.Z. is partially supported by NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Novelty detection: a reviewpart 1: statistical approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="2481" to="2497" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novelty detection: A reviewpart 2: neural network based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="2499" to="2521" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive survey of numeric and symbolic outlier mining techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Agyemang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alhajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="521" to="538" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An overview of anomaly detection techniques: Existing solutions and latest technological trends</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Netw</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3448" to="3470" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of outliers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H M</forename><surname>Rahmatullah Imon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev: Comput Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: a survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Outlier detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev: Data Mining Knowledge Disc</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="268" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection for discrete sequences: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="839" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Outlier detection for high dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">When is &quot;nearest neighbor&quot; meaningful?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Database Theory (ICDT)</title>
		<meeting>the 7th International Conference on Database Theory (ICDT)<address><addrLine>Jerusalem, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="217" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The TV-Tree: an index structure for high-dimensional data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="517" to="542" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Near neighbor search in large metric spaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 21st International Conference on Very Large Data Bases (VLDB)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The X-Tree: an index structure for high-dimensional data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 22nd International Conference on Very Large Data Bases (VLDB)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">/sam databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Braunmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno>DOI:10.1002</idno>
	</analytic>
	<monogr>
		<title level="m">Fast parallel similarity search in multimedia Statistical Analysis and Data Mining</title>
		<meeting><address><addrLine>Tucson, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Proceedings of the ACM International Conference on Management of Data (SIGMOD)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The SR-tree: an index structure for high-dimensional nearest neighbor queries</title>
		<author>
			<persName><forename type="first">N</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Tucson, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="369" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cost model for nearest neighbor search in high-dimensional data space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</title>
		<meeting>the 16th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems<address><addrLine>Tucson, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing (STOC)<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 24th International Conference on Very Large Data Bases (VLDB)<address><addrLine>New York City, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Searching in high-dimensional spaces: Index structures for improving the performance of multimedia databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="373" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the geometry of similarity search: dimensionality curse and concentration of measure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pestov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Process Lett</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="47" to="51" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is the nearest neighbor in high dimensional spaces?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 26th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="506" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the surprising behavior of distance metrics in high dimensional space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Database Theory (ICDT)</title>
		<meeting>the 8th International Conference on Database Theory (ICDT)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="420" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The concentration of fractional distances</title>
		<author>
			<persName><forename type="first">D</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="873" to="886" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the existence of obstinate results in vector space models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 33rd International Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient algorithms for mining outliers from large data sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LOF: identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Theory of nearest neighbors indexability</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Database Syst (TODS)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="838" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Density-based indexing for approximate nearest-neighbor queries</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 5th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustering high dimensional data: a survey on subspace clustering, patternbased clustering, and correlation clustering</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Knowledge Disc Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Subspace clustering techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Database Systems</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2873" to="2875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev: Data Mining Knowledge Disc</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="364" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Clustering high dimensional data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Knowledge Disc</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="340" to="350" />
		</imprint>
	</monogr>
	<note type="report_type">Wiley Interdiscip Rev</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Subspace clustering of high dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 4th SIAM International Conference on Data Mining (SDM)<address><addrLine>Buena Vista, FL</addrLine></address></meeting>
		<imprint>
			<publisher>Lake</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting</title>
		<author>
			<persName><forename type="first">K.-G</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Softw Technol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="255" to="271" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subspace clustering for high dimensional data: a review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="105" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Iterative projected clustering by subspace mining</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance based subspace clustering with flexible dimension partitioning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Data Engineering (ICDE)</title>
		<meeting>the 23rd International Conference on Data Engineering (ICDE)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1250" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A general framework for increasing the robustness of PCAbased correlation clustering algorithms</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<meeting>the 20th International Conference on Scientific and Statistical Database Management (SSDBM)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="418" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finding non-redundant, statistically significant regions in high dimensional data: a novel approach to projected and subspace clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 14th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="533" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global correlation clustering based on the Hough transform</title>
		<author>
			<persName><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Anal Data Mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="111" to="127" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Example-based robust outlier detection in high dimensional datasets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kitagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 5th IEEE International Conference on Data Mining (ICDM)<address><addrLine>Houston, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="829" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Angle-based outlier detection in high-dimensional data</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 14th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="444" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">OutRank: ranking outliers in high dimensional data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Steinhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Data Engineering (ICDE) Workshop on Ranking in Databases (DBRank)</title>
		<meeting>the 24th International Conference on Data Engineering (ICDE) Workshop on Ranking in Databases (DBRank)<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="600" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Re-designing distance functions and distance-based applications for high dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="18" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distinctiveness-sensitive nearest-neighbor search for efficient similarity retrieval of multimedia information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 17th International Conference on Data Engineering (ICDE)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Finding generalized projected clusters in high dimensional space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Independent Quantization: an index compression technique for high-dimensional data spaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 16th International Conference on Data Engineering (ICDE)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="577" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An adaptive and efficient dimensionality reduction algorithm for high-dimensional indexing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 19th International Conference on Data Engineering (ICDE)<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On high dimensional indexing of uncertain data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 24th International Conference on Data Engineering (ICDE)<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1460" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Is the distance compression effect overstated? Some theory and experimentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>France</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carrol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM)</title>
		<meeting>the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM)<address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="280" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Can shared-neighbor distances defeat the curse of dimensionality?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<meeting>the 22nd International Conference on Scientific and Statistical Database Management (SSDBM)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="482" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Geometric representation of high dimension, low sample size data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Royal Stat Soc: Ser B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="427" to="444" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The remarkable simplicity of very high dimensional data: application of model-based clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Classif</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="249" to="277" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distance metrics for high dimensional nearest neighborhood recovery: compression and normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>France</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Sci</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="92" to="110" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">When is &apos;nearest neighbour&apos; meaningful: a converse theorem and implications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Durrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kabán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Complex</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="397" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the &quot;dimensionality curse&quot; and the &quot;self-similarity blessing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-U</forename><surname>Pagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="111" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The properties of highdimensional data spaces: implications for exploring gene and protein expression data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Ressom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Gehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev Cancer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A survey on enhanced subspace clustering, Data Mining Knowledge Disc</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalkrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-012-0258-x</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Subspace clustering, ensemble clustering, alternative clustering, multiview clustering: What can we learn from each other?</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiClust: 1st International Workshop on Discovering, Summarizing and Using Multiple Clusterings Held in Conjunction with KDD 2010</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Quality of similarity rankings in time series</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bernecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Symposium on Spatial and Temporal Databases (SSTD)</title>
		<meeting>the 12th International Symposium on Spatial and Temporal Databases (SSTD)<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="422" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The curse of dimensionality in data mining and time series prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Franc ¸ois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Work-Conference on Artificial Neural Networks (IWANN)</title>
		<meeting>the 8th International Work-Conference on Artificial Neural Networks (IWANN)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="758" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Identification of Outliers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Nearest neighbors in high-dimensional data: the emergence and influence of hubs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2487" to="2531" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Timeseries classification in many intrinsic dimensions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 10th SIAM International Conference on Data Mining (SDM)<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="677" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The role of hubness in clustering highdimensional data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mladenić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<meeting>the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)<address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scale-free networks: a decade and beyond</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="issue">5939</biblScope>
			<biblScope unit="page" from="412" to="413" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A unified approach for mining outliers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference of the Centre for Advanced Studies on Collaborative research (CASCON)</title>
		<meeting>the conference of the Centre for Advanced Studies on Collaborative research (CASCON)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Distance-based outliers: algorithms and applications</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tucanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="237" to="253" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fast outlier detection in high dimensional spaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Angiulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge Discoverys (PKDD)</title>
		<meeting>the 6th European Conference on Principles of Data Mining and Knowledge Discoverys (PKDD)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient biased sampling for approximate clustering and outlier detection in large datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berchthold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1187" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mining distance-based outliers in near linear time with randomization and a simple pruning rule</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwabacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 9th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An efficient referencebased approach to outlier detection in large datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zaïane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 6th IEEE International Conference on Data Mining (ICDM)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Enhancing effectiveness of outlier detections for low density patterns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<meeting>the 6th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="535" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Mining top-n local outliers in large databases</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 7th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Ranking outliers using symmetric neighborhood relationship</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), Singapore</title>
		<meeting>the 10th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), Singapore</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mining outliers with faster cutoff update and space utilization</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Szeto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1292" to="1301" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Distance-based outlier detection: consolidation and renewed bearing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Orair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1469" to="1480" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">R-Trees: a dynamic index structure for spatial searching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guttman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The R*-Tree: an efficient and robust access method for points and rectangles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the ACM International Conference on Management of Data (SIGMOD)<address><addrLine>Atlantic City, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 25th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Nearest neighbors in high-dimensional spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Discrete and Computational Geometry</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Goodman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>O'rourke</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="877" to="892" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Extensions of Lipschitz mappings into a Hilbert space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference in Modern Analysis and Probability</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On variants of the Johnson-Lindenstrauss lemma</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matoušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Struct Algor</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="156" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Database-friendly random projections</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems</title>
		<meeting>the 20th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnson-Lindenstrauss with binary coins</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Syst Sci</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="671" to="687" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The Johnson-Lindenstrauss transform: an empirical study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX) SIAM</title>
		<meeting>the Workshop on Algorithm Engineering and Experiments (ALENEX) SIAM<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">On the distance concentration awareness of certain data reduction techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kabán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="277" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Sur une courbe, qui remplit toute une aire plane</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathe Ann</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="160" />
			<date type="published" when="1890">1890</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">A computer oriented geodetic data base and a new technique in file sequencing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Morton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>International Business Machines Co</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Ueber die stetige Abbildung einer Linie auf ein Flächenstück</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Ann</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="459" to="460" />
			<date type="published" when="1891">1891</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Hilbert R-tree: an improved R-tree using fractals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 20th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Santiago de Chile, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Efficient pruning schemes for distance-based outlier detection</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalkrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<meeting>the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="160" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Fast mining of distance-based outliers in high-dimensional datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Otey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Disc</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Locality sensitive outlier detection: A ranking driven approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 27th International Conference on Data Engineering (ICDE)<address><addrLine>Hannover, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Finding local anomalies in very high dimensional space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 10th IEEE International Conference on Data Mining (ICDM)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A near-linear time approximation algorithm for angle-based outlier detection in highdimensional data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 18th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">LOCI: Fast outlier detection using the local correlation integral</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kitagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 19th International Conference on Data Engineering (ICDE)<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="315" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Outlier mining in large highdimensional data sets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Angiulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Analysis of the clustering properties of the Hilbert spacefilling curve</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="141" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">High dimensional similarity search with space filling curves</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 17th International Conference on Data Engineering (ICDE)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="615" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Distance-preserving dimensionality reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev: Data Mining Knowledge Disc</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Robust estimates, residuals, and outlier detection with multiresponse data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gnanadesikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="124" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Outlier identification in high dimensions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Filzmoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maronna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Stat Data Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1694" to="1711" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Quality assessment of dimensionality reduction: rank-based criteria</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1431" to="1443" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Feature extraction for outlier detection in high-dimensional spaces</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalkrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Proc Track</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">HiCS: high contrast subspaces for density-based outlier ranking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Böhm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 28th International Conference on Data Engineering (ICDE)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Feature bagging for outlier detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 11th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Mining outliers with ensemble of heterogeneous detectors on random subspaces</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalkrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Database Systems for Advanced Applications (DASFAA)</title>
		<meeting>the 15th International Conference on Database Systems for Advanced Applications (DASFAA)<address><addrLine>Tsukuba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="368" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Converting output scores from outlier detection algorithms into probability estimates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 6th IEEE International Conference on Data Mining (ICDM)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Interpreting and unifying outlier scores</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 11th SIAM International Conference on Data Mining (SDM)<address><addrLine>Mesa, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">On evaluation of outlier rankings and outlier scores</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wojdanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 12th SIAM International Conference on Data Mining (SDM)<address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1047" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Finding intensional knowledge of distance-based outliers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 25th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Density-based clustering</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev: Data Mining and Knowledge Disc</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 20th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Santiago de Chile, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">On using class-labels in evaluation of clusterings</title>
		<author>
			<persName><forename type="first">I</forename><surname>Färber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiClust: 1st International Workshop on Discovering, Summarizing and Using Multiple Clusterings Held in Conjunction with KDD 2010</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Evaluation of multiple clustering solutions</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd MultiClust Workshop: Discovering, Summarizing and Using Multiple Clusterings Held in Conjunction with ECML PKDD 2011</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">An effective and efficient algorithm for high-dimensional outlier detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="221" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">HOSminer: a system for detecting outlying subspaces of highdimensional data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 30th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1265" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">An unbiased distance-based outlier detection approach for highdimensional data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalkrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Database Systems for Advanced Applications (DASFAA)</title>
		<meeting>the 16th International Conference on Database Systems for Advanced Applications (DASFAA)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="138" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Subspace outlier mining in large multimedia databases</title>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Universes and Local Patterns</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">DUSC: dimensionality unbiased subspace clustering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 7th IEEE International Conference on Data Mining (ICDM)<address><addrLine>Omaha, NE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="409" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">EDSC: efficient density-based subspace clustering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management (CIKM)<address><addrLine>Napa Valley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1093" to="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Outlier detection in axis-parallel subspaces of high dimensional data</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<meeting>the 13th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Adaptive outlierness for subspace outlier ranking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 19th ACM Conference on Information and Knowledge Management (CIKM)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1629" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Statistical selection of relevant subspace projections for outlier ranking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 27th International Conference on Data Engineering (ICDE)<address><addrLine>Hannover, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="434" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Efficient query processing in arbitrary subspaces using vector approximations</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<meeting>the 18th International Conference on Scientific and Statistical Database Management (SSDBM)<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="184" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Faster exact histogram intersection on large data collections using inverted VAfiles</title>
		<author>
			<persName><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Image and Video Retrieval (CIVR)</title>
		<meeting>the 3rd International Conference on Image and Video Retrieval (CIVR)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="455" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Similarity search in arbitrary subspaces under L p -norm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 24th International Conference on Data Engineering (ICDE)<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Subspace similarity search using the ideas of ranking and top-k retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bernecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Emrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Data Engineering (ICDE) Workshop on Ranking in Databases (DBRank)</title>
		<meeting>the 26th International Conference on Data Engineering (ICDE) Workshop on Ranking in Databases (DBRank)<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Subspace similarity search: Efficient k-nn queries in arbitrary subspaces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bernecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Emrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<meeting>the 22nd International Conference on Scientific and Statistical Database Management (SSDBM)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">BeyOND -unleashing BOND</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bernecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Very Large Data Bases (VLDB) Workshop on Ranking in Databases (DBRank)</title>
		<meeting>the 37th International Conference on Very Large Data Bases (VLDB) Workshop on Ranking in Databases (DBRank)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">LoOP: local outlier probabilities</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1649" to="1652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Non-parametric estimation of a multivariate probability density</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Epanechnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Prob Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Correlation clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ludwig-Maximilians-Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="54" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Deriving quantitative models for correlation clusters</title>
		<author>
			<persName><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 12th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">SOREX: Subspace outlier ranking exploration toolkit</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gerwert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<meeting>the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Evaluating clustering in subspace projections of high dimensional data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 35th International Conference on Very Large Data Bases (VLDB)<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1270" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">The WEKA data mining software: an update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Visual evaluation of outlier detection models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wojdanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Database Systems for Advanced Applications (DASFAA)</title>
		<meeting>the 15th International Conference on Database Systems for Advanced Applications (DASFAA)<address><addrLine>Tsukuba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="396" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Spatial outlier detection: data, algorithms, visualizations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hettab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Symposium on Spatial and Temporal Databases (SSTD)</title>
		<meeting>the 12th International Symposium on Spatial and Temporal Databases (SSTD)<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="512" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A new local distancebased outlier detection approach for scattered real-world data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<meeting>the 13th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A unified approach to detecting spatial outliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GeoInformatica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">SLOM: a new measure for local spatial outliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Inform Syst (KAIS)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="412" to="429" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">GLS-SOD: a generalized local statistical approach for spatial outlier detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Boedihardjo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 16th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1069" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Evaluation of clusterings -metrics and visual support</title>
		<author>
			<persName><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Data Engineering (ICDE)</title>
		<meeting>the 28th International Conference on Data Engineering (ICDE)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Redefining clustering for high-dimensional applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="225" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Subspace and projected clustering: experimental evaluation and analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Inform Syst (KAIS)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="326" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
