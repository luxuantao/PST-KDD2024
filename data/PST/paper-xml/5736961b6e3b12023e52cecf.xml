<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Automatic Learning-based Framework for Robust Nucleus Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Xing</forename><surname>Fuyong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">Y</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Crayton</forename><forename type="middle">Pruitt</forename><surname>Family</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">J. Crayton Pruitt Family Department of Biomedical Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Automatic Learning-based Framework for Robust Nucleus Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC18A4FA4FE923DB6BCF97CF546B546B</idno>
					<idno type="DOI">10.1109/TMI.2015.2481436</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2481436, IEEE Transactions on Medical Imaging This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2481436, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Nucleus segmentation</term>
					<term>deep convolutional neural network</term>
					<term>sparse representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for accurate and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, to which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>image analysis. However, it is prone to over-segmentation. To address this limitation, marker-based watershed has been proposed and achieved great success in nucleus/cell segmentation. Lin et al. <ref type="bibr" target="#b2">[3]</ref> have proposed a gradient weighteddistance transform method to locate nucleus centroids, which are viewed as markers for watershed segmentation of nuclei in 3D fluorescence microscopy images. Yang et al. <ref type="bibr" target="#b3">[4]</ref> have exploited conditional erosion to detect nucleus markers for watershed segmentation in fluorescent nuclei images, and another mathematical morphology based marker detection is reported in <ref type="bibr" target="#b4">[5]</ref>, which supports watershed transform in RNA interference (RNAi) images. In addition, Cheng and Rajapakes <ref type="bibr" target="#b5">[6]</ref> have employed an adaptive H-minima transform <ref type="bibr" target="#b6">[7]</ref> to extract nucleus markers for the subsequent watershed segmentation in Drosophila Kc167 cell images, Schmitt and Hasse <ref type="bibr" target="#b7">[8]</ref> have detected the markers by localizing the gravitation centers of mass clumped cells with kernel-based iterative radial voting, and Mao et al. <ref type="bibr" target="#b8">[9]</ref> have presented a two-step supervised learning approach to locate nuclei in bladder cancer images. The choice of marker detection algorithms for watershed segmentation highly depends on the specific applications, and it is difficult to apply a general rule. Meanwhile, the aforementioned detection methods are not robust to image noise or require careful parameter selection, and thus might need further processing to merge or split falsely segmented regions from watershed.</p><p>Another popular type of nucleus or cell segmentation methods is based on graphs or Voronoi diagrams. Al-Kofahi et al. <ref type="bibr" target="#b9">[10]</ref> have first detected nuclei seeds using Euclidean distance map-based Laplacian-of-Gaussian, and subsequently applied a graph cut algorithm with α-expansion <ref type="bibr" target="#b10">[11]</ref> and graph coloring for final nucleus segmentation in histopathology images. However, it might require user's interaction to merge or split false initial segmentation. Bernardis and Yu <ref type="bibr" target="#b11">[12]</ref> have proposed a graph partitioning algorithm, which defines two grouping cues composed of feature similarity-based short-range attraction and feature dissimilarity-based long-range repulsion, for cell segmentation in fluorescence microscopy images. However, this method does not well delineate cell boundaries. Yang et al. <ref type="bibr" target="#b12">[13]</ref> have applied a concave vertex graph to automatic image analysis of histopathology specimens. By minimizing a cost function based on cellular characteristics, the shotest path in the constructed graph is calculated to separate the touching cells. The dual graph for a Voronoi diagram, Delaunay triangulation, is used to segment clumps of nuclei in <ref type="bibr" target="#b13">[14]</ref>, where points of maximum curvature are used as vertices for Delaunay triangulation, and a constraint satisfaction network is applied to decomposition of the edge hypotheses. Another concave vertex-based touching cell splitting algorithm is presented in <ref type="bibr" target="#b14">[15]</ref>, which iteratively segments cells based on dominant concave region detection. These methods heavily rely on accurate concave point detection, which might be not easy to achieve in challenging histopathology images. Wu et al. <ref type="bibr" target="#b15">[16]</ref> have split interacting fibroblast cells by seeking the shortest path in an adjacent graph, but it might exhibit high time cost when dealing with large-scale images. Similarly, Jones et al. <ref type="bibr" target="#b16">[17]</ref> have proposed a shortest path searching algorithm within a specifically defined manifold for Drosophila cell segmentation, but it fails to handle weak cell boundaries in challenging cases.</p><p>Active contour or surface models are also commonly used for medical image segmentation. Fatakdawala et al. <ref type="bibr" target="#b17">[18]</ref> have proposed an expectation maximization (EM) driven geodesic active contour <ref type="bibr" target="#b18">[19]</ref> with an overlap resolution (EMaGACOR) scheme, which can automatically detect and segment lymphocytes on histopathology images. Similarly, Chang et al. <ref type="bibr" target="#b19">[20]</ref> have applied a level set model with color and scale constraints to nuclei segmentation, and Padfield et al. <ref type="bibr" target="#b20">[21]</ref> have added a size constraint to the geodesic model <ref type="bibr" target="#b18">[19]</ref> to extract G2 phase nuclei of eukaryotic cells. In <ref type="bibr" target="#b21">[22]</ref>, level set techniques are used to detect, segment, and track leukocytes, which are constrained with the shape, size, and internal intensity homogeneity of the objects. However, these methods do not explicitly handle multiple touching objects, and need further processing to split nucleus or cell clumps. Yan et al. <ref type="bibr" target="#b22">[23]</ref> have proposed a repulsive level set model to penalize cell overlapping by introducing an interactive scheme between cells, Qi et al. <ref type="bibr" target="#b23">[24]</ref> have adopted a seed detection-based repulsive level set model to segment overlapping breast cancer cells in tissue microarray (TMA) images, and Mosaliganti et al. <ref type="bibr" target="#b24">[25]</ref> have used a coupled level set model to segment cells in 3D fluorescent images, which respects underlying intensity and shape distributions. However, these implicit deformable models usually exhibit high computational cost for a large number of cells. Some improved level set implementation methods have been proposed for fast segmentation, including narrow band <ref type="bibr" target="#b25">[26]</ref>, graph-vertex coloring <ref type="bibr" target="#b26">[27]</ref>, convex optimization <ref type="bibr" target="#b27">[28]</ref>, and more advanced techniques <ref type="bibr" target="#b28">[29]</ref>. Despite significant efficiency improvement, usually implicit active contour models are slower than corresponding parametric models <ref type="bibr" target="#b29">[30]</ref>. In addition, although topology preserving level set <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> is proposed, generally implicit models that allow one initial contour to split or merge do not take advantage of known topological constraints. This is not preferred when the object positions are detected in advance. The parametric gradient vector flow (GVF) model <ref type="bibr" target="#b32">[33]</ref>, on the other hand, is combined with color gradient and l 2 e robust estimation in <ref type="bibr" target="#b33">[34]</ref> for unsupervised segmentation of lymphocyte cellular images. However, this method does not handle touching cells. Cai et al. <ref type="bibr" target="#b34">[35]</ref> have presented a repulsive GVF snake model to segment and track axons in fluorescence microscopy images, but it assumes the rough shapes and areas of objects are known a priori, and this assumption does not hold for automatic nucleus or cell segmentation in histopathology images. A more general coupled parametric active contour model is presented in <ref type="bibr" target="#b29">[30]</ref>, which introduces a region-based term into the traditional deformable model <ref type="bibr" target="#b35">[36]</ref> to prevent contour overlapping. However, it calculates the repulsion from all the other contours in the image, which might be computationally expensive for a large number of objects.</p><p>Histopathology images often exhibit significant variations in staining preparations, and thus simple color convolution usually under-or over-estimates the nuclei <ref type="bibr" target="#b36">[37]</ref>. Furthermore, nucleus touching or overlapping presents more challenges for color convolutions. In order to reduce the sensitivity to batch effects and tumor heterogeneity, color normalization is often used before nucleus or cell segmentation. Kothari et al. <ref type="bibr" target="#b37">[38]</ref> have exploited color maps to normalize the color space from sample images to reference images, and afterward separated the nuclei from the background with a linear discriminant analysis classifier. This color segmentation is quite effective to handle histopathology images but it is not designed to handle touching objects. Chang et al. <ref type="bibr" target="#b36">[37]</ref> have applied the same color normalization technique to testing images, and achieved nucleus regions within a multi-reference graph cut framework. Thereafter, concave points are detected to build Delaunay triangulation graphs, to which geometric reasoning techniques are introduced to partition overlapped nuclei. This method relies on robust concave point detection. For our dataset, it is challenging to achieve clean masks for accurate concavity detection.</p><p>Nucleus/cell segmentation methods that only rely on image appearances may be insufficient to handle cells with missing or misleading boundaries, which often occur on touching/overlapping cells. In order to tackle these limitations, high level shape priors are introduced to level set models <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> for better segmentation of partially occluded objects. Leventon et al. <ref type="bibr" target="#b40">[41]</ref> have computed the shape prior by applying principle component analysis (PCA) to the signed distance functions of the geodesic active contours, and similarly, another PCA-based shape prior is incorporated into an active contour model for medical image segmentation <ref type="bibr" target="#b41">[42]</ref>. Cremers et al. <ref type="bibr" target="#b42">[43]</ref> have used a related signed distance function <ref type="bibr" target="#b43">[44]</ref> to represent a shape and introduced a labeling function to constrain the active domain of the shape prior. Recently, Ali et al. <ref type="bibr" target="#b44">[45]</ref> have incorporated object shape priors into a level set formulation to handle occlusion, and it does not require point correspondence for shape representation, which are required for the active shape model <ref type="bibr" target="#b45">[46]</ref>. However, all the level set based implementations exhibit high computational cost, and most of them are defined for single object segmentation and need additional efforts for simultaneous segmentation of multiple touching objects. Cai et al. <ref type="bibr" target="#b46">[47]</ref> have applied shape constraints to the GVF model <ref type="bibr" target="#b32">[33]</ref> to segment neurons, and Park et al. <ref type="bibr" target="#b47">[48]</ref> have formulated the shape priors with a Gaussian mixture model and applied expectation maximum to shape inference for nanoparticle segmentation. However, both methods require clean edge maps to obtain desired segmentation, which are not easy to achieve in pathology images with background clutter. Sparse representation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> has recently been successfully applied to shape prior modeling for lung, liver, and cerebellum segmentation. In comparison with the PCA-based active shape model, sparse representation-based shape modeling is robust to false appearance cues and preserve local details, even though they are not statistically important <ref type="bibr" target="#b48">[49]</ref>. These approaches are designed to segment single organ instead of multiple objects, and are not able to be directly applied to nucleus segmentation.</p><p>In this paper, we propose a robust learning-based framework (see Figure <ref type="figure" target="#fig_1">2</ref>) for accurate nucleus segmentation by combing both bottom-up and top-down information. In the training stage, we learn a deep convolutional neural network (CNN) model and a compact nucleus shape repository with a novel selection-based dictionary learning algorithm. In the testing stage, the CNN model is applied to the images to generate probability maps followed by iterative region merging for shape initialization, one per nucleus. Thereafter, the proposed nucleus segmentation algorithm performs shape deformation using a local repulsive deformable model and shape inference Fig. <ref type="figure">3</ref>. Left: The flowchart of the shape initialization. Given an input image (a-1), small patches are cropped with a sliding window technique and fed into the learned CNN model (a-2) to generate a probability map (a-3), to which an iterative region merging algorithm (a-4) is used for nucleus shape marker generation (a-5,a-6). Right: The procedure of iterative region merging. Based on the noise-eliminated probability map (b-1), the inverse of the distance map (b-2) is calculated and used to generate initial markers (b-3) with Hminima transform. These initial markers iteratively expand based on distance values until markers merge into each other; during the merging procedure, the markers who will merge in next iteration are recorded and smoothed as the final shape markers (b-6). For those non-touching nuclei, the predicted nucleus regions are smoothed with a morphology operation and viewed as final shape markers. The image (b-4) in the bottom right is an intermediate result of marker growing. The image (b-5) is the result of the last iterative region merging, which merge two distinct markers. Therefore, the markers before this iteration are recorded as initial shapes.</p><p>using the shape prior derived from the sparse shape model.</p><p>A preliminary version of our work <ref type="bibr" target="#b50">[51]</ref> is presented in the International Conference of Medical Image Computing and Computer Assisted Intervention-MICCAI 2013. Compared with the short conference version, each step of the proposed approach is explained in detail in this paper. Specifically, a deep convolutional neural network that is robust to inhomogeneous intensity and image noise is newly introduced to initialize shapes for nucleus segmentation. In addition, a novel repulsive deformable model with locality constraint is applied to touching nucleus segmentation with the combination of a selection-based nucleus shape prior model, and it is more robust to image noises than the balloon snake model in <ref type="bibr" target="#b50">[51]</ref>. Finally, the proposed framework is extensively tested on three types of different datasets, and the experiments are redesigned to compare with six state-of-the-arts. An extensive evaluation of CNN-based probability map generation (shape initialization) and parameter selection for the proposed algorithm are investigated and provided in this paper. In summary, the contributions of this paper are: 1) A deep CNN-based shape initialization is introduced to generate initial shapes, which learns hierarchical feature representation from raw images (YUV color space) and is relatively insensitive to image noise and inhomogeneous intensity; 2) An integrated framework consisting of selection-based sparse shape prior and efficient repulsive deformable model is proposed for contour evolution, which can effectively separate touching or overlapping nuclei;</p><p>3) The proposed framework is extensively tested on multiple types of clinical datasets using a range of different tissues and stain preparations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SHAPE INITIALIZATION</head><p>In digitized histopathology images, it is not infrequent that tumor nuclei form dense clumps and overlap with one another. In order to achieve nucleus segmentation with shape preservation that facilitates the subsequent morphological feature  <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Probability Map Generation</head><p>In our algorithm, the probability map is calculated from the classification results. Given an input image, each pixel is assigned with a probability indicating how likely it belongs to part of a nucleus. This problem is solved by training a classifier based on a deep CNN using supervised learning. Recently, CNN has been proven to be effective for object detection, segmentation, and classification in nature and medical image analysis <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. In particular, Ciresan et al. <ref type="bibr" target="#b56">[57]</ref> have used a deep neural network algorithm to detect mitosis in breast cancer histology images, which can correctly separate mitosis from non-mitosis. Instead of relying on handcrafted features which need to be specifically designed, CNN can automatically learn multi-level hierarchies of features which are invariant to irrelevant variations of samples while preserving relevant information <ref type="bibr" target="#b57">[58]</ref>. This learned feature representation exhibits both strong expressive and discriminative powers that can enhance the classification performance.</p><p>1) Convolutional Neural Network: A CNN <ref type="bibr" target="#b58">[59]</ref> usually consists of successive pairs of convolutional and max-pooling layers, followed by several fully-connected layers. The deep architecture extracts hierarchical features from the input images, and the last layer, which is usually chosen as a sigmoid or softmax activation function, outputs probability distributions over classes based on the learned features <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. The CNN structure used for brain tumor in our algorithm is listed in Table <ref type="table">I</ref>, and the architectures for NET and breast cancer datasets are similar except that the input patch sizes are 39×39 (64 output feature maps) and 45 × 45, respectively.</p><p>In our deep architecture, multiple layers are stacked such that the output of the (l -1)-th layer will serve as the input of the l-th layer. A convolutional layer learns a set of convolutional filters which will be used to calculate output feature maps, with all units in a feature map sharing the same weights. Suppose the l-th layer is a convolutional layer, a distinct set of feature maps will be selected from the output of the (l -1)-th layer as the input to the l-th layer to compute the j-th feature map P l j . This procedure can be written as</p><formula xml:id="formula_0">P l j = f ( i P l-1 i * O l ij + β l j ),<label>(1)</label></formula><p>where i ∈ [0, N l-1 in -1], and j ∈ [0, N l out -1] denote the indices of the selected input feature maps and output feature maps respectively, with N l-1 in and N l out representing the number of the input and output feature map of the l-th layer. The β l j and O l ij represent learnable biases and convolutional kernels corresponding to the i-th input feature map and the j-th output feature map. The function f , defined as f (x) = max(0, x), represents the rectified linear units (ReLU) <ref type="bibr" target="#b61">[62]</ref>, which can significantly improve the time cost at the training stage <ref type="bibr" target="#b59">[60]</ref>.</p><p>The max pooling layer summarizes the activities and picks up the max values over a neighborhood region in each feature map <ref type="bibr" target="#b62">[63]</ref>, which not only reduces feature dimensionality but also introduces local shift and translation invariance to the neural network. This operation is usually performed separately in each feature map. The convolutional-pooling layers are stacked to learn local hierarchical features, based on which the fully-connected layers learn more higher level feature representation for classification. Different from the convolutional layers sharing weights, each neuron in fully-connected layers makes connections to all the neurons in the previous layer. The last layer is a softmax layer (fully-connected), which produces probability distributions over classes <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and the outputs of the softmax function can be interpreted as category probabilities <ref type="bibr" target="#b63">[64]</ref>. In our application, it is a binary classification problem such that the last layer has only two neurons.</p><p>2) CNN Training and Testing: Our CNN is trained using raw pixel values (the YUV color space) of small image patches with certain size, centered on the pixel itself. Therefore, each pixel corresponds to a vectorized feature. We manually annotate the geometric centers of nuclei on the training images. The patches whose centers are closer than several pixels (chosen as 4) to the corresponding annotated nucleus centers are labeled as positive, otherwise negative. In order to achieve rotation invariance, all positive patches are rotated by 90 • ,180 • , and 270 • . This can also produce more positive training samples, which are significant when there exist less real nucleus patches than non-nucleus patches in the training images. Figure <ref type="figure">4</ref> shows the learned filters and the feature maps in the first layer of the CNN model on the NET dataset.</p><p>After training, we apply the learned CNN model to small patches cropped from a new testing image using a pixel-wise sliding window with the same size as training patches. Those patches partly lying outside the image boundary are handled with zero padding. For each testing image, the CNN model with the sliding-window technique creates a probability map where each pixel is assigned a probability being close to nucleus centers, and those lying on the background would be with lower probabilities. This feed-forward procedure of probability map generation is carried out using the CUDA parallel  computing platform on graphic processing units (GPUs), and the computational time can be improved at runtime. Figure <ref type="figure">5</ref> shows the comparative foreground segmentation between a simple color deconvolution method <ref type="bibr" target="#b64">[65]</ref> and the used CNN model. As we can see, the CNN model is more robust to intensity inhomogeneity and background clutter, and it can achieve much better performance than the color deconvolution method on the challenging datasets such as breast cancer.</p><p>In our CNN model, the last layer of the CNN model is a softmax layer with two neurons, which output two probabilities (sum to 1) that the center pixel of the input image patch is positive and negative, respectively. The larger the probability value is, the more likely the input patch belongs to the corresponding category. A positive probability value larger than 0.5 indicates that the patch is less likely to be negative (the corresponding negative probability is smaller than 0.5). Therefore, we apply a threshold q 1 = 0.5 to the positive probability map, which denotes how likely each pixel to be positive, to remove those pixels with probabilities lower than threshold q 1 before further analysis. In addition, there may be some small noise-caused regions. We use the region size as a criterion and eliminate those regions with size less than a threshold (q 2 ) before performing the subsequent shape initialization. We select the value of q 2 by considering the original scale in terms of stain preparation. The NET image data is captured at 20× magnification with about 344 nm/pixel, and the other two are generated at 40× magnification with around 250 nm/pixel. Based on observations, we select q 2 as 30, 70 and 70 pixels for NET, brain tumor, and breast cancer, respectively, and they are proper to eliminate image noises, which have much smaller areas than real nuclei. Therefore, the regions whose area are smaller than q 2 are viewed as noises and eliminated without further consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shape Initialization</head><p>For non-touching or non-overlapping nuclei, the local maxima on the obtained probability map can be viewed as markers for the subsequent nucleus segmentation. However, it is challenging to detect correct extrema for overlapping nuclei. In <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, H-minima transform <ref type="bibr" target="#b6">[7]</ref> is exploited to suppress undesired minima and calculate initial markers for the subsequent segmentation. Inspired by this idea, we propose an iterative region merging algorithm to calculate the initial shapes to the best extent possible. Based on the probability map, we first calculate the distance map <ref type="bibr" target="#b65">[66]</ref>, where each pixel corresponds to a value measuring the shortest distance from the background. Next, H-minima transform is applied to the inverse of the distance map to achieve desired minima. For each connected region on the probability map, we iteratively extend the markers based on the distance values until all the markers merge; during the merging procedure, we record the markers who will merge in next iteration. Finally, we exploit a simple morphology operation to smooth the markers, which can effectively preserve the nucleus shapes and are used as initial shapes for the subsequent nucleus segmentation. For better illustration, we visualize the procedure of iterative region merging in the right panel of Figure <ref type="figure">3</ref>. For the nontouching or non-overlapping nuclei, the predicted nucleus regions are smoothed to form the final shape markers. In this scenario, it can reduce the running time due to no iterative region growing for these nuclei. </p><formula xml:id="formula_1">N i (dt 0 ) 2. for connected region c i ∈ C 3. dt = dt 0 4. if N i (dt) &gt; 1 5. while N i (dt) &gt; 1 6. dt = dt + 1 7. Do M G(c i , dt) and get N i (dt) 8.</formula><p>for connected region c ij ∈ c i 9.</p><p>Record corresponding markers as initial shapes 10.</p><p>which will merge into c ij in next iteration 11.</p><p>end for 12.</p><p>end while 13. else 14.</p><p>Record corresponding markers as initial shapes 15.</p><p>end if 16.</p><p>Marker smoothing 17. end for Compared with the method presented in <ref type="bibr" target="#b5">[6]</ref> that stops all the markers' growing for each connected region when the first merging occurs, our algorithm only prevents those will merge in next iteration from growing but keeps others expanding. The markers grow based on the distance values on the complement image, where the pixels with the same values actually form a level curve. In this way, the final markers will exhibit the largest areas before merging, and thus they can preserve nucleus shapes to the greatest extent without unnecessary early termination of growing. This is especially important for the touching/overlapping nuclei exhibiting large size variations. Let C = {c i } denote the set of connected regions, HT (I, dt) mean the H-minima transform on the inverse of the distance map I with threshold dt, M G(c i , dt) represent the marker growing operation on c i with step size dt, N i (dt) be the number of minima in c i , and c ij be the j-th region component in c i , the details of the iterative merging algorithm are listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NUCLEUS SEGMENTATION</head><p>Given the initial shapes (contours) calculated from Section II, we propose to combine bottom-up and top-down information together to achieve nucleus delineation considering the fact that nucleus boundaries are often weak or even missing. In addition, the proposed algorithm can handle misleading cues due to inhomogeneous intensity or background clutter in the digitized specimens. Sparse shape model has shown to be more effective than PCA-based shape prior due to its insensitiveness to object occlusion <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. However, using all training shapes is inefficient during sparse reconstruction on a large dataset at run-time. KSVD <ref type="bibr" target="#b66">[67]</ref> is a popular dictionary learning algorithm, but it is not designed as a discriminative and selection-based dictionary learning method with respect to classification and segmentation. In this work, we propose a novel and robust selection-based dictionary learning algorithm for nucleus shape modeling. Different from KSVD, this method directly selects the most representative nucleus shapes from the training dataset as dictionary bases. The robustness of the dictionary learning method is achieved by minimizing an integrated square error with a sparse constraint. In order to simultaneously and efficiently segment multiple nuclei, we combine a top-down shape prior model and a bottom-up deformable model with locality and repulsion constraints. The proposed algorithm alternately performs shape deformation using the efficient local repulsive deformable model, and shape inference using the shape prior derived from the sparse shape model. The flowchart of nucleus segmentation is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selection-based Sparse Shape Model</head><p>In this paper, nucleus shape v ∈ R 2m is represented by the concatenated 2D coordinates of m landmarks which are automatically detected by the rules: 1) The two endpoints of the major axis of the shape are selected as major landmarks.</p><p>2) All the other landmarks are evenly interpolated along the shape. Given N nucleus shapes {v i } N i=1 aligned by Procrustes analysis, sparsity-based shape modeling aims to find a compact shape dictionary</p><formula xml:id="formula_2">B = [b 1 b 2 , ...b K ] ({b k ∈ R 2m } K</formula><p>k=1 are bases) and a sparse coefficient α such that any aligned shape v can be represented with a few bases: v = Bα + , where is the residual. The dictionary learning can be formulated as</p><formula xml:id="formula_3">min B,{αi} N i=1 ||v i -Bα i || 2 + λ||α i || 1 ,<label>(2)</label></formula><p>For a large nucleus shape dataset, it is intuitive to select a subset of the data as a shape repository that can sufficiently represent the whole dataset. This summarization can help remove outliers that are not the true representatives of the dataset and might reduce the computational time for runtime optimization due to the decreased object-space dimension. Based on these considerations, we propose a novel selectionbased dictionary learning method for sparse representation by minimizing a locality-constrained integrated squared error (ISE). Scott <ref type="bibr" target="#b67">[68]</ref> has theoretically shown that minimizing the ISE is equal to minimizing the objective function:</p><formula xml:id="formula_4">g(x|θ) 2 dx -2 N N i=1 g(x i |θ)</formula><p>, where g(x|θ) is a parametric model with parameter θ and N is the number of data points {x i } N i=1 . In the spare shape model, we have v i = Bα i + i , where i is the residual for the i-th shape. Therefore, we can model the residual density with function g( |θ) and minimize the objective function as follows</p><formula xml:id="formula_5">min θ J(θ) = min θ [( g( |θ) 2 d - 2 N N i=1 g( i |θ)) +λ N i=1 K k=1 |α ik |||v i -b k || 2 ], s. t. 1 T α i = 1, ∀i,<label>(3)</label></formula><p>where i = v i -Bα i and α i = [α i1 α i2 ... α iK ] T . The first two terms form the L 2 E criteria, which is robust to outliers <ref type="bibr" target="#b67">[68]</ref>. The last term constrains local representation of bases with weighted sparse codes, and is used to encourage each nucleus to be sufficiently represented by its neighboring dictionary bases for similarity preserving, which is essential in the sparse reconstruction. The constraint 1 T α i = 1, ∀i, ensures the shift-invariance. The residual is modeled with multivariate normal distribution: i ∼ N (0, σ 2 I 2m ). In this way g( i |θ) = ξφ( i |0, σ 2 I 2m ), where ξ denotes the percentage of the inlier shapes that need to be estimated and φ is the probability density function of multivariate normal distribution. Based on (3), the dictionary B and sparse coefficients {α i } N i=1 can be calculated by estimating θ = {ξ, B, α 1 , α 2 , .., α N , σ 2 }.</p><p>Equation ( <ref type="formula" target="#formula_5">3</ref>) can be solved by performing dictionary basis selection and coefficient computation alternately. As J(θ) in ( <ref type="formula" target="#formula_5">3</ref>) is differentiable with respect to {b k } K k=1 , projection based-gradient descent is utilized for minimization to update the bases, which are directly-selected shapes within each iteration. For coefficient calculation, we keep the dictionary fixed. Based on the sparse reconstruction criterion, the sparse coding objective function can be rewritten as:</p><formula xml:id="formula_6">min {αi} i∈A [ i∈A ||v i -Bα i || 2 + λ K k=1 |α ik |||v i -b k || 2 ], s. t. 1 T α i = 1, i ∈ A, (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where A is the set of indices corresponding to estimated inlier shapes. Locality-constrained linear coding (LLC) <ref type="bibr" target="#b68">[69]</ref> is applied to (4) for coefficient computation, where the neighboring bases are defined in terms of the Euclidean distances between the shape and dictionary bases. Let B 0 be the initial dictionary where the bases are randomly selected from the dataset, LLC is used in (4) to compute the current coefficients. Meanwhile, the active set A is updated with the indices corresponding to the N • ξ shapes with the smallest reconstruction errors || i || 2  2 in each iteration. At the t-th step, θ t = {ξ t , B t , α t 1 , α t 2 , .., α t N , σ t 2 }, and</p><formula xml:id="formula_8">B t = [b t 1 , b t 2 , ..., b t K ],</formula><p>denote the gradient of J(θ) in (3):</p><formula xml:id="formula_9">∂J(θ)</formula><p>∂b k , ∂J(θ) ∂σ 2 , and ∂J(θ) ∂ξ , as J b k , J σ 2 and J ξ , respectively. The basis b k is updated by selecting the shape v l which has the largest correlation between the displacement and the current b t k :</p><formula xml:id="formula_10">COR(v l , b t k , J b k ) = (v l -b t k ) T (-J b k ) ||(v l -b t k )|| 2 || -J b k || 2 ,<label>(5)</label></formula><p>Let A t represent a set of indices corresponding to the current estimated inliers, the current reconstruction error E t and the ISE error F t are defined as</p><formula xml:id="formula_11">E t = N i=1,i∈A t || t i || 2 2 N i=1 z(i∈A t ) ,<label>(6)</label></formula><formula xml:id="formula_12">F t = ξ t 2 φ(0|0, 2σ t 2 I 2m ) -2ξ t N N i=1 φ( t i |0, σ t 2 I 2m ),<label>(7</label></formula><p>) where z(x) is the indicator function. Assume that E t min is the current reconstruction error, E t rep is the reconstruction error after replacing the k-th basis with v l , F t min is the current ISE error, and F t rep is the ISE error after replacing the kth basis with v l , then the replacement will be performed only if E t min &gt; E t rep and F t min &gt; F t rep . The σ 2 and ξ are updated in the negative gradient directions:</p><formula xml:id="formula_13">(σ t+1 ) 2 = σ t 2 -∆h σ 2 • J σ 2 , ξ t+1 = ξ t -∆h ξ • J ξ ,<label>(8)</label></formula><p>where ∆h σ 2 and ∆h ξ represent the learning rates. The details are listed in Algorithm 2.</p><p>Algorithm 2: Selection-based dictionary learning using ISE Input: Pre-aligned nucleus shapes {v 1 , v 2 , ..., v N }.</p><p>Output:</p><formula xml:id="formula_14">Dictionary Φ = [φ 1 , φ 2 , ...φ K ]. 0. Initialize Φ.</formula><p>1. for t = 1 : T (T is maximum value of iteration) 2. α i ← solution of (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>for k = 1 : K 4.</p><p>Compute COR(v l , Φ t k , J φ k ) using ( <ref type="formula" target="#formula_10">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Compute Due to large shape variations of objects, it might be not suitable to learn a single shape prior model for all the shape instances. Inspired by <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, we build multiple subpopulation shape prior models based on clustered shapes. Specifically, we group the aligned training shapes into several clusters with k-means, and learn one shape prior model for each group. For runtime optimization, we align the testing shape to each mean shape and select the one with smallest alignment error to be the active model, and perform shape inference using (4) with the learned dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shape Deformation</head><p>In this step, we propose an efficient shape deformation method based on the Chan-Vese model for nucleus segmentation. The Chan-Vese model <ref type="bibr" target="#b71">[72]</ref> is formulated based on the well-known Mumford-Shah functional <ref type="bibr" target="#b72">[73]</ref>, and consists of two region-based data fitting terms, one for foreground and the other for background, and several regularization terms. Denote by I the image and v the contour, the energy function J(v) is</p><formula xml:id="formula_15">J(v) = Ω (I(x) -u) 2 dx + Ω0 (I(x) -u 0 ) 2 dx +γ|v| + η|Ω|,<label>(9)</label></formula><p>where Ω and Ω 0 denote the regions inside and outside the contour v, u and u 0 represent the average intensity of Ω and Ω 0 , respectively, and the last two terms are the length of v with weight γ and the area of Ω with parameter η, respectively. Due to intensity inhomogeneity and noise inside nuclei, it might be not sufficient to only apply the original Chan-Vese model in <ref type="bibr" target="#b8">(9)</ref> to nucleus segmentation on digitized specimens. In order to enhance the robustness, we add an edge detector into (9) combining with the region-based data fitting term to better move contours towards nucleus boundaries. In addition, the model in ( <ref type="formula" target="#formula_15">9</ref>) is not designed to separate touching or overlapping objects. Given multiple initial contours (shapes), the active contours may cross one another during their evolvements. To address this limitation, a repulsive term is introduced into (9) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Instead of moving each contour independently, the introduced interaction is modeled concurrently among all contours during their evolutions. Consider an image I containing N nuclei, denoted by v i (i = 1, ..., N ) with v i representing the i-th contour. The energy function J for nucleus segmentation combining the driving and repulsive mechanisms thus can be expressed as follows:</p><formula xml:id="formula_16">J = λ 1 N i=1 Ωi (I(x) -u i ) 2 dx + λ 2 Ω0 (I(x) -u 0 ) 2 dx + λ 3 N i=1 1 0 e(v i (s))ds + ω N i=1 N j=1,j =i Ωi∩Ωj 1 dx + N i=1 γ|v i |,<label>(10)</label></formula><p>where Ω i and Ω 0 represent the region inside v i and outside all the contours (background), the third term with e(v i (s)) is the edge detector and chosen as</p><formula xml:id="formula_17">-||∇I(v i (s))|| 2 (s ∈ [0, 1]</formula><p>is the parameter for contour representation), and the fourth term denotes the repulsion preventing contours from crossing with each other. We remove the area term because its weight η is usually chosen as 0 in object segmentation (also in our implementation).</p><p>For each nucleus, the model in <ref type="bibr" target="#b9">(10)</ref> requires calculation of repulsion from all the other nuclei on the image, and this leads to high time cost when N is large. However, practically each nucleus is often surrounded by a limited number (M &lt;&lt; N ) of adjacent nuclei, and only its neighboring nuclei make dominant repulsive contributions to its shape deformation during contour evolution. This suggests that we can deform shape v i in its local coordinate system for computational efficiency, and it can be implemented by simply using v i 's M nearest neighbors V i . We thus replace the repulsion term in <ref type="bibr" target="#b9">(10)</ref> with REP :</p><formula xml:id="formula_18">REP = ω N i=1 j∈Vi Ωi∩Ωj 1 dx,<label>(11)</label></formula><p>The repulsion in <ref type="bibr" target="#b10">(11)</ref> reduces the computation complexity significantly from O(N 2 ) to O(N • M ) due to M &lt;&lt; N . In our segmentation framework, since the V i can be pre-determined using the proposed deep CNN-based iterative region merging algorithm, minimizing the energy function with ( <ref type="formula" target="#formula_18">11</ref>) is much faster compared with using the repulsion from all the other nuclei on the image. Substituting ( <ref type="formula" target="#formula_18">11</ref>) into (10) and using the Euler-Lagrange equations associated to the minimization of (10), we can get the following evolution equation</p><formula xml:id="formula_19">∂v i ∂t = | ∂v i ∂s |n i (-λ 1 (I -u i ) 2 + λ 2 (I -u 0 ) 2 -λ 3 ∇e(v i ) -ω j∈Vi z j (v i ) + γρ(v i )), (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where n i is the normal unit vector of v i , and z j (x) represents the indicator function: z j (x) = 1 if x ∈ Ω j , otherwise 0. ρ(•) denotes the curvature. Given the initial shapes, we can iteratively evolve the contours toward desired nucleus boundaries. Following <ref type="bibr" target="#b29">[30]</ref>, we solve the deformable model with parametric active contours, and this facilitates the sparse shape modeling in the proposed framework. This is different from the models presented in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b44">[45]</ref>, where the level set formulation that represents contours implicitly is used for energy minimization. The proposed model can take advantage of known object topology such that evolving contours are not allowed to split or merge, while generally level set models are designed to handle topology changes and thus it might generate undesired contours due to intensity heterogeneity. Compared with the shape prior based level set in <ref type="bibr" target="#b39">[40]</ref>, which only use a single shape template, we exploit a shape dictionary that allows much stronger representation power to constrain the contour deformation.</p><p>Given initial contours, the proposed segmentation framework alternately performs shape deformation with the repulsive active contour model and shape inference with sparse shape prior. The shapes always expand from inside nuclei, one per nucleus, and evolve towards nucleus boundaries. In the ac-tive contour model, contours move based on image appearance information until equation ( <ref type="formula" target="#formula_19">12</ref>) reaches a stable state, where the associated energy function achieves a minimum value; in the shape inference stage, contours evolve based on high level shape prior to constrain the shapes. This alternative operation scheme of combing bottom-up and top-down information has been successfully applied to biomedical image segmentation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The proposed framework has been extensively tested on three types of histopathology images: brain tumor, NET, and breast cancer. In each dataset, each image corresponds to The initial depth parameter for H-minima transform is set as dt 0 = 1. The λ in (3,4) is a regularization parameter, controlling the trade-off between the error and the locality penalty, and it is set as 5 × 10 -3 . For the repulsive deformable model in <ref type="bibr" target="#b11">(12)</ref>, λ 1 and λ 2 correspond to the first two data fitting terms, λ 3 and ω weight the contributions of the edge detector and the repulsive term, respectively, and γ controls the intrinsic properties of the contours. The parameter ω and the neighborhood size M are important in nucleus segmentation, and we have experimentally shown their effects on the segmentation performance in Section IV-C. For the other parameters, we set λ 1 = λ 2 = 1, λ 3 = 0.2, and γ = 1 by following the suggestions in <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The number of shape clusters depends on the shape variations of the nuclei, and in our experiments we found that three clusters are sufficient to model nucleus shape prior on each dataset. Finally, each nucleus contour is represented by 60 control points. The parameters are fixed for all three datasets, which demonstrates the generality of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nucleus Detection with Shape Initialization</head><p>Shape initialization is actually nucleus detection in our segmentation framework, which provides nucleus positions for the subsequent active contour model. We conduct both qualitative and quantitative analysis to evaluate the nucleus detection. The gold standard of each nucleus center is calculated as the mean of the annotated contour, and the detected nucleus centers are the local maxima on the probability map. Figure <ref type="figure" target="#fig_5">6</ref> shows the shape initialization using the deep CNN model and the iterative region merging algorithm on one sample slide of the breast cancer dataset. We can see that many nuclei are correctly detected. Figure <ref type="figure" target="#fig_6">7</ref> shows the comparative detection results between the proposed CNN model and scalable SVM <ref type="bibr" target="#b73">[74]</ref>, random forest (RF) <ref type="bibr" target="#b74">[75]</ref>, and deep belief network (DBN) <ref type="bibr" target="#b75">[76]</ref> on three datasets. As one can tell, the proposed method produces much better performance than the others, especially on the more challenging breast cancer dataset.</p><p>We apply the metric in <ref type="bibr" target="#b79">[80]</ref> for quantitative analysis, which defines the gold standard regions as circular regions within 15 pixels for every nucleus center. A detected nucleus centroid is considered to be a true positive (TP) only if it lies within the gold standard region; otherwise, it is considered as a false positive (FP). Each TP is matched with the nearest gold standard nucleus center. The gold standard nucleus centers that are not matched by any detected results are considered to be false negatives (FN). Based on these definitions, we can compute the precision (P), recall(R), and F 1 score as Fig. <ref type="figure">9</ref>. The procedure of shape evolution on one sample breast cancer image. From top to bottom and from left to right: original image, initial shapes, intermediate shape evolution before using shape prior model for inference, intermediate shape evolution after using shape prior model for inference, shape deformation and inference in last iteration. Note that all the nuclei in the connected regions touching image boundaries are ignored. </p><formula xml:id="formula_21">P = T P T P + F P , R = T P T P + F N , F 1 = 2P R P + R .<label>(13)</label></formula><p>Table II lists the evaluation of nucleus detection for different methods. It is clear that the proposed CNN model produces the best results, and the lowest standard deviations in almost all metrics demonstrates the strong reliability of our method. Specifically, SVM might need sophisticated feature design to provide desired results. RF and DBN produce fair performance on the NET images but much lower detection accuracy on the other two, which are more challenging. The proposed approach outperforms the traditional classification methods (SVM and RF) on all the datasets due to the learned feature representations, which are robust for object classification. The reason why the DBN model provides low accuracy might be that the current DBN network is not very deep; a deeper DBN might be able to provide better performance, but this will require much higher training and testing time cost.</p><p>(a) Original images (b) MS <ref type="bibr" target="#b76">[77]</ref> (c) ISO <ref type="bibr" target="#b77">[78]</ref> (d) SUP <ref type="bibr" target="#b78">[79]</ref> (e) GCC <ref type="bibr" target="#b9">[10]</ref> (f) MWS (g) RLS <ref type="bibr" target="#b23">[24]</ref> (h) Proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nucleus Segmentation</head><p>The segmentation results using our method on six sample slide digitized images are shown in Figure <ref type="figure" target="#fig_7">8</ref>. For better illustration, the procedure of shape evolution is shown in Figure <ref type="figure">9</ref>. Many nuclei exhibit weak boundaries on the NET dataset, and the brain tumor and breast cancer images have heavy background clutter and dense nucleus clustering. The proposed algorithm can accurately detect and segment thousands of nuclei with shape preservation. Figure <ref type="figure" target="#fig_8">10</ref> gives the comparative segmentation results on six representative image patches including mean shift (MS) <ref type="bibr" target="#b76">[77]</ref>, isoperimetric graph partition (ISO) <ref type="bibr" target="#b77">[78]</ref>, superpixel (SUP) <ref type="bibr" target="#b78">[79]</ref>, markerbased watershed (MWS), graph-cut and coloring (GCC) <ref type="bibr" target="#b9">[10]</ref>, repulsive level set (RLS) <ref type="bibr" target="#b23">[24]</ref>, and the proposed framework. It is clear that MS, ISO, and SUP are general segmentation algorithms that require further processing for touching object segmentation. The contours created by MWS does not well describe nucleus boundaries. GCC is unable to handle weak boundaries on nucleus overlapping regions, and therefore it is prone to under-segmentation. RLS provides more accurate nucleus boundaries than MWS and GCC; however, it does not preserve topology and therefore may generate some small holes (false splitting contours) inside/outside individual nuclei due to intensity inhomogeneity. In addition, it does not take advantage of the nucleus shape priors either. On the contrary, ours can effectively handle nucleus overlapping because of the constraints of object topology preserving and learned shape prior, and therefore provides more accurate segmentation results (last column of Figure <ref type="figure" target="#fig_8">10</ref>).</p><p>In order to quantitatively analyze the pixel-wise segmentation accuracy, we apply multiple metrics including Dice similarity coefficient (DSC), Hausdorff distance (HD), and mean absolute distance (M AD), to the evaluation of the algorithm. Let Ω sr and Ω gt represent the regions inside the automatic segmentation contour v sr and the gold standard contour v gt , respectively, the metrics are defined:  <ref type="bibr" target="#b76">[77]</ref>, ISO <ref type="bibr" target="#b77">[78]</ref>, (SUP) <ref type="bibr" target="#b78">[79]</ref>, MWS, GCC <ref type="bibr" target="#b9">[10]</ref>, RLS <ref type="bibr" target="#b23">[24]</ref>, and the proposed. The 80% column represents the sorted 80% highest accuracy among all the results. It is clear that the proposed method provides the best performance, especially in terms of HD that calculates the largest error for each segmentation. This is attributed to the fact that compared with the other methods without shape constraints, our approach can correct the corrupted nucleus boundaries. Meanwhile, the proposed method produces the smallest standard deviation of the metrics, which indicates its stronger reliableness. We also perform a two-sample t-test <ref type="bibr" target="#b80">[81]</ref> between the proposed approach and each of the comparative methods. Except that our method is statistically equal to RLS <ref type="bibr" target="#b23">[24]</ref> in terms of MAD on the breast cancer dataset (p-value is approximately 0.76), it is significantly better than all the comparative methods in all the other cases (p-value is smaller than 0.05). The average running time for the proposed method is approximately 54 seconds on an image of about 220 × 230 pixels, where shape initialization accounts for 49 seconds, and 5 seconds for shape deformation and inference. We have applied a fast scanning technique <ref type="bibr" target="#b81">[82]</ref> to the CNN prediction in the testing stage, and significantly improved the time cost, with about 6 seconds for pixel-wise classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Analysis 1) Repulsion Weight:</head><p>The parameter ω in (12) controls the penalty of the repulsion between contours of touching or overlapping nuclei. Figure <ref type="figure" target="#fig_9">11</ref> shows the DSC, HD, and MAD segmentation accuracy in the brain tumor, NET, and breast cancer datsets. When ω = 0, the algorithm reduces to a traditional parametric region-based model, and each contour evolves independently such that adjacent contours might cross each other. In this way, touching or overlapping nuclei would not be correctly separated such that the segmentation accuracy will be very low. With the increase of ω, the accuracy grows gradually. However, when ω is sufficiently large, the repulsion receives much larger penalty such that the non-overlapping rule would be strictly obeyed and the segmentation for overlapping nuclei might not preserve correct shapes and sizes, which are important for subsequent nucleus-level morphology feature extraction. This is more obvious in the breast cancer  The M = 0 corresponds to no repulsive term considered in <ref type="bibr" target="#b11">(12)</ref>. The accuracy is low due to the fact that touching or overlapping nuclei are not correctly segmented. The execution time is extremely high when M = N -1 (6055, 10855, and 14460 seconds on brain tumor, NET, and breast cancer, respectively), which represents the case that the repulsion from all other nuclei in the image are calculated. When M = 5, also used in our experiments, the execution time is about 81, 187 and 221 seconds, respectively, which is more than 60 times faster than the one with globally repulsive deformable model. The proposed method in ( <ref type="formula" target="#formula_19">12</ref>) is a regional-based model, so more image background that might contain noises will contribute to contour evolution when a large M (which usually corresponds to a large region) is used. By contrast, a small local region can preserve intensity smoothness. Therefore, the segmentation accuracy slightly decreases when M is larger than a threshold, as shown in Figure <ref type="figure" target="#fig_10">12</ref>. Practically, one can select a proper M by providing a rough estimation to improve both segmentation performance and computational efficiency instead of using M = N -1.</p><p>3) Dictionary Size: To evaluate the proposed dictionary learning algorithm, we learn multiple dictionaries with different sizes as 5%, 10%, 20%, 30%, 50%, and 100% of the number of all training shapes. The 100% means that the dictionary is equal to the data matrix which collects all the training samples. The evaluation based on DSC, HD, and M AD is listed in Table <ref type="table" target="#tab_5">IV</ref>. As we can see, a learned compact dictionary can achieve approximately equal segmentation accuracy as all the data. We also conduct a two sample ttest <ref type="bibr" target="#b80">[81]</ref> between different level learned dictionaries and the full dictionary, and it shows that a compact dictionary can consistently provide statistically equal performance as all the data. This might be because the proposed dictionary learning algorithm is sufficient to create a reference repository with strong expressive power. Due to the fast locality-constrained linear coding, the time cost at run-time does not exhibit large variation with respect to the dictionary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND CONCLUSION</head><p>In this work, we propose a novel nucleus segmentation framework using deep convolutional neural network and selection-based sparse shape model. The approach starts with a deep learning-based iterative region merging algorithm to initialize the contours, and thereafter alternately performs efficient bottom-up shape deformation and robust top-down shape inference to achieve accurate nucleus segmentation. The comparative experiments demonstrate its superior performance. For fair comparison, all the source (or binary) codes of the other state-of-the-arts, except the marker-controlled watershed which is implemented by us, are obtained from the corresponding authors.</p><p>The proposed shape initialization method is robust to image noise and inhomogeneous intensity. Due to the learned feature representation from the CNN model, we can obtain reliable probability maps which can facilitate the subsequent shape inference. Another major contribution in this paper is a novel repulsive deformable model with sparsity-based shape constraint. On one hand, compared with the nonparametric model, level set, which is generally designed to handle topology changes, the proposed repulsive model is a parametric model that can strictly preserve object topology. On the other hand, the learned shape dictionaries constrain the deformable model with shape priors such that it can effectively handle weak or missing nucleus boundaries. The proposed framework is a general method that can be extended to other applications.</p><p>The sparse representation-based shape prior model has been experimentally proved to be superior to the conventional PCAbased or smooth shape prior on medical image segmentation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. A compact learned dictionary whose size is much smaller than the number of available training samples not only can provide equal performance in nucleus segmentation but also might improve the computational complexity due to the reduced data size.</p><p>We also observe that for repulsive deformable models including level set-based implementation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> or parametric representation <ref type="bibr" target="#b50">[51]</ref>, it is reasonable and necessary  to perform shape deformation in its local system for each individual contour when calculating the repulsion, because each object is only surrounded by a limited number of nuclei, which is much smaller than the total number of nuclei in the entire image. In this way, significant improvement of the computational time can be achieved if we choose a smaller and proper size of the neighbors.</p><p>Since currently the proposed approach is mainly implemented with Matlab, it might be not well scalable to largescale images, and a C/C++ implementation will improve the computational cost. Furthermore, considering that whole-slide scanned histopathological images are usually with very large sizes (e.g. 10000 × 10000), we plan to further reduce the running time of our method using cloud computing techniques.</p><p>By dividing the whole image into multiple partially-overlapped tiles and distributing them onto different workers, concurrent cell segmentation can be achieved using a master-worker manner in the Spark cloud computing platform <ref type="bibr" target="#b82">[83]</ref>. Our future work is to implement the proposed method with cloud computing techniques so that it can be adaptive to large-scale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head><p>This research is funded, by part, by NIH R01 AR065479-02.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample images of brain tumor, pancreatic neuroendocrine tumor, and breast cancer. Many nuclei touch with each other, exhibit weak nucleus boundaries, or have significant shape variations. The background clutter in the breast cancer image presents significant challenges as well.</figDesc><graphic coords="2,56.40,58.12,161.72,90.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The flowchart of the proposed framework.</figDesc><graphic coords="3,48.96,53.14,243.78,158.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. (a) Learned kernels in the first layer of the used CNN model on the NET dataset. (b) One sample NET image patch (the YUV color space). (c) Several randomly selected feature maps in the first layer. The feature maps are generated by feeding the sample input image to the CNN model.</figDesc><graphic coords="5,115.12,58.74,98.43,101.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Iterative region mergingInput: Probability map and initial depth dt 0 for H-minima transform Output: Initial shapes 1. Do HT (I, dt 0 ) and get</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Shape initialization using the proposed algorithm on one sample image of the breast cancer dataset. Top left: original image; Top right: probability map from CNN; Bottom left: initial nucleus shapes from iterative region merging; Bottom right: original image overlaid with initial nucleus shapes. Note that all the nuclei in the connected regions touching image boundaries are ignored.</figDesc><graphic coords="8,91.82,190.55,212.69,127.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparative nucleus detection using different methods on several sample patches of brain tumor (rows 1 ∼ 2), NET (rows 3 ∼ 4), and breast cancer (rows 5 ∼ 6). Note that the nuclei touching image boundaries are ignored.</figDesc><graphic coords="9,115.39,450.94,73.90,73.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Segmentation results using the proposed algorithm on six sample images of the brain tumor (top), NET (middle), and breast cancer (bottom) datasets. Note that all the nuclei in the connected regions touching image boundaries are ignored.</figDesc><graphic coords="10,59.48,215.23,119.05,73.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparative segmentation using different methods on several sample patches of brain tumor (rows 1 ∼ 2), NET (rows 3 ∼ 4), and breast cancer (rows 5 ∼ 6). MWS, RLS, and the proposed use the same initialization. Note that the nuclei touching image boundaries are ignored.</figDesc><graphic coords="12,56.79,394.22,59.53,62.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The performance of the proposed segmentation algorithm with respect to the repulsion weight ω in (12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. The performance of the proposed segmentation algorithm with respect to the number of nearest neighbors M in<ref type="bibr" target="#b11">(12)</ref>. From top to bottom: running time, mean of the DSC, HD, and MAD, and standard deviation of the DSC, HD, and MAD on brain tumor, NET, breast cancer datsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EVALUATION</head><label>II</label><figDesc>OF NUCLEUS DETECTION ON THREE DATASETS IN TERMS OF MEAN AND STANDARD DEVIATION (STD) OF PRECISION, RECALL AND F 1 .21 0.82/0.17 0.74/0.16 0.60/0.15 0.94/0.04 0.72/0.12 0.56/0.18 0.65/0.14 0.58/0.12 RF 0.53/0.20 0.94/0.12 0.66/0.18 0.71/0.10 0.88/0.10 0.78/0.08 0.36/0.14 0.74/0.15 0.46/0.14 DBN 0.52/0.19 0.79/0.20 0.60/0.17 0.81/0.09 0.77/0.09 0.78/0.05 0.39/0.14 0.67/0.12 0.47/0.11 proposed 0.72/0.20 0.88/0.15 0.77/0.16 0.84/0.08 0.93/0.05 0.88/0.04 0.71/0.14 0.88/0.09 0.78/0.08</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SCORE.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Brain tumor (Mean/STD)</cell><cell cols="2">NET (Mean/STD)</cell><cell></cell><cell cols="3">Breast cancer (Mean/STD)</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>SVM</cell><cell>0.72/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARATIVE</head><label>III</label><figDesc>PIXEL-WISE SEGMENTATION ACCURACY ON BRAIN TUMOR, NET, AND BREAST CANCER. FOR EACH METRIC (DSC, HD, MAD), THE MEAN, MEDIAN, STANDARD DEVIATION (STD), AND THE SORTED 80% HIGHEST ACCURACY AMONG ALL THE RESULTS ARE LISTED. sr (s), v gt ) denotes the minimum distance from point s to the contour v gt , sup means the supremum, and |v sr | represents the length of v sr . A large DSC, or a small HD/M AD indicates high segmentation accuracy.Table III displays the DSC, HD and M AD values using MS</figDesc><table><row><cell>brain</cell><cell></cell><cell>DSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HD</cell><cell></cell><cell></cell><cell></cell><cell>MAD</cell></row><row><cell></cell><cell cols="7">Mean Median STD 80% Mean Median</cell><cell>STD</cell><cell>80%</cell><cell cols="3">Mean Median STD 80%</cell></row><row><cell>MS</cell><cell>0.74</cell><cell>0.84</cell><cell></cell><cell>0.24 0.90</cell><cell></cell><cell>9.98</cell><cell>4.84</cell><cell cols="2">11.95 16.29</cell><cell>5.23</cell><cell>3.23</cell><cell>5.29 6.79</cell></row><row><cell>ISO</cell><cell>0.70</cell><cell>0.75</cell><cell></cell><cell cols="3">0.19 0.84 10.15</cell><cell>5.67</cell><cell cols="2">12.41 13.62</cell><cell>6.63</cell><cell>4.97</cell><cell>6.49 7.83</cell></row><row><cell>SUP</cell><cell>0.75</cell><cell>0.84</cell><cell></cell><cell cols="3">0.20 0.89 10.89</cell><cell>5.00</cell><cell cols="2">11.07 19.03</cell><cell>5.32</cell><cell>3.16</cell><cell>5.01 8.04</cell></row><row><cell>GCC</cell><cell>0.81</cell><cell>0.86</cell><cell></cell><cell>0.13 0.90</cell><cell></cell><cell>7.07</cell><cell>4.12</cell><cell>6.47</cell><cell>10.63</cell><cell>3.85</cell><cell>2.92</cell><cell>2.95 4.85</cell></row><row><cell>MWS</cell><cell>0.81</cell><cell>0.86</cell><cell></cell><cell>0.15 0.92</cell><cell></cell><cell>7.21</cell><cell>4.56</cell><cell>6.30</cell><cell>11.06</cell><cell>3.57</cell><cell>2.50</cell><cell>3.04 5.06</cell></row><row><cell>RLS</cell><cell>0.80</cell><cell>0.83</cell><cell></cell><cell>0.11 0.90</cell><cell></cell><cell>8.51</cell><cell>5.00</cell><cell>8.52</cell><cell>14.14</cell><cell>4.59</cell><cell>2.46</cell><cell>6.53 5.58</cell></row><row><cell>proposed</cell><cell>0.85</cell><cell>0.89</cell><cell></cell><cell>0.11 0.93</cell><cell></cell><cell>5.06</cell><cell>2.91</cell><cell>5.26</cell><cell>6.37</cell><cell>3.26</cell><cell>2.18</cell><cell>2.89 4.41</cell></row><row><cell>NET</cell><cell></cell><cell>DSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HD</cell><cell></cell><cell></cell><cell></cell><cell>MAD</cell></row><row><cell></cell><cell cols="7">Mean Median STD 80% Mean Median</cell><cell>STD</cell><cell>80%</cell><cell cols="3">Mean Median STD 80%</cell></row><row><cell>MS</cell><cell>0.66</cell><cell>0.72</cell><cell></cell><cell>0.20 0.82</cell><cell></cell><cell>7.01</cell><cell>4.74</cell><cell>5.88</cell><cell>10.50</cell><cell>4.35</cell><cell>3.43</cell><cell>2.57 6.12</cell></row><row><cell>ISO</cell><cell>0.48</cell><cell>0.50</cell><cell></cell><cell cols="3">0.20 0.67 10.02</cell><cell>5.94</cell><cell cols="2">13.65 10.58</cell><cell>8.14</cell><cell>6.81</cell><cell>6.93 8.89</cell></row><row><cell>SUP</cell><cell>0.75</cell><cell>0.81</cell><cell></cell><cell>0.17 0.87</cell><cell></cell><cell>6.62</cell><cell>3.73</cell><cell>6.85</cell><cell>9.62</cell><cell>3.77</cell><cell>2.53</cell><cell>3.29 4.78</cell></row><row><cell>GCC</cell><cell>0.61</cell><cell>0.69</cell><cell></cell><cell>0.21 0.77</cell><cell></cell><cell>6.37</cell><cell>4.29</cell><cell>4.23</cell><cell>10.10</cell><cell>5.00</cell><cell>3.87</cell><cell>2.58 6.78</cell></row><row><cell>MWS</cell><cell>0.82</cell><cell>0.89</cell><cell></cell><cell>0.17 0.93</cell><cell></cell><cell>4.12</cell><cell>2.75</cell><cell>3.51</cell><cell>6.41</cell><cell>2.33</cell><cell>1.38</cell><cell>5.53 3.18</cell></row><row><cell>RLS</cell><cell>0.84</cell><cell>0.86</cell><cell></cell><cell>0.09 0.91</cell><cell></cell><cell>2.71</cell><cell>1.46</cell><cell>3.11</cell><cell>4.01</cell><cell>2.26</cell><cell>1.69</cell><cell>1.93 2.79</cell></row><row><cell>proposed</cell><cell>0.92</cell><cell>0.95</cell><cell></cell><cell>0.09 0.98</cell><cell></cell><cell>2.41</cell><cell>1.35</cell><cell>2.99</cell><cell>3.14</cell><cell>1.58</cell><cell>0.98</cell><cell>1.75 2.10</cell></row><row><cell>breast</cell><cell></cell><cell>DSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HD</cell><cell></cell><cell></cell><cell></cell><cell>MAD</cell></row><row><cell></cell><cell cols="7">Mean Median STD 80% Mean Median</cell><cell>STD</cell><cell>80%</cell><cell cols="3">Mean Median STD</cell><cell>80%</cell></row><row><cell>MS</cell><cell>0.49</cell><cell>0.51</cell><cell></cell><cell cols="3">0.25 0.74 20.83</cell><cell>17.77</cell><cell cols="3">15.59 28.70 13.17</cell><cell>11.74</cell><cell>8.27 18.63</cell></row><row><cell>ISO</cell><cell>0.56</cell><cell>0.58</cell><cell></cell><cell cols="3">0.19 0.75 17.19</cell><cell>15.40</cell><cell cols="3">10.54 24.36 11.59</cell><cell>10.38</cell><cell>5.82 15.89</cell></row><row><cell>SUP</cell><cell>0.68</cell><cell>0.72</cell><cell></cell><cell cols="3">0.18 0.84 17.14</cell><cell>13.89</cell><cell cols="2">13.10 24.72</cell><cell>9.32</cell><cell>7.74</cell><cell>6.02 13.29</cell></row><row><cell>GCC</cell><cell>0.59</cell><cell>0.62</cell><cell></cell><cell cols="3">0.23 0.81 16.84</cell><cell>15.59</cell><cell cols="3">10.40 23.91 10.75</cell><cell>9.43</cell><cell>6.57 15.64</cell></row><row><cell>MWS</cell><cell>0.73</cell><cell>0.79</cell><cell></cell><cell cols="3">0.20 0.88 11.12</cell><cell>9.66</cell><cell>7.25</cell><cell>14.80</cell><cell>6.66</cell><cell>5.08</cell><cell>5.49</cell><cell>9.24</cell></row><row><cell>RLS</cell><cell>0.77</cell><cell>0.81</cell><cell></cell><cell cols="3">0.15 0.88 10.50</cell><cell>8.12</cell><cell>7.46</cell><cell>14.76</cell><cell>6.30</cell><cell>4.80</cell><cell>4.95</cell><cell>8.25</cell></row><row><cell>proposed</cell><cell>0.80</cell><cell>0.86</cell><cell></cell><cell>0.15 0.92</cell><cell></cell><cell>8.60</cell><cell>6.37</cell><cell>6.77</cell><cell>13.08</cell><cell>6.24</cell><cell>4.53</cell><cell>4.90</cell><cell>9.61</cell></row><row><cell></cell><cell></cell><cell cols="2">DSC =</cell><cell>2|Ω sr ∩ Ω gt | |Ω sr | + |Ω gt |</cell><cell cols="2">,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HD = max{sup</cell><cell cols="2">d(v sr (s), v gt ), sup</cell><cell cols="4">d(v gt (s), v sr )},</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s</cell><cell></cell><cell>s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">M AD =</cell><cell cols="5">1 0 d(v sr (s), v gt )|v sr (s)|ds 2|v sr |</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+</cell><cell cols="4">1 0 d(v gt (s), v sr )|v gt (s)|ds 2|v gt |</cell><cell>, (14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>where d(v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>dataset, which exhibits more touching or overlapping nuclei. Therefore, it is necessary to choose a proper value for ω, such as 2.5 (used in our experiments), to achieve desired segmentation accuracy.2) Nearest Neighbor Size: The nearest neighbor size M in (12) plays a significant role in our algorithm. In Figure 12, We calculate the segmentation accuracy and running time with different nearest neighbor size {M = 0, 1, 2, 4, 6, 8, 10, 15, 30, 60, 100, 200} on three images, one per dataset. The image size for brain tumor, NET, and breast cancer is approximately 1121 × 601, 1324 × 817, and 1712 × 952 with N = 221, 572, 565 nuclei, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV EFFECTS</head><label>IV</label><figDesc>OF DICTIONARY SIZE ON BRAIN TUMOR, NET, AND BREAST CANCER. FOR EACH METRIC (DSC, HD, MAD), THE MEAN AND STANDARD DEVIATION (STD) ARE LISTED. .11 5.15/5.40 3.28/2.91 0.92/0.09 2.47/3.09 1.60/1.79 0.80/0.15 8.67/6.85 6.24/4.88 10% 0.85/0.11 5.15/5.42 3.29/2.94 0.92/0.09 2.46/3.08 1.59/1.78 0.80/0.15 8.67/6.87 6.27/4.92 20% 0.85/0.11 5.02/5.27 3.33/2.89 0.92/0.09 2.44/3.00 1.59/1.77 0.80/0.15 8.62/6.76 6.26/4.93 30% 0.85/0.11 5.14/5.39 3.21/2.81 0.92/0.10 2.48/3.09 1.60/1.87 0.80/0.15 8.70/6.83 6.28/4.94 50% 0.85/0.11 5.12/5.39 3.21/2.81 0.92/0.09 2.46/3.06 1.59/1.84 0.80/0.15 8.69/6.82 6.28/4.95 100% 0.85/0.11 4.99/5.06 3.28/2.81 0.92/0.09 2.38/2.89 1.59/1.74 0.80/0.15 8.59/6.72 6.30/4.93</figDesc><table><row><cell></cell><cell cols="3">Brain tumor (Mean/STD)</cell><cell></cell><cell>NET (Mean/STD)</cell><cell></cell><cell cols="3">Breast cancer (Mean/STD)</cell></row><row><cell></cell><cell>DSC</cell><cell>HD</cell><cell>MAD</cell><cell>DSC</cell><cell>HD</cell><cell>MAD</cell><cell>DSC</cell><cell>HD</cell><cell>MAD</cell></row><row><cell>5%</cell><cell>0.85/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cell segmentation: 50 years down the road [life sciences]</title>
		<author>
			<persName><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine, IEEE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="140" to="145" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histopathological image analysis: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="147" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A hybrid 3D watershed algorithm incorporating gradient cues and object models for automatic segmentation of nuclei in confocal image stacks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Guzowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry A</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nuclei segmentation using markercontrolled watershed, tracking using mean-shift, and kalman filter in time-lapse microscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ciruits Syst</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2405" to="2414" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards automated cellular image segmentation for RNAi genomewide screening</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perrimon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">3749</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation of clustered nuclei with shape markers and marking function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rajapakse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="741" to="748" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
		<title level="m">Morphological Image Analysis: Principles and Applications</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Radial symmetries based decomposition of cell clusters in binary and gray level images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1923" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised learning-based cell image segmentation for p53 immunohistochemistry</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1163" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved automatic detection and segmentation of cell nuclei in histopathology images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lassoued</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="852" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding dots: segmentation as popping out regions from boundaries</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bernardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic image analysis of histopathology specimens using concave vertex graph</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="page" from="833" to="841" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Delaunay triangulation approach for segmenting clumps of nuclei</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Biomed. Imag. (ISBI)</title>
		<imprint>
			<biblScope unit="page" from="9" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Partitioning histopathological images: an integrated framework for supervised colortexture segmentation and cell splitting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belkacem-Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1661" to="1677" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical partial matching and segmentation of interacting cells</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">7510</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Voronoi-based segmentation of cell on image manifolds</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Biomed. Image Applicat</title>
		<imprint>
			<biblScope unit="volume">3765</biblScope>
			<biblScope unit="page" from="535" to="543" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Expectationmaximization-driven geodesic active contour with overlap resolution (EMaGACOR): application to lymphocyte segmentation on breast cancer histopathology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fatakdawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhanot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1676" to="1689" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geodesic active contours</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="79" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multireference level set for the characterization of nuclear morphology in glioblastoma multiforme</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3460" to="3467" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal cell cycle phase analysis using level sets and fast marching methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Padfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Level set analysis for leukocyte detection and tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Acton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="562" to="572" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic segmentation of high-throughput RNAi fluorescent cellular images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust segmentation of overlapping cells in histopathology specimens using parallel seed detection and repulsive level set</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="754" to="765" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection of spatially correlated objects in 3D images using appearance models and coupled active contours</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mosaliganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gouaillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Noche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Obholzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Megason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">5762</biblScope>
			<biblScope unit="page" from="641" to="648" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmenting and tracking fluoresent cells in dynamic 3D microscopy with coupled active surfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guillen-Aghion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Olivo-Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1396" to="1410" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cell segmentation using coupled level sets and graph-vertex coloring</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rpalaniappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bunyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="101" to="108" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast globally optimal segmentation of cells in fluorescence microscopy images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bergeest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">6891</biblScope>
			<biblScope unit="page" from="645" to="652" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Advanced level-set-based cell tracking in time-lapse fluorescence microscopy</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dzyubachyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Van Cappellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Essers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="852" to="867" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coupled parametric active contours</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Olivo-Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1838" to="1842" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A topology preserving level set method for geometric deformable models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="755" to="768" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-repelling snakes for topologypreserving segmentation models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Snakes, shapes, and gradient vector flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation based on robust estimation and color active contour models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="486" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Repulsive force based snake model to segment and track neuronal axons in 3D microscopy image stacks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1608" to="1620" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Snakes: active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Invariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="670" to="682" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic batch-invariant color segmentation of histological cancer images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hassberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chaudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Biomed. Imag. (ISBI)</title>
		<imprint>
			<biblScope unit="page" from="657" to="660" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shape priors for level set representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rousson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Level set based shape prior segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1164" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Statistical shape influence in geodesic active contours</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A priori information in image segmentation: energy functional based on shape statistical model and image information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="425" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards recognition-based variational segmentation using shape priors and dynamic labeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Scale Space Theories Comput. Vis</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2695</biblScope>
			<biblScope unit="page">388400</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matching distance functions: A shape-to-area variational approach for globalto-local registration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rousson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="775" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An integrated region-, boundary-, shapebased active contour for multiple object overlap resolution in histological imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1448" to="1460" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shapeconstrained repulsive snake method to segment and track neurons in 3D microscopy images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Biomed. Imag. (ISBI)</title>
		<imprint>
			<biblScope unit="page" from="538" to="541" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Segmentation, inference and classification of partially overlapping nanoparticles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="669" to="681" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable segmentation via sparse shape representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="451" to="458" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deformable segmentation via sparse shape representation and dictionary learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Ana</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1385" to="1396" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust selection-based sparse shape model for lung cancer image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">8151</biblScope>
			<biblScope unit="page" from="404" to="412" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning-based feature representation for AD/MCI classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="583" to="590" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2852" to="2860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arevalo Ovalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Gonzlez</forename><surname>Osorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network,&quot; in Int</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="246" to="253" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Sym. Cir. System. (ISCAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn. (ICML)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn. (ICML)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Quantification of histochemical staining by color deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ruifrok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Quant. Cytol. Histol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="291" to="299" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R J</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Parametric statistical modeling by minimum integrated squared error</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="274" to="285" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hierarchical shape statistical model for segmentation of lung fields in chest radiographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="page" from="417" to="424" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cross modality deformable segmentation using hierarchical clustering and learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="1033" to="1041" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Optimal approximation by piecewise smooth functions and associated variational problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Applied Math</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="685" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">BudgetedSVM: A toolbox for scalable SVM approximations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3813" to="3817" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Piotr&apos;s Computer Vision Matlab Toolbox (PMT)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<ptr target="http://vision.ucsd.edu/∼pdollar/toolbox/doc/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Isoperimetric graph partitioning for image segmetentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="469" to="475" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Guiding model search using segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1417" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning to detect cells using non-overlapping extremal regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">7510</biblScope>
			<biblScope unit="page" from="348" to="356" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Handbook of Biological Statistics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Sparky House Publishing</publisher>
			<pubPlace>Baltimore, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Fast image scanning with deep max-pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4034" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Skeletal muscle cell segmentation using distributed convolutional neural network,&quot; in Accepted to High Performance Computing (HPC) Workshop associated with Int</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
