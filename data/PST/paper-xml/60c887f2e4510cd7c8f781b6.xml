<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MART: Motion-Aware Recurrent Neural Network for Robust Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
							<email>hefan@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hling@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MART: Motion-Aware Recurrent Neural Network for Robust Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce MART, Motion-Aware Recurrent neural network (MA-RNN) for Tracking, by modeling robust longterm spatial-temporal representation. In particular, we propose a simple, yet effective context-aware displacement attention (CADA) module to capture target motion in videos. By seamlessly integrating CADA into RNN, the proposed MA-RNN can spatially align and aggregate temporal information guided by motion from frame to frame, leading to more effective representation that benefits a tracker from motion when handling occlusion, deformation, viewpoint change etc. Moreover, to deal with scale change, we present a monotonic bounding box regression (mBBR) approach that iteratively predicts regression offsets for target object under the guidance of intersection-over-union (IoU) score, guaranteeing non-decreasing accuracy. In extensive experiments on five benchmarks, including GOT-10k, LaSOT, TC-128, OTB-15 and VOT-19, our tracker MART consistently achieves state-of-the-art results and runs in real-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking has been one of the most important components in computer vision with many applications such as robotics, surveillance and so on. For a robust tracker, one of the core problems is how to design an effective feature representation for target appearance modeling <ref type="bibr" target="#b54">[56]</ref> so that the tracker can well deal with various challenges in videos such as occlusion, deformation, view changes, etc.</p><p>Early approaches usually leverage various hand-crafted representation (e.g., pixel value <ref type="bibr" target="#b3">[4]</ref>, HoG <ref type="bibr" target="#b22">[24]</ref>) for tracking. Recently, inspired by powerful deep networks <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b29">31]</ref>, researchers have resorted to deep representation for tracking and achieved significant improvement <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b60">62]</ref>. Despite considerable advancement, most existing trackers focus on spatial feature representation of current frame for tracking, while leaving rich temporal information under explored. Consequently, their performances may degrade when target feature is corrupted by challenges such as occlusion, deformation, etc.</p><p>A natural remedy for this problem is to use both spatial and temporal representation for appearance modeling. This way, current frame can be effectively enhanced for tracking with extra support from historical frames, even when difficult challenges occur. A recent representative effort in <ref type="bibr" target="#b62">[64]</ref> develops such a spatial-temporal representation for tracking. Since target features are usually not spatially aligned between frames due to motion, this method estimates optical flow using a sub-network (FlowNet <ref type="bibr" target="#b13">[14]</ref>) to capture motion dynamics of the target for spatial feature alignment. Despite upgrading performance, this method can be improved in two aspects: (1) Efficiency. To obtain optical flow, a large extra optical network is integrated into feature representation network, resulting in more computation and inefficient tracking inference. <ref type="bibr" target="#b1">(2)</ref> Long-term representation. In tracker <ref type="bibr" target="#b62">[64]</ref>, spatial-temporal representation is achieved by warping and aggregating features on a short fixed temporal window, making it difficult to obtain a long-term representation, which is desired for tracking.</p><p>To handle the above issues and obtain an efficient longterm spatial-temporal representation, we propose a novel Motion-Aware Recurrent neural network (MA-RNN) for Tracking. Specifically, an MA-RNN consists of two parts, a context-aware displacement attention (CADA) module and an RNN. The RNN component, implemented based on Con-vGRU <ref type="bibr" target="#b0">[1]</ref>, aims at long-term representation by learning to aggregate temporal features. Due to motion dynamics, however, targets are usually spatially misaligned across frames (see Fig. <ref type="figure" target="#fig_0">1</ref> for an example). In such situation, direct aggregation of features into RNN may even be detrimental to the representation. Attacking this problem, we propose a simple, yet effective CADA module for efficient motion capture and apply it to align feature aggregation in RNN for robust representation. In particular, the motion dynamics of a target are captured by modeling displacement of each unit on feature maps across frames. To achieve this, we match each unit in current frame to a local region in last frame, and compute a soft attention score map to represent such displacement. For robustness, context of each unit is taken into consideration for matching. After obtaining the displacement attention score map, we propagate it to guide spatial Based on the spatial-temporal representation, we apply a classifier to localize target. To handle scale change, we propose a separate scale estimation component using multistep bounding box regression (BBR). The core is a targetaware BBR-IoU network that, given a reference target, predicts the offsets of its bounding box and IoU score for regression result. The IoU score is used to guide iterative bounding box regression. Since the IoU score of the candidate box is guaranteed to be non-decreasing, we call our method monotonic bounding box regression (mBBR). As a single step of regression usually works well (as observed in recent studies as well <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref>), mBBR works efficiently most of time, with very few iterations by using IoU score as an early-stop condition.</p><p>To evaluate our tracker, we conduct experiments on five popular benchmarks including GOT-10k <ref type="bibr" target="#b23">[25]</ref>, LaSOT <ref type="bibr" target="#b15">[16]</ref>, TC-128 <ref type="bibr" target="#b36">[38]</ref>, OTB-15 <ref type="bibr" target="#b57">[59]</ref> and VOT-19 <ref type="bibr" target="#b28">[30]</ref>. The proposed MART consistently achieves state-of-the-art results <ref type="foot" target="#foot_0">1</ref> .</p><p>In summary, the contributions in this paper include: (i) a novel motion-aware recurrent neural network (MA-RNN) for spatial-temporal representation for tracking; (ii) a novel context-aware displacement attention (CADA) module for target motion dynamic capture and spatial feature alignment; (iii) a simple yet effective monotonic bounding box regression (mBBR) for accurate target scale estimation; and (iv) state-of-the-art performances on five benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual tracking has been extensively studied in recent years. In the following we discuss the most related work and refer readers to <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b45">47]</ref> for recent tracking surveys.</p><p>Deep Tracking. Inspired by success in image classification <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b29">31]</ref>, deep feature has been used for tracking and demonstrated state-of-the-art performance <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b55">57]</ref>. One of recent trends is to integrate correlation filter tracking with deep feature to learn a discriminative classification model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b62">64]</ref>. Especially, the work of <ref type="bibr" target="#b62">[64]</ref> leverages temporal information for tracking. Despite robustness, these trackers cannot well deal with heavy scale variation. To handle this issue, the work of <ref type="bibr" target="#b7">[8]</ref> decomposes tracking into localization and estimation subtasks and adopts IoU prediction <ref type="bibr" target="#b25">[27]</ref> for scale estimation, achieving significant improvement. Another trend is to learn a similarity measurement for tracking using Siamese networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">50]</ref>. Owing to balanced accuracy and efficiency, the work of <ref type="bibr" target="#b1">[2]</ref> has been improved with many extensions <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b56">58]</ref>. Notably, by integrating with region proposal network (RPN) <ref type="bibr" target="#b43">[45]</ref>, the work of <ref type="bibr" target="#b31">[33]</ref> greatly improves Siamese tracker in dealing with scale change, which is further enhanced by incorporating distractor detection <ref type="bibr" target="#b61">[63]</ref>, multi-stage mechanism <ref type="bibr" target="#b16">[18,</ref><ref type="bibr">54]</ref> and deeper architecture <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b60">62]</ref>.</p><p>Our work is related to but different from <ref type="bibr" target="#b62">[64]</ref> that uses a large FlowNet <ref type="bibr" target="#b13">[14]</ref> for motion capture. In contrast, we propose an efficient CADA module for motion dynamics. Besides, we model a long-term representation using RNNs, which differs from <ref type="bibr" target="#b62">[64]</ref> with a short-term representation. Our work is relevant to <ref type="bibr" target="#b7">[8]</ref> by sharing the similar spirit of decomposing tracking into two sub-problems. However, our solutions to the two sub-problems are very different than those in <ref type="bibr" target="#b7">[8]</ref>. In addition, IoU prediction <ref type="bibr" target="#b25">[27]</ref> is employed in both <ref type="bibr" target="#b11">[12]</ref> and our work, but in different ways. The work of <ref type="bibr" target="#b7">[8]</ref> directly utilizes IoU score for scale estimation, while our approach uses it to guide bounding box regression. Different from the multi-step BBR in <ref type="bibr" target="#b16">[18,</ref><ref type="bibr">54]</ref>, ours is more adaptive by using IoU score to guide BBR.</p><p>An interesting observation is that, our CADA module can be connected to the recent Siamese tracker <ref type="bibr" target="#b1">[2]</ref> that learns a similarity measurement for tracking. The difference is, the displacement, obtained by matching, is directly used at box-level for target localization in <ref type="bibr" target="#b1">[2]</ref>, while we use it at pixel-level to capture motion dynamics and guide spatial feature alignment. Besides, our CADA module leverages contextual information for robust matching.</p><p>RNN for Spatial-temporal Representation. RNN has been extensively explored in computer vision owing to its capacity in modeling long-term feature representation including video object segmentation <ref type="bibr" target="#b49">[51]</ref>, video action classification/recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, video object detection <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b58">60]</ref>.</p><p>Our approach is closely related to <ref type="bibr" target="#b0">[1]</ref> that proposes convolutional gated recurrent unit networks (ConvGRU) and then applies it for action recognition. Implemented based on ConvGRU, our work MA-RNN is different than Con-vGRU in two main aspects. First, MA-RNN aims at distinguishing target/background in videos instead of classifying a video clip. Second, MA-RNN uses a motion-aware mod- ule to capture dynamics for robust representation in RNN, which is important for visual tracking.</p><p>Motion Dynamic Modeling. As a crucial component in video understanding, how to capture and model motion dynamics attracts increasing attention. A common solution is to compute optical flow between frames. Inspired by deep learning, the accuracy of optical flow has been greatly improved (e.g., FlowNet/FlowNet 2.0 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">26]</ref>). Despite this, it is time-consuming to obtain such motion information with optical flow when considering efficiency requirement in tracking. An alternative solution for motion modeling is to explicitly compute displacement of pixels by performing correlation <ref type="bibr" target="#b17">[19]</ref>. Unlike <ref type="bibr" target="#b17">[19]</ref> that aims at detecting object movements for video object detection, we utilize CADA to align features in RNNs for tracking. Besides, we take into consideration context information of each unit to better capture motion dynamics, which significantly differs from <ref type="bibr" target="#b17">[19]</ref>.</p><p>3. Motion-Aware RNN for Tracking (MART)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Following paradigm in <ref type="bibr" target="#b7">[8]</ref>, we decompose tracking into two exclusive sub-tasks, including target localization and target scale estimation. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the overview of our tracking algorithm.</p><p>In target localization, for a test image, we employ a backbone network (i.e., ResNet <ref type="bibr" target="#b21">[23]</ref> pre-trained on Ima-geNet <ref type="bibr" target="#b12">[13]</ref>) to extract initial feature representation x t . Considering the gap between classification and tracking tasks, we apply an extra conv layer to transform the initial feature x t to f t . Together with spatial feature f t−1 and spatialtemporal representation H t−1 from last frame, f t is fed to MA-RNN for spatial-temporal representation H t in current frame. Classification is performed on H t to obtain the target position.</p><p>Target scale estimation is performed using mBBR. With the target position by localization component, we sample a set of candidate proposals {B i c } around target position using previous scale information. These proposals {B i c } and the test image feature x t , together with initial bounding box B 1 and reference image feature x 1 , are sent to mBBR for fianl target scale estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion-Aware RNN (MA-RNN)</head><p>In this work, we aim at learning a robust long-term spatial-temporal representation for object tracking. For this purpose, a natural choice is RNN that captures long-term representation by aggregating temporal information from frame to frame. Considering the importance of spatial information in 2D images/videos, the work of <ref type="bibr" target="#b0">[1]</ref> replaces linear product operation in GRU-RNN <ref type="bibr" target="#b5">[6]</ref> with convolution and proposes ConvGRU for action recognition, achieving superior performance. Mathematically, ConvGRU can be formulated as follows,</p><formula xml:id="formula_0">z t = φ(W f z * f t + U f z * H t−1 + b z ) r t = φ(W f r * f t + U f r * H t−1 + b r ) H t = ϕ(W f H * f t + U f H * (r t • H t−1 ) + b H ) H t = z t • H t + (1 − z t ) • H t−1 (1)</formula><p>where f t and H t−1 are feature input to ConvGRU and spatial-temporal representation in last frame,</p><formula xml:id="formula_1">W f z , U f z , W f r , U f r , W f H , U f H represent</formula><p>convolutional kernels and are end-to-end learned, z t and r t are update and reset gates, and H t denotes spatial-temporal representation in current frame. ' * ' and '•' are convolution operation and Hadamard product, respectively. The bias terms in Eq. ( <ref type="formula">1</ref>) are omitted for simplicity.</p><p>From Eq. ( <ref type="formula">1</ref>), we observe that the temporal information is derived through direct aggregation between features f t and H t−1 (see Fig. <ref type="figure" target="#fig_3">3</ref>   again). To deal with this issue, we propose the MA-RNN with a key innovative CADA module, which efficiently captures motion flow to guide temporal feature aggregation over frames. The idea is to compute the displacement of a unit (on feature map) to capture motion flow. Since target object usually moves smoothly or slowly in videos, we compute the displacement of a unit within a local region. In specific, for a feature unit f t (p, q) at position (p, q) on feature map f t , the CADA module matches f t (p, q) and each feature unit within a small region centered at (p, q) on feature map f t−1 . The attentional matching score map, denoted by S, is then utilized to transform the spatial-temporal representation H t−1 to align with feature f t . Mathematically, the matching process by CADA can be formulated as follows,</p><formula xml:id="formula_2">Sp,q(i, j) = sim ft(p, q), ft−1(p + i, q + j) i,j∈{−d,••• ,d} sim ft(p, q), ft−1(p + i, q + j)<label>(2)</label></formula><p>where S p,q (i, j) denotes the normalized similarity score for feature unit f t−1 (p + i, q + j) on f t−1 , d controls the size of local region, and sim(•, •) computes the similarity between two feature units, which can be defined by dot product for its simplicity, i.e., sim</p><formula xml:id="formula_3">f t (p, q), f t−1 (p+i, q+j) = f t (p, q) • f t−1 (p + i, q + j).</formula><p>For the same units of target between different frames, their contexts are similar to each other. Therefore, we introduce contextual information of each unit into similarity computation. For a unit f t (p, q), its context region feature C t (p, q) is defined as a set of unit features for a local region centered at (p, q) with size k (excluding itself) as follows (also see the dashed red rectangle in Fig. <ref type="figure" target="#fig_3">3 (b)</ref>),</p><formula xml:id="formula_4">Ct(p, q) = {ft(p + i, q + j) ∀ i, j ∈ {−k, • • • , k}}/{ft(p, q)}<label>(3)</label></formula><p>Considering deformation of target, we apply max pooling operation to C t (p, q), and obtain the contextual feature unit</p><formula xml:id="formula_5">f C t (p, q) as f C t (p, q) = maxpool(C t (p, q))<label>(4)</label></formula><p>Therefore, we re-write sim(•, •) as containing two weighted terms,</p><formula xml:id="formula_6">sim ft(p, q), ft−1(p + i, q + j) = αft(p, q) • ft−1(p + i, q + j) + (1 − α)f C t (p, q) • f C t−1 (p + i, q + i)<label>(5)</label></formula><p>where the α adjusts the importance of contextual information.</p><p>Note that, our CADA module shares the similar spirit with recent Siamese tracker <ref type="bibr" target="#b1">[2]</ref> that computes a soft attentional matching score between a template and a search region for tracking. Different from <ref type="bibr" target="#b1">[2]</ref>, however, we utilize CADA module to capture motion flow and align spatial features in RNN. Besides, we consider contextual information in CADA for more robust matching.</p><p>With Eq. ( <ref type="formula" target="#formula_2">2</ref>), MA-RNN is obtained using attention score S p,q (i, j) to align spatial features in RNN, and mathematically expressed as followed as,</p><formula xml:id="formula_7">Ht−1 (p, q) = i,j∈{−d,••• ,d} H t−1 (p + i, q + j)S p,q (i, j) z t = φ(W f z * f t + U f z * Ht−1 ) r t = φ(W f r * f t + U f r * Ht−1 ) H t = ϕ(W f H * f t + U f H * (r t • Ht−1 )) H t = z t • H t + (1 − z t ) • Ht−1 (6)</formula><p>This way, MA-RNN is able to learn a robust long-term spatial-temporal representation. Fig. <ref type="figure" target="#fig_3">3</ref> (b) illustrates the spatial feature alignment in RNN by CADA module. It is worth noticing that, our CADA module is computed within 3ms, which is much more efficient than optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Target Localization with MA-RNN</head><p>With long-term spatial-temporal representation by MA-RNN, we perform classification for target localization. To adapt to target appearance change, the classifier requires online update. Inspired by the simplicity and efficiency of classifier in <ref type="bibr" target="#b7">[8]</ref>, we adopt the same classification network that consists of two convolutional layers and is defined as follows,</p><formula xml:id="formula_8">ψ(H; {w 2 , w 1 }) = σ(w 2 * δ(w 1 * H)) (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where H is the spatial-temporal feature representation, {w 2 , w 1 } denote convolutional kernels, σ(•) and δ(•) are parametric exponential linear unit (PELU) <ref type="bibr" target="#b50">[52]</ref> and identity activation functions.</p><p>Inspired by discriminative correlation filter trackers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b46">48]</ref>, the classification model is learned by minimizing the square loss, similar to <ref type="bibr" target="#b7">[8]</ref>, as follows,</p><formula xml:id="formula_10">L cls = n i=1 γ i ψ(H i ; {w 2 , w 1 })−y i 2 + j λ j w j 2<label>(8)</label></formula><p>where y i denotes a 2D Gaussian label, γ i represents importance of each training sample, and λ j the regularization parameter.</p><p>It is worth noting that, different from <ref type="bibr" target="#b7">[8]</ref> that adopts feature representation (i.e., x t in Fig. <ref type="figure" target="#fig_1">2</ref>) extracted from pre-trained ResNet <ref type="bibr" target="#b21">[23]</ref>, our classification model is built on long-term spatial-temporal representation (i.e., H t in Fig. <ref type="figure" target="#fig_1">2</ref>) by MA-RNN. Considering the gap between classification and tracking tasks, we apply an extra conv layer, followed by a ResNet backbone, for feature transformation before feeding it to MA-RNN (see Fig. <ref type="figure" target="#fig_1">2</ref>). During training, the additional conv layer, MA-RNN and classification network are jointly end-to-end trained using the ADAM optimizer <ref type="bibr" target="#b26">[28]</ref>. In inference phase, only the classification network is updated. For efficiency, we utilize strategy in <ref type="bibr" target="#b7">[8]</ref> for online learning of classifier. Please refer to <ref type="bibr" target="#b7">[8]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Target Scale Estimation with m-BBR</head><p>While providing target position, the localization component cannot well estimate target scale. Inspired by detection community (e.g., <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b43">45]</ref>), recent trackers use either one or multi-stage BBR (e.g., <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr">54,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b61">63]</ref>) or IoUNet (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>) to estimate target scale. The former method is efficient by predicting offsets within one forward pass while lack of localization confidence reasoning, which may results in non-monotonic regression problem <ref type="bibr" target="#b25">[27]</ref>. The latter one is reliable, however, requires both forward pass w.r.t. computing IoU score and backward pass w.r.t. computing gradients for box update. In addition, the IoU increment is relatively small compared to regression. Thus, it often needs multiple iterations for final scale estimation. Taking advantages of both the aforementioned approaches, we introduce a multi-step iterative method, i.e., monotonic bounding box regression (mBBR), for target scale estimation. The core of mBBR is a BBR-IoU network that first predicts regression offsets for a box and then perform IoU prediction <ref type="bibr" target="#b25">[27]</ref> for the regression result. To implement BBR-IoU network, we employ the similar feature modulation strategy in <ref type="bibr" target="#b7">[8]</ref>. Fig. <ref type="figure" target="#fig_4">4</ref> shows the pipeline of BBR-IoU network, and due to space limited, we refer readers to supplementary material for its detailed architecture.</p><p>As demonstrated in Fig. <ref type="figure" target="#fig_4">4</ref> </p><formula xml:id="formula_11">L est = i={1,2,3,4} L smooth (r i , r * i ) + L mse (O c , O * c )<label>(9)</label></formula><p>where (r * 1 , r * 2 , r * 3 , r * 4 ) are the regression labels of box B c and O * c denotes the IoU between B ′ c and groundtruth target bounding box in the test image. L smooth is smooth-L 1 loss <ref type="bibr" target="#b18">[20]</ref> and L mse the mean square loss.</p><p>Once training completed, we utilize BBR-IoU network to iteratively estimate target scale under the guidance of IoU. If IoU score is above an acceptable threshold θ IoU , we directly output regression for scale estimation; if it is decreased compared to last iteration, we stop regression and output offsets of last iteration for scale estimation. Otherwise, regression iterates over new box until reaching to the maximum number N reg of regressions. Owing to the guidance by IoU score, mBBR guarantees that the regression accuracy is monotonically increasing. Algorithm 1 summa-  rizes the working pipeline of mBBR. Note that, our mBBR significantly differs from [18, 54] with multi-step BBR and <ref type="bibr" target="#b7">[8]</ref> with IoU prediction. The methods of <ref type="bibr" target="#b16">[18,</ref><ref type="bibr">54]</ref> perform a fix number of steps of regression for scale estimation. In addition, localization confidence is ignored in each step, which may result in nonmonotonic regression issue <ref type="bibr" target="#b25">[27]</ref>. By contrast, we leverage IoU score to guide each step of regression and use it as an early-stop condition. By doing so, we can achieve adaptive regression with monotonically increasing accuracy. In <ref type="bibr" target="#b7">[8]</ref>, a candidate box is refined within two stages: computing IoU score in forward pass and adjusting candidate box using gradient descent method in backward pass. These two stages are iteratively repeated for a fixed time. Unlike <ref type="bibr" target="#b7">[8]</ref>, our approach directly predicts offsets for scale estimation. Since a single step of regression works well in most cases, our mBBR stops within very few iterations most of time, which is much more efficient.</p><formula xml:id="formula_12">(r i 1 , r i 2 , r i 3 , r i 4 ), O i c ← BBR-IOU(I 1 , B 1 , I T , B i c ); 7 B i+1 c ← Regressing B i c using (r i 1 , r i 2 , r i 3 , r i 4 ); 8 B ← B i+1 c ; 9 if O i c ≥ θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Tracking</head><p>Training. We train the target localization and estimation parts separately. For the target localization component, the transformation conv layer, MA-RNNs and classification are end-to-end trained using ADAM method <ref type="bibr" target="#b26">[28]</ref> on videos. To reduce redundancy, we sparsely sample a frame with a random interval ∈ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> to form a new sequence for training. For each frame, we sample a square patch with an area of about 5 × 5 times the target and randomly shift it. These image patches are resized to a fixed size and then sent for training. For scale estimation, BBR-IoU network is trained on image pairs with each consisting of a reference image patch and a test image patch. These image pairs are sampled from the same video with a maximum gap of 50 frames. For reference image, we sample a square patch with an area of about 5×5 times centered at the target. We sample a similar image patch for test image, with perturbations in position and scale as in <ref type="bibr" target="#b7">[8]</ref>. The reference and test image patches are resized to the same resolution before training. For each image pair, we randomly generate 16 candidate boxes in the test image and ensure each one with a minimum 0.6 IoU with the groundtruth bounding box. Data augmentation strategies, such as image flipping, rotation and color jittering, are adopted.</p><p>Visual Tracking. We split tracking into target localization and target scale estimation. For each sequence, we precompute the feature for reference image in target scale estimation. When a new frame arrives, we extract a region of interest based on tracking result of last frame and calculate its spatial-temporal feature representation, which is fed to the classification network for predicting target position. For robustness, we sample a set of M initial target proposals around the target position, and apply mBBR for scale estimation. The final target scale is determined by the refined box with the maximum IoU score. To adapt to target appearance changes, we collect, every V frames, historical spatial-temporal representations from up to U frames for online updating of the classification model. In order to facilitate update, we utilize the strategy in <ref type="bibr" target="#b7">[8]</ref> for online learning during tracking. Note that, MA-RNN does not need online update as they have learned to model generic spatialtemporal representation for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation. We implement MART in python using PyTorch <ref type="bibr" target="#b41">[43]</ref> on an Nvidia GTX-1080 GPU. We employ pre-trained ResNet-50 <ref type="bibr" target="#b21">[23]</ref> as our backbone network and freeze the parameters in both training and tracking. Our MART runs at a speed of around 31 fps. We train the target localization branch using training splits of LaSOT <ref type="bibr" target="#b15">[16]</ref>, GOT-10k <ref type="bibr" target="#b23">[25]</ref> and VID <ref type="bibr" target="#b44">[46]</ref>. We train for 50 epochs using ADAM <ref type="bibr" target="#b26">[28]</ref>. The learning rate starts from 10 −3 with a decay of 0.1 every 10 epochs. The channel of spatial-temporal representation H t is 256. The time step for training MA-RNNs is empirically set to 10. The d, k and α are set to 5, 1 and 0.8, respectively. For BBR-IoU network, we use training splits of LaSOT <ref type="bibr" target="#b15">[16]</ref>, GOT-10k <ref type="bibr" target="#b23">[25]</ref> and VID <ref type="bibr" target="#b44">[46]</ref> and COCO <ref type="bibr" target="#b37">[39]</ref>. We train for 50 epochs with ADAM <ref type="bibr" target="#b26">[28]</ref> using learning rate of 10 −3 with a decay of 0.1 every 10 epochs. The IoU threshold θ IoU is set to 0.85. The maximum number N reg of iterations is 3. The V and U are set to 20 and 50, and the number M of initial proposals for scale estimation is set to 5. GOT-10k <ref type="bibr" target="#b23">[25]</ref> is proposed to assess short-term tracking performance. We evaluate MART on the server provided by the organizers using the testing split with 180 sequences. The performance is measured using average overlap (AO) and success rate (SR) with different thresholds 0.5 and 0.75. Tab. 1 demonstrates the comparisons to other trackers, showing that MART achieves the best performance under all metrics. Specifically, MART obtains AO of 0.628, SR 0.50 of 0.732 and SR 0.75 of 50.4, outperforming the second best tracker DiMP <ref type="bibr" target="#b2">[3]</ref> with AO of 0.611, SR 0.50 of 0.717 and SR 0.75 of 0.492 by 1.7%, 1.5% and 1.2%, respectively. In comparison with ATOM <ref type="bibr" target="#b7">[8]</ref> with 0.556 AO, 0.634 SR 0.50 and 0.403 SR 0.75 , we obtain significant gains by 7.2%, 9.8% and 10.2%, showing the advance of spatialtemporal representation by our MA-RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment on LaSOT</head><p>LaSOT <ref type="bibr" target="#b15">[16]</ref> is a large-scale dataset consisting of 1,400 sequences. Following the protocol, we utilize 1,120 sequences for training and the rest 280 for testing. We compare MART with 12 state-of-the-art tracking algorithms (DiMP <ref type="bibr" target="#b2">[3]</ref>, ATOM <ref type="bibr" target="#b7">[8]</ref>, SiamRPN++ <ref type="bibr" target="#b30">[32]</ref>, C-RPN <ref type="bibr" target="#b16">[18]</ref>, SiamDW <ref type="bibr" target="#b60">[62]</ref>, MDNet <ref type="bibr" target="#b40">[42]</ref>, SiamFC <ref type="bibr" target="#b1">[2]</ref>, StructSiam <ref type="bibr" target="#b59">[61]</ref>, DSiam <ref type="bibr" target="#b19">[21]</ref>, ECO <ref type="bibr" target="#b8">[9]</ref>, STRCF <ref type="bibr" target="#b32">[34]</ref> and TRACA <ref type="bibr" target="#b6">[7]</ref>).</p><p>The results, evaluated using success plot, are demonstrated in Fig <ref type="figure">5 (a)</ref>. We observe that, our MART achieves the best performance with 0.571 success score, outperforming the second best DiMP <ref type="bibr" target="#b2">[3]</ref> by 0.3%. In comparison to ATOM <ref type="bibr" target="#b7">[8]</ref> with 0.523 success score, we obtain considerable gains by 4.8%. Our MART outperforms SiamRPN++ <ref type="bibr" target="#b30">[32]</ref> by 7.5% in success plot. In addition, compared to multi-step regression approach for tracking in C-RPN <ref type="bibr" target="#b16">[18]</ref> with 0.455 success score, our tracker reveals clear improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment on TC-128</head><p>TC-128 <ref type="bibr" target="#b36">[38]</ref> consists of 128 fully annotation colorful videos. Following <ref type="bibr" target="#b36">[38]</ref>, we employ success plot in one-pass evaluation (OPE) for evaluation. We compare our MART to 9 state-of-the-art trackers (DiMP <ref type="bibr" target="#b2">[3]</ref>, SiamRPN++ <ref type="bibr" target="#b30">[32]</ref>, ATOM <ref type="bibr" target="#b7">[8]</ref>, SiamFC <ref type="bibr" target="#b1">[2]</ref>, ECO <ref type="bibr" target="#b8">[9]</ref>, C-COT <ref type="bibr" target="#b11">[12]</ref>, PTAV [17], DeepSRDCF <ref type="bibr" target="#b10">[11]</ref> and HCF <ref type="bibr" target="#b39">[41]</ref>), and the results are shown in Fig. <ref type="figure">5 (b</ref>). From Fig. <ref type="figure">5</ref> (b), we observe that, our MART achieves the best result with success score of 0.621, outperforming the second best DiMP <ref type="bibr" target="#b2">[3]</ref> with 0.609 success score by 1.2%. In comparison with ATOM that applies only spatial feature for tracking and achieves 0.590 success score, our approach obtains significant gains of 3.1%, showing the advantages of spatial-temporal representation in robust localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiment on OTB-2015</head><p>OTB-2015 <ref type="bibr" target="#b57">[59]</ref> contains 100 fully annotated sequences. We employ success plot metric in OPE to assess different algorithms. We compare our proposed MART to 12 state-ofthe-art trackers (DiMP <ref type="bibr" target="#b2">[3]</ref>, SiamRPN++ <ref type="bibr" target="#b30">[32]</ref>, ATOM <ref type="bibr" target="#b7">[8]</ref>, C-RPN <ref type="bibr" target="#b16">[18]</ref>, DaSiamRPN <ref type="bibr" target="#b61">[63]</ref>, SiamRPN <ref type="bibr" target="#b31">[33]</ref>, Grad-Net <ref type="bibr" target="#b33">[35]</ref>, SA-Siam <ref type="bibr" target="#b20">[22]</ref>, ACT <ref type="bibr" target="#b4">[5]</ref>, SiamFC <ref type="bibr" target="#b1">[2]</ref>, ECO-HC <ref type="bibr" target="#b8">[9]</ref> and TRACA <ref type="bibr" target="#b6">[7]</ref>), and the results are shown in Fig. <ref type="figure">5 (c)</ref>.</p><p>We can see from Fig. <ref type="figure">5</ref> (c), our approach achieves competitive result with success score of 0.678 compared to SiamRPN++ <ref type="bibr" target="#b30">[32]</ref> and DiMP <ref type="bibr" target="#b2">[3]</ref>. In comparison with ATOM that applies only spatial feature for tracking and achieves 0.655 success score, our approach obtains significant gains of 2.3%, showing the advantages of spatialtemporal representation in robust localization. C-RPN <ref type="bibr" target="#b16">[18]</ref> proposes a cascade architecture that employs multi-step regressions for scale estimation, and obtains 0.663 success score. Different from <ref type="bibr" target="#b16">[18]</ref>, our multi-step regressions are guided by the IoU score, and outperform C-RPN <ref type="bibr" target="#b16">[18]</ref> by 2.5% in terms of precision and success plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiment on VOT-2019</head><p>VOT-2019 <ref type="bibr" target="#b28">[30]</ref> contains 60 sequences which are developed by replacing 12 less representative videos in VOT-2018 <ref type="bibr" target="#b27">[29]</ref> with more challenging ones. Similar to VOT-2018, each tracker is evaluated with EAO, accuracy and robustness. We compare our MART with several recent topperformance trackers from VOT-2019, and Tab. 2 demonstrates the results. DiMP <ref type="bibr" target="#b2">[3]</ref> achieves the best EAO of 0.379. Our MART obtains promising result with EAO of 0.356, which significantly outperforms ATOM <ref type="bibr" target="#b7">[8]</ref> with EAO of 0.292 and SiamRPN++ <ref type="bibr" target="#b30">[32]</ref> with EAO of 0.285.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Experiment</head><p>To validate the effect of different components, we conduct ablation experiments on LaSOT tst <ref type="bibr" target="#b15">[16]</ref> regarding the target localization and target estimation.</p><p>Tab. Figure <ref type="figure">5</ref>. Comparisons of our MART and other state-of-the-art trackers on LaSOT <ref type="bibr" target="#b15">[16]</ref>, TC-128 <ref type="bibr" target="#b36">[38]</ref> and OTB-2015 <ref type="bibr" target="#b57">[59]</ref>. Our method achieves the best results on LaSOT and TC-128 and performs favorably against many trackers on OTB-2015. adding RNN (i.e., ConvGRU <ref type="bibr" target="#b0">[1]</ref>) for temporal representation, the performance is improved to 0.539, showing the advantage of leveraging temporal cue for tracking. To effectively enhance spatial-temporal representation, we propose to apply a matching based displacement attention module to capture motion for feature alignment in RNN, and push the SUC score to 0.565 with significant gain of 2.5%, which suggests the importance of feature alignment for robust representation. By incorporating contextual information, the SUC score is further improved by 0.6% from 0.565 to 0.571. The ablative experiments in Tab. 3 clearly evidence the effectiveness of our MA-RNN. Tab. 4 demonstrates the results of our algorithm with different strategies for target scale estimation. As shown in Tab. 4, with one step of bounding box regression, we achieve 0.546 SUC score. When directly adding more regressions, the SUC score is improved to 0.567 while the speed is decreased from 42 fps to 26 fps. Unlike direct use of multi-step regression, we develop an adaptive regression approach under the guidance of IoU prediction, and achieve better performance with 0.571 SCU score and faster speed with 31 fps. Besides, we compare our mBBR with IoU prediction in <ref type="bibr" target="#b7">[8]</ref> that estimates scale via iterative forward and backward passes. When replacing mBBR with IoU prediction network, we observe slight gain of 0.3% in SUC score from 0.571 to 0.574 while significant speed decrease from 31 fps to 23 fps, showing more balanced performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we explore spatial-temporal representation for visual tracking. In specific, we introduce novel motionaware recurrent neural networks to simultaneously capture motion dynamics of target and align spatial temporal features, leading to more effective representation. Based on spatial-temporal representation, we develop an online classification model for target localization. In addition, for target scale estimation, we introduce a monotonic multi-step regression approach that utilizes the IoU prediction score to guide bounding box regression in each step. Integrating the target localization and scale estimation, our tracker MART achieves state-of-the-art results on five benchmark and runs in real-time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Misalignment caused by target motion. The yellow patches in (b) and (c) have the same spatial positions with yellow patch in (a) while they do not semantically align with it. Instead, the green patches in (b) and (c) with slight displacements are semantically better aligned with the yellow patch in (a).</figDesc><graphic url="image-1.png" coords="2,130.46,72.45,75.61,75.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The MART tracking pipeline, including target localization and scale estimation. The localization branch, based on our MA-RNNs, provides target position to generate candidate proposals, which are sent to mBBR for scale estimation.</figDesc><graphic url="image-4.png" coords="3,481.33,123.18,62.52,62.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a)), which ignores the misalignment problem caused by motion dynamics in videos (see Fig.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison between feature aggregation in existing RNN (e.g., ConvGRU<ref type="bibr" target="#b0">[1]</ref>) and our MA-RNN. Instead of directly aggregating features, we leverage CADA module to align and guide feature aggregation for better spatial-temporal representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Illustration of BBR-IOU network that regresses target bounding box and performs IoU prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, BBR-IoU takes as inputs the reference image I 1 , the initial bounding box B 1 , the test image I T and a candidate bounding box B c , and outputs a 4d regression offset vector (r 1 , r 2 , r 3 , r 4 ) and IoU score O c for regressed box B ′ c (B ′ c is obtained by applying regression offsets to B c ). To train the BBR-IoU network, we use the follow loss function,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 5 /</head><label>15</label><figDesc>Monotonic Bounding Box Regression 1 Input: I 1 , B 1 , I T , B c , θ IoU , N reg , trained model BBR-IoU; 2 Output: Final refined target bounding box B; 3 B 1 c = B c , B = B c ; 4 for i = 1 to N reg do *regression and IoU prediction*/ 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on GOT-10k<ref type="bibr" target="#b23">[25]</ref>. The best two scores are highlighted in red and blue colors, respectively.</figDesc><table><row><cell>Tracker</cell><cell>AO</cell><cell cols="2">SR0.50 SR0.75</cell></row><row><cell cols="2">ECO-HC [9] 0.286</cell><cell>0.276</cell><cell>0.096</cell></row><row><cell cols="2">CFNet [53] 0.293</cell><cell>0.265</cell><cell>0.087</cell></row><row><cell cols="2">MDNet [42] 0.299</cell><cell>0.303</cell><cell>0.099</cell></row><row><cell cols="2">HCF [41] 0.315</cell><cell>0.297</cell><cell>0.088</cell></row><row><cell cols="2">ECO [9] 0.316</cell><cell>0.309</cell><cell>0.111</cell></row><row><cell cols="2">SiamFC [2] 0.348</cell><cell>0.353</cell><cell>0.098</cell></row><row><cell cols="2">SPM [54] 0.513</cell><cell>0.593</cell><cell>0.359</cell></row><row><cell cols="2">ATOM [8] 0.556</cell><cell>0.634</cell><cell>0.402</cell></row><row><cell cols="2">DiMP-50 [3] 0.611</cell><cell>0.717</cell><cell>0.492</cell></row><row><cell cols="2">MART 0.628</cell><cell>0.732</cell><cell>0.504</cell></row><row><cell cols="3">4.1. Experiment on GOT-10k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref> shows the ablation experiments on target localization. Without any temporal information aggregation, the baseline tracker achieves 0.531 success (SUC) score. When</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell cols="7">Success plots of OPE on LaSOT tst</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="5">Success plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="5">Success plots of OPE</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Success rate</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MART [0.571] DiMP-50 [0.568] ATOM [0.523] SiamRPN++ [0.496] C-RPN [0.455] MDNet [0.397] SiamDW [0.385]</cell><cell>Success rate</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MART [0.621] DiMP-50 [0.609] ECO [0.597] ATOM [0.590]</cell><cell>Success rate</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiamRPN++ [0.696] DiMP-50 [0.688] MART [0.678] C-RPN [0.663] DaSiamRPN [0.658] SA-Siam [0.656] ATOM [0.655]</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.336]</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">C-COT [0.567]</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO-HC [0.643]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">StructSiam [0.335]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN++ [0.564]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiamRPN [0.637]</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSiam [0.333]</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.544]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ACT [0.625]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO [0.324]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DeepSRDCF [0.536]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GradNet [0.625]</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">STRCF [0.308]</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.497]</cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TRACA [0.603]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TRACA [0.257]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HCF [0.482]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiamFC [0.582]</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">(a) Comparisons on LaSOT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Comparisons on TC-128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(c) Comparisons on OTB-2015</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons on VOT-2019<ref type="bibr" target="#b28">[30]</ref>. The best two scores are highlighted in red and blue colors, respectively.</figDesc><table><row><cell cols="3">Tracker EAO Accuracy Robustness</cell></row><row><cell>DCFST [30] 0.361</cell><cell>0.589</cell><cell>0.321</cell></row><row><cell>SiamCRF [30] 0.330</cell><cell>0.625</cell><cell>0.296</cell></row><row><cell>SPM [54] 0.275</cell><cell>0.577</cell><cell>0.507</cell></row><row><cell>SiamRPN++ [32] 0.285</cell><cell>0.599</cell><cell>0.482</cell></row><row><cell>SiamDW [62] 0.299</cell><cell>0.600</cell><cell>0.467</cell></row><row><cell>ATOM [8] 0.292</cell><cell>0.603</cell><cell>0.411</cell></row><row><cell>DiMP-50 [3] 0.379</cell><cell>0.594</cell><cell>0.278</cell></row><row><cell>MART 0.356</cell><cell>0.607</cell><cell>0.362</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on target localization with spatial-temporal representation.</figDesc><table><row><cell>Component</cell><cell cols="2">MART</cell><cell></cell></row><row><cell>RNN</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Displacement Attention</cell><cell></cell><cell>✓</cell><cell>✓</cell></row><row><cell>Contextual Information</cell><cell></cell><cell></cell><cell>✓</cell></row><row><cell cols="4">SUC (%) 53.1 53.9 56.5 57.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on target scale estimation with different strategies.</figDesc><table><row><cell></cell><cell>IoU net-</cell><cell></cell><cell>mBBR</cell><cell></cell></row><row><cell></cell><cell>work [8]</cell><cell cols="3">One step wo / IoU w / IoU</cell></row><row><cell>SUC (%)</cell><cell>57.4</cell><cell>54.6</cell><cell>56.7</cell><cell>57.1</cell></row><row><cell>Speed (fps)</cell><cell>23</cell><cell>42</cell><cell>26</cell><cell>31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The code will be available at https://hengfan2010.github. io/projects/MART/MART.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is supported in part by NSF Grants IIS-2006665 and IIS-1814745.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2008">2016. 1, 2, 3, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>David S Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">A</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yui Man</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time&apos;actor-critic&apos;tracking</title>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-aware deep feature compression for high-speed visual tracking</title>
		<author>
			<persName><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyeoup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Heng Fan and Haibin Ling</publisher>
			<date type="published" when="2017">2019. 2, 6, 7, 8 [17. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2019. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName><forename type="first">Anfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2016. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>João F Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2018 challenge results</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2019 challenge results</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
				<imprint>
			<date type="published" when="2008">2019. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 5, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradnet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep visual tracking: Review and experimental comparison</title>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="338" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Anthony Dick, and Anton Van Den Hengel. A survey of appearance models in visual object tracking</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2007">2015. 1, 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><forename type="middle">M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning spatial-aware regressions for visual tracking</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Roi pooled correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parametric exponential linear unit for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Gigu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brahim</forename><surname>Chaib-Draa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spm-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2019. 2, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Understanding and diagnosing visual tracking systems</title>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning attentions: residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Object tracking benchmark</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 5, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
