<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-10">10 November 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaohua</forename><surname>Huang</surname></persName>
							<email>xiaohua.huang@ee.oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Learning Science</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<region>JiangSu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>gyzhao@ee.oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Hong</surname></persName>
							<email>xhong@ee.oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
							<email>wenming_zheng@seu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Learning Science</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<region>JiangSu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Child Development and Learning Science (Ministry of Education)</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<region>JiangSu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-10">10 November 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">FDCA47E5CA3CDFE1A3843924F4FEAD85</idno>
					<idno type="DOI">10.1016/j.neucom.2015.10.096</idno>
					<note type="submission">Received 24 April 2015 Received in revised form 26 September 2015 Accepted 27 October 2015 Communication by Su-Jing Wang</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Micro-expression LBP-TOP Vector quantization Discriminative</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spontaneous facial micro-expression analysis has become an active task for recognizing suppressed and involuntary facial expressions shown on the face of humans. Recently, Local Binary Pattern from Three Orthogonal Planes (LBP-TOP) has been employed for micro-expression analysis. However, LBP-TOP suffers from two critical problems, causing a decrease in the performance of micro-expression analysis. It generally extracts appearance and motion features from the sign-based difference between two pixels but not yet considers other useful information. As well, LBP-TOP commonly uses classical pattern types which may be not optimal for local structure in some applications. This paper proposes SpatioTemporal Completed Local Quantization Patterns (STCLQP) for facial micro-expression analysis. Firstly, STCLQP extracts three interesting information containing sign, magnitude and orientation components. Secondly, an efficient vector quantization and codebook selection are developed for each component in appearance and temporal domains to learn compact and discriminative codebooks for generalizing classical pattern types. Finally, based on discriminative codebooks, spatiotemporal features of sign, magnitude and orientation components are extracted and fused. Experiments are conducted on three publicly available facial micro-expression databases. Some interesting findings about the neighboring patterns and the component analysis are concluded. Comparing with the state of the art, experimental results demonstrate that STCLQP achieves a substantial improvement for analyzing facial micro-expressions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Micro-expression is a subtle and involuntary facial expression. It usually occurs when a person is consciously trying to conceal all signs of how he is feeling. Unlike regular facial expressions, it is difficult to hide micro-expression reaction. As a result, the importance of micro-expression study is apparent in many potential applications for the security field. Psychological research shows that facial micro-expressions generally remain less than 0.2 s, as well they are very subtle <ref type="bibr" target="#b9">[8]</ref>. Short duration and subtle change causes human difficulties in recognizing facial microexpressions. In order to improve this ability, the microexpression training tool developed by Ekman and his team was used to train people to better recognize micro-expressions. Even so, human can achieve just around 40% recognition accuracy <ref type="bibr" target="#b11">[10]</ref>.</p><p>Therefore, there is a great need for a high-quality automatic system to recognize facial micro-expressions.</p><p>Some earlier studies on automatic facial micro-expression analysis have primarily focused on posed or synthetic facial micro-expressions <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30]</ref>. Recently, researchers have conducted spontaneous facial micro-expression analysis <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b44">43]</ref>. In contrast to posed facial micro-expressions, spontaneous facial micro-expressions can reveal genuine emotions that people try to conceal. It is very challenging to extract useful information from subtle changes of micro-expressions. It is noted that geometrybased or appearance-based feature extraction method has been commonly employed to analyze facial expressions. Specifically, geometric-based features represent the face geometry, such as the shapes and locations of facial landmarks, but they are sensitive to global changes, such as pose change and illumination variation. Instead, appearance-based features describe the skin texture of faces. Among these methods, Local Binary Pattern from Three Orthogonal Planes (LBP-TOP) has demonstrated its simplicity and efficiency for facial expression recognition <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b51">50]</ref>. As a result, LBP-TOP has been widely used in micro-expression analysis <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b7">6]</ref>.</p><p>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Pfister et al. <ref type="bibr" target="#b28">[27]</ref> proposed a spontaneous micro-expression recognition method using LBP-TOP. Yan et al. <ref type="bibr" target="#b43">[42]</ref> applied LBP-TOP on their CASME 2 database achieving micro-expression recognition rate of 63.41%. As well, other works used LBP-TOP to investigate whether micro-facial movement sequences can be different from neutral face sequences <ref type="bibr" target="#b7">[6]</ref>.</p><p>However, it should be noted that there is still a gap to achieve a high-quality micro-expression analysis. Consequently, several works have attempted to improve the LBP-TOP. Ruiz-Hernandez and Pietikäinen <ref type="bibr" target="#b33">[32]</ref> used the re-parameterization of second order Gaussian jet on the LBP-TOP achieving promising microexpression recognition result on the SMIC database <ref type="bibr" target="#b28">[27]</ref>. Wang et al. <ref type="bibr" target="#b38">[37]</ref> extracted Tensor features from Tensor Independent Color Space (TICS) for micro-expression recognition, but their results on the CASME 2 database showed no improvement if we compare their highest achievable accuracies with the previous results. Furthermore, Wang et al. <ref type="bibr" target="#b39">[38]</ref> used Local Spatiotemporal Directional Features (LSDF) with robust principal component analysis for micro-expressions, but yet did not obtain improvement for micro-expression analysis. In addition, recent work <ref type="bibr" target="#b40">[39]</ref> reduced redundant information of LBP-TOP by using Six Intersection Points (LBP-SIP) to obtain better performance than the LBP-TOP. Even so, there is still much room for improvement in the recognition performance.</p><p>In our preliminary work <ref type="bibr" target="#b18">[17]</ref>, Completed Local Quantized Pattern (CLQP) was proposed by using the completed information and vector quantization to improve the performance of the original LQP proposed by Hussain and Triggs <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19]</ref>. It achieved considerable results on texture classification and neonatal facial expression classification tasks. This paper proposes Spatio-Temporal Completed Local Quantized Pattern (STCLQP) by extending our spatial domain approach <ref type="bibr" target="#b18">[17]</ref> for micro-expression analysis. In this work, STCLQP exploits three useful information, including sign-based, magnitude-based and orientation-based difference of pixels. Furthermore, STCLQP designs compact and discriminative codebooks for spatiotemporal domain. Different from our preliminary work, this work considers a more discriminative codebook and an application in spatiotemporal domain.</p><p>To explain the concepts of our approach, the paper is organized as follows. Section 2 discusses recent related work. Section 3 describes the completed local quantized patterns (CLQP) in spatial domain. Section 4 provides the extension of CLQP to spatiotemporal domain and explains its implementation to microexpression analysis. Section 5 discusses parameter settings, datasets and provides experimental results with relevant discussion. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The video feature extraction problem has been addressed from different perspectives. Some works describe a video clip from the shape features. In <ref type="bibr" target="#b25">[24]</ref>, Liu et al. proposed a sketch-based method to organize video clip, where they used sketch annotations to enhance the narrations. The sketch-based approach achieves context-aware sketch recommendation for video shape extraction. In <ref type="bibr" target="#b0">[1]</ref>, Belongie et al. proposed the shape context using a circular local pattern and histogram to measure similarity between shapes. In <ref type="bibr" target="#b35">[34]</ref>, Shin and Chun used eighteen major feature points defined in MPEG 4, and then applied the dense optical flow method to track the feature points for sequential frames. In <ref type="bibr" target="#b21">[20]</ref>, Jain et al. used the shape feature around the eyebrows, eyes, nose, chin, inner lips and outer lips to describe the feature of each frame in video clip.</p><p>On the other hand, there are a few works to use a texture descriptor to describe the appearance and motion features of the video clip. As we know, Gabor and Local Binary Pattern (LBP) are two most representative ones for facial expression recognition. Since Gabor feature is related to the perception in human visual system, some facial expression recognition systems to date have utilized the Gabor energy filters <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b48">47]</ref>. Instead, LBP is simple to implement, fast to compute and has led to high accuracy in texture-based recognition tasks <ref type="bibr" target="#b27">[26]</ref>. In recent years significant progress has been made in using LBP for facial expression recognition <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b34">33]</ref>. Due to its simplicity, the LBP operator was further extended to video sequences <ref type="bibr" target="#b51">[50]</ref>, named the LBP from Three Orthogonal Planes (LBP-TOP) of a space time volume. Basically, the LBP-TOP description is formed by calculating the LBP features from the planes and concatenating the histograms. In addition, LBP-TOP has efficient computation. Thus, it has become attractive for researchers in many fields.</p><p>In recent years, many extensions of LBP-TOP are developed in an application such as human action recognition <ref type="bibr" target="#b26">[25]</ref> and lip reading <ref type="bibr" target="#b1">[2]</ref>. Local Ternary Pattern from Three Orthogonal Planes (LTP-TOP) proposed by Nanni et al. <ref type="bibr" target="#b26">[25]</ref> quantized intensity differences of neighboring pixels and center pixel into three levels to increase the robustness against noise. However, the LTP-TOP is sensitive to the quantization levels. In <ref type="bibr" target="#b1">[2]</ref>, for increasing the robustness against intensity noise, Local Ordinal Contrast Pattern (LOCP) used a pairwise ordinal contrast measurement of pixels from a circular neighborhood starting at the center pixel.</p><p>Recently, Completed Local Binary Pattern from Three Orthogonal Planes (CLBP-TOP) was proposed to exploit the useful information from intensity level <ref type="bibr" target="#b29">[28]</ref>. In their work, completed local binary pattern <ref type="bibr" target="#b14">[13]</ref> was extended into temporal domain. Magnitude and center pixel are severed as the complementary to the LBP-TOP, increasing the robustness of the LBP-TOP against noise. However, central pixel intensity information is very sensitive to noises caused by such things as illumination changes <ref type="bibr" target="#b52">[51]</ref>. Alternatively, several researches <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b17">16]</ref> have demonstrated that orientation is useful because of its robustness to illumination changes. In <ref type="bibr" target="#b49">[48]</ref>, Zhang et al. proposed the Histogram of Gabor Phase Pattern, in which they combine the spatial histogram and the Gabor phase information coding schemes. In <ref type="bibr" target="#b41">[40]</ref>, Xie et al. proposed local Gabor XOR patterns, which encodes the Gabor phase by using local XOR pattern. In <ref type="bibr" target="#b37">[36]</ref>, Vu and Caplier presented multiple features combining patterns of oriented edge magnitude and patterns of dominant orientations. Therefore, it begs a question if orientation would be more effective for LBP-TOP.</p><p>Furthermore, LBP-TOP and CLBP-TOP inherit from the sparse sampling problem, yielding inadequate spatiotemporal descriptors. In fact, LBP histograms with small a number of bins tend to fail to provide enough discriminative information about the image appearance <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b47">46]</ref>. Instead, as the number of increases, more discriminative information will be provided, although this will cause the number of local patterns to increase exponentially. For example, with 8 sampling points around each pixel, there are 256 possible local patterns, while with 16 sampling points the dimensionality of histogram is 65,536. More importantly, the histogram becomes extremely sparse given a limited number of pixels. For example, it is observed that only half of the 65,536 local patterns in the case of 16-point sampling occur in spontaneous micro-expression database <ref type="bibr" target="#b23">[22]</ref>.</p><p>To address the sparseness problem, some specific codebooks were designed to reduce the number of possible codes and make the resulting histogram compact and evenly distributed. For example, in <ref type="bibr" target="#b27">[26]</ref>, Ojala et al. presented a type of codebook, namely 'uniform pattern', which consists of several binary patterns containing at most two bitwise transitions from 0 to 1 or vice versa when the bit pattern is traversed circularly. Although similar methods to uniform coding are well-established to reduce the dimensionality, they are at best limited palliatives for a serious problem, in which the codebook size is exponentially grown with the number of local sampling points and quantization depth. Furthermore, it is uncertain that uniform coding is generalized for non-circular and multi-circular structures. Thus, it wonders if another compact method can be developed to provide appropriate codebook for various sizes and expressiveness of local pattern representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Completed local quantized pattern in spatial domain</head><p>To address the sparseness problem, many typical coding methods, such as uniform patterns <ref type="bibr" target="#b27">[26]</ref>, have been designed by considering rotation and scale invariant properties. However, with number of sampling points increasing, these coding methods are not optimal and compact. Our preliminary work <ref type="bibr" target="#b18">[17]</ref> proposed Completed Local Quantization Pattern (CLQP) to create a more robust LBP. This approach consists of three stages: (1) component extraction, (2) a codebook obtained by using vector quantization, and (3) local pattern encoding. Fig. <ref type="figure" target="#fig_0">1</ref> shows the procedure for CLQP. For briefly description, Table <ref type="table" target="#tab_0">1</ref> describes the representation of the used mathematical symbols in CLQP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Component extraction</head><p>We suppose that an image can be represented as fξ x;y g, where ξ x;y is the gray-level intensity or orientation angle of pixel ðx; yÞ. For a spatial coordinate ðx; yÞ, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the local pattern can be formulated as follows:</p><p>x ! ¼ ½f ðξ x;y ; ξ x 1 ;y 1 Þ; f ðξ x;y ; ξ x 2 ;y 2 Þ; …; f ðξ x;y ; ξ x P ;y P Þ; ð1Þ where f ðξ x;y ; ξ x i ;y i Þ is the formula comparing values such as graylevel intensities of two pixels, ðx i ; y i Þ is the neighbor sampling points of ðx; yÞ, and P is the number of neighbor sampling points. For ðx i ; y i Þ, it can be sampled as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. This section will in detail describe the extraction of f ðξ x;y ; ξ x i ;y i Þ, which involves the orientation-based, sign-based and magnitude-based difference operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Orientation-based difference extraction</head><p>Essentially, orientation-based difference extraction aims to encode two dominant orientations between two pixels in an image. This scheme consists of three stages. Firstly, it calculates dominant orientation of the pixel. Secondly, it quantifies dominant orientation into T levels. Finally, it compares dominant orientation of the central pixel with its neighbor pixels.</p><p>(1) Dominant orientation of the pixel: To date, several methods, such as an image gradient <ref type="bibr" target="#b5">[4]</ref> and Sobel filters <ref type="bibr" target="#b13">[12]</ref>, can be used to calculate dominant orientation of each pixel. Gabor filters and Gaussian recursive transformation have been recently proposed to obtain dominant orientation of each pixel <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b49">48]</ref>. In their methods, orientation-estimation filters are designed by using a mask with various size.</p><p>Following <ref type="bibr" target="#b12">[11]</ref>, a set of N Gaussian kernels is designed to estimate dominant orientation of a pixel. Given a mask patch with D Â D size, the n-th Gaussian kernel is defined as the difference between two oriented with shifted kernels as follows:</p><formula xml:id="formula_0">G θn ¼ G À θn À G þ θn P x;y ½ðG À θn ÀG þ θn Þ Á hðG À θn ÀG þ θn Þ ;<label>ð2Þ</label></formula><p>where  </p><formula xml:id="formula_1">G À θn ¼ 1 2πσ 2 exp À ðx À σ cos θ n Þ 2 þðy À σ sin θ n Þ 2 2σ 2 ! ; G þ θn ¼ 1 2πσ 2 exp À ðx þ σ cos θ n Þ 2 þðyþ σ sin θ n Þ 2 2σ 2 ! ; hðG À θn À G þ θn Þ ¼ 1; G À θn À G þ θn 4 0 0; G À θn À G þ θn r 0 8 &lt; : ;</formula><formula xml:id="formula_2">θ n ¼ n Â 2π N ; n ¼ 0; …; N À 1.</formula><p>The response of Gaussian kernel defines the contrast magnitude of a local edge at its pixel location. The dominant orientation of pixel ðx; yÞ is obtained with the orientation of a kernel that gave the maximum response,</p><formula xml:id="formula_3">θ x;y ¼ arg max θn X p;q g x À p;y À q G θn ;<label>ð3Þ</label></formula><p>where g x À p;y À q is gray-level intensity of a pixel ðx À p; yÀ qÞ in an image, p; q</p><formula xml:id="formula_4">¼ À⌊ D 2 c; À⌊ D 2 cþ1; …; ⌊ D 2 cÀ1; ⌊ D 2 c È É ,<label>and</label></formula><formula xml:id="formula_5">⌊ D 2 c means the floor of D 2 .</formula><p>(2) Quantification of dominant orientation: With Gaussian kernels, we obtain a new image containing dominant orientation. We suppose that dominant orientations of the center pixel ðx; yÞ and its neighbors ðx i ; y i Þ are θ x;y and θ x i ;y i , respectively. Generally, we could compute the difference of orientation angle <ref type="bibr" target="#b36">[35]</ref> and subsequently code it as 0=1 by setting a threshold value. However, it is difficult to find the optimal threshold value. Instead, some works <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b41">40]</ref> have quantified phase or orientation into several levels, making it easy to compare the relationship between dominant orientations. Thus, the quantification function is used to quantify dominant orientation:</p><formula xml:id="formula_6">ξ ¼ mod θ x;y 2π T þ 0:5 6 6 6 6 4 7 7 7 7 5; T 0 B @ 1 C A;<label>ð4Þ</label></formula><p>where T is number of quantification level.</p><p>(3) Relationship of dominant orientations: Following LBP, we aim to exploit the relationship of a pixel and its surrounding neighbors on orientation-quantified image. The dominant orientation bin of pixel ðx; yÞ and its surrounding pixels ðx i ; y i Þ are denoted as ξ and ξ i , respectively. Thus, their relationship is calculated as follows:</p><formula xml:id="formula_7">f ðξ x;y ; ξ x i ;y i Þ ¼ ξ È ξ i ¼ 0; ξ ¼ ξ i 1; ξaξ i ( ;<label>ð5Þ</label></formula><p>where P is the number of the sampling points. Discussion: In previously mentioned procedure, N and T are important to orientation based difference extraction. The dominant orientation estimation is sensitive to N. Small N will make orientation estimation inaccurate. Instead, large N will cause expensive computation. Consequently, appropriate N would provide beneficial performance to orientation estimation. In <ref type="bibr" target="#b41">[40]</ref>, Xie et al. showed that the appropriate quantification level could achieve a balance between robustness to orientation variation and representation power of local patterns. Additionally, in <ref type="bibr" target="#b37">[36]</ref>, Vu et al. discussed how the quantification level could affect the local pattern for the orientation in neighbor structure. Thus, T performs a trade-off role between the robustness to orientation variation and representation power of local patterns. The effect of N and T will be examined in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Sign-based and magnitude-based difference extraction</head><p>The sign-based and magnitude-based information are important to a face descriptor. Let ξ x i ;y i ði ¼ 1; …; PÞ denote the gray-level intensity of P sampling points around ðx; yÞ. Following <ref type="bibr" target="#b14">[13]</ref>, the difference between the center pixel and its surrounding pixel is calculated as d i ¼ ξ x i ;y i À ξ x;y . It is further decomposed into sign and magnitude as follows:</p><formula xml:id="formula_8">d i ¼ signðd i Þnj d i j ;<label>ð6Þ</label></formula><p>where</p><formula xml:id="formula_9">signðd i Þ ¼ 1; d i Z 0 0; d i o 0</formula><p>( is the sign of d i and j d i j is the magnitude of d i .</p><p>The sign pattern of ðx; yÞ has the same formulation (binary) as the LBP operator. It can be represented as ½S 1 ; …; S P . For a magnitude pattern ½M 1 ; …; M P , it is simply converted into a consistent format with that of a sign pattern by a threshold δ. Here we set it as the mean value of j d i j from the whole image. The magnitude pattern can be denoted as</p><formula xml:id="formula_10">M i ¼ 1; j d i j Zδ 0; j d i j oδ ( .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Formulation of components</head><p>Through the extraction, for a pixel ðx; yÞ, three components are obtained as the formulations of ½O 1 ; …; O P , ½S 1 ; …; S P and ½M 1 ; …; M P for orientation, sign and magnitude, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Efficient vector quantization</head><p>Hussain and Triggs <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19]</ref> applied vector quantization to learn codebook for various local pattern neighborhoods, such as Disc 5 (in Fig. <ref type="figure" target="#fig_1">2(b</ref>)), since vector quantization produces compact codebook to different applications. Motivated by their work, we adopt vector quantization to obtain codebooks for three components. For convenient purpose, we take the orientation component as an example for interpreting our method.</p><p>Given training images, all local orientation patterns are denoted as x q ! ðq ¼ 1; …; Q Þ, where Q is the total number of pixels from these images. The vector quantization aims to quantize them into K quantized depths. K-means clustering is a common way in vector quantization for generating representative "visual words" <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b19">18]</ref>. Theoretically, the k-means clustering partitions all observations into a set Ψ n ¼ fΨ 1 ; Ψ 2 ; …; Ψ K g as to minimize the withincluster sum of squares as follows: </p><formula xml:id="formula_11">Ψ n ¼ arg min Ψ X K k ¼ 1 X x ! q A Ψ k J x ! q À μ k J 2 ;<label>ð7Þ</label></formula><formula xml:id="formula_12">Ψ n ¼ arg min b Ψ X K k ¼ 1 X y ! j A b Ψ k J w j y ! j À b μ k J 2 :<label>ð8Þ</label></formula><p>Another important issue related to the efficiency of k-means is to initialize clustering centers. Different choices of an initialization substantially affect the speed of divergence of training procedure and also the performance. Instead of random sampling, we exploit the dominant local patterns, i.e., the most frequently occurred patterns, as initialization. In the implementation, we firstly sort local patterns by the descending order of occurrence, and secondly select the first K local patterns as the initialization of the clustering centers. Our preliminary work <ref type="bibr" target="#b18">[17]</ref> had demonstrated that objective function in Eq. ( <ref type="formula" target="#formula_12">8</ref>) converged very fast.</p><p>In order to guarantee fast implementation, a codebook is offline built by mapping unique local patterns Y ¼ ½ y ! 1 ; y ! 2 ; …; y ! J to the nearest clustering centers. Motivated by <ref type="bibr" target="#b27">[26]</ref>, we propose to use a 'miscellaneous' label to make codebook robust and compact. In the implementation of this label, we take the following steps:</p><p>(1) we set the threshold λ to categorize local patterns into genuine and fake ones; (2) for genuine one, we assign the label of the closest cluster to it; and (3) for fake one, we assign an extra label ðK þ 1Þ to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local pattern encoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typically, local pattern x</head><p>! can be encoded by a predefined codebook Ω, such as 'uniform pattern' <ref type="bibr" target="#b27">[26]</ref> or our learned codebook by vector quantization. Fig. <ref type="figure" target="#fig_2">3</ref> describes the procedure encoding a local pattern by using a codebook Ω on a facial image.</p><p>Generally, a facial image is divided into B blocks. For the b-th block, we can obtain local orientation pattern ½O 1 ; …; O P for a pixel ðx; yÞ. Based on the codebook Ω, its corresponding index can be quickly searched. For example, the local pattern '0000000' is programmed by its corresponding index '5'. The same procedure is applied to all pixels, thus formulating all local orientation patterns into a histogram H b . For the facial image, histograms H b ðb ¼ 1; …; BÞ from B blocks are concatenated into one feature vector H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extension to the spatiotemporal domain</head><p>In the previous section, the CLQP features are constructed for static-image analysis and obtained acceptable results on texture classification. Recently, LBP-TOP has been proposed for microexpression analysis <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b23">22]</ref>. It combines motion and appearance information together by using three orthogonal planes, as shown in Fig. <ref type="figure">4</ref>. Motivated by LBP-TOP, we intend to extend CLQP to the spatiotemporal domain for dynamic micro-expression analysis, named SpatioTemporal Completed Local Quantized Pattern (STCLQP). In this work, STCLQP resolves the problem that the typical coding method may be not optimal to the LBP-TOP. On the other hand, STCLQP makes local patterns more compact and discriminative by developing codebooks based on Fisher criterion.</p><p>The method is described in Fig. <ref type="figure" target="#fig_5">5</ref>. It consists of four stages: (1) components extraction, (2) using vector quantization to obtain codebook (including local pattern pool and vector quantization), (3) local pattern encoding and (4) discriminative and compact codebook selection. The component extraction method (in Section 3.1) and an efficient vector quantization (in Section 3.2) are employed to the first and second stages, respectively. In this section, we explain in detail the last two stages. For convenient purpose, we take the orientation component as an example for interpreting our method. The same procedure is applied to sign and magnitude components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Local pattern encoding in the spatiotemporal domain</head><p>In spatiotemporal domain, LBP-TOP considers to use three orthogonal planes for representing an image sequences. In original LBP-TOP, 'uniform pattern' <ref type="bibr" target="#b27">[26]</ref> is commonly utilized to encode all three planes. Due to this, it is noted that in the implementation of the STCLQP, three codebooks are required for the orientation component. We denote codebooks as Ω XY , Ω XT , and Ω YT for XY, XT and YT planes.</p><p>Assume that a video clip is divided into B blocks, for all pixels in volume, we can obtain local patterns x ! XY for XY plane. With the codebook Ω XY , it is easy to search the corresponding index for x ! XY . The same method is utilized to all pixels in XY plane. Finally, the indexes are statistically formulated as one histogram. Histograms of all blocks are concatenated into one feature H XY , which represents the appearance feature of facial images.</p><p>For two other planes (XT and YT), histograms are extracted to the same way of XY plane. For STCLQP, histograms from three orthogonal planes are concatenated into one feature vector. In summary, local pattern encoding procedure in the spatiotemporal domain is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discriminative and compact codebook selection</head><p>Previously mentioned in Section 3.2, the weighted k-mean method produces an efficient and fast way to vector quantization. However, the weighted k-mean method did not consider the discriminative information. In order to design discriminative codebook, Fisher criterion and local pattern encoding are employed to each plane. In this section, we intend to produce Ω K n ;η for the plane η, where ηAfXY; XT; YTg, and K n means the optimal clustering size for the plane η. Here, the procedure on XY plane is taken as an example. The same way are also utilized to two other planes. When there is no ambiguity, η is omitted for clarity.</p><p>We suppose that we have a set of codebook size ½K 1 ; …; K V . For the codebook size K v , K v A fK 1 ; …; K V g, its corresponding codebook Ω Kv is calculated by using vector quantization. With local pattern encoding method (in Section 4.1), we can obtain its corresponding features:</p><formula xml:id="formula_13">H ¼ ½H 1 ; …; H B ;<label>ð9Þ</label></formula><p>where B is number of blocks, and </p><formula xml:id="formula_14">H b A R Kv .</formula><formula xml:id="formula_15">m I;b ¼ 1 C X C c ¼ 1 2 L c ðL c À 1Þ X Lc i ¼ 2 X i À 1 j ¼ 1 dðH c;i b ; H c;j b Þ;<label>ð10Þ</label></formula><formula xml:id="formula_16">s 2 I;b ¼ X C c ¼ 1 X Lc i ¼ 2 X i À 1 j ¼ 1 ðdðH c;i b ; H c;j b ÞÀm I;b Þ 2 ;<label>ð11Þ</label></formula><p>where H c;j b denotes the histogram extracted from the j-th sample </p><formula xml:id="formula_17">m E;b ¼ 2 CðC À 1Þ X C À 1 u ¼ 1 X C v ¼ u þ 1 1 L u L v X Lu i ¼ 1 X Lv j ¼ 1 dðH u;i b ; H v;j b Þ;<label>ð12Þ</label></formula><formula xml:id="formula_18">s 2 E;b ¼ X C À 1 u ¼ 1 X C v ¼ u þ 1 X Lu i ¼ 1 X Lv j ¼ 1 ðdðH u;i b ; H v;j b ÞÀm E;b Þ 2 :<label>ð13Þ</label></formula><p>Finally, the Fisher score for the codebook Ω Kv can be computed by:</p><formula xml:id="formula_19">w ¼ X B b ¼ 1 ðm I;b Àm E;b Þ 2 s 2 I;b þ s 2 I;b :<label>ð14Þ</label></formula><p>With Fisher criterion, the local histogram features are discriminative, if the means of intra and extra classes are far apart and the variances are small. For each codebook size, we obtain Fisher weight by using the above-mentioned method. The optimal codebook size K n and its corresponding codebook Ω K n are chosen with respect to the maximal Fisher weights. Our proposed approach is described in Algorithm 2.</p><p>Algorithm 2. Discriminative and compact codebook selection for motion and spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation of STCLQP</head><p>Given a video sequence, through component extraction (in Section 3.1), we can obtain sign, magnitude and orientation components. For the sign component, compact and discriminative codebook Ω η of each plane is obtained by using an efficient vector quantization and codebook selection. Subsequently, local patterns are mapped into Ω η , and finally generate features H η . For sign component, features of three planes H XY , H XT and H YT are concatenated into one feature vector H ðSÞ . Features from magnitude and orientation components, H ðMÞ and H ðOÞ , are extracted to the same way of the sign component.</p><p>Concatenation of histograms is commonly used to combine histograms of three components. However, it would bring curse of dimensionality to classification. Alternatively, we intend to use feature-level fusion on feature subspace. In our scheme, we apply supervised locality preserving projection <ref type="bibr" target="#b16">[15]</ref> to sign, magnitude and orientation components, thus obtaining feature space U S , U M and U O for sign, magnitude and orientation components, respectively. Therefore, the final feature vectors from sign (S), magnitude (M) and orientation (O) components can be formulated as,</p><formula xml:id="formula_20">H STCLQP ¼ ½U 0 S H ðSÞ ; U 0 M H ðMÞ ; U 0 O H ðOÞ :<label>ð15Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the proposed method, we exploit the sign-based, magnitude-based and orientation-based information (in Section 3.1). For the orientation, it is found that the number of Gaussian kernels and quantification level are two critical parameters. Furthermore, the codebook size (i.e., the number of clustering K in Section 4.2) and number of local pattern sampling (i.e., P and R as shown in Fig. <ref type="figure" target="#fig_1">2</ref>) are very important to STCLQP. In the experiments, Spontaneous Micro-expression Corpus (SMIC) <ref type="bibr" target="#b23">[22]</ref>, CASME <ref type="bibr" target="#b42">[41]</ref> and CASME 2 <ref type="bibr" target="#b43">[42]</ref> databases are used to evaluate the performance of STCLQP. Preliminary results of CLQP on texture classification and neonatal facial expression recognition were presented in <ref type="bibr" target="#b18">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Micro-expression database</head><p>The SMIC database consists of 16 subjects (6 females and 10 males) with 164 spontaneous micro-expressions (positive, negative and surprise) which were recorded in a controlled scenario using 100 fps camera with resolution of 640 Â 480. Here, two tasks in SMIC database are investigated: micro-expression detection (micro/non-micro) and recognition (positive/negative/surprise).</p><p>The CASME dataset contains spontaneous 1500 facial movements filmed under 60 fps camera. Among them, 195 microexpressions were coded so that the first, peak and last frames were tagged. Referring to the work of <ref type="bibr" target="#b42">[41]</ref>, we select 171 facial micro-expression videos that contain disgust, surprise, repression and tense micro-expressions.</p><p>The CASME 2 database includes 247 spontaneous facial microexpressions recorded by a 200 fps camera. These samples are coded with the onset and offset frames, as well as tagged with AUs and emotion. There are five classes of the micro-expressions in this database: happiness, surprise, disgust, repression and others. One task in CASME 2 database is investigated: micro-expression recognition for five classes.</p><p>In order to distinguish each task on three databases, we denote micro-expression detection and micro-expression recognition on the SMIC database as 'detection' and 'pos/neg/sur', respectively, while for micro-expression recognition on the CASME and CASME 2 as '4-class' and '5-class', respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental setup and protocol</head><p>We evaluate the proposed approach by using a leave-onesubject-out validation in all databases, in which samples from one subject are used as test and the rest are used as training. In all experiments, facial landmarks of facial images are detected by using Active Shape Models <ref type="bibr" target="#b6">[5]</ref>, and then all facial images are normalized and cropped into the same size. The temporal interpolation method <ref type="bibr" target="#b50">[49]</ref> is used to interpolate the frames of the high speed videos into 10 frames. All facial images are divided into 8 Â 8 blocks for 'pos/neg/sur' (SMIC), '4-class' (CASME) and '5-class' (CASME 2), while 5 Â 5 blocks for 'detection' (SMIC). For a fair comparison, we use Support Vector Machine with linear kernel as classifier in all experiments <ref type="bibr" target="#b2">[3]</ref>. The optimal value of the cost parameter was determined using the grid search strategy, where optimal values of cost parameter were searched exponentially in the range of ½10 À 6 ; 10 6 . Following <ref type="bibr" target="#b42">[41]</ref>, mean recognition accuracy is used to measure the performance. Since mean recognition accuracy would allow for false bias, Area Under the Curve (AUC) <ref type="bibr" target="#b15">[14]</ref> and F1-measure <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b45">44]</ref> for multi-class problem are also considered as measurements to evaluate the parameters of STCLQP and compare algorithms. For F1-measurement, it is defined as</p><formula xml:id="formula_21">F ¼ 1 C P C i ¼ 1 2p i Âr i p i þ r i</formula><p>, where p i and r i are the precision and recall of the i-th class, respectively, and C are the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of orientation parameters</head><p>So far many works can be exploited to approximate dominant orientation θ in image processing. Specifically, θ can be calculated by arctan hxnI hynI , where h x and h y are filters used to approximate the differentiation operator along the image horizontal and vertical direction respectively. Possible choices for h x and h y include central difference estimators of various orders and discrete approximations to the first derivative of the Gaussian. In the experiments, we compare Gaussian filter with two commonly used filters for h x and h y . One is an image gradient, where h x ¼ ½À101 and h y ¼ ½À101 T , the other is Sobel operator with two 3 Â 3 kernels <ref type="bibr" target="#b13">[12]</ref>.</p><p>For Gaussian filter, T and N are two important parameters in orientation pattern. T plays a critical role in computing the neighborhood relationship for all filters. It also decides the quantification level for dominant orientation. In experiments, we test three quantization levels of 2, 4 and 8. On the other hand, N is the key parameter in Gaussian filter, since dominant orientation estimation is dependent on N. We evaluate the effect of N on the performance when N ¼ 4, 8 and 16. In order to suppress the influence of codebook, we use 'uniform pattern' <ref type="bibr" target="#b27">[26]</ref> to encode local patterns for orientation component.</p><p>Table <ref type="table" target="#tab_3">2</ref> shows recognition accuracy by using different dominant orientation estimation methods. These results give some interesting findings as follows:</p><p>(1) As seen from this (2) It is found that using more Gaussian filters can increase recognition rates of all tasks. However, the deeper quantization level T cannot improve the performance, since features will be more sparse. In the implementation of orientation, it is found that the performance of Gaussian filters achieves promising results when T is 4. (3) Using bigger mask patch size (Gaussian-Mask5) for Gaussian filters can contribute better performance to 'detection', 'pos/ neg/sur', '4-class' and '5-class' than Gaussian-Mask3, because using bigger mask patch size can capture sufficient pixel information for more accurate dominant orientation estimation.</p><p>These results demonstrate that Gaussian filter's strategy achieves better performance than Gradient filter and Sobel filter. They further show that Gaussian filter's method can be appropriately used in estimating domain orientation. In the following experiments, the number of Gaussian filters N and the quantization level T are set to 16 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of an efficient vector quantization</head><p>The purpose of this experiment aims to investigate how vector quantization (in Section 3.2) affects the performance of STCLQP in micro-expression analysis. Since the codebook size is an important factor in vector quantization, we mainly investigate the influence of codebook size and compare our proposed efficient vector quantization with the typical coding method, 'uniform pattern'. For convenience, we use 8 neighborhood sampling pixels around the center pixel with radius as 3 (i.e., <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b9">8)</ref> as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>). Here, we set the same codebook size for all codebooks on three orthogonal planes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Influence of codebook size</head><p>Fig. <ref type="figure" target="#fig_6">6</ref> clearly shows the performance of three components with different codebook sizes for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. From this figure, we have some findings as follows:</p><p>(1) For the sign component (in Fig. <ref type="figure" target="#fig_6">6(a)</ref>), the highest accuracy is achieved with the increasing codebook size. And they are 67.38%, 57.93%, 40.94% and 50.20% when the codebook size is set to 30, 40, 50 and 70 for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. (2) For the magnitude component (in Fig. <ref type="figure" target="#fig_6">6(b</ref>)), it is noted that the codebooks with the large codebook size have better performance for 'detection' and '5-class', while the codebook with the small codebook size offers promising result for 'pos/neg/ sur'. The accuracies are 65.24%, 41.46%, 44.44% and 44.53% when the codebook size is set to 50, 10, 10 and 80 for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. (3) For the orientation component (in Fig. <ref type="figure" target="#fig_6">6(c</ref>)), we find that the influence of codebook size is similar to sign component for 'detection' and '5-class'. For 'pos/neg/sur', the performance is achieved when the codebook size is 60; with the increasing codebook size, the performance is remained. From Fig. <ref type="figure" target="#fig_6">6</ref>(c), it can be seen that the results are 72.87%, 56.10%, 36.84% and 48.99% when the codebook size is set to 20, 70, 60 and 50 for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. The results using vector quantization are shown in Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">4</ref>. As seen from Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">4</ref>, it is found that the optimal codebook size stays in range of <ref type="bibr">[10 80</ref>], when sign, magnitude and orientation components achieve the best performance. It further shows that the smallest codebook size cannot represent the statistical property of all bins, while large size could provide abundant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Comparison to 'uniform pattern'</head><p>For demonstrating vector quantization, we compare our method with 'uniform pattern' in this section. We use 'uniform pattern' <ref type="bibr" target="#b27">[26]</ref> to encode the sign, magnitude and orientation components, respectively. The recognition accuracy and F1 score are shown in Tables <ref type="table" target="#tab_7">5</ref> and<ref type="table">6</ref>, respectively.</p><p>Comparing Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_7">5</ref>, it is seen that for sign component, the increasing rates of recognition accuracies using vector quantization are 0.61%, 4.27%, 3.51% and 3.74% for 'detection', 'pos/neg/ sur', '4-class' and '5-class', respectively. As seen from Tables <ref type="table" target="#tab_5">4</ref> and<ref type="table">6</ref>, F1 scores have been increased at the increasing rate of 0.0424, 0.0051, 0.0627 and 0.0455 for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively.</p><p>For magnitude component, the recognition accuracies using vector quantization is increased at 2.44%, 1.83%, 5.26% and 4.85% for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. F1 scores are increased at 0.021, 0.0237, 0.1172 and 0.0476 for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. Specifically for '4-class' F1 score is substantially increased.  For orientation component, using vector quantization obtains promising performance at the increasing recognition rate of 2.14%, 4.89% and 4.68% for 'detection', 'pos/neg/sur' and '4-class', respectively. F1 scores are increased at 0.0337, 0.0207 and 0.0587 for 'detection', 'pos/neg/sur' and '4-class', respectively. Furthermore, we show the recognition rate of vector quantization for sign, magnitude and orientation, when codebook size is 60 in Table <ref type="table" target="#tab_8">7</ref>. Comparing with Table <ref type="table" target="#tab_7">5</ref>, even using the similar number for both encoding methods, at most of cases, vector quantization still performs better than 'uniform pattern'. These comparisons show that vector quantization can make original LBP-TOP better. Furthermore, they show STCLQP with small codebook size achieves better performance than 'uniform pattern'. The small codebook size can make system fast and save storage cost. For example, in the implementation, for sign component, the classification and local pattern encoding phases take average 2.3 ms and 2.12 ms for each sample using codebook, respectively, while using 'uniform pattern' they take about 6.78 ms and 5.201 ms, respectively, when they are carried out on Malab 2013a with Intel Core i5 3.10 GHz. For storage, the dimension of sign component using 'uniform pattern' is 11,328 (8 Â 8 Â 3 Â 59), while using codebook is 7680 (8 Â 8 Â 3 Â 40). Therefore, an advantage of vector quantization is to reduce the dimension and get more compact features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation of discriminative codebook selection</head><p>As previously mentioned in Section 4.2, the proposed STCLQP can optimally choose the discriminative codebook for each plane and each component. Next, we further examine the performance using codebook selection. The detailed results in three tasks based on recognition accuracy are described as follows:</p><p>( From comparative results in four tasks, we can see that discriminative codebook selection can considerably raise the performance of three components. This can partly be explained by the present codebooks, which can provide discriminative information to the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Effect of various neighboring sampling</head><p>In this experiment, we aim to investigate the effect of local pattern neighborhood, which is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. For parameter setup, we employ discriminative vector quantization and take orientation component into consideration.</p><p>We use 8 sampling pixels around a center pixel with radius as 3 (i.e., <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b9">8)</ref>) as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), the recognition accuracies of 'detection', 'pos/neg/sur', '4-class and '5-class' are 73.78%, 57.93%, 40.94% and 50.20%, respectively. While we increase the number of sampling points yet keep the same radius (i.e., <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b17">16)</ref>), the recognition accuracies are raised by 0.3%, 0.61%, 0.31% and 1.1% for 'detection', 'pos/neg/sur', '4-class and '5-class' respectively. Furthermore, Disc 5 (24 sampling points, in Fig. <ref type="figure" target="#fig_1">2(b</ref>)) is employed. The performance is further improved by 0.56%, 2.02% and 0.88% for 'pos/neg/ sur', '4-class' and '5-class' respectively. Therefore, increasing sampling points boosts the performance of micro-expression analysis. One reason is that more sampling points provide more compact information and extract sufficient structure around the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Analysis of components</head><p>Tables <ref type="table">8</ref> and<ref type="table" target="#tab_10">9</ref> show results for different components of STCLQP using Disc 5 . It is found that sign and orientation components performs better than magnitude component in the most of cases except on CASME database, because CASME has distinct characteristic of low video rate and little motion intensity change. The results of using only M were constantly lower than S þM in the most of cases. The best results for STCLQP are achieved by using all three components. This finding agrees with Pfister <ref type="bibr" target="#b29">[28]</ref> and Guo <ref type="bibr" target="#b14">[13]</ref> who found magnitude and sign useful for face analysis and texture classification. It is interesting to note that even though the tasks are quite different (texture recognition on visual data and spontaneous micro-expression analysis) the results of the component division experiments follow the same pattern. In general, SþM þO yields the best results, followed closely by S þO, SþM and MþO, and more distantly by S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Algorithm comparison</head><p>Previously mentioned, STCLQP achieves the best accuracies of 75.31%, 64.02%, 57.31% and 58.39% for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. In this section, we compare STCLQP with LBP-TOP, CLBP-TOP, local ordinary contrast pattern (LOCP-TOP) <ref type="bibr" target="#b1">[2]</ref>, spatiotemporal local monogenic binary pattern (STLMBP) <ref type="bibr" target="#b17">[16]</ref>, spatiotemporal cuboids descriptor (Cuboids) <ref type="bibr">[7]</ref> and LBP-SIP <ref type="bibr" target="#b40">[39]</ref>.</p><p>Results on recognition accuracy, average F1-score and mean AUC on three databases are reported in Tables 10-12, respectively. As can be seen from three tables, cuboid feature method performs poorly in four tasks. Instead, spatiotemporal feature descriptors work better than cuboid features.</p><p>(1) We can see that STCLQP outperforms CLBP-TOP, although both methods employ sign and magnitude components. STCLQP alternatively uses codebook to replace 'uniform pattern', so that it can obtain more compact and discriminative information than CLBP-TOP. In addition, orientation component is included in STCLQP.</p><p>(2) As well, we can see that STCLQP beats STLMBP. As we know, STLMBP exploits the similar way to obtain orientation and sign features. But one main difference is STLMBP first used monogenic filter to extract new images. Comparing with STLMBP and CLBP-TOP, we see that using codebook can strengthen micro-expression features. (3) We compare STCLQP with LBP-SIP <ref type="bibr" target="#b40">[39]</ref> on the three databases.</p><p>It is found that our method appears to outperform LBP-SIP on all databases. In STCLQP, it exploits more information while LBP-SIP aims to improve efficiency of LBP-TOP using intersect neighborhoods.</p><p>We provide confusion matrices for four tasks on three databases using STCLQP, as shown in Fig. <ref type="figure">7</ref>. On micro-expression detection task, STCLQP easily detects most of micro-expression videos out of non-micro expression videos. For micro-expression recognition on SMIC database, STCLQP achieves the lowest accuracy for recognizing 'positive' class, and STCLQP has the same performance for recognizing 'negative' and 'surprise' classes.</p><p>On CASME and CASME 2 databases, micro-expression classes become more complicated and diversity, for example, CASME 2 database contains five classes. From the confusion matrix on CASME dataset, it is found that STCLQP works the best on recognizing 'disgust', followed closely by 'tense', 'repression' and 'surprise'. Among four classes, 'repression' and 'tense' are two easily confused micro-expressions for STCLQP. Furthermore, it is found that samples from 'disgust' class is falsely classified to 'tense' class. On CASME 2 dataset, 'repression' and 'disgust' are the most difficult micro-expressions to recognize, while 'surprise' and 'others' classes are easy to recognize. It is found that samples of 'happiness', 'disgust' and 'repression' are falsely classified into 'others' micro-expression category. In <ref type="bibr" target="#b43">[42]</ref>, Yan et al. annotated microexpression video clips that cannot be labeled according to FACS into one class. This annotation may cause STCLQP falsely classifying 'happiness', 'disgust' and 'repression' into 'others' class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.">Discussion</head><p>Overall, for spontaneous micro-expressions, the effects of codebook size, sampling structure and components are examined on three spontaneous micro-expression databases. Tables 10-12 compare the proposed method with other methods under different performance evaluation metric. We see that STCLQP provides considerably improvement for recognizing spontaneous micro-expressions.</p><p>Moreover, the codebook size substantially affects the performance of STCLQP. It is also dependent on the neighboring structures. Fortunately, less bins in a codebook can achieve better accuracy than by using uniform pattern (59 bins). The advantage is that it automatically learns the statistical patterns from the dataset. In other words, the codebook is specific to the dataset. We further prove that the discriminative codebook selection strategy can substantially raise the performance in recognizing spontaneous micro-expressions. It also avoids from the manual parameter selection from many parameters. Additionally, we also see that STCLQP appears to be explicitly improved by using more local pattern neighborhoods.</p><p>Additionally, we report the effect of all components and different combination to micro-expression analysis. It is interesting to find that all three components can improve the performance of spontaneous micro-expression analysis. We also find that the orientation component can perform better than sign component in some cases, for example, in micro-expression detection. This is first time to combine those three components for LBP-TOP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In recent years, facial micro-expression analysis has been an active and challenging research in psychology and computer vision, because the durations of micro-expressions are very short and only subtle changes are involved. We proposed a spatiotemporal completed local quantized patterns, which exploits more useful information and learns compact and discriminative codebook for micro-expression analysis. In our approach, orientation component is considered as complementary to sign and magnitude components for LBP features. Furthermore, an efficient vector quantization and Fisher criterion are utilized to obtain the compact and discriminative codebook. Finally, features of three components are concatenated into one feature based on feature subspace. We demonstrated the proposed feature descriptor is efficient and can provide advantageous performance by comparing with other methods on three spontaneous micro-expression databases. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of completed local binary pattern: (1) Three kinds of information (local sign, magnitude and orientation patterns) are extracted from the image. (2) Three separate codebooks are learned by using vector quantization, where S, M, and O are referred to sign, magnitude and orientation, respectively. (3) The sign, magnitude and orientation patterns are mapped into their corresponding codebook by using a codebook, and three histograms are concatenated into one vector.</figDesc><graphic coords="3,82.77,553.83,420.02,161.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of commonly used local pattern neighborhoods: (a) The circular neighborhoods ðR; PÞ with P sampling points ðx i ; y i Þ and radius R around the central point ðx; yÞ, and (b) multi-circle neighborhoods Disc 5 with 24 sampling points, where the black point represents a central point ðx; yÞ and the red ones are local sampling points ðx i ; y i Þ. The pixel values are bi-linearly interpolated whenever the sampling point is not in the center of a pixel. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc><graphic coords="4,140.37,58.62,324.48,168.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of mapping the local pattern into a codebook Ω. Given an image, we use orientation-based difference extraction to obtain local pattern of each pixel. Based on a codebook Ω, local pattern is assigned into its corresponding index. Finally, all local patterns are formulated as one histogram.</figDesc><graphic coords="5,112.82,639.27,359.97,84.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Based on Fisher criterion, the difference of the same class should be small while the difference of the various classes should be large. Here we take H b of the b-th block as an example. For a C class problem, let the similarities of different samples of the same expression compose the intra-class similarity, and those of samples from different expressions compose the extra-class similarity. The mean m I;b and the variance s 2 I;b of intra-class similarities for each block can be computed by as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Algorithm 1 .</head><label>41</label><figDesc>Fig. 4. Procedure of LBP-TOP on micro-expression analysis [50]. (a) Block volume. (b) LBP features from three orthogonal planes. (c) Appearance and motion feature for block volume.</figDesc><graphic coords="6,123.14,58.62,359.06,119.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Spatiotemporal completed local binary pattern: (1) We extract the sign, magnitude and orientation components for each plane (one of XY, XT and YT), and formulate the local pattern pool. (2) Fixing the number of clustering K, k-means clustering method is employed to quantize the local pattern pool for obtaining the codebook. (3) Based on specific codebook, the pool can be encoded. (4) The encoded features are further used to calculate the Fisher score w Kη for the number of clusterings, where η is XY, XT and YT. Fisher scores are used to choose the discriminative and compact codebook for each component in each plane. For convenience, here we take the sign component as an example.</figDesc><graphic coords="8,92.52,58.62,420.23,168.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Recognition accuracies of sign, magnitude and orientation components with different codebook size K (in Section 4.2), where 'detection' and 'pos/neg/sur' are conducted on SMIC, while '4-class' on CASME and '5-class' on CASME 2.</figDesc><graphic coords="10,90.65,58.62,423.83,399.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Xiaohua</head><label></label><figDesc>Huang received the B.S. degree in communication engineering from Huaqiao University, Quanzhou, China, in 2006. He received his Ph.D. degree in Computer Science and Engineering from University of Oulu, Oulu, Finland, in 2014. He has been a scientist researcher in the Center for Machine Vision Research, Department of Computer Science and Engineering, University of Oulu since 2015. He has authored or co-authored more than 20 papers in journals and conferences, and has served as a reviewer for journals and conference. His current research interests include facial expression recognition, micro-expression analysis, grouplevel emotion recognition, multi-modal emotion recognition and texture classification. Guoying Zhao received the Ph.D. degree in computer science from the Chinese Academy of Sciences, Beijing, China, in 2005. She is currently an Associate Professor with the Center for Machine Vision Research, University of Oulu, Finland, where she has been a researcher since 2005. In 2011, she was selected to the highly competitive Academy Research Fellow position. She has authored or co-authored more than 110 papers in journals and conferences, and has served as a reviewer for many journals and conferences. She has lectured tutorials at ICPR 2006, ICCV 2009, and SCIA 2013, and authored/edited three books and two special issues in journals. Zhao was a Co-Chair of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,82.83,58.62,419.75,292.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Representation of mathematical symbols in CLQP.</figDesc><table><row><cell>Symbol</cell><cell>Representation</cell></row><row><cell>ðx; yÞ</cell><cell>Coordinate of pixel</cell></row><row><cell>ξ</cell><cell>The gray-level intensity or orientation angle of a pixel</cell></row><row><cell>f ðÁ; ÁÞ</cell><cell>The formula comparing values of two pixels</cell></row><row><cell>P</cell><cell>The number of neighbor sampling points</cell></row><row><cell>G</cell><cell>Gaussian kernel</cell></row><row><cell>θ</cell><cell>The dominant orientation of a pixel</cell></row><row><cell>N</cell><cell>The number of Gaussian kernels</cell></row><row><cell>T</cell><cell>The number of quantification level</cell></row><row><cell>K</cell><cell>The number of codebook size</cell></row><row><cell>Ω</cell><cell>The codebook</cell></row><row><cell>x !</cell><cell>The local pattern</cell></row><row><cell>H</cell><cell>A histogram of one facial block</cell></row><row><cell>B</cell><cell>The number of blocks in facial image</cell></row><row><cell>H</cell><cell>The feature of facial image</cell></row><row><cell>S</cell><cell>The sign component</cell></row><row><cell>M</cell><cell>The magnitude component</cell></row><row><cell>O</cell><cell>The orientation component</cell></row></table><note><p>and σ is a root mean square deviation of Gaussian distribution, ðx; yÞ is the coordinate of cell patch, and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where μ k is the mean of local patterns in Ψ k , and K is the number of clustering centers.It is found that k-means clustering usually requires much time and huge memory size in training procedure. In our case, local patterns that occur several times are calculated repeatedly in clustering. For example, '00000000' occurs 20 times in an image. It therefore leads to great redundancy in calculation. To address this problem, we introduce a weight W of local patterns for k-means clustering. In specific, we can obtain J unique local patterns Y ¼</figDesc><table><row><cell>½ y ! 1 ; y ! 2 ; …; y !</cell></row></table><note><p><p><p>J and the number of occurrences of local pattern W ¼ ½w 1 ; w 2 ; …; w J , where JðJ⪡Q Þ is the number of local patterns, for example, J ¼ 256 for 8-point sampling. Eq. (</p>7</p>) consequently becomes c</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mask5) yields the highest accuracies of 70.73%, 51.21%, 32.16% and 50.20% for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively, when the number of Gaussian filters N and quantization level T are set to 16 and 4, respectively.</figDesc><table /><note><p>table, Gradient filter's method obtains the accuracies of 67.38%, 50.61%, 33.33% and 43.72% for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively, while for Sobel filter's approach, 64.94%, 50%, 32.16% and 38.06% for 'detection', 'pos/neg/sur', '4-class' and '5-class', respectively. Instead, our method using Gaussian filters with Mask5 (Gaussian-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Recognition accuracy of local orientation pattern using various filters, where we compare Gaussian filter with Gradient and Sobel filters. N and T represent the level of orientation estimation and quantization level, respectively. Gaussian-Mask3 and Gaussian-Mask5 represent Gaussian kernels are designed based on a mask patch with 3 Â 3 and 5 Â 5 pixel sizes, respectively.</figDesc><table><row><cell>Filter</cell><cell>[N, T]</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Gradient</cell><cell>½À; 2</cell><cell>55.18</cell><cell>41.46</cell><cell>27.49</cell><cell>37.25</cell></row><row><cell></cell><cell>½À; 4</cell><cell>60.67</cell><cell>50.61</cell><cell>32.75</cell><cell>43.72</cell></row><row><cell></cell><cell>½À; 8</cell><cell>67.38</cell><cell>42.68</cell><cell>33.33</cell><cell>42.92</cell></row><row><cell></cell><cell>½À; 16</cell><cell>64.63</cell><cell>41.46</cell><cell>30.99</cell><cell>41.30</cell></row><row><cell>Sobel</cell><cell>½À; 2</cell><cell>56.40</cell><cell>43.29</cell><cell>29.24</cell><cell>36.44</cell></row><row><cell>½À; 4</cell><cell>58.84</cell><cell>50.00</cell><cell>27.49</cell><cell>30.37</cell><cell></cell></row><row><cell>½À; 8</cell><cell>64.33</cell><cell>43.90</cell><cell>30.99</cell><cell>30.77</cell><cell></cell></row><row><cell>½À; 16</cell><cell>64.94</cell><cell>38.41</cell><cell>32.16</cell><cell>38.06</cell><cell></cell></row><row><cell>Gaussian-Mask3</cell><cell>½4; 2</cell><cell>64.02</cell><cell>39.63</cell><cell>29.24</cell><cell>43.72</cell></row><row><cell>½8; 2</cell><cell>58.85</cell><cell>40.24</cell><cell>26.32</cell><cell>37.65</cell><cell></cell></row><row><cell>½8; 4</cell><cell>66.46</cell><cell>48.78</cell><cell>30.41</cell><cell>45.75</cell><cell></cell></row><row><cell>½16; 4</cell><cell>66.77</cell><cell>49.39</cell><cell>31.58</cell><cell>49.80</cell><cell></cell></row><row><cell>½16; 8</cell><cell>70.73</cell><cell>46.34</cell><cell>31.58</cell><cell>40.08</cell><cell></cell></row><row><cell>Gaussian-Mask5</cell><cell>½4; 2</cell><cell>63.72</cell><cell>42.68</cell><cell>25.73</cell><cell>38.46</cell></row><row><cell>½8; 2</cell><cell>58.84</cell><cell>42.68</cell><cell>26.32</cell><cell>36.03</cell><cell></cell></row><row><cell>½8; 4</cell><cell>68.90</cell><cell>48.78</cell><cell>29.82</cell><cell>47.37</cell><cell></cell></row><row><cell>½16; 4</cell><cell>70.73</cell><cell>51.21</cell><cell>32.16</cell><cell>50.20</cell><cell></cell></row><row><cell>½16; 8</cell><cell>67.99</cell><cell>46.34</cell><cell>29.24</cell><cell>43.32</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Recognition accuracy (%) of sign, magnitude and orientation components encoded by using vector quantization. The number in the bracket is codebook size.</figDesc><table><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign</cell><cell>67.38 (30)</cell><cell>57.93 (40)</cell><cell>40.94 (50)</cell><cell>50.20 (70)</cell></row><row><cell>Magnitude</cell><cell>65.24 (50)</cell><cell>41.46 (10)</cell><cell>44.44 (10)</cell><cell>44.53 (80)</cell></row><row><cell>Orientation</cell><cell>72.87 (20)</cell><cell>56.10 (70)</cell><cell>36.84 (60)</cell><cell>48.99 (50)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 F1</head><label>4</label><figDesc></figDesc><table><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign</cell><cell>0.6675 (30)</cell><cell>0.5808 (40)</cell><cell>0.386 (50)</cell><cell>0.4696 (70)</cell></row><row><cell>Magnitude</cell><cell>0.6506 (50)</cell><cell>0.4183 (10)</cell><cell>0.3997 (10)</cell><cell>0.3982 (80)</cell></row><row><cell>Orientation</cell><cell>0.7262 (20)</cell><cell>0.5518 (70)</cell><cell>0.2945 (60)</cell><cell>0.4261 (50)</cell></row></table><note><p>score of sign, magnitude and orientation components encoded by using vector quantization. The number in the bracket is codebook size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1)</ref> In 'detection', the results are reported as 68.29%, 65.85%, and 73.78% for sign, magnitude and orientation components, respectively. Using fixed codebook size can obtain 67.38%, 65.24% and 72.87% for sign, magnitude and orientation components, respectively.(2) In the task 'pos/neg/sur', the results are reported as 59.10%, 43.29%, and 57.93% for sign, magnitude and orientation com-</figDesc><table><row><cell>ponents, respectively, while using fixed codebook size can</cell></row><row><cell>only achieve the recognition rate of 57.93%, 41.46% and 56.10%</cell></row><row><cell>for sign, magnitude and orientation components, respectively.</cell></row><row><cell>(3) In '4-class', the results are reported as 42.11%, 46.78%, and</cell></row><row><cell>40.94% for sign, magnitude and orientation components,</cell></row><row><cell>respectively. One can obtain 40.94%, 44.44% and 36.84% using</cell></row><row><cell>fixed codebook size for sign, magnitude and orientation</cell></row><row><cell>components, respectively.</cell></row><row><cell>(4) In '5-class', the results are reported as 52.55%, 44.94%, and</cell></row><row><cell>50.20% for sign, magnitude and orientation components,</cell></row><row><cell>respectively. One can obtain 50.20%, 44.53% and 48.99% using</cell></row><row><cell>fixed codebook size for sign, magnitude and orientation</cell></row><row><cell>components, respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Recognition accuracy (%) of sign, magnitude and orientation components encoded by using 'uniform pattern'<ref type="bibr" target="#b27">[26]</ref>.</figDesc><table><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign</cell><cell>66.77</cell><cell>53.66</cell><cell>37.43</cell><cell>46.46</cell></row><row><cell>Magnitude</cell><cell>62.80</cell><cell>39.63</cell><cell>39.18</cell><cell>39.68</cell></row><row><cell>Orientation</cell><cell>70.73</cell><cell>51.21</cell><cell>32.16</cell><cell>50.20</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">F1 score of sign, magnitude and orientation components encoded by using 'uniform</cell></row><row><cell>pattern' [26].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign</cell><cell>0.6624</cell><cell>0.5384</cell><cell>0.3233</cell><cell>0.4241</cell></row><row><cell>Magnitude</cell><cell>0.6269</cell><cell>0.3973</cell><cell>0.2825</cell><cell>0.3506</cell></row><row><cell>Orientation</cell><cell>0.7055</cell><cell>0.5181</cell><cell>0.2358</cell><cell>0.4383</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Recognition accuracy (%) of sign, magnitude and orientation components encoded by using vector quantization, where codebook size is 60.</figDesc><table><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign</cell><cell>66.46</cell><cell>56.10</cell><cell>38.01</cell><cell>48.99</cell></row><row><cell>Magnitude</cell><cell>64.63</cell><cell>39.02</cell><cell>38.01</cell><cell>42.91</cell></row><row><cell>Orientation</cell><cell>72.56</cell><cell>55.49</cell><cell>36.84</cell><cell>46.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc>F1 score using spatiotemporal feature descriptors on SMIC, CASME and CASME 2, where we reproduce results of LBP-SIP using leave-one-subject-out protocol.</figDesc><table><row><cell>Methods</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>LBP-TOP [50]</cell><cell>0.6624</cell><cell>0.5384</cell><cell>0.3233</cell><cell>0.4241</cell></row><row><cell>STLMBP [16]</cell><cell>0.6681</cell><cell>0.5439</cell><cell>0.4458</cell><cell>0.4142</cell></row><row><cell>LOCP-TOP [2]</cell><cell>0.6155</cell><cell>0.5135</cell><cell>0.2259</cell><cell>0.3879</cell></row><row><cell>CLBP-TOP (S þ M) [28]</cell><cell>0.6838</cell><cell>0.5569</cell><cell>0.3895</cell><cell>0.5285</cell></row><row><cell>Cuboids [7]</cell><cell>0.6033</cell><cell>0.3032</cell><cell>0.2228</cell><cell>0.2754</cell></row><row><cell>LBP-SIP [39]</cell><cell>0.5434</cell><cell>0.4492</cell><cell>0.3327</cell><cell>0.4480</cell></row><row><cell>STCLQP</cell><cell>0.7402</cell><cell>0.6381</cell><cell>0.5</cell><cell>0.5836</cell></row><row><cell>Table 12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Area under the curve (%) using spatiotemporal feature descriptors on SMIC, CASME</cell></row><row><cell cols="5">and CASME 2, where we reproduce results of LBP-SIP using leave-one-subject-out</cell></row><row><cell>protocol.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>LBP-TOP [50]</cell><cell>66.77</cell><cell>60.65</cell><cell>59.62</cell><cell>64.23</cell></row><row><cell>STLMBP [16]</cell><cell>67.38</cell><cell>57.08</cell><cell>57.64</cell><cell>59.73</cell></row><row><cell>LOCP-TOP [2]</cell><cell>61.59</cell><cell>57.17</cell><cell>56.14</cell><cell>62.96</cell></row><row><cell>CLBP-TOP (S þ M) [28]</cell><cell>69.21</cell><cell>55.69</cell><cell>58.66</cell><cell>65.45</cell></row><row><cell>Cuboids [7]</cell><cell>60.37</cell><cell>54.14</cell><cell>55.52</cell><cell>54.78</cell></row><row><cell>LBP-SIP [39]</cell><cell>55.49</cell><cell>55.62</cell><cell>60.80</cell><cell>58.29</cell></row><row><cell>STCLQP</cell><cell>75.31</cell><cell>67.25</cell><cell>68.93</cell><cell>66.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>F1 score of components and their combination in STCLQP.</figDesc><table><row><cell>Component</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>Sign (S)</cell><cell>0.7005</cell><cell>0.5908</cell><cell>0.4224</cell><cell>0.5143</cell></row><row><cell>Magnitude (M)</cell><cell>0.6699</cell><cell>0.4485</cell><cell>0.4534</cell><cell>0.3781</cell></row><row><cell>Orientation (O)</cell><cell>0.7452</cell><cell>0.5899</cell><cell>0.4122</cell><cell>0.4701</cell></row><row><cell>S þM</cell><cell>0.7070</cell><cell>0.5608</cell><cell>0.5324</cell><cell>0.5366</cell></row><row><cell>S þO</cell><cell>0.7340</cell><cell>0.6186</cell><cell>0.5052</cell><cell>0.5584</cell></row><row><cell>M þ O</cell><cell>0.6867</cell><cell>0.5853</cell><cell>0.53</cell><cell>0.5399</cell></row><row><cell>S þM þ O</cell><cell>0.7402</cell><cell>0.6381</cell><cell>0.56</cell><cell>0.5835</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Comparison under recognition accuracy (%) with spatiotemporal feature descriptors on SMIC, CASME and CASME 2, where we reproduce results of LBP-SIP using leave-one-subject-out protocol.</figDesc><table><row><cell>Methods</cell><cell>SMIC</cell><cell></cell><cell>CASME</cell><cell>CASME2</cell></row><row><cell></cell><cell>Detection</cell><cell>pos/neg/sur</cell><cell>4-class</cell><cell>5-class</cell></row><row><cell>LBP-TOP [50]</cell><cell>66.77</cell><cell>53.66</cell><cell>37.43</cell><cell>46.46</cell></row><row><cell>STLMBP [16]</cell><cell>67.38</cell><cell>54.88</cell><cell>46.20</cell><cell>47.77</cell></row><row><cell>LOCP-TOP [2]</cell><cell>61.59</cell><cell>51.22</cell><cell>31.58</cell><cell>48.91</cell></row><row><cell>CLBP-TOP (Sþ M) [28]</cell><cell>69.21</cell><cell>56.10</cell><cell>45.31</cell><cell>53.28</cell></row><row><cell>Cuboids [7]</cell><cell>60.37</cell><cell>32.32</cell><cell>33.33</cell><cell>36.03</cell></row><row><cell>LBP-SIP [39]</cell><cell>55.49</cell><cell>44.51</cell><cell>36.84</cell><cell>46.56</cell></row><row><cell>STCLQP</cell><cell>75.31</cell><cell>64.02</cell><cell>57.31</cell><cell>58.39</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X.Huang  et al. / Neurocomputing 175 (2016) 564-578</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Academy project by Academy of Finland under Grant 277395, Academy fellow project by Academy of Finland under Grant 251286 and 283125, and Infotech Oulu under Grant 24001161. This work was partly supported by the National Basic Research Program of China under Grant 2015CB351704, the National Natural Science Foundation of China (NSFC) under Grants 61231002 and 61201444, the Natural Science Foundation of Jiangsu Province under Grant BK20130020, the Ph.D. Program Foundation of Ministry Education of China under grant 20120092110054.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local ordinal contrast pattern histograms for spatiotemporal, lip-based speaker authentication</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="602" to="612" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Confusion matrix of each class using STCLQP on (a) &apos;detection&apos; on SMIC, (b) &apos;pos/neg/sur&apos; on SMIC</title>
		<imprint/>
	</monogr>
	<note>4-class&apos; on CASME and (d) &apos;5-class&apos; on CASME 2</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WLD: a robust local image descriptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Micro-facial movements: an investigation on spatio-temporal descriptors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Costen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lansley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leightley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ECCV workshop on Spontaneous Behavior Analysis</title>
		<meeting>eeding of ECCV workshop on Spontaneous Behavior Analysis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="111" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<meeting>IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lie catching and micro expressions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Philosophy of Deception</title>
		<editor>
			<persName><forename type="first">Ed</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Clancy</forename><surname>Martin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="118" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting local binary patterns from image key points: application to automatic facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Scandinavian Conference on Image Analysis</title>
		<meeting>Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I see how you feel: training laypeople and professionals to recognize fleeting emotions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herbasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Meeting of International Communication Association</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature-based detection of facial landmarks from neutral and expressive facial images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gizatdinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Surakka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="139" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="414" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A completed modeling of local binary pattern operator for texture classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1657" to="1663" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple generalisation of the area under the ROC curve for multiple class classification problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Till</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="186" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatiotemporal local monogenic binary patterns for facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="243" to="246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Texture description with completed local quantized patterns</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Scandinavian Conference on Image Analysis</title>
		<meeting>Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual recognition using local quantized patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="716" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Napoleon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Conference on Computer Vision</title>
		<meeting>British Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial expression recognition with temporal modeling of shapes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Correlated label propagation with application to multi-label learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1719" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A spontaneous microexpression database: Inducement, collection and baseline</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression extracted automatically from video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="615" to="625" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A sketch-based approach for interactive organization of video clips</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local ternary patterns from three orthogonal planes for human action clssification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5125" to="5128" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognising spontaneous facial microexpressions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1449" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differentiating spontaneous from posed facial expressions within a generic facial expression recognition framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="868" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facial micro-expressions recognition using high speed camera and 3-D gradient descriptor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Polikovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kameda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Crime Detection and Prevention</title>
		<meeting>International Conference on Crime Detection and Prevention</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection and measurement of facial microexpression characteristics for psychological analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Polikovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kameda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Technical Report</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">98</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<title level="m">Computer Vision using Local Binary Patterns</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Encoding local binary patterns using reparameterization of the second order Gaussian jet</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary pattern: a comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatio-temporal facial expression recognition using optical flow and HMM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Softw. Eng. Artif. Intell. Netw. Parallel/Distrib. Comput</title>
		<imprint>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subspace learning from image gradient orientations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2454" to="2466" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining patterns of orientations and magnitudes for face recognition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Biometrics</title>
		<meeting>International Joint Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Micro-expression recognition using dynamic textures on tensor independent color space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4678" to="4683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Micro-expression recognition using robust principal component analysis and local spatiotemporal directional features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV Workshop on Spontaneous Behavior Analysis</title>
		<meeting>ECCV Workshop on Spontaneous Behavior Analysis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="325" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LBP with six intersection points: reducing redundant information in LBP-TOP for micro-expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="525" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fusing local patterns of gabor magnitude and phase for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1349" to="1361" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CASME database: a dataset of spontaneous micro-expressions collected from neutralized faces</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CASME II: an improved spontaneous micro-expression database and the baseline evaluation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quantifying micro-expressions with constraint local model and local binary pattern</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV Workshop on Spontaneous Behavior Analysis</title>
		<meeting>ECCV Workshop on Spontaneous Behavior Analysis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An evaluation of statistical approaches to text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evaluating bag-of-the-visual-words representations in scene classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Multimedia Information Retrieval</title>
		<meeting>the International Workshop on Multimedia Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="192" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient image appearance description using dense sampling based local binary patterns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ylioinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="375" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Comparison between geometrybased and Gabor-wavelets-based facial expression recognition using multilayer perceptron</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Histogram of Gabor phase patterns (HGPP): a novel object representation approach for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="68" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An image-based visual speech animation system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1420" to="1432" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary pattern with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rotation invariant image and video description with local binary pattern features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1465" to="1467" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
