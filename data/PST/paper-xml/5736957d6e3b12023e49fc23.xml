<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Two-Layer Recurrent Neural Network for Nonsmooth Convex Optimization Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sitian</forename><surname>Qin</surname></persName>
							<email>qinsitian@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoping</forename><surname>Xue</surname></persName>
							<email>xiaopingxue@263.net</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>264209</postCode>
									<settlement>Weihai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Two-Layer Recurrent Neural Network for Nonsmooth Convex Optimization Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CFA4F382D3BF03585B2805AC1E13AEA</idno>
					<idno type="DOI">10.1109/TNNLS.2014.2334364</idno>
					<note type="submission">received April 8, 2013; revised January 11, 2014 and May 24, 2014; accepted June 26, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Global convergence</term>
					<term>Lyapunov function</term>
					<term>nonsmooth convex optimization</term>
					<term>two-layer recurrent neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a two-layer recurrent neural network is proposed to solve the nonsmooth convex optimization problem subject to convex inequality and linear equality constraints. Compared with existing neural network models, the proposed neural network has a low model complexity and avoids penalty parameters. It is proved that from any initial point, the state of the proposed neural network reaches the equality feasible region in finite time and stays there thereafter. Moreover, the state is unique if the initial point lies in the equality feasible region. The equilibrium point set of the proposed neural network is proved to be equivalent to the Karush-Kuhn-Tucker optimality set of the original optimization problem. It is further proved that the equilibrium point of the proposed neural network is stable in the sense of Lyapunov. Moreover, from any initial point, the state is proved to be convergent to an equilibrium point of the proposed neural network. Finally, as applications, the proposed neural network is used to solve nonlinear convex programming with linear constraints and L 1 -norm minimization problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where x = (x 1 , x 2 , . . . , x n ) T ∈ R n is the vector of decision variables, f : R n → R is the objective function which is convex but may be nonsmooth, g(x) = (g 1 (x), g 2 (x), . . . , g p (x)) T : R n → R p is a p-dimensional vector-valued function and g i is also convex but may be nonsmooth (i = 1, 2, . . . , p), A ∈ R m×n is a full row-rank matrix (i.e., rank(A) = m ≤ n), and b</p><formula xml:id="formula_0">= (b 1 , b 2 , . . . , b m ) T ∈ R m .</formula><p>Without loss of generality, in this paper, we assume that nonlinear convex programming (1) has at least an optimal solution. Nonlinear programming <ref type="bibr">(1)</ref> arises in a broad variety of scientific and engineering applications, such as optimal control, structure design, signal processing, electrical network, robot control, and power system planning <ref type="bibr">[1]</ref>. Recently, people derive many numerical algorithms to solve nonlinear programming <ref type="bibr">(1)</ref>. However, since the computing time greatly depends on the dimension and the structure of (1), the conventional numerical algorithm is usually less effective for nonlinear programming <ref type="bibr">(1)</ref>. Then, one possible and promising approach to solve nonlinear programming <ref type="bibr">(1)</ref> with high dimension and complex structure in real time is to employ recurrent neural networks based on circuit implementation <ref type="bibr" target="#b1">[2]</ref>. The neurodynamics for nonlinear programming <ref type="bibr">(1)</ref> has numerous potential applications, such as the coordination of multimanipulator systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the model predictive control <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the pattern classification <ref type="bibr" target="#b6">[7]</ref>, and so on.</p><p>Since Tank and Hopfield <ref type="bibr" target="#b7">[8]</ref> first proposed a neural network for linear programming, recurrent neural networks for the optimization problem <ref type="bibr">(1)</ref>, and their engineering applications have been widely investigated <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b26">[27]</ref>. For example, Kennedy and Chua <ref type="bibr" target="#b8">[9]</ref> improved the neural network in <ref type="bibr" target="#b7">[8]</ref> by developing a neural network with a finite penalty parameter to solve nonlinear programming. To avoid using penalty parameters, Xia and Wang <ref type="bibr" target="#b10">[11]</ref> presented a novel recurrent neural network for nonlinear convex programming subject to nonlinear inequality constraints. Under the condition that the objective function is convex and all constraint functions are strictly convex, they proved that the state of the proposed neural network was globally convergent to an exact optimal solution. To overcome the strict condition on the constraint function in <ref type="bibr" target="#b10">[11]</ref>, Yang and Cao <ref type="bibr" target="#b11">[12]</ref> extended the neural network proposed in <ref type="bibr" target="#b10">[11]</ref> and proved the related results under the condition that the objective function and all constraint functions are convex. Gao <ref type="bibr" target="#b12">[13]</ref> presented a projection-type neural network for solving the nonlinear convex programming problem <ref type="bibr">(1)</ref> with bounded constraints in real time.</p><p>Meanwhile, it is well known that nonsmooth optimization plays an important role in many engineering applications, such as manipulator control, signal processing, sliding mode, and so on. Forti et al. <ref type="bibr" target="#b18">[19]</ref> proposed a generalized neural network to solve a much wider class of nonsmooth nonlinear programming problems in real time. The generalized neural network in <ref type="bibr" target="#b18">[19]</ref> was a gradient system of differential inclusions. For more general nonsmooth programming, Xue and Bian <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> proposed some recurrent neural networks based on subgradient and penalty function method. However, penalty function method is effective relying on exact penalty parameters, and it is difficult to estimate penalty parameters in real applications. Cheng et al. <ref type="bibr" target="#b27">[28]</ref> proposed a nonsmooth recurrent neural network to solve the nonsmooth convex optimization problem <ref type="bibr">(1)</ref>. The proposed neural network in <ref type="bibr" target="#b27">[28]</ref> could deal with the nonsmooth convex optimization problem with a larger class of constraints, and was not based on any penalty method. However, the neural network in <ref type="bibr" target="#b27">[28]</ref> had a complex structure.</p><p>To reduce the model complexity, some one-layer recurrent neural networks were proposed <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. Guo et al. <ref type="bibr" target="#b28">[29]</ref> presented a one-layer recurrent neural network for solving pseudoconvex optimization problems subject to linear equality constraints. Moreover, they proved the finite-time state convergence to the feasible region defined by the equality constraints. Based on penalty function method, Liu et al. <ref type="bibr" target="#b29">[30]</ref> presented another one-layer recurrent neural network to solve pseudoconvex optimization problems subject to linear equality and box constraints. To solve linear programming, Liu et al. <ref type="bibr" target="#b24">[25]</ref> proposed a novel neural network based on the gradient method. In <ref type="bibr" target="#b24">[25]</ref>, they proved that the state of the proposed neural network was globally convergent to exact optimal solutions in finite time. Convergence in finite time or convergence in the presence of nonisolated equilibria is a remarkable phenomenon for some recurrent neural networks, and receives numerous attentions <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref>. For example, in <ref type="bibr" target="#b25">[26]</ref>, by means of quantitative evaluation of the Łojasiewicz exponent at the equilibrium points, for nonconvex quadratic programming, each trajectory of the neural network in <ref type="bibr" target="#b18">[19]</ref> is either exponentially convergent, or convergent in finite time, toward a singleton belonging to the set of constrained critical points. Recently, by introducing a regularization control item, Bian and Xue <ref type="bibr" target="#b35">[36]</ref> proposed a new neural network to solve a class of nonsmooth convex optimization problems. Under some mild assumptions, they proved that the state of proposed neural network converged to the feasible region in finite time and to the particular element in the optimal solution set with the smallest norm.</p><p>Inspired by previous studies, in this paper, we propose a simplified two-layer neural network to solve the nonlinear programming <ref type="bibr">(1)</ref>. The contributions of this paper are listed as follows. First, unlike neural networks in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b29">[30]</ref>, the proposed neural network in this paper is not based on penalty method, which means that we need not select an exact parameter in advance. Second, the proposed neural network has a low model complexity. For solving the nonsmooth convex optimization problem (1), neural networks in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b27">[28]</ref> have three-layer structure. Obviously, the structure of the proposed neural network in this paper is simpler than the neural networks in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b27">[28]</ref>. Third, based on the more general conditions, we prove the stability of the proposed neural network. In this paper, we only assume that f and g in (1) are convex. It is a basic hypothesis for the convex optimization problem. Meanwhile, by some simple transformations, A in (1) can be transformed into a full row-rank matrix and the equality constraint Ax = b remains unchanged. Hence, without loss of generality, we also assume that A is a full row-rank matrix.</p><p>The remainder of this paper is organized as follows. In Section II, we will present some preliminaries and Karush-Kuhn-Tucker (KKT) conditions for the nonsmooth convex optimization problem <ref type="bibr">(1)</ref>. In Section III, we will introduce a simplified two-layer neural network and prove that the equilibrium point set of the proposed neural network is equivalent to the KKT optimality set of the nonsmooth convex optimization problem <ref type="bibr">(1)</ref>. Then, the convergence analysis of the proposed neural network is studied in Section IV. In Section V, we will propose two examples to illustrate our results. Finally, Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>For the convenience of later discussions, we present some definitions and properties, which are needed in the remainder of this paper. We refer readers to <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">[38]</ref> for more thorough discussions.</p><p>Let R n be an n-dimensional real Euclidean space with inner product &lt; •, • &gt;, and induced norm • . A set-valued map </p><formula xml:id="formula_1">F : K ⊆ R n → R n is a map that to each point x ∈ K , there corresponds a nonempty set F(x) ⊆ R n . A set-valued map F : K ⊆ R n → R n with</formula><formula xml:id="formula_2">Gr (F) = {(x, y) ∈ K × R n : y ∈ F(x)}.</formula><p>Definition 2.1: Let f be Lipschitz near a given point x 0 ∈ R n , and v any other vector in R n . The generalized directional derivative of f at x 0 in the direction v, denoted by f • (x 0 ; v), is defined as follows:</p><formula xml:id="formula_3">f • (x 0 ; v) = lim sup y→x 0 ,t ↓0 f (y + tv) -f (y) t .</formula><p>The Clarke subdifferential of f at x 0 is given by </p><formula xml:id="formula_4">∂ f (x 0 ) = {ξ ∈ R n : f • (x 0 ; v) ≥ ξ, v for all v in R n } which is a subset of R n . Note that if f is Lipschitz of rank K near x 0 , then ∂ f (x 0 ) is a nonempty, convex, compact subset of R n ,</formula><formula xml:id="formula_5">) = {ξ ∈ R n : f (x 0 ) -f (x) ≤ ξ, x 0 - x , ∀x ∈ R n }; 2) ∂ f (•) is maximal monotone, i.e., x -x 0 , v -v 0 ≥ 0 for any v ∈ ∂ f (x) and v 0 ∈ ∂ f (x 0 ); 3) ∂ f (•) is upper semicontinuous. Definition 2.2: f is said to be regular at x provided: 1) for all v ∈ R n , the usual one-sided directional derivative f (x; v) exists; 2) for all v ∈ R n , f (x; v) = f • (x; v).</formula><p>Obviously, any convex function is regular. Regular function has a very important property (i.e., chain rule), which has been used in many papers <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>Lemma 2.1 (Chain Rule <ref type="bibr" target="#b28">[29]</ref>): If V (x) : R n → R is regular and x(t) : [0, +∞) → R n is absolutely continuous on any compact interval of [0, +∞), then x(t) and V (x(t)) : [0, +∞) → R are differentiable, and</p><formula xml:id="formula_6">V (x(t)) = ξ, ẋ(t) ∀ξ ∈ ∂ V (x(t))</formula><p>for a.e. t ∈ [0, +∞).</p><p>Without loss of generality, in this paper, we assume that the nonlinear convex programming (1) has at least an optimal solution. We next introduce nonsmooth KKT conditions of the nonlinear programming <ref type="bibr">(1)</ref>. According to [28, Th. 1], we have the following.</p><p>Lemma 2.2: Let f and g be convex, then x * is an optimal solution of nonlinear convex programming (1) if and only if there exist μ * ∈ R p and ν * ∈ R m such that</p><formula xml:id="formula_7">0 ∈ ∂ f (x * ) + ∂g(x * ) T μ * + A T v * 0 = μ * -(μ * + g(x * )) + 0 = Ax * -b (2)</formula><p>where ∂g(x T , and (t) + = max{t, 0}.</p><formula xml:id="formula_8">* ) T = (∂g 1 (x * ), ∂g 2 (x * ), . . . , ∂g p (x * )) T , (μ * + g(x * )) + = ((μ * 1 + g 1 (x * )) + , (μ * 2 + g 2 (x * )) + , . . . , (μ * p + g p (x * )) + )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NEURAL NETWORK MODEL</head><p>Based on Lemma 2.2, we propose a two-layer recurrent neural network to solve the nonlinear convex programming (1), with the following dynamical equations:</p><formula xml:id="formula_9">⎧ ⎪ ⎨ ⎪ ⎩ ẋ(t) ∈ -(I -P)[∂ f (x(t)) + ∂g(x(t)) T μ(t)] -A T h(Ax(t) -b) 2 μ(t) = -μ(t) + (μ(t) + g(x(t))) +<label>(3)</label></formula><p>where μ = (μ + g(x)) + , I is the identity matrix,</p><formula xml:id="formula_10">P = A T (A A T ) -1 A, h(x) = ( h(x 1 ), h(x 2 ), . . . , h(x m )) T</formula><p>, and its component is defined as</p><formula xml:id="formula_11">h(x i ) = ⎧ ⎪ ⎨ ⎪ ⎩ 1, if x i &gt; 0 [-1, 1], if x i = 0 -1, if x i &lt; 0 (4) and ∂g(x(t)) T = (∂g 1 (x(t)), ∂g 2 (x(t)), . . . , ∂g p (x(t))) T .</formula><p>Nonsmooth neural network (3) can be realized by a generalized circuit. For more details on the generalized circuit, readers can refer to <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b35">[36]</ref>. Here, we propose a generalized circuit implementation of neural network (3) for the following simple optimization problem:</p><formula xml:id="formula_12">minimize f (x 1 , x 2 ) = x 1 + |x 2 | subject to g(x 1 , x 2 ) = x 1 + x 2 ≤ 0a 1 x 1 + a 2 x 2 = 1. (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>The implementation of F = ∂ f (x) + ∂g(x) T μ can be simulated as the block diagram in Fig. <ref type="figure">1</ref>, and the implementation of h is simulated in <ref type="bibr" target="#b18">[19]</ref>. Then, the architecture of neural network (3) for the optimization problem ( <ref type="formula" target="#formula_12">5</ref>) can be shown in  Fig. <ref type="figure">2</ref>, where</p><formula xml:id="formula_14">(h 1 , h 2 ) T = h(Ax -b), [e i j ] 2×2 = I -P, and A = (a 1 , a 2 ).</formula><p>In Table <ref type="table" target="#tab_2">I</ref>, we show a comparison of the proposed neural network (3) with several other existing neural networks for solving the nonsmooth convex optimization problem (1). Here,</p><formula xml:id="formula_15">X 1 = {x ∈ R n : g(x) ≤ 0} and X 2 = {x ∈ R n : Ax = b}.</formula><p>It is easy to see that the proposed neural network (3) has less number of neurons than neural networks in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b27">[28]</ref>. Meanwhile, although neural networks in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and <ref type="bibr" target="#b38">[39]</ref> have low model complexity, more additional assumptions or penalty parameters are introduced to ensure the stability of these neural networks. More comparisons and related examples will be introduced in Section IV.</p><p>For the convenience of later discussions, we introduce the definition of a solution to a Cauchy problem associated to neural network (3) in the sense of Filippov <ref type="bibr" target="#b39">[40]</ref> as follows.</p><p>Definition 3.1:</p><formula xml:id="formula_16">A vector function (x(•), μ(•)) T is said to be a state of neural network (3) on [0, T ), if (x(•), μ(•)) T is absolutely continuous on [0, T ) with initial condition x(0) = x 0 , μ(0) = μ 0 , and for almost all (a.e.) t ∈ [0, T ) ⎧ ⎪ ⎨ ⎪ ⎩ ẋ(t) ∈ -(I -P)[∂ f (x(t)) + ∂g(x(t)) T μ(t)] -A T h(Ax(t) -b) 2 μ(t) = -μ(t) + (μ(t) + g(x(t))) + .</formula><p>Equivalently, there exist measurable functions</p><formula xml:id="formula_17">γ (t) ∈ ∂ f (x(t)), η(t) = (η 1 (t), η 2 (t), . . . , η p (t)) ∈ ∂g(x(t)), and ξ(t) ∈ h(Ax(t) -b), such that ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)] -A T ξ(t) 2 μ(t) = -μ(t) + (μ(t) + g(x(t))) + for almost all t ∈ [0, T ). Definition 3.2: (x * , μ * ) T is called an equilibrium point of the neural network (3) if 0 ∈ (I -P)[∂ f (x * ) + ∂g(x * ) T μ * ] + A T h(Ax * -b) 0 = -μ * + (μ * + g(x * )) + i.e., there exist γ * ∈ ∂ f (x * ), η * ∈ ∂g(x * ) and ξ * ∈ h(Ax * -b) such that 0 = (I -P)(γ * + η * T μ * ) + A T ξ * 0 = -μ * + (μ * + g(x * )) + . Theorem 3.1: For any initial point (x(0), μ(0)) T = (x 0 , μ 0 ) T ∈ R n+ p</formula><p>, there is at least a local solution (x(t), μ(t)) T of neural network (3) defined on a maximal interval [0, T ), for some T ∈ (0, +∞]. Moreover, the state x(t) will reach the equality feasible region X 2 = {x ∈ R n |Ax = b} in finite time and stay there thereafter. And</p><formula xml:id="formula_18">μ(t) ≥ 0 if μ(0) ≥ 0. Proof: Since F(x, μ) = -(I -P)[∂ f (x) + ∂g(x) T μ] -A T h(Ax -b) -μ + (μ + g(x)) + is U.S.C. with nonempty convex compact values, then for any initial point (x 0 , μ 0 ) T ∈ R n+ p , there exists at least a state (x(t), μ(t)) T of neural network (3) with x(0) = x 0 and μ(0) = μ 0 , t ∈ [0, T ) [37, Th. 3, p. 98]. That is, there exist measurable functions γ (t) ∈ ∂ f (x(t)), η(t) ∈ ∂g(x(t)), and ξ(t) ∈ h(Ax(t) -b), such that ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)] -A T ξ(t) 2 μ(t) = -μ(t) + (μ(t) + g(x(t))) +<label>(6)</label></formula><p>for almost all t ∈ [0, T ).</p><formula xml:id="formula_19">Let B(x) = Ax -b 1 , where • 1 is the 1-norm of R n .</formula><p>Obviously, B(x) is convex and regular. According to the chain rule (Lemma 2.1), for a.e. t ∈ [0, T ), we have</p><formula xml:id="formula_20">d dt B(x(t)) = ζ T A ẋ(t) ∀ζ ∈ h(Ax(t) -b).</formula><p>It is clear that</p><formula xml:id="formula_21">A(I -P) = A(I -A T (A A T ) -1 A) = A-A = 0. Hence, taking ζ = ξ(t) ∈ h(Ax(t) -b) which is from (6), we have d dt B(x(t)) = ξ(t) T A ẋ(t) = -ξ(t) T A(I -P)[γ (t) + η(t) T μ(t)] -ξ(t) T A A T ξ(t) = -ξ(t) T A A T ξ(t) ≤ -λ M (A A T ) ξ(t) 2<label>(7)</label></formula><p>where λ M (A A T ) &gt; 0 stands for the operation of taking the maximum eigenvalue of A A T . Obviously, if Ax(t) = b, by the definition of h in (4), ξ(t) 2 ≥ 1, which implies that</p><formula xml:id="formula_22">d dt B(x(t)) ≤ -λ M (A A T ). (<label>8</label></formula><formula xml:id="formula_23">)</formula><p>Suppose that there exists</p><formula xml:id="formula_24">t 0 &gt; 0 such that x(0) = x 0 / ∈ X 2 and x(t) / ∈ X 2 for t ∈ [0, t 0 ].</formula><p>Then, integrating both sides of (8) from 0 to t 0 , we have</p><formula xml:id="formula_25">B(x(t 0 )) -B(x(0)) ≤ -λ M (A A T )t 0 that is 0 ≤ Ax(t 0 ) -b 1 ≤ Ax(0) -b 1 -λ M (A A T )t 0 . Therefore, t 0 ≤ Ax(0) -b 1 /λ M (A A T ). That is, when t &gt; Ax(0) -b 1 /λ M (A A T ), x(t) ∈ X 2 ,</formula><p>i.e., the state x(t) will reach the equality feasible region X 2 = {x ∈ R n |Ax = b} in finite time and an upper bound of the hit time is</p><formula xml:id="formula_26">t s = Ax(0) -b 1 /λ M (A A T ). Moreover, if x(t) leaves X 2 at t 1 &gt; t s ,</formula><p>then there must exist the interval (t 1 , t 2 ) such that x(t) / ∈ X 2 for any t ∈ (t 1 , t 2 ) and Ax(t 1 )b 1 = 0. Hence, by <ref type="bibr" target="#b6">(7)</ref>, we have</p><formula xml:id="formula_27">Ax(t 2 ) -b 1 ≤ Ax(t 1 ) -b 1 -λ M (A A T )(t 2 -t 1 ) = -λ M (A A T )(t 2 -t 1 ) &lt; 0 ( 9 )</formula><p>which leads a contradiction. Hence, the state x(t) reaches X 2 in finite time and stays there thereafter. On the other hand, according to (6), we have</p><formula xml:id="formula_28">d dt (e 1 2 t μ(t)) = 1 2 e 1 2 t (μ(t) + g(x(t))) + ≥ 0 which implies that e 1 2 t μ(t) ≥ e 0 μ(0) = μ(0). Hence, μ(t) ≥ 0 if μ(0) ≥ 0.</formula><p>Now, we study the uniqueness of the state of neural network (3) starting at a given initial point.</p><p>Theorem 3.2: For any initial point</p><formula xml:id="formula_29">(x 0 , μ 0 ) T ∈ X 2 × R p , there is a unique solution (x(t), μ(t)) T of neural network (3) with (x(0), μ(0)) T = (x 0 , μ 0 ) T . Proof: Let (x(t), μ(t)) T be a state of neural network (3) with initial point (x(0), μ(0)) T = (x 0 , μ 0 ) T ∈ X 2 × R p , i.e., there exist measurable functions γ (t) ∈ ∂ f (x(t)), η(t) ∈ ∂g(x(t)), and ξ(t) ∈ h(Ax(t) -b), such that ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)] -A T ξ(t) 2 μ(t) = -μ(t) + [μ(t) + g(x(t))] +<label>(10)</label></formula><p>for a.e. t. By Theorem 3.1 and the initial point</p><formula xml:id="formula_30">(x 0 , μ 0 ) T ∈ X 2 × R p , the solution (x(t), μ(t)) T of neural network (3)</formula><p>will stay in X 2 × R p , i.e., Ax(t) = b for all t ≥ 0. Then, by <ref type="bibr" target="#b9">(10)</ref> </p><formula xml:id="formula_31">0 = A ẋ(t) = -A(I -P)[γ (t) + η(t) T μ(t)] -A A T ξ(t) = -A A T ξ(t)</formula><p>which means that ξ(t) = 0 for all t ≥ 0, since A A T is invertible by the assumption in <ref type="bibr">(1)</ref>. Hence, the state x(t) also satisfies</p><formula xml:id="formula_32">ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)]. (<label>11</label></formula><formula xml:id="formula_33">)</formula><p>Suppose there exists another state (x (t), μ (t)) T of neural network (3) with the same initial condition (x (0), μ (0</p><formula xml:id="formula_34">)) T = (x 0 , μ 0 ) T ∈ X 2 × R p , i.e., there exist measurable functions γ (t) ∈ ∂ f (x (t)), η (t) ∈ ∂g(x (t)), and ξ (t) ∈ h(Ax (t)-b), such that ẋ (t) = -(I -P)[γ (t) + η (t) T μ (t)] -A T ξ (t) 2 μ (t) = -μ (t) + [μ (t) + g(x (t))] +<label>(12)</label></formula><p>for a.e. t. Similar to the above discussion, (x (t), μ (t)) T also satisfies</p><formula xml:id="formula_35">ẋ (t) = -(I -P)[γ (t) + η (t) T μ (t)] Ax (t) = b (<label>13</label></formula><formula xml:id="formula_36">)</formula><p>for a.e. t.</p><formula xml:id="formula_37">Noting that μ 2 = (μ + g(x)) + 2 = p i=1 [(μ i + g i (x)) + ] 2 and [(μ i + g i (x)) + ] 2 = (μ i + g i (x)) 2 if μ i + g i (x) &gt; 0 0 o t h e r w i s e</formula><p>we have μ 2 is convex with respect to (x, μ), and</p><formula xml:id="formula_38">∂ μ 2 = 2∂g(x) T μ 2μ . (<label>14</label></formula><formula xml:id="formula_39">)</formula><p>Hence, by the maximal monotonicity of convex subdifferential and (10), <ref type="bibr" target="#b11">(12)</ref>, and ( <ref type="formula" target="#formula_38">14</ref>), we have</p><formula xml:id="formula_40">(x(t) -x (t)) T (η(t) T μ -η (t) T μ ) + (μ(t) -μ (t)) T (μ(t) -μ (t)) ≥ 0 (<label>15</label></formula><formula xml:id="formula_41">)</formula><p>for a.e. t ≥ 0. Then, according to chain rule (Lemma 2.1) and ( <ref type="formula" target="#formula_32">11</ref>) and ( <ref type="formula" target="#formula_35">13</ref>)</p><formula xml:id="formula_42">d dt 1 2 x(t) -x (t) 2 + μ(t) -μ (t) 2 = (x(t) -x (t)) T ( ẋ(t) -ẋ (t)) + 2(μ(t) -μ (t)) T ( μ(t) -μ (t)) = (μ(t) -μ (t)) T (-μ(t) + μ(t) + μ (t) -μ (t)) + (x(t) -x (t)) T (I -P)(-[γ (t) + η(t) T μ(t)] + [γ (t) + η (t) T μ (t)]). (<label>16</label></formula><formula xml:id="formula_43">)</formula><p>On the other hand, since</p><formula xml:id="formula_44">Ax(t) = Ax (t) = b for t ≥ 0 (x(t) -x (t)) T P = (x(t) -x (t)) T A T (A A T ) -1 A = (Ax(t) -Ax (t)) T (A A T ) -1 A = 0.</formula><p>Hence, by the maximal monotonicity of convex subdifferential and ( <ref type="formula" target="#formula_40">15</ref>)</p><formula xml:id="formula_45">d dt 1 2 x(t) -x (t) 2 + μ(t) -μ (t) 2 = (x(t) -x (t)) T (-[γ (t) + η(t) T μ(t)] + [γ (t) + η (t) T μ (t)]) + (μ(t) -μ (t)) T (-μ(t) + μ(t) + μ (t) -μ (t)) = -(x(t) -x (t)) T (γ (t) -γ (t)) -μ(t) -μ (t) 2 -[(x(t) -x (t)) T (η(t) T μ -η (t) T μ ) + (μ(t) -μ (t)) T (μ(t) -μ (t))] + 2(μ(t) -μ (t)) T (μ(t) -μ (t)) ≤ -μ(t) -μ (t) 2 + 2(μ(t) -μ (t)) T (μ(t) -μ (t)) ≤ μ(t) -μ (t) 2 . (<label>17</label></formula><formula xml:id="formula_46">)</formula><p>Obviously, (•) + is globally Lipschitz, i.e., |(s) + -(t) + | ≤ |s -t| for all s, t ∈ R. Then</p><formula xml:id="formula_47">μ(t) -μ (t) 2 = p i=1 [(μ i (t) + g i (x(t))) + -(μ i (t) + g i (x (t))) + ] 2 ≤ p i=1 [(μ i (t) + g i (x(t))) -(μ i (t) + g i (x (t)))] 2 ≤ 2 μ(t) -μ (t) 2 + 2 g(x(t)) -g(x (t)) 2<label>(18)</label></formula><p>According to the property of convex function [38, Proposition 2.2.6], g is locally Lipschitz on R n . Especially, g is locally Lipschitz on the initial point x 0 ∈ X 2 ⊆ R n , i.e., there exist r &gt; 0 and L &gt; 0 such that</p><formula xml:id="formula_48">g(x) -g(y) ≤ L x -y (<label>19</label></formula><formula xml:id="formula_49">)</formula><p>for all x, y ∈ B(x 0 , r ). Meanwhile, by above assumption, (x(0), μ(0)) T = (x (0), μ (0)) T = (x 0 , μ 0 ) T ∈ X 2 × R p . Hence, there exists t 0 &gt; 0 such that both x(t) and x (t) lie in B(x 0 , r ) for all t ∈ [0, t 0 ]. Then, by <ref type="bibr" target="#b18">(19)</ref>, for all t ∈ [0, t 0 ]</p><formula xml:id="formula_50">g(x(t)) -g(x (t)) ≤ L x(t) -x (t) . (<label>20</label></formula><formula xml:id="formula_51">)</formula><p>Therefore, according to the inequalities ( <ref type="formula" target="#formula_45">17</ref>), <ref type="bibr" target="#b17">(18)</ref>, and ( <ref type="formula" target="#formula_50">20</ref>), for all t ∈ [0, t 0 ], we have</p><formula xml:id="formula_52">d dt 1 2 x(t) -x (t) 2 + μ(t) -μ (t) 2 ≤ 2 μ(t) -μ (t) 2 + 2L x(t) -x (t) 2 ≤ M 1 2 x(t) -x (t) 2 + μ(t) -μ (t) 2<label>(21)</label></formula><p>where M = max{4L, 2} &gt; 0. Hence, by Gronwall's inequality, for all t ∈ [0, t 0 ], we have 1 2</p><formula xml:id="formula_53">x(t) -x (t) 2 + μ(t) -μ (t) 2 ≤ 1 2 x(0) -x (0) 2 + μ(0) -μ (0) 2 e Mt = 0 (22)</formula><p>which means Next, similar to the above proof, and by the fact that g is locally Lipschitz on x(t 0 ), there exists t 1 &gt; t 0 such that x(t) = x (t), μ(t) = μ (t), for all t ∈ [t 0 , t 1 ]. Then, step by step, we obtain the uniqueness of the state of neural network <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_54">x(t) = x (t), μ(t) = μ (t), for all t ∈ [0, t 0 ]. (<label>23</label></formula><formula xml:id="formula_55">)</formula><p>Since the equilibrium point is a stationary solution of differential equation, by Theorem 3.1, if (x * , μ * ) T is an equilibrium point of neural network (3), then we must have that Ax * = b. We next prove that x * is just an optimal solution of the nonlinear convex programming <ref type="bibr">(1)</ref>.</p><p>Theorem 3.3: Let (x * , μ * ) T be an equilibrium point of neural network (3), then x * is an optimal solution to the nonlinear convex programming <ref type="bibr">(1)</ref>. Conversely, if x * is an optimal solution to the nonlinear convex programming (1), then there exists μ * &gt; 0 such that (x * , μ * ) T is an equilibrium point of neural network <ref type="bibr" target="#b2">(3)</ref>.</p><p>Proof: Let (x * , μ * ) T be an equilibrium point of neural network (3)</p><formula xml:id="formula_56">0 ∈ (I -P)[∂ f (x * ) + ∂g(x * ) T μ * ] + A T h(Ax * -b) 0 = -μ * + (μ * + g(x * )) + .</formula><p>Then, by Theorem 3.1, we have Ax * = b and μ * ≥ 0. Meanwhile, taking</p><formula xml:id="formula_57">ν * = (A A T ) -1 A(∂ f (x * ) + ∂g(x * ) T μ * ) + h(Ax * -b), we have 0 ∈ (I -P)[∂ f (x * ) + ∂g(x * ) T μ * ] + A T h(Ax * -b) = ∂ f (x * ) + ∂g(x * ) T μ * + A T v * .</formula><p>Hence, by Lemma 2.2, x * is an optimal solution to the nonlinear convex programming (1). Conversely, let x * be an optimal solution to the nonlinear convex programming <ref type="bibr">(1)</ref>. Then, by Lemma 2.2, there exists (μ * , v * ) T such that the equalities in (2) hold. Hence</p><formula xml:id="formula_58">0 ∈ (I -P)[∂ f (x * ) + ∂g(x * ) T μ * + A T v * ] = (I -P)[∂ f (x * ) + ∂g(x * ) T μ * ].</formula><p>On the other hand, since 0 ∈ h(Ax *b), we have 0</p><formula xml:id="formula_59">∈ (I -P)[∂ f (x * ) + ∂g(x * ) T μ * ] + h(Ax * -b).</formula><p>Combining with equality (2), it is obvious that (x * , μ * ) T is an equilibrium point of neural network (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONVERGENCE ANALYSIS</head><p>In this section, we analyze the global convergence of neural network <ref type="bibr" target="#b2">(3)</ref>. First, we prove the following Lemma, which plays an important role in proving the stability of neural network (3).</p><p>Lemma 4.1:</p><formula xml:id="formula_60">If f (x, y) : R n × R n → R is convex and f is continuously differentiable with respect to x, then ∂ f (x 0 , y 0 ) = ∂ x f (x 0 , y 0 ) × ∂ y f (x 0 , y 0 )</formula><p>where ∂ f (x 0 , y 0 ) is the subdifferential of f at (x 0 , y 0 ), ∂ x f (x 0 , y 0 ) is the partial subdifferential of f (•, y 0 ) at x 0 , and ∂ y f (x 0 , y 0 ) is similarly defined.</p><p>Proof: From [38, Proposition 2.3.15], we have that</p><formula xml:id="formula_61">∂ f (x 0 , y 0 ) ⊆ ∂ x f (x 0 , y 0 ) × ∂ y f (x 0 , y 0 ).</formula><p>Hence, we only need to prove</p><formula xml:id="formula_62">∂ x f (x 0 , y 0 ) × ∂ y f (x 0 , y 0 ) ⊆ ∂ f (x 0 , y 0 ).</formula><p>Since f is continuously differentiable with respect to x, ∂ x f (x 0 , y 0 ) reduces to a point ∇ x f (x 0 , y 0 ). Then, for any p y ∈ ∂ y f (x 0 , y 0 ), we next prove that (∇ x f (x 0 , y 0 ), p y ) ∈ ∂ f (x 0 , y 0 ). At first, by the differential mean value theorem, for any</p><formula xml:id="formula_63">(v 1 , v 2 ) ∈ R n × R n , we have lim t →0 + f (x 0 + tv 1 , y 0 + tv 2 ) -f (x 0 , y 0 + tv 2 ) t = lim t →0 + ∇ x f (x 0 + sv 1 , y 0 + tv 2 )tv 1 t (s ∈ [0, t]) = ∇ x f (x 0 , y 0 )v 1 = lim t →0 + f (x 0 + tv 1 , y 0 ) -f (x 0 , y 0 ) t .</formula><p>Therefore, for any</p><formula xml:id="formula_64">(v 1 , v 2 ) ∈ R n × R n (∇ x f (x 0 , y 0 ), p y ) T (v 1 , v 2 ) = ∇ x f (x 0 , y 0 ) T v 1 + p T y v 2 ≤ f x (x 0 , y 0 ; v 1 ) + f y (x 0 , y 0 ; v 2 ) = lim t →0 + f (x 0 + tv 1 , y 0 ) -f (x 0 , y 0 ) t + lim t →0 + f (x 0 , y 0 + tv 2 ) -f (x 0 , y 0 ) t = lim t →0 + f (x 0 + tv 1 , y 0 + tv 2 ) -f (x 0 , y 0 + tv 2 ) t + lim t →0 + f (x 0 , y 0 + tv 2 ) -f (x 0 , y 0 ) t = lim t →0 + f (x 0 + tv 1 , y 0 + tv 2 ) -f (x 0 , y 0 ) t = f (x 0 , y 0 ; v 1 , v 2 )</formula><p>which implies that (∇ x f (x 0 , y 0 ), p y ) ∈ ∂ f (x 0 , y 0 ) by Definition 2.1. Theorem 4.1: Each equilibrium point of neural network (3) is stable in the sense of Lyapunov. Moreover, for any initial point (x 0 , μ 0 ) T ∈ R n × R p , the state of neural network (3) is convergent to an equilibrium point of neural network <ref type="bibr" target="#b2">(3)</ref>.</p><p>Proof: The proof will be divided into two steps as follows.</p><p>We firstly prove that for any initial point, the state of neural network (3) exists for t ∈ [0, +∞) and each equilibrium point of neural network (3) is stable in the sense of Lyapunov.</p><p>According to the assumptions in the nonlinear convex programming (1) and Theorem 3.3, there exists at least an equilibrium point of neural network (3). Without loss of generality, we let (x * , μ * ) T be an equilibrium point of neural network (3). That is, there exist γ * ∈ ∂ f (x * ), η * ∈ ∂g(x * ), and</p><formula xml:id="formula_65">ξ * ∈ h(Ax * -b) such that 0 = (I -P)(γ * + η * T μ * ) + A T ξ * 0 = -μ * + (μ * + g(x * )) + . (<label>24</label></formula><formula xml:id="formula_66">)</formula><p>Multiplying (I -P) by both sides of ( <ref type="formula" target="#formula_65">24</ref>), we have</p><formula xml:id="formula_67">0 = (I -P) 2 (γ * + η * T μ * ) + (I -P)A T ξ * = (I -P)(γ * + η * T μ * ). (<label>25</label></formula><formula xml:id="formula_68">)</formula><p>Construct the following Lyapunov function:</p><formula xml:id="formula_69">V (x, μ) = f (x) -f (x * ) + 1 2 μ 2 - 1 2 μ * 2 -(x -x * ) T (γ * + η * T μ * ) -(μ -μ * ) T μ * + 1 2 x -x * 2 + 1 2 μ -μ * 2 (26)</formula><p>where μ = (μ + g(x))</p><formula xml:id="formula_70">+ . It is well known that μ 2 is convex on R n × R p [13]. Let ϕ(x, μ) = f (x) + 1 2 μ 2 .</formula><p>Obviously, ϕ is convex with respect to (x, μ). Then, by Lemma 2.2</p><formula xml:id="formula_71">ϕ(x, μ)-ϕ(x * , μ * ) ≥ (γ * +η * T μ * ) T (x -x * )+μ * T (μ-μ * )</formula><p>which implies that</p><formula xml:id="formula_72">V (x, μ) ≥ 1 2 x -x * 2 + 1 2 μ -μ * 2 ≥ 0. (<label>27</label></formula><formula xml:id="formula_73">)</formula><p>That is, V (x, μ) is positive definite and radially unbounded. Meanwhile, the subdifferential of V (x, μ) with respect to x is</p><formula xml:id="formula_74">∂ x V (x, μ) = ∂ f (x) + μ T ∂g(x) -(γ * + η * T μ * ) + x -x * (28)</formula><p>and the subdifferential of V (x, μ) with respect to μ is</p><formula xml:id="formula_75">∂ μ V (x, μ) = μ + μ -2μ * . (<label>29</label></formula><formula xml:id="formula_76">)</formula><p>Let (x(t), μ(t)) T be a state of neural network (3) with the initial point (x(0), μ(0)) T = (x 0 , μ 0 ) T , i.e., there exist measurable functions γ (t) ∈ ∂ f (x(t)), η(t) = (η 1 (t), η 2 (t), . . . , η p (t)) ∈ ∂g(x(t)), and ξ(t) ∈ h(Ax(t)b), such that</p><formula xml:id="formula_77">ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)] -A T ξ(t) 2 μ(t) = -μ(t) + (μ(t) + g(x(t))) +<label>(30)</label></formula><p>for a.e. t. By Theorem 3.1, the state x(t) will reach the equality feasible region X 2 = {x ∈ R n |Ax = b} in finite time and stay there thereafter. Without loss of generality, we only consider the stability of neural network (3) with x(t) ∈ X 2 , i.e., Ax(t) = b for all t ≥ 0. Then, similar to <ref type="bibr" target="#b10">(11)</ref>, we have</p><formula xml:id="formula_78">ẋ(t) = -(I -P)[γ (t) + η(t) T μ(t)]. (<label>31</label></formula><formula xml:id="formula_79">)</formula><p>Because (I -P) <ref type="formula" target="#formula_67">25</ref>), we have</p><formula xml:id="formula_80">= (I -P) 2 , (x(t) -x * ) T P = (x(t) - x * ) T A T (A A T ) -1 A = (Ax(t) -Ax * ) T (A A T ) -1 A = 0, and 0 = (I -P)(γ * + η * T μ * ) by (</formula><formula xml:id="formula_81">[γ (t) + η(t) T μ -(γ * + η * T μ * ) + x -x * ] T ẋ(t) = -[γ (t) + η(t) T μ(t)] T (I -P)[γ (t) + η(t) T μ -(γ * + η * T μ * ) + x(t) -x * ] = -(I -P)[γ (t) + η(t) T μ(t)] 2 -(γ * + η * T μ * ) T (I -P)[γ (t) + η(t) T μ(t)] -(x(t) -x * ) T (I -P)[γ (t) + η(t) T μ(t) -γ * -η * T μ * ] = -ẋ(t)] 2 -(x -x * ) T (γ (t) -γ * ) -(x(t) -x * ) T (η(t) T μ(t) -η * T μ * ). Meanwhile 1 2 (μ + μ -2μ * ) T (-μ + μ) = 1 2 (-μ + μ + 2μ -2μ * ) T (-μ + μ) = -2 μ(t) 2 + (-μ + μ) T (μ -μ * ).</formula><p>Hence, by chain rule (i.e., Lemma 2.1) and Lemma 4.1, for a.e. t ≥ 0, we have</p><formula xml:id="formula_82">d dt V (x(t), μ(t)) = [γ (t) + η(t) T μ -(γ * + η * T μ * ) + x(t) -x * ] T ẋ(t) + 1 2 (μ + μ -2μ * ) T (-μ + μ) = -ẋ(t) 2 -(x(t) -x * ) T (γ (t) -γ * ) -(x(t) -x * ) T (η(t) T μ(t) -η * T μ * ) -2 μ(t) 2 + (-μ(t) + μ(t)) T (μ(t) -μ * ). (<label>32</label></formula><formula xml:id="formula_83">)</formula><p>From the convexities of f and g, we have</p><formula xml:id="formula_84">⎧ ⎪ ⎨ ⎪ ⎩ (x(t) -x * ) T (γ (t) -γ * ) ≥ 0 g(x * ) -g(x(t)) -η T (t)(x * -x(t)) ≥ 0 g(x(t)) -g(x * ) -η * T (x(t) -x * ) ≥ 0. (<label>33</label></formula><formula xml:id="formula_85">)</formula><p>Substituting ( <ref type="formula" target="#formula_84">33</ref>) into ( <ref type="formula" target="#formula_82">32</ref>), one has</p><formula xml:id="formula_86">d dt V (x(t), μ(t)≤ -ẋ(t) 2 -2 μ(t) 2 +μ(t) T g(x * ) -μ * T g(x * ) -μ(t) T g(x(t)) + μ * T g(x(t)) +(-μ(t) + μ(t)) T (μ -μ * ). (<label>34</label></formula><formula xml:id="formula_87">)</formula><p>From <ref type="bibr" target="#b23">(24)</ref>, it is clear that</p><formula xml:id="formula_88">μ(t) T g(x * ) = g T (x * )(μ + g(x)) + ≤ 0 μ * T g(x * ) = 0.</formula><p>On the other hand, noting that</p><formula xml:id="formula_89">μ + g(x) -(μ + g(x)) + = (μ + g(x)) -≤ 0 μ T (μ + g(x)) -= (μ + g(x)) + T (μ + g(x)) -= 0</formula><p>and by <ref type="bibr" target="#b33">(34)</ref>, we have</p><formula xml:id="formula_90">d dt V (x(t), μ(t)) ≤ -ẋ(t) 2 -2 μ(t) 2 + μ(t) T g(x * ) -μ * T g(x * ) + (μ * -μ) T [μ(t) + g(x(t)) -(μ(t) + g(x(t))) + ] ≤ -ẋ(t) 2 -2 μ(t) 2 + μ * (μ(t) + g(x(t))) - -μ(t) T (μ(t)+g(x(t))) -≤ -ẋ(t) 2 -2 μ(t) 2 . (35) Hence, 0 ≤ V (x(t), μ(t)) ≤ V (x(0), μ(0)) &lt; +∞, for t &gt; 0.</formula><p>On the other hand, since V is radially unbounded, then for any initial point (x 0 , μ 0 ) T ∈ R n × R p , the state (x(t), μ(t)) T of neural network (3) is bounded, which means that (x(t), μ(t)) T exists for t ∈ [0, +∞). In addition, according to <ref type="bibr" target="#b34">(35)</ref>, the equilibrium point (x * , μ * ) T of neural network (3) is stable in the sense of Lyapunov. And by the arbitrariness of (x * , μ * ) T , each equilibrium point of neural network (3) is stable in the sense of Lyapunov. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p><p>We next prove that for any initial point, the state of neural network (3) is convergent to an equilibrium point of neural network <ref type="bibr" target="#b2">(3)</ref>.</p><p>Let</p><formula xml:id="formula_91">H (x, μ) = inf (I -P)[γ + η T μ] 2 + 1 2 μ -μ 2 : γ ∈ ∂ f (x), η ∈ ∂g(x) . (<label>36</label></formula><formula xml:id="formula_92">)</formula><p>It is clear that H (x, μ) = 0 with Ax = b if and only if (x, μ) is an equilibrium point of neural network (3). Due to the boundedness of (x(t), μ(t)) T , there exists a convergent subsequence</p><formula xml:id="formula_93">{(x(t k ), μ(t k )) T |0 ≤ t 1 ≤ t 2 ≤ • • • }, and t k → +∞ such that (x(t k ), μ(t k )) T → ( x, μ) T . By Theorem 3.1, it is obvious that A x = b.</formula><p>We next prove H ( x, μ) = 0, which means that ( x, μ) T is an equilibrium point of neural network (3). First, due to the boundedness of (x(t), μ(t)) T and (30), we choose a sufficiently large constant M &gt; 0 such that</p><formula xml:id="formula_94">ẋ(t) + μ(t) ≤ M for all t ≥ 0. Suppose H ( x, μ) = 0, then H ( x, μ) &gt; 0.</formula><p>According to Theorem 5 on page 52 in <ref type="bibr" target="#b36">[37]</ref>, H (x, μ) is lower semicontinuous with respect to (x, μ). Hence, there exist ε &gt; 0 and δ &gt; 0, such that</p><formula xml:id="formula_95">H (x, μ) &gt; ε (37) for all (x, μ) ∈ B( x, μ; δ) = {(x, μ) ∈ R n × R p : x -x + μ -μ &lt; δ}. Since (x(t k ), μ(t k )) T → ( x, μ) T ,</formula><p>there exists a positive integer N, such that</p><formula xml:id="formula_96">x(t k ) -x + μ(t k ) -μ &lt; δ 2 for all k &gt; N. When t ∈ [t k -δ 4M , t k + δ 4M ] and k &gt; N, we have x(t) -x + μ(t) -μ ≤ x(t) -x(t k ) + x(t k ) -x + μ(t) -μ(t k ) + μ(t k ) -μ ≤ M|t -t k | + δ 2 ≤ δ.</formula><p>By <ref type="bibr" target="#b36">(37)</ref>, we have</p><formula xml:id="formula_97">H (x(t), μ(t)) &gt; ε for all t ∈ [t k -δ/4M, t k + δ/4M] and k &gt; N. Hence +∞ 0 H (x(t), μ(t))dt ≥ k≥N t k -δ 4M ,t k + δ 4M H (x(t), μ(t))dt ≥ k≥N t k -δ 4M ,t k + δ 4M εdt = k≥N δ 2M ε = +∞. (<label>38</label></formula><formula xml:id="formula_98">)</formula><p>On the other hand, from ( <ref type="formula" target="#formula_72">27</ref>) and ( <ref type="formula">35</ref>), there exists a constant V 0 such that lim</p><formula xml:id="formula_99">t →+∞ V (x(t), μ(t)) = V 0 .</formula><p>Hence, by <ref type="bibr" target="#b34">(35</ref></p><formula xml:id="formula_100">) +∞ 0 H (x(t), μ(t))dt = lim s→+∞ s 0 H (x(t), μ(t))dt ≤ -lim s→+∞ s 0 V (x(t), μ(t))dt = -lim s→+∞ [V (x(t), μ(t)) -V (x(0), μ(0))] = V (x(0), μ(0)) -V 0 &lt; +∞.</formula><p>Obviously, it contradicts with <ref type="bibr" target="#b37">(38)</ref>. Hence, H ( x, μ) = 0, which means that ( x, μ) T is an equilibrium point of neural network (3), and x is an optimal solution to the nonlinear convex programming <ref type="bibr">(1)</ref>. That is, there exist</p><formula xml:id="formula_101">γ ∈ ∂ f ( x), η ∈ ∂g( x) such that ⎧ ⎪ ⎨ ⎪ ⎩ 0 = (I -P)( γ + η T μ) 0 = -μ + ( μ + g( x)) + A x = b.</formula><p>Finally, we prove that lim t →+∞ (x(t), μ(t)) T = ( x, μ) T , which completes the proof. We construct another Lyapunov function as follows:</p><formula xml:id="formula_102">W (x, μ) = f (x) -f ( x) + 1 2 μ 2 - 1 2 μ 2 -(x -x) T ( γ + η T μ) -(μ -μ) T μ + 1 2 x -x 2 + 1 2 μ -μ 2 .</formula><p>By similar analysis above, we have</p><formula xml:id="formula_103">W (x, μ) ≥ 1 2 x -x 2 + 1 2 μ -μ 2 d dt W (x(t), μ(t)) ≤ 0.<label>(39)</label></formula><p>Clearly, W ( x, μ) = 0. By the continuity of W , for any ε &gt; 0, there exists δ &gt; 0 such that</p><formula xml:id="formula_104">W (x, μ) = |W (x, μ) -W ( x, μ)| &lt; ε (40) when x -x 2 + μ -μ 2 &lt; δ. Meanwhile, since (x(t k ), μ(t k )) T → ( x, μ) T , there exists t N such that x(t N ) -x 2 + μ(t N ) -μ 2 &lt; δ. (<label>41</label></formula><formula xml:id="formula_105">)</formula><p>Hence, combining with (39), <ref type="bibr" target="#b39">(40)</ref>, and (41), we have From Theorem 3.1, there exists t 0 &gt; 0 such that the state of neural network (3) reaches the equality feasible region in finite time t 0 and stay there thereafter. Hence, from Theorem 3.2, the state of neural network (3) is unique for t ∈ [t 0 , +∞).</p><formula xml:id="formula_106">1 2 x(t) -x 2 + 1 2 μ(t) -μ 2 ≤ W (x(t), μ(t)) ≤ W (x(t N ), μ(t N )) ≤ ε for all t &gt; t N ,</formula><p>Remark 2: The nonlinear convex programming (1) has been studied extensively by many researchers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Compared with existed neural networks, the neural network (3) is not based on penalty method, and has low model complexity. For example, in <ref type="bibr" target="#b27">[28]</ref>, the authors proposed a three-layer recurrent neural network to solve the optimization problem <ref type="bibr">(1)</ref>.</p><p>On the other hand, just as said in Remark 4 of <ref type="bibr" target="#b27">[28]</ref>, after some simple translation, the proposed neural network in <ref type="bibr" target="#b27">[28]</ref> can be simplified as a two-layer recurrent neural network. However, in <ref type="bibr" target="#b27">[28]</ref>, it is only proved that the state of the simplified neural network is exponentially convergent to the equality feasible region X 2 = {x : Ax = b}, not in finite time. It is possible that the state of the simplified neural network is exponentially convergent to X 2 , but never attain X 2 . Hence, to solve optimization problem (1), the initial point of the simplified neural network in <ref type="bibr" target="#b27">[28]</ref> should be chosen in X 2 . Obviously, it is not an easy job, especially in high dimension Euclidean space. In this paper, the state of neural network (3) is convergent to the equality feasible region X 2 in finite time, which means that we need not choose some special initial point in advance.</p><p>Remark 3: Recently, to solve the nonlinear programming (1) efficiently, some simplified neural networks were proposed <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However, most of these simplified neural networks were based on some stricter hypothesizes. For example, in <ref type="bibr" target="#b38">[39]</ref>, a one-layer neural network was proposed to solve (1) based on the following assumption: int(X 1 ) X 2 is nonempty and bounded.</p><p>Moreover, the neural network in <ref type="bibr" target="#b38">[39]</ref> was also based on penalty function method. Meanwhile, to avoid penalty parameters, Bian and Xue in <ref type="bibr" target="#b35">[36]</ref> proposed another neural network based on the assumption <ref type="bibr" target="#b41">(42)</ref>. Obviously, the hypothesizes in this paper are weaker than those in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b38">[39]</ref>, and neural network (3) is not based on penalty function method. Here, we introduce an example to show the advantage of neural network <ref type="bibr" target="#b2">(3)</ref>.</p><p>Example 1: Consider the following nonsmooth optimization problem:</p><formula xml:id="formula_108">min f (x) = 10(x 1 + x 2 ) 2 + (x 1 -2) 2 + 20|x 3 -3| + e x 3 s. t. g(x) = (x 1 + 3) 2 + x 2 -36 ≤ 02x 1 + 5x 3 = 7. (43)</formula><p>Obviously, f (x) and g(x) are convex. Moreover, after simple calculation, we have {(1, x 2 , 1)</p><formula xml:id="formula_109">T ∈ R 3 : x 2 &lt; 20} ⊆ int{x ∈ R 3 : (x 1 + 3) 2 + x 2 -36 ≤ 0} {x ∈ R 3 : 2x 1 + 5x 3 = 7}. It is easy to see that the set {(1, x 2 , 1) T ∈ R 3 : x 2 &lt; 20}</formula><p>is unbounded, which means that neural networks in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b38">[39]</ref> may not be capable of solving this problem. On the other hand, the proposed neural network (3) can be used to solve problem <ref type="bibr" target="#b42">(43)</ref>. Here, we use the proposed neural network (3) to solve problem <ref type="bibr" target="#b42">(43)</ref>. Fig. <ref type="figure" target="#fig_3">3</ref> displays the transient behavior of (x 1 (t), x 2 (t), x 3 (t)) T of neural network (3) with three random initial points A 1 = (-1, 0, 1, 6), A 2 = (-0.5, 0.5, 1.5, 6), and A 3 = (0, 1.5, 2, 6). It is easy to see that states of neural network (3) starting at different initial points are all convergent to the unique equilibrium point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATIONS AND NUMERICAL EXAMPLES</head><p>In this section, two examples are proposed to show the effectiveness of neural network (3). The simulation is conducted on MATLAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nonlinear Convex Programming With Linear Constraints</head><p>As a special case of the nonlinear convex programming (1), we study the following nonlinear convex programming with linear constraints:</p><formula xml:id="formula_110">minimize f (x) subject to Ax = b (44) where x = (x 1 , x 2 , . . . , x n ) T ∈ R n , f : R n → R is the objective function which is convex but may be nonsmooth, A ∈ R m×n is a full row-rank matrix (i.e., rank(A) = m ≤ n), b = (b 1 , b 2 , . . . , b n ) T ∈ R n .</formula><p>According to the discussions in Section IV, the related neural network (3) reduces to the following one-layer neural network:</p><formula xml:id="formula_111">ẋ(t) ∈ -(I -P)∂ f (x(t)) -A T h(Ax(t) -b) (<label>45</label></formula><formula xml:id="formula_112">)</formula><p>where I , P and h are similarly defined in (3). Theorem 5.1: 1) For any initial point x(0) ∈ R n , there is at least a solution x(t) of neural network (45), which will reach X 2 = {x ∈ R n |Ax = b} in finite time and stay there thereafter. 2) x * is an equilibrium point of neural network (45) if and only if x * is an optimal solution to problem <ref type="bibr" target="#b43">(44)</ref>. 3) Each equilibrium point of neural network (45) is stable in the sense of Lyapunov, and the state of neural network (45) is convergent to an equilibrium point of neural network (45). Remark 4: Recently, to reduce the model complexity, some one-layer neural networks were proposed <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Especially, Guo et al. <ref type="bibr" target="#b28">[29]</ref> used neural network (45) to solve problem <ref type="bibr" target="#b43">(44)</ref> with pseudoconvex function f . In addition, they proved that the state of the neural network (45) was globally convergent to the equilibrium point set. Obviously, according to Theorem 5.1, when f is convex, neural network (45) is convergent to an equilibrium point, which improves the conclusions of <ref type="bibr" target="#b28">[29]</ref>. Next example illustrates it.</p><p>Example 2: Consider the following optimization problem:</p><formula xml:id="formula_113">minimize f (x) = 1 2 (x 1 + x 2 ) 2 subject to x 1 + x 2 + x 3 = 7.<label>(46)</label></formula><p>In this problem, the objective function f is convex. Here, we use neural network (45) to solve this problem. By Theorem 5.1, for any initial point x(0) ∈ R n , the state of neural network (45) is convergent to an equilibrium point of neural network (45). Obviously, the optimal solution set of problem ( <ref type="formula" target="#formula_113">46</ref>) is <ref type="figure" target="#fig_4">4</ref> shows the transient behavior of x(t) based on neural network (45) with eight random initial points A i (i = 1, 2, . . . , 8). From Fig. <ref type="figure" target="#fig_4">4</ref>, one can see that from different initial point A i , the state of neural network (45) is convergent to different equilibrium point of neural network (45), which all is the optimal solution of problem (46).</p><formula xml:id="formula_114">= {x = (x 1 , x 2 , x 3 )|x 1 + x 2 = 0, x 3 = 7}. Fig.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. L 1 -Norm Minimization Problem</head><p>L 1 -norm minimization problem, also known as the constrained least absolute deviation problem, refers to finding the minimum L 1 -norm solution subject to some constraints, which can be shown as where C ∈ R n×n , d ∈ R n , g(x) = (g 1 (x), g 2 (x), . . . , g p (x)) T : R n → R p is a p-dimensional vector valued function, A ∈ R m×n is a full row-rank matrix (i.e., rank</p><formula xml:id="formula_115">minimize Cx -d 1 subject to g(x) ≤ 0 Ax = b, x ∈<label>(47)</label></formula><formula xml:id="formula_116">(A)= m ≤ n), b = (b 1 , b 2 , . . . , b n ) T ∈ R n , ⊆ R n is a convex set.</formula><p>L 1 -norm minimization problem plays an important role in engineering applications, such as manipulator control, signal processing, support vector machine, nonlinear curve fitting problem, and so on <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>. Obviously, L 1 -norm minimization problem (47) is a nonsmooth convex optimization problem if g(x) is convex. We simulate the neural network (49) by MATLAB. Fig. <ref type="figure" target="#fig_5">5</ref> displays the trajectory of the state variables x 1 , x 2 , x 3 , μ of neural network (49) with three different initial points (-0.6, 1, -0.2, 1.2) T , (-1.5, 0.5, 0, 1.6) T , and (-1, 1.5, 0.4, 1.8) T . In Fig. <ref type="figure" target="#fig_5">5</ref>, one can easily see that the states of neural network (49) with different initial points are all convergent to (-1, 1, 0, 1.44) T , which is an equilibrium point of neural network (49). Hence, the optimal solution of L 1 -norm optimization problem (48) is x * = (-1, 1, 0) T and Cx *d 1 = 10.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presents a two-layer recurrent neural network modeled by a differential inclusion for solving nonsmooth convex optimization problem subject to convex inequality and linear equality constraints. The equilibrium point set of the proposed neural network is equivalent to the KKT optimality set of the nonsmooth convex optimization problem. Each equilibrium point of the proposed neural network is stable in the sense of Lyapunov, and the state of the proposed neural network starting at any initial point is convergent to an equilibrium point. Compared with existing neural network models, this neural network has two-layer structure and does not depend on any penalty parameters, which means that it is more convenient for design and implementation. As applications, three examples are proposed to show the effectiveness of the proposed neural network. Meanwhile, it is well known that many nonlinear programming problems can be formulated as nonconvex optimization problems. Hence, proposing a simplified recurrent neural network for nonconvex optimization problems (especially pseudoconvex optimization problems) is our further work.</p><p>He was an Assistant Professor with the University  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Block diagram of F by circuits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Trajectories of the states (x 1 (t), x 2 (t), x 3 (t)) T of neural network (3) with three different initial points in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Transient behaviors of x(t) of neural network (45) with eight random initial points in Example 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Transient behaviors of state variables of neural network (45) in Example 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 3 :f</head><label>3</label><figDesc>Consider the following L 1 -norm optimization problem: minimize Cxd 1 subject to g(x) ≤ 0 Ax = b It is clear that problem (48) is a nonsmooth convex optimization problem. Neural network (3) associated to (48) can be described asẋ ∈ -(I -P)[∂ f (x) + ∂g(x) T μ] -A T h(Axb) 2 μ = -μ + (μ + g(x)) + (x) = |x 1 + 3.5x 3 -1.5| + |2x 2 + 1.6x 3 -3.8|+|x 1 + x 2 + x 3 + 6.2|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 of</head><label>5</label><figDesc>Harbin Institute of Technology, Weihai, China, 6 from 2010 to 2013, where he is currently an Asso-7 ciate Professor. His current research interests include 8 neural network theory and optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9 Xiaoping</head><label>9</label><figDesc>Xue was born in Inner Mongolia, China, 10 in 1963. He received the Ph.D. degree in mathemat-11 ics from the Harbin Institute of Technology, Harbin, 12 China, in 1991. 13 He has been a Full Professor of Mathematics with 14 the Harbin Institute of Technology since 1997, where 15 he was an Assistant Professor from 1991 to 1992 16 and an Associate Professor from 1992 to 1997. He 17 has authored more than 80 scientific papers and two 18 monographs. His current research interests include</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>nonempty values is said to be upper semicontinuous (U.S.C.) at x ∈ K if for any open set U y containing F( x), there exists a neighborhood U x of x such that F(U x ) ⊆ U y . If K is closed, F has nonempty closed values and is bounded in a neighborhood of each point x ∈ K , then F is upper semicontinuous on K if and only if the graph of F (denoted by Gr (F)) is closed, where Gr (F) is defined as</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc></figDesc><table /><note><p><p>OF RELATED NEURAL NETWORKS FOR SOLVING THE CONVEX PROGRAMMING</p>(1)    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>which implies that lim In Theorem 4.1, it is proved that for any initial point, the state of neural network (3) exists for t ∈ [0, +∞).</figDesc><table><row><cell>(x(t), μ(t)) T = t →+∞ ( x, μ) T . Therefore, for any initial point (x 0 , μ 0 ) T ∈ R n × R p ,</cell></row><row><cell>the state (x(t), μ(t)) T of neural network (3) is convergent</cell></row><row><cell>to an equilibrium point of neural network (3). Meanwhile,</cell></row><row><cell>from different initial point, the state of neural network (3)</cell></row><row><cell>may be convergent to different equilibrium point of neural</cell></row><row><cell>network (3).</cell></row><row><cell>Remark 1:</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Editor-in-Chief, the associate editor, and the anonymous reviewers for their insightful and constructive comments, which helped to enrich the content and improve the presentation of this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China under Grant 11271099, Grant 11126218, and Grant 11101107, and in part by the China Post-Doctoral Science Foundation under Project 2013M530915.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>network theory.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nonlinear Programming: Theory and Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Shetty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Unbehauen</surname></persName>
		</author>
		<title level="m">Neural Networks for Optimization and Signal Processing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A dual neural network for redundancy resolution of kinematically redundant manipulators subject to joint limits and joint velocity limits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="658" to="667" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed adaptive coordinated control of multi-manipulator systems using neural networks</title>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robot Intell</title>
		<meeting>Robot Intell</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A recurrent neural network for hierarchical control of interconnected dynamic systems</title>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Nikiforuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="466" to="481" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constrained multi-variable generalized predictive control using a dual neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural network model with bounded-weights for pattern classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L W</forename><surname>Nuttle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1411" to="1426" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple &apos;neural&apos; optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural networks for nonlinear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="554" to="562" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel recurrent neural network for solving nonlinear optimization problems with inequality constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1340" to="1353" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A recurrent neural network for nonlinear convex optimization subject to nonlinear inequality constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1385" to="1394" />
			<date type="published" when="2004-07">Jul. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The optimization technique for solving a class of non-differentiable programming based on neural network method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal., Real World Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1108" to="1114" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel neural network for nonlinear convex programming</title>
		<author>
			<persName><forename type="first">X.-B</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="621" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A recurrent neural network with exponential convergence for solving convex quadratic program and related linear piecewise equations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1003" to="1015" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamical analysis of neural networks of subgradient system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2347" to="2352" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamical behavior of a class of nonsmooth gradient-like systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2632" to="2641" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global exponential stability and global convergence in finite time of neural networks with discontinuous activations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A recurrent neural network for solving nonlinear convex programs subject to linear constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="386" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized neural network for nonsmooth nonlinear programming problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1741" to="1754" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous activation function for linear programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1366" to="1383" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous hard-limiting activation function for quadratic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="558" to="570" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for support vector machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1261" to="1269" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subgradient-based neural networks for nonsmooth convex optimization problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2391" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subgradient-based neural networks for nonsmooth nonconvex optimization problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1024" to="1038" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel recurrent neural network with finite-time convergence for linear programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2962" to="2978" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convergence of neural networks for programming problems via a nonsmooth Łojasiewicz inequality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1471" to="1486" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new one-layer neural network for linear and quadratic programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="918" to="929" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network for non-smooth convex optimization problems with application to the identification of genetic regulatory networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="714" to="726" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for pseudoconvex optimization subject to linear equality constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1892" to="1900" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for constrained pseudoconvex optimization and its application for dynamic portfolio optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Worst-case complexity of smoothing quadratic regularization methods for non-Lipschitzian optimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1718" to="1741" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Finite-time convergent recurrent neural network with a hard-limiting activation function for constrained optimization with piecewise-linear objective functions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="601" to="613" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invariance principle and complete stability for cellular neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II, Exp. Briefs</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="202" to="206" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Absolute stability of analytic neural networks: An approach based on finite trajectory length</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2460" to="2469" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global exponential stability and global convergence in finite time of delayed neural networks with infinite gain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1449" to="1463" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural network for solving constrained convex optimization problems with global attractivity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="710" to="723" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Aubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cellina</surname></persName>
		</author>
		<title level="m">Differential Inclusions</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Optimization and Nonsmooth Analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for constrained nonsmooth optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1323" to="1333" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differential equations with discontinuous right-hand side</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Filippov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="231" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A neural network model for non-smooth optimization over a compact convex subset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Netw.-Int. Soc. Nutrigenet. Nutrigenom. (ISNN)</title>
		<imprint>
			<biblScope unit="volume">3971</biblScope>
			<biblScope unit="page" from="344" to="349" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pedestrian detection in images via cascaded L1-norm minimization learning method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2573" to="2583" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The comparison of 1 and 2 -norm minimization methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bektaş</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Şişman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Phys. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1721" to="1727" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph cuts via 1 norm minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhusnurmath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sitian Qin was born in Shandong</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">Oct. 2008. 1981</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1866" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">He received the Ph.D. degree in mathematics from 2 the Harbin Institute of Technology</title>
		<imprint>
			<pubPlace>Harbin, China</pubPlace>
		</imprint>
	</monogr>
	<note>3 in 2010</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
