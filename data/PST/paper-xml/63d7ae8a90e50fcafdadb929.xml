<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional ?-VAE for De Novo Molecular Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Richards</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Austen</forename><forename type="middle">M</forename><surname>Groener</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">De</forename><surname>Novo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional ?-VAE for De Novo Molecular Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Drug-discovery</term>
					<term>Recurrent VAE</term>
					<term>Molecular Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has significantly advanced and accelerated de novo molecular generation. Generative networks, namely Variational Autoencoders (VAEs) can not only randomly generate new molecules, but also alter molecular structures to optimize specific chemical properties which are pivotal for drugdiscovery. While VAEs have been proposed and researched in the past for pharmaceutical applications, they possess deficiencies which limit their ability to both optimize properties and decode syntactically valid molecules. We present a recurrent, conditional ?-VAE which disentangles the latent space to enhance post hoc molecule optimization. We create a mutual information driven training protocol and data augmentations to both increase molecular validity and promote longer sequence generation. We demonstrate the efficacy of our framework on the ZINC-250k dataset, achieving SOTA unconstrained optimization results on the penalized LogP (pLogP) and QED scores, while also matching current SOTA results for validity, novelty and uniqueness scores for random generation. We match the current SOTA on QED for top-3 molecules at 0.948, while setting a new SOTA for pLogP optimization at 104.29, 90.12, 69.68 and demonstrating improved results on the constrained optimization task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Druglikeness is an essential ingredient for conducting virtual screening of molecules in early-stage drug discovery-prior to synthesis and testing <ref type="bibr" target="#b0">[1]</ref>. Fig. <ref type="figure">1</ref>: Presented here is a survey of molecular generation studies which utilize the ZINC-250k benchmark dataset. Techniques fall into one of several families, which includes variational autoencoders (VAE), generative adversarial networks (GAN), recurrent neural networks (RNN), graph convolutional networks (GCN), monte carlo (MC), reinforcement learning (RL), and combinations of multiple methods (indicated by a "+"). We compare these against our own method proposed in this work (?-CVAE) which make use of conditional variational autoencoders to achieve state-of-the-art performance by a large margin. Top-3 best scores for each method (ranked by pLogP) are presented.</p><p>Factors such as solubility in fat and water determine if an orally administered drug can both reach and penetrate the cell membrane and are often described by molecular properties such as the octanol-water partition coefficient (Log P) and the quantitative estimate of drug-likeness (QED). While such properties can be measured experimentally they can also be predicted computationally, allowing for brute-force search of candidate molecules. Increasingly, the latest models and techniques from the field of artificial intelligence are being brought to bear on the challenges of drug development, enabling quicker, cheaper, and more effective means of discovery <ref type="bibr" target="#b1">[2]</ref>.</p><p>The fundamental objective of de novo drug synthesis is to uncover molecular structures with desired properties of interest. It has been estimated that the number of realistic drug-like molecules which could ever be synthesized ranges between 10 23 and 10 60 <ref type="bibr" target="#b2">[3]</ref>, immediately highlighting the benefits of favoring intelligent search strategies over brute force ones. The most successful of these intelligent search strategies promises to accelerate the development of novel drugs by significantly decreasing the number of wet-lab experiments needed to discover candidate molecules.</p><p>Three categories deep learning model architectures have been consistently explored for de novo molecular generation: autoencoders, graph networks, and reinforcement learning. This paper focuses on advancing autoencoder based methods, specifically Variational Autoencoders (VAEs). Semantically similar to standard autoencoders, VAEs have been widely used <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> to encode a molecule (usually represented via textual strings) into a lower dimensional latent space, from which it is decoded back to the starting molecule. VAEs that learn from molecular strings rely heavily on recurrent modules within the network, such as long short-term memory (LSTM) cells <ref type="bibr" target="#b11">[12]</ref>, gated recurrent units (GRUs) <ref type="bibr" target="#b12">[13]</ref>, and self-attention mechanisms <ref type="bibr" target="#b13">[14]</ref>, to learn relationships between user-defined "tokens", which can be atoms, bonds, rings, or any combination of substructures or functional groups. Recurrent modules, model architecture design principles, and training protocols used in these studies are mainly driven by advancements in the field of Natural Language Processing (NLP) <ref type="bibr" target="#b14">[15]</ref>.</p><p>Optimization routines performed after training (post hoc) are used to intelligently navigate the learned latent space to optimize some molecular property. Popular choices for these post hoc routines include Bayesian Optimization (BO) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref> and Particle Swarm Optimization (PSO) <ref type="bibr" target="#b15">[16]</ref>. These techniques have been used in both constrained and unconstrained testing environments to maximize properties such as pLogP and QED.</p><p>Other benchmarks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref> gauge how well these optimization routines can re-locate an existing molecule, find molecules with diverging similarity metrics, and locate molecules with varying levels of fingerprint similarity.</p><p>In this paper, we advance and simplify existing techniques to condition the latent space of a VAE used for molecular generation and prove that a well-conditioned latent space facilitates post hoc molecular optimization for maximizing pLogP and QED scores. Furthermore, we develop a robust training protocol which automatically adjusts the KL divergence loss weight to maximize the posterior mutual information. We analyze common constraints and limitations from previously published textual based (e.g. SMILES) VAEs for maximizing the pLogP score and present simple solutions for overcoming these deficiencies. As a product of these changes and enhancements, we produce SOTA results for the ZINC-250k unconstrained pLogP benchmark, while closely matching SOTA for the random generation (Gaussian prior) benchmark's validity, novelty, and uniqueness scores, and produce a new SOTA for validity without check portion of the benchmark. Our proposed framework is designed to be customizable so that other NLP architectures can easily supplant the existing encoder and decoder networks. Further, our latent conditioning method is extensible to arbitrary new molecular properties, so that other de novo molecular optimization tasks can be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vanilla Variational Autoencoder (VAE) for Sequence Generation</head><p>A Variational Autoencoder (VAE) is a generative network derived from a standard autoencoder (AE) framework that observes a space x in terms of a prior distribution over a latent space p(z) and conditional likelihood of generating a data sample from a latent space p ? (x|z) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. The optimal parameter set ? * that maximizes the probability of constructing x is given by ? * = argmax x n i=1 log p ? (x i ) <ref type="bibr" target="#b19">[20]</ref>. Then the true log likelihood log p ? (x i ) can be formulated as log p ? (x i ) = log p ? (x i |z)p ? (z). Since this is intractable, the evidence lower bound (ELBO) is instead maximized, defined in Eq. 1. ELBO relies on a variational approximation to the posterior distribution q ? (z|x) to maximize the log likelihood.</p><formula xml:id="formula_0">L(x; ?, ?) = E q ? (z|x) [log p ? (x|z)] -D KL (q ? (z|x)||p(z)) ? log p(x).<label>(1)</label></formula><p>Following the ELBO formulation, the VAE is comprised of a probabilistic encoder q ? (z|x) and decoder p ? (x|z). The encoder is parameterized by ? and learns a functional mapping between input x to the posterior multivariate Gaussian distribution in the latent space z. The decoder, parameterized by ? learns to reconstruct x from the variational distribution z. The VAE seeks to minimize the reconstruction term in ELBO such that the encoder generates meaningful latent vectors which the decoder can subsequently reconstruct. The Kullback-Leibler (KL) loss minimizes the KL divergence between the approximate posterior q ? (z|x) and the Gaussian prior distribution p(z) ? N (0, I). An appealing advantage of the VAE over a standard AE is that it can be used as a post hoc generator, capable of decoding lower dimensional latent vectors via a trained decoder.</p><p>The vanilla VAE usually suffers from posterior collapse, a phenomenon by which the network learns a trivial local optimum of the ELBO objective, causing the variational posterior to be severely misrepresented as the true posterior <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. He et al. <ref type="bibr" target="#b20">[21]</ref> demonstrate that posterior collapse can be mathematically represented as a local optimum of VAEs whereby both the encoder and decoder networks equivalently represent the latent output z as the prior distribution, shown in Eq. 2. The authors from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref> show that posterior collapse simply occurs as a direct result of an underrepresented (or lagging) reconstruction loss. During training, vanilla (imbalanced) VAEs will equally weight the reconstruction and KL loss terms which leads to the network quickly optimizing the KL term, locating an inaccurate local optimum that poorly approximates the true posterior.</p><formula xml:id="formula_1">q ? (z|x) = p ? (z|x) = p(z) ?x ? R<label>(2)</label></formula><p>Higgins et al. <ref type="bibr" target="#b21">[22]</ref> introduced the ?-VAE to combat posterior collapse and therefore improve, or disentangle, the latent representation of samples (image-based datasets). The authors <ref type="bibr" target="#b21">[22]</ref> introduced a hyperparameter ? in the loss function that is applied statically during training, which led to significant improvements in disentanglement scores over the vanilla VAE implementation. The resultant ELBO formulation is shown in Eq. 3, where ? is simply a multiplicative weight to the KL loss term. Other works <ref type="bibr" target="#b22">[23]</ref> have shown that dynamically altering ? during training can lead to more optimal results. Bowman et al. <ref type="bibr" target="#b22">[23]</ref> propose a training scheduler which anneals (linearly increasing, although other approaches such as cyclic ramp and de-ramp have been used) ? during training and clips it to an empirically derived optimal maximum. Additionally, Yan et al. <ref type="bibr" target="#b9">[10]</ref> show that this linear annealing process can be effectively used for disentangling latent representations for a ?-VAE that was specifically engineered to generate molecular SMILES strings. Yan et al. <ref type="bibr" target="#b9">[10]</ref> empirically derive an optimal maximum of ? = 0.1 for the ZINC-250k dataset.</p><formula xml:id="formula_2">L(x; ?, ?) = E q ? (z|x) [log p ? (x|z)] -?D KL (q ? (z|x)||p(z)) ? log p(x), 0 ? ? &lt; 1 (3)</formula><p>Though the ?-VAE is a robust solution to mitigate posterior collapse and to improve training, molecular generation from such a network can still be limited. Typically, molecular generation tasks are goaldirected, meaning that they involve using a post hoc optimization framework which targets specific molecular properties known a priori. Such a framework usually employs an optimizer which intelligently navigates a learned posterior, basing its trajectory on intermediate properties from decoded molecules. Vanilla ?-VAEs lack sufficient network constraints and additional functionality to condition the posterior distribution on such properties. Conditional VAEs (CVAEs) can solve this by generating molecules with specific target properties that are imposed by explicitly defined condition vectors. Prior research, e.g. Lim et al 's CVAE <ref type="bibr" target="#b23">[24]</ref>, however shows that CVAEs struggle to disentangle the latent molecular representations (evident by PCA/t-SNE reduction techniques of the posterior), and achieve low molecular validity when decoding. Other approaches like G?mez-Bombarelli et al. <ref type="bibr" target="#b3">[4]</ref> reframe the standard VAE architecture to promote more latent space disentangling by additionally regressing targets from latent representations. G?mez-Bombarelli et al 's VAE illustrates sufficient latent space separation based on the respective targets, but achieves low reconstruction and validity during decoding, while also struggling to produce molecules with high-valued penalized logP scores, where a vanilla ?-VAE <ref type="bibr" target="#b9">[10]</ref> demonstrated better results (without imposing any conditional constraints). Other unique conditioning techniques for VAEs (including graph-based approaches) have been proposed <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> which demonstrate competitive reconstruction and validity rates, as well as low inference times, but have not been applied to common constrained or unconstrained benchmarks. One of the core methods presented in our paper introduces an alternative to conditional sampling which instead directly encodes property information into the latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Posterior Conditioning Techniques</head><p>We first discuss how probabilistic regression techniques can be used to as a conditioning method, then provide our simplifications and other contributions. Zhao et al. <ref type="bibr" target="#b26">[27]</ref> made significant contributions to disentangling the latent space of a standard VAE by conditioning the latent vectors on continuous, task-dependent features.</p><p>Rather than sampling from a single Gaussian prior N (0, I), Zhao et al. condition z on a target property c, effectively formulating a conditional distribution p(z|c) which captures a property-specific prior on latent representations. The regression network predicting p(z|c) is referred to as a latent generator <ref type="bibr" target="#b26">[27]</ref>. We are able to disentangle the target property c from the prior through the construction of p(z|c), shown in Eq. 4. Here, altering c induces a change in the mean of the resultant Gaussian distribution. Therefore, continuous targets can be encoded in the mean of the distribution.</p><formula xml:id="formula_3">p(z|c) ? N (z; u T c, ? 2 I), u T u = 1<label>(4)</label></formula><p>However, c is paired with and dependent on respective input samples x, giving the true posterior the final form of: p(z, c|x). If c is unknown during training or for semi-supervised applications, a probabilistic regressor q ? (c|x) is required. Zhao et al. realize q ? (c|x) with a standard feed-forward network to regress continuous targets c from inputs x. q ? (c|x) is enforced during training with an additional loss term log q ? (c|x) to minimize the difference between the predicted target and its respective ground truth. Finally, to regulate the conditional distribution p(z, c|x), the KL loss term is altered to instead minimize the KL loss between the conditional prior p(z|c) and the posterior generated by the encoder p ? (z|x), shown in Eq. 5. The conditional component of our ?-CVAE is derived from p(z, c|x) since the approximated posterior is conditioned on continuous targets.</p><formula xml:id="formula_4">L(x, c; ?, ?, ?) = E q ? (z|x) [log p ? (x|z)] -D KL (q ? (z|x)||p(z|c)) -log q ? (c|x) ? log p(x).</formula><p>(5)</p><p>For our study where target, c i , is known for each molecular input x i , we instead opt to simply remove the probabilistic regressor q ? (c|x) and latent generator p(z|c), and instead formulate a new multivariate Gaussian prior p(? i ) ? N (? i , I), where ?i is the standardized target ?i = ci-?c ?c , while ? c and ? c are the mean and standard deviation computed across all targets respectively. Hence, we aim to minimize the KL divergence between the q ? (z|x) and N (?, I), rather than using a zero-mean Gaussian prior (p(z) ? N (0, I)), which effectively minimizes the difference between the mean of the posterior and the respective standardized targets. We refer to this latent disentanglement method as "explicit latent conditioning" (ELC). ELC enables disentanglement by altering latent vectors without imposing additional networks, losses, or altering GRU (or other recurrent units) states which complicate training and increase the complexity and size of the network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. ELC allows us to architect the latent space such that molecules with similar values of ? fall within close proximity. Further, this disentanglement method can be generalized to any desired molecular properties external to pLogP and QED such as optoelectronic properties <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, chemical reactivity, melting point, and solubility <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Although this paper focuses on univariate molecular properties, practitioners often seek to generate molecules with multiple constraints or desired ranges of multiple properties. ELC can be easily extended to multivariate cases to facilitate multi-property constrained optimization tasks.</p><p>We show in Sec. 3.4 that disentangling the latent space with this technique dramatically enhances unconstrained optimization performance. Our ELBO objective function has the final form shown in Eq. 6</p><formula xml:id="formula_5">L(x; ?, ?) = E q ? (z|x) [log p ? (x|z)] -?D KL (q ? (z|x)||p(?)) ? log p(x) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mutual Information</head><p>While several dynamic ? schedulers have been introduced in previous works to regulate the fractional impact of the KL loss term to avoid posterior collapse, we still require a metric for monitoring whether posterior collapse is occurring. Hoffman and Johnson <ref type="bibr" target="#b32">[33]</ref> derive the Mutual Information (MI) of a VAE, shown in Eq. 7</p><formula xml:id="formula_6">I q =E p d (x) [D KL (q ? (z|x)||p(z))]- D KL (q ? (z)||p(z)),<label>(7)</label></formula><p>Yan et al. showed that I q ? 0 is indicative of posterior collapse in a VAE, while their well-trained network achieves a maximum of I q ? 4.8. We use MI as a metric for both monitoring the quality of latent space representations and as a mechanism for deriving the best value for ? at each training step. We use the same method as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref> for computing mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MI-centric Training Protocol</head><p>After a thorough assessment of prior work <ref type="bibr" target="#b9">[10]</ref>, it is evident that the pivotal mechanism behind successfully training any VAE for molecular generation is the ability to dynamically alter the KL-loss weight, ?. Further, properly balancing the KL loss during training such that the posterior mutual information is maximized results in higher chemical validity, novelty, and uniqueness during the decoding phase (metrics used in <ref type="bibr" target="#b33">[34]</ref> and presented in Table <ref type="table">1</ref>).</p><p>We find that widely used scheduling methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> for adjusting ? during training are suboptimal as their rigid design prevents the posterior MI from being maximized. Further, the selection of parameters used in the chosen scheduling method (ramp, de-ramp, cyclic timing, among other hyperparamters) is crucial to optimizing network performance. Optimal hyperparamter values are difficult to approximate, especially when altering major architectural features and loss functions, while executing a robust search algorithm to optimize can be compute and time intensive. We instead opt for a more versatile and dynamic approach which involves using a non-linear proportional-integral (PI) controller to alter ? during training such that the posterior MI reaches a targeted value known a priori. Shao et al. <ref type="bibr" target="#b34">[35]</ref> present such a method, which approximates the ideal value for ? at each training step following Eq. 8.</p><formula xml:id="formula_7">?(t) = K p 1 + exp(e(t)) -K i t j=0 e(j) + ? min ,<label>(8)</label></formula><p>From Eq. 8, e(t) is the error function that guides the PI controller at training step t and is originally defined by e(t) = v kl -vkl (t) <ref type="bibr" target="#b34">[35]</ref>, where vkl (t) is the KL loss at the current training step and v kl is the target KL loss. Following the empirically derived values from <ref type="bibr" target="#b34">[35]</ref>, we set K p and K i to 0.01 and 0.001, respectively. Targeting an arbitrarily low KL loss leads to naive latent representations and can induce posterior collapse <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, hence careful selection of v kl is required. More importantly, an acceptable value for v kl does not guarantee maximization of the posterior mutual information, which is our goal. We redefine e(t) to instead target an optimal posterior mutual information score v M I . Therefore the PID controller modifies ? based on the difference between the MI value at each step with the targeted value. However, similar to v kl in <ref type="bibr" target="#b34">[35]</ref>, we find that the selection of v M I is sensitive, as choosing an arbitrary high value induces a sustained low ? value that results in poor KL losses. We empirically deduce, aligned with past work <ref type="bibr" target="#b9">[10]</ref>, that a v M I of ? 4.85 leads to optimal and stable training results. Following previously described schedulers, we initialize ? at 0 to introduce a larger focus on minimizing the reconstruction loss during the beginning epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">?-CVAE Architecture</head><p>The final architecture of our proposed ?-CVAE network is illustrated in Fig. <ref type="figure">2</ref>, which consists of two major components: an encoder and decoder. For the encoder, we transform each tokenized input sequence to continuous multi-dimensional vectors via a standard embedding layer, which is further enhanced by using a positional embedding layer, encoding the token position in each embedding vector. The remainder of the encoder uses a basic multi-head attention driven framework employed in the vanilla Transformer architecture <ref type="bibr" target="#b13">[14]</ref>. For the decoder, we opt to use a similar architecture to the one used in <ref type="bibr" target="#b9">[10]</ref> which incorporates Gated Recurrent Units (GRUs) <ref type="bibr" target="#b12">[13]</ref> over multi-head attention layers (used in the Transformer framework). We also improve our training performance by using teacher forcing in our decoder network which was proven to lead to more stable training in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">VAE Sequence Generation Limitations and Remedies</head><p>Previous methods for de novo molecular generation which rely on tokenized strings (e.g. SMILES, InChI) have poorly optimized the penalized logP property for both constrained and unconstrained optimization, whereas graph-based methods have generally prevailed. We conjecture that this poor performance can be attributed to two major deficiencies: (1) insufficient memory for generating long sequences and (2) an absence of specific tokens used to represent larger molecules in string format. We elaborate on each deficiency and present solutions to each in the following sections, while we show results of each remedy in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Sequence Length Constraints</head><p>The penalized logP metric <ref type="bibr" target="#b7">[8]</ref> measures the estimated octanol-water partition coefficient (logP) while simultaneously accounting for ring size and synthetic accessibility, shown in Eq. 9.</p><formula xml:id="formula_8">y(m) = logP (m) -SA(m) -cycle(m),<label>(9)</label></formula><p>A more accurate algorithm for estimating logP was introduced by Wildman and Crippen <ref type="bibr" target="#b36">[37]</ref>. This involves Fig. <ref type="figure">2</ref>: Main ?-CVAE architecture, which uses a multi-head attention driven encoder network to produce q ? (z|x) followed by a GRU-based decoder network that produces p ? (x|z). Latent samples produced by the encoder are to compute the KL loss term, while decoder outputs have a softmax activation applied and are passed into a categorical cross-entropy (CCE) loss to drive the decoder outputs to the original tokenized input sequences. a simple summation of individual logP scores from molecular constituents known a priori <ref type="bibr" target="#b36">[37]</ref>, given by Eq. 10, where for some molecule m, n i is the number of atoms of type i and a i is the logP of type i.</p><formula xml:id="formula_9">logP (m) = i n i a i<label>(10)</label></formula><p>It is evident from Eq. 10 that as we continue to append relevant constituents (as defined in <ref type="bibr" target="#b36">[37]</ref>) the logP score will continue to increase accordingly. Hence, to produce molecules with large logP scores a generative model must be able to accomplish two things: (1) identify molecular constituents with positively contributing logP scores, and more importantly, (2) it must have the capacity to produce larger molecules (i.e. longer sequences). Current best molecular generation networks trained on ZINC-250k for unconstrained penalized logP maximization have demonstrated the capacity to produce large structures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>, whereas traditional VAE based networks have been limited to much smaller structures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>NLP-based sequence generation models-relying on functional components like multi-head attention, LSTMs, or GRUs-which do not make training protocol changes or incorporate syntax-awareness are typically incapable of producing output lengths much larger than their input lengths. As a result, vanilla VAEs used for SMILES sequence generation are inherently limited by the absence of longer sequences in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Insufficient Token Representation</head><p>In addition to inadequacies in sequence length the second major limiting factor in achieving higher pLogP scores with VAEs is the insufficiency of tokens present in the initial text corpus. In order to generate molecules with different geometries, number of rings, and bond types, the associated SMILES tokens must be present in the embedding vocabulary so that the network can appropriately utilize them. The clearest example of how this can handicap a model's ability to generate new molecules is the lack of numerical tokens (e.g. 1, 2, 3) which denote a particular ring. The highest ring number in the ZINC-250k dataset is 8, meaning that only 8 unique rings can be formed within the entire molecule. Current best molecular generation networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> have consistently shown that molecules with the  highest pLogP scores consist of multiple concatenated rings (large than 8). As a result we hypothesize here that a more diverse molecular corpus should assist us in achieving higher quality predictions using our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Dataset Expansion for Overcoming Deficiencies</head><p>We remedy the deficiencies described above by marginally expanding the ZINC-250k dataset to include samples with longer sequences. We first create a library of substructures by aggregating the lowest pLogP scores within the dataset. Following this, we generate ? 5000 new samples by randomly appending these substructures to existing molecules at random (syntactically correct) locations. We ensure that the pLogP scores of the new larger molecules do not exceed the highest pLogP score in the ZINC-250k dataset overall. In other words, we are careful not to allow our generative model to benefit from newly created data with inherently better properties. Not only does this expanded corpus provide the model with longer molecules, but it also provides new tokens which can be used to represent new types of structures. We emphasize here that this expanded dataset is only used to train our model on the unconstrained optimization task, while the constrained optimization task (discussed in Sec 3.4) uses the original ZINC-250k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We use the ZINC-250k dataset <ref type="bibr" target="#b4">[5]</ref> for all of our experiments. ZINC-250k was constructed by taking 250, 000 random samples from the ZINC database <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Training</head><p>All networks described in this paper were implemented using Keras <ref type="bibr" target="#b40">[41]</ref> and TensorFlow <ref type="bibr" target="#b41">[42]</ref>. All training was performed on an NVIDIA GeForce RTX 2070 GPU, with 8GB of memory.</p><p>We train the ?-CVAE model using separate Adam optimizers <ref type="bibr" target="#b42">[43]</ref> for the encoder and decoder networks, starting with a learning rate (LR) of 3 ? 10 -3 and 1 ? 10 -3 , respectively. We employ LR decay callbacks such that when a plateau is detected (no CCE loss or KL loss improvements over 3 epochs) then the respective LR is decreased by a factor of 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sampling from Gaussian Prior: Generation and Reconstruction</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we evaluate the ability of our ?-CVAE and non-conditional ?-VAE models to generate valid structures strictly from a Gaussian prior N (0, I). Following <ref type="bibr" target="#b33">[34]</ref>, we also report model performance on the following metrics: validity (% chemically valid), uniqueness (% unique and valid), novelty (% falling outside the training dataset), and N.U.V. (% novel, unique, and valid).</p><p>During the decoding process, we opt to use the partial SMILES parser <ref type="bibr" target="#b48">[49]</ref>, which checks the syntax of incomplete (partial) SMILES strings, the ability to  Table <ref type="table">1</ref>: Generation performance on ZINC250K dataset sampling from a normal distribution (N (0, I)). * indicates that we instead sampled from N (U(-1, 1), I). For our ?-CVAE networks, we condition on penalized logP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% Validity</head><p>% Validity w/o check % Uniqueness % Novelty % N.U.V.</p><formula xml:id="formula_10">CVAE [4] 0.7 N/A N/A N/A N/A GVAE [5] 7.2 N/A N/A N/A N/A GraphVAE [6]</formula><p>13.5 N/A N/A N/A N/A SD-VAE <ref type="bibr" target="#b6">[7]</ref> 43.5 N/A N/A N/A N/A JT-VAE <ref type="bibr" target="#b7">[8]</ref> 100 N/A 100 100 100 GCPN <ref type="bibr" target="#b43">[44]</ref> 100 20 99.97 100 99.97 MRNN <ref type="bibr" target="#b44">[45]</ref> 100 65 99.89 100 99.89 GraphNVP <ref type="bibr" target="#b45">[46]</ref> 42.6 ? 1.6 N/A 94.8 ? 0.6 100 40.38 GRF <ref type="bibr" target="#b46">[47]</ref> 73.4 ? 0.62 N/A 53.7 ? 2.13 100 39.42 All SMILES VAE <ref type="bibr" target="#b10">[11]</ref> 98.5 ? 0.1 N/A 100.00 0.00 99.96 N/A GraphAF <ref type="bibr" target="#b47">[48]</ref> 100 68 99.10 100 99.10 MoFlow <ref type="bibr" target="#b33">[34]</ref> 100.00 ? 0.00 81.76 ? 0.21 99.99 ? 0.01 100.00 ? 0.00 99.99 ? 0.01 GF-VAE <ref type="bibr" target="#b8">[9]</ref> 100.00 N/A 100.00 100.00 100.00 kekulize aromatic systems, among other checks. We use this parser to guide the decoding process such that if an invalid (intermediate) SMILES string is detected with the network's current prediction of the next token in the decoded sequence, then we instead select the next most probable token. This selection process is repeated until either a valid partial SMILES string is found or all tokens have been exhausted. We find that using the partial SMILES parser improves decoding performance. However, to be consistent with <ref type="bibr" target="#b33">[34]</ref>, we compute a final metric that measures the validity without performing any intermediate corrections, denoted as validity w/o check. To compute this, we use a standard decod-ing process that does not employ the partial SMILES parser or any other intermediate syntax checks.</p><formula xml:id="formula_11">?</formula><p>To remain consistent with <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>, we sample 10, 000 latent vectors from N (0, I) and decode over 5 separate runs and record the mean and standard deviation across all runs for each metric in Table <ref type="table">1</ref>. Notably, we set a new SOTA in the validity w/o check metric at 98.28?0.06%. Across all other metrics our ?-CVAE and ?-VAE remain competitive with other models, falling short of the best scores in most cases by less than a few percentage points.</p><p>Additionally, we test our ?-CVAE by sampling from N (U(-1, 1), I) to demonstrate that varying the mean of the Gaussian prior (on which pLogP is conditioned) induces more diverse molecules, as only sampling from N (0, I) will generate molecules with an approximated pLogP score of 0. This is evidenced by the increase in Uniqueness score from 88.69% to 93.83% and N.U.V. score from 80.11% to 88.40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unconstrained Optimization</head><p>We follow the JT-VAE paper <ref type="bibr" target="#b7">[8]</ref> by performing an unconstrained optimization benchmark on the ZINC-250k dataset to maximize the pLogP and Quantitative Estimate of Drug-likeness (QED) <ref type="bibr" target="#b49">[50]</ref> scores.</p><p>Since the ?-CVAE is conditioned on specific molecular properties, we train two separate networks: one conditioned on pLogP scores, and the other on QED scores. As discussed in Sec. 2.2, the properties must be standardized to create a constrained Gaussian prior p(? i ) ? N (? i , I). We note here that for the pLogP network we employ a simple standardization function ?plogp = c plogp -?c ?c . However, since the QED scores have a smaller range of [0, 1], we empirically determine that scaling QED scores using a quantile transform to follow a normal distribution, denoted as Q N (0,I) (x), produce better results: ?QED = Q N (0,I) (c QED ) .</p><p>The unconstrained optimization benchmark seeks to maximize both the plogP and QED scores. We adopt the Molecular Swarm Optimization (MSO) <ref type="bibr" target="#b15">[16]</ref> framework for posterior optimization of single properties, as this has demonstrated better results and reduced search latency than the widely used method of coupling a sparse Gaussian process (SGP) with Bayesian Optimization (BO) <ref type="bibr" target="#b7">[8]</ref>. We initialize the optimizer at ? 2000 randomly selected molecules. We execute the PSO routine for ? 1000 steps. Table <ref type="table" target="#tab_1">2</ref> shows the re-sults of the benchmark, with our ?-CVAE outperforming the top-3 molecules of the current SOTA method for maximizing pLogP scores, while also matching the highest QED scores. We showcase the molecular structures of the top-3 generated molecules with the highest pLogP scores in Fig. <ref type="figure" target="#fig_5">5</ref>. Further, we overlay heatmaps onto each of the molecular structures to visualize the atomic contributions to the non-penalized LogP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Constrained Optimization</head><p>Similar to Sec. 3.4, we follow the JT-VAE <ref type="bibr" target="#b7">[8]</ref> constrained optimization benchmark setup. The 800 molecules with the lowest pLogP scores are selected from the original test set as starting molecules to be modified. Improvement is measured by finding the increase in pLogP scores between the modification with the highest score and the respective original molecule. Similarity is measured by computing the Tanimoto similarity between each modification the respective starting molecule. Success is measured by the percent of successful modification, i.e. the percent of modifications that have exceeded the scores of their respective starting molecules.</p><p>For this benchmark we choose to disable the conditional sampling portion of our network, and instead train a separate ?-VAE that simply seeks to minimize the KL divergence between q ? (z|x) and a zero-mean Gaussian prior p(z) ? N (0, I), but still utilizes the training protocol outlined in Sec. 2.4. The constrained optimization task demands that the network optimize the pLogP scores while remaining within a predetermined similarity boundary ?. Though introducing the conditional sampling mechanism into the network helps us dramatically improve on the unconstrained opti- mization benchmark, we find that it is not as advantageous for the constrained benchmark. Since molecules with high Tanimoto similarity can have large differences in pLogP scores, we hypothesize that conditioning on pLogP rearranges the latent space in such a way that molecules with similar semantic structure are pushed far apart in latent space, which is sub-optimal for the constrained optimization problem.</p><p>MSO is run for a maximum of 100 steps on each of the starting molecules. We use 3 swarms each with 450 particles. The optimizer starting point is critical to establish as it can dramatically improve the final improvement of the molecule and reduce search latency. We choose starting points for each molecule m i by encoding the initial molecules to their respective latent vectors z i using our trained encoder q ? (x|z), and apply N (? = 0, ? = 1.0) to each vector to produce semantically similar neighbors ?i . Since molecules with similar structures (semantic similarities) should be within close proximity to one another in latent space, we can assist the constrained optimization by using this method to locate initial molecules with high Tanimoto similarity to z i . We find that this approach produces a more robust starting point for the MSO routine to operate, while choosing a ? that is too high can negatively influence results or significantly increase optimization time.  For each step, we evaluate the fitness of the decoded molecules via the following reward function from <ref type="bibr" target="#b56">[57]</ref>: </p><formula xml:id="formula_12">R(s) = p(m i ) -? ? (? -SIM(m i , m 0 )) if SIM(m i , m 0 ) &lt; ? p(m i ) otherwise</formula><p>SIM is defined as the Tanimoto similarity between the decoded molecules m i and the candidate molecule m 0 , we choose ? = 100 (same value used in the original work <ref type="bibr" target="#b56">[57]</ref>), and p(m) indicates computing the pLogP score of m i . We use RDKFringerprints <ref type="bibr" target="#b57">[58]</ref>, a "Daylightlike" substructure fingerprint method, to compute the Tanimoto similarity. We find that this reward function facilitates the search as decoded molecules that have not met the Tanimoto threshold are still used to guide the swarm, rather than naively returning a null improvement.</p><p>Our ?-VAE model performs very well in the constrained benchmark. In terms of improvement, it outperforms previous methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, while demonstrating a slightly higher mean improvement than the current SOTA <ref type="bibr" target="#b52">[53]</ref>. Although other methods have demonstrated slightly higher success rates, our method achieves a competitive 98.25%. These optimal results further validate our training protocol, as optimally targeting sufficient posterior MI increases the average improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Posterior Dimensionality Reduction</head><p>We conduct a two-dimensional (2D) Principal Component Analysis (PCA) on the learned posterior of both the standard ?-VAE and ?-CVAE. For the ?-VAE, this allows us to visualize the Tanimoto similarity between a single candidate molecule and neighboring molecules in latent space. The plots shown in Fig. <ref type="figure" target="#fig_6">6</ref> are 2D PCA plots, where each point on the plots represents a projected latent vector from an encoded molecule from the ZINC-250k validation set.</p><p>We compute the Tanimoto similarity at several annuli centered around a candidate molecule m i . We show that as we traverse the molecular space, moving from more distant annuli (2 &lt;= R &lt; 3) to closer ones (0.0 &lt;= R &lt; 0.1), the average Tanimoto similarity between the m i and all other molecules within each annulus increases from SIM = 0.130 to SIM = 0.144 respectively. That is, as we approach m i in reduced latent space, neighboring molecules become more structurally similar.</p><p>The 2D PCA plot for the conditional VAE is shown in Fig. <ref type="figure" target="#fig_6">6 (b)</ref>. Each point in this space represents a projected latent vector from an encoded molecule, where color is used to represent penalized logP score. As is evident from the plot, we have successfully im-posed target-property dependent structure in the latent space (grouping molecules with similar penalized LogP scores); effectively disentangling the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we revive a seemingly obsolete technique for de novo molecular generation and achieve new SOTA results for several benchmarks. Textual based VAEs have shown little promise when compared to more modern graph and flow based models. However, we show how simple architectural changes, more robust and flexible training protocols, and specific techniques for overcoming dataset limitations can dramatically enhance the performance of VAE models.</p><p>A second, major contribution of this work is to demonstrate how the latent space of VAE models can be purposefully structured to our advantage-either in terms of semantic structure (Tanimoto similarity) or in terms of desired target properties (pLogP). This is achieved by modifying the KL divergence loss to encode directly to the desired molecular property in the posterior. Consequently, we emphasize here that this simple strategy can be extended to guide the molecular optimization for any desired property (or set of properties) for which we wish to discover new candidate molecules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Training and validation CCE (reconstruction, or log likelihood) loss per epoch. (b) Training and validation posterior Mutual Information per epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Training and validation losses and metrics collected over 200 epochs.</figDesc><graphic url="image-4.png" coords="7,300.69,89.45,226.76,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>8. Each network (one trained for each optimization task) is trained for 200 epochs. The training and validation losses (CCE and KL-divergence) are shown in Fig. 4 (a). Both losses indicate stable training. We also show the mutual information recorded per epoch in Fig. 4 (b). The training protocol we introduce in Sec. 2.4 demonstrates its effectiveness in keeping the mutual information balanced during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) No partial SMILES parser (validity check) to guide the decoding process. (b) With partial SMILES parser</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Molecules generated by randomly sampling N (0, I) and decoding.</figDesc><graphic url="image-6.png" coords="8,300.68,89.45,226.77,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Top-3 molecules generated from our ?-CVAE with the highest pLogP scores. Each molecule has a heatmap overlay which colorizes the atomic contributions to the LogP (non-penalized) score, cycling from red to blue which indicates the highest and lowest contributions respectively.</figDesc><graphic url="image-7.png" coords="9,55.10,89.45,141.73,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: PCA of posterior of the validation set, where each plotted point indicates a projected latent vector of a molecule into 2D space. (a) shows results from the ?-VAE with no latent space conditioning, where the points are colorized based on the Tanimoto similarity between a single candidate molecule (darker indicates higher similarity). (b) shows results from the ?-CVAE conditioned on pLogP, where the points are colorized and sized by the molecule's respective pLogP score (darker and larger indicates a higher pLogP score).</figDesc><graphic url="image-10.png" coords="11,51.42,89.45,226.77,226.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,297.19,304.75,271.23,271.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="6,146.95,89.45,283.47,283.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the top 3 property scores of generated molecules for both penalized logP and QED scores.</figDesc><table><row><cell>Method</cell><cell>1st</cell><cell cols="3">Penalized logP 2nd</cell><cell>3rd</cell><cell>1st</cell><cell>QED 2nd</cell><cell>3rd</cell></row><row><cell>ZINC (Dataset)</cell><cell cols="2">4.52</cell><cell>4.30</cell><cell></cell><cell>4.23</cell><cell>0.948</cell><cell>0.948</cell><cell>0.948</cell></row><row><cell>CVAE [4]</cell><cell cols="2">1.98</cell><cell>1.42</cell><cell></cell><cell>1.19</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GVAE [5]</cell><cell cols="2">2.94</cell><cell>2.89</cell><cell></cell><cell>2.80</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>ORGAN [51, 52]</cell><cell cols="2">3.63</cell><cell>3.49</cell><cell></cell><cell>3.44</cell><cell>0.896</cell><cell>0.824</cell><cell>0.820</cell></row><row><cell>SD-VAE [7]</cell><cell cols="2">4.04</cell><cell>3.50</cell><cell></cell><cell>2.96</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>JT-VAE [8]</cell><cell cols="2">5.30</cell><cell>4.93</cell><cell></cell><cell>4.49</cell><cell>0.925</cell><cell>0.911</cell><cell>0.910</cell></row><row><cell>Re-Balanaced VAE [10]</cell><cell cols="2">5.32</cell><cell>5.28</cell><cell></cell><cell>5.23</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GF-VAE [9]</cell><cell cols="2">6.47</cell><cell>4.42</cell><cell></cell><cell>4.42</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>BBRT-Seq2Seq [52]</cell><cell cols="2">6.74</cell><cell>6.47</cell><cell></cell><cell>6.42</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>GCPN [44]</cell><cell cols="2">7.98</cell><cell>7.85</cell><cell></cell><cell>7.80</cell><cell>0.948</cell><cell>0.947</cell><cell>0.946</cell></row><row><cell>BBRT-JTNN [52]</cell><cell cols="2">10.13</cell><cell>10.10</cell><cell></cell><cell>9.91</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>MRNN [45]</cell><cell cols="2">10.34</cell><cell>10.19</cell><cell cols="2">10.14</cell><cell cols="2">0.948 0.948</cell><cell>0.947</cell></row><row><cell>GraphAF [48]</cell><cell cols="2">12.23</cell><cell>11.29</cell><cell cols="2">11.05</cell><cell cols="2">0.948 0.948</cell><cell>0.947</cell></row><row><cell>unitMCTS-38 [39]</cell><cell cols="2">12.63</cell><cell>12.6</cell><cell cols="2">12.55</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>MNCE-RL OEU [53]</cell><cell cols="2">14.49</cell><cell>14.44</cell><cell cols="2">14.36</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>MNCE-RL [53]</cell><cell cols="2">18.33</cell><cell>18.18</cell><cell cols="2">18.16</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>T-LBO-1 [38]</cell><cell cols="2">24.06</cell><cell>22.84</cell><cell cols="2">21.26</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>MSO [16]</cell><cell cols="2">26.10</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>0.948</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PGFS [54]</cell><cell cols="2">27.22</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>0.948</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>JT-VAE (k = 103, retraining[40])</cell><cell cols="2">27.84</cell><cell>27.59</cell><cell cols="2">27.21</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>unitMCTS-84 [39]</cell><cell cols="2">29.20</cell><cell>28.76</cell><cell cols="2">28.73</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>All SMILES VAE (KL-unscaled) [11]</cell><cell cols="2">29.80</cell><cell>29.76</cell><cell cols="2">29.11</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>T-LBO-2 [38]</cell><cell cols="2">34.83</cell><cell>31.1</cell><cell cols="2">29.21</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>T-LBO-3 [38]</cell><cell cols="2">38.57</cell><cell>34.83</cell><cell cols="2">34.63</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>?-VAE (Our)</cell><cell cols="2">6.53</cell><cell>6.47</cell><cell></cell><cell>6.44</cell><cell cols="2">0.948 0.948 0.948</cell></row><row><cell>?-CVAE (Our)</cell><cell cols="7">104.29 90.12 69.68 0.948 0.948 0.948</cell></row></table><note><p>"N/A" indicates that no results exist from the original paper.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Constrained penalized logP optimizations re-</cell></row><row><cell>sults with a Tanimoto similarity threshold of ? = 0.4.</cell></row><row><cell>RDKFingerprints were used to compute the Tanimoto</cell></row><row><cell>similarity. "N/A" indicates that no results exist from</cell></row><row><cell>the original paper.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structure-based virtual screening: an overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1047" to="1055" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alisson Marques da Silva, and Alex Gutterres Taranto. Structurebased virtual screening: From classical to artificial intelligence</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bechelane</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Let?cia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Assis</surname></persName>
		</author>
		<author>
			<persName><surname>Alves De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Chemistry</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">343</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimation of the size of druglike chemical space based on gdb-17 data</title>
		<author>
			<persName><forename type="first">Pavel</forename><forename type="middle">G</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Madzhidov</surname></persName>
		</author>
		<author>
			<persName><surname>Varnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="675" to="679" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjam?n</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<title level="m">Grammar variational autoencoder</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Syntax-directed variational autoencoder for structured data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GF-VAE: A Flow-Based Variational Autoencoder for Molecule Generation</title>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1181" to="1190" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Re-balancing variational autoencoder loss for molecule sequence generation</title>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">All smiles variational autoencoder</title>
		<author>
			<persName><forename type="first">Artem</forename><surname>Zaccary Alperstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cherkasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolfe</forename><surname>Tyler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert learns (and teaches) chemistry</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Srouji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Andreas Steffen, Hans Briem, Noe Frank, and Djork-Arn? Clevert. Efficient multi-objective molecular optimization in a continuous latent space</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floriane</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ChemRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guacamol: Benchmarking models for de novo molecular design</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Fiscato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwin</forename><forename type="middle">H S</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><forename type="middle">C</forename><surname>Vaucher</surname></persName>
		</author>
		<idno type="PMID">30887799</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From autoencoder to beta-vae. lilianweng.github.io/lil-log</title>
		<author>
			<persName><forename type="first">Lilian</forename><surname>Weng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lagging inference networks and posterior collapse in variational autoencoders</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<title level="m">Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Molecular generative model based on conditional variational autoencoder for de novo molecular design</title>
		<author>
			<persName><forename type="first">Jaechang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongok</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Conditional constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rigoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol?</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional molecular design with deep generative models</title>
		<author>
			<persName><forename type="first">Seokho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2018-07">Jul 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variational autoencoder for regression: Application to brain aging analysis</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Honnorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An attentiondriven long short-term memory network for high throughput virtual screening of organic photovoltaic candidate molecules</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Solar Energy</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page" from="43" to="50" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Message-passing neural networks for highthroughput polymer screening</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">W</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Nolan Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">R</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">E</forename><surname>Nimlos</surname></persName>
		</author>
		<author>
			<persName><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">234111</biblScope>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The harvard clean energy project: Large-scale computational screening and design of organic photovoltaics on the world community grid</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Olivares-Amaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sule</forename><surname>Atahan-Evrenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Amador-Bedolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roel</forename><surname>Sanchez-Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aryeh</forename><surname>Gold-Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Brockway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<date type="published" when="2011-08-02">2, 08 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from the harvard clean energy project: The use of neural networks to accelerate materials discovery</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Pyzer-Knapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Functional Materials</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="9" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Design of high transmission color filters for solar cells directed by deep q-learning</title>
		<author>
			<persName><forename type="first">Iman</forename><surname>Sajedian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Rho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Solar Energy</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="670" to="676" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ELBO surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moflow: An invertible flow model for generating molecular graphs</title>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Huajie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dachun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<title level="m">Controlvae: Controllable variational autoencoder</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prediction of physicochemical parameters by atomic contributions</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wildman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Crippen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="868" to="873" />
			<date type="published" when="1999-09">09 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">High-dimensional bayesian optimisation with variational autoencoders and deep metric learning</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Grosnit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasul</forename><surname>Tutunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Maraval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan-Rhys</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Cowen-Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Bou-Ammar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Goal directed molecule generation using monte carlo tree search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Rajasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><surname>Ravindran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sample-efficient optimization in the latent space of deep generative models via weighted retraining</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Molecularrnn: Generating realistic molecular graphs with optimized properties</title>
		<author>
			<persName><forename type="first">Mariya</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junier</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexandr</forename><surname>Isayev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Kaushalya</forename><surname>Madhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katushiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nakago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Abe</surname></persName>
		</author>
		<title level="m">Graphnvp: An invertible flow model for generating molecular graphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph residual flow for molecular graph generation</title>
		<author>
			<persName><forename type="first">Shion</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirotaka</forename><surname>Akita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graphaf: a flow-based autoregressive model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">partialsmiles</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaia</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?my</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sorel Muresan</surname></persName>
		</author>
		<author>
			<persName><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2012-02">02 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Objectivereinforced generative adversarial networks (organ) for sequence generation models</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guimaraes</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Outeiral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">Cunha</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Black box recursive translations for molecular optimization</title>
		<author>
			<persName><forename type="first">Farhan</forename><surname>Damani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishnu</forename><surname>Sresht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Reinforced molecular optimization with neighborhood-controlled grammars</title>
		<author>
			<persName><forename type="first">Chencheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning to navigate the synthetically accessible chemical space using reinforcement learning</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sufeng</forename><surname>Sattarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashaswi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Molecular hypergraph grammar with its application to molecular optimization</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kajino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tomasz Danel, and Micha l Warcho l. Mol-cyclegan: a generative model for molecular optimization</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Pocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Rataj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Optimization of molecules via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Zhenpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">10 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<title level="m">Rdkit: Open-source cheminformatics</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
