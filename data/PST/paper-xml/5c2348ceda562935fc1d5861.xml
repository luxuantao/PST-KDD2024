<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KDGAN: Knowledge Distillation with Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
							<email>xiaojiew94@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>rui.zhang@unimelb.edu.au</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Corresponding author 32nd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2018)</postCode>
									<settlement>Montr√©al, Lake Lake mead Nice lake</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>ysun@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
							<email>jianzhong.qi@unimelb.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Twitter Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KDGAN: Knowledge Distillation with Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">26C054404C4DFD6678B8DD76633CF769</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In machine learning, it is common that more resources such as input features <ref type="bibr" target="#b46">[47]</ref> or computational resources <ref type="bibr" target="#b22">[23]</ref>, which we refer to as privileged provision, are available at the stage of training a model than those available at the stage of running the deployed model (i.e., the inference stage). Figure <ref type="figure" target="#fig_0">1</ref> shows an example application of image tag recommendation, where more input features (called privileged information <ref type="bibr" target="#b46">[47]</ref>) are available at the training stage than those available at the inference stage. Specifically, the training stage has access to images as well as image titles and comments (textual information) as shown in Figure <ref type="figure" target="#fig_1">1a</ref>, whereas the inference stage only has access to images themselves as shown in Figure <ref type="figure" target="#fig_1">1b</ref>. After a smart phone user uploads an image and is about to provide tags for the image, it is inconvenient to type tags on the phone and thinking about tags for the image also takes time, so it is very useful to recommend tags based on the image as shown in Figure <ref type="figure" target="#fig_1">1b</ref>. Another example application is unlocking mobile phones by face recognition. We usually deploy face recognition models on mobile phones so that legit users can unlock the phones without depending on remote services or internet connections. The training stage may be done on a powerful server with significantly more computational resources than the inference stage, which is done on a mobile phone. Here, a key problem is how to use privileged provision, i.e., resources only accessible for training, to train a model with great inference performance <ref type="bibr" target="#b28">[29]</ref>.</p><p>Typical approaches to the problem are based on knowledge distillation (KD) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. As shown by the left half of Figure <ref type="figure">2</ref>, KD consists of a classifier and a teacher <ref type="bibr" target="#b28">[29]</ref>. To operate for resourceconstrained inference, the classifier does not use privileged provision. On the other hand, the teacher uses privileged provision by, e.g., having a larger model capacity or taking more features as input. Once trained, the teacher outputs a distribution over labels called soft labels <ref type="bibr" target="#b28">[29]</ref> for each training instance. Then, the teacher trains the classifier to predict the soft labels via a distillation loss such as the L2 loss on logits <ref type="bibr" target="#b6">[7]</ref>. This training process is often called "distilling" the knowledge in the teacher into the classifier <ref type="bibr" target="#b22">[23]</ref>. Since the teacher normally cannot perfectly model the true data distribution, it is difficult for the classifier to learn the true data distribution from the teacher.</p><p>Generative adversarial networks (GAN) provide an alternative way to learn the true data distribution. Inspired by Wang et al. <ref type="bibr" target="#b48">[49]</ref>, we first present a naive GAN (NaGAN) with two players. As shown by the right part of Figure <ref type="figure">2</ref>, NaGAN consists of a classifier and a discriminator. The classifier serves as a generator that generates relevant labels given an instance while the discriminator aims to distinguish the true labels from the generated ones. The classifier learns from the discriminator to perfectly model the true data distribution at the equilibrium via adversarial losses. One limitation of NaGAN is that a large number of training instances and epochs is normally required to reach equilibrium <ref type="bibr" target="#b14">[15]</ref>, which restricts its applicability to domains where collecting labeled data is expensive. The slow training speed is because in such a two-player framework, the gradients from the discriminator to update the classifier often vanish or explode during the adversarial training <ref type="bibr" target="#b3">[4]</ref>. It is challenging to train a classifier to learn the true data distribution with limited training instances and epochs.</p><p>To address this challenge, we propose a three-player framework named KDGAN to distill knowledge with generative adversarial networks. As shown in Figure <ref type="figure">2</ref>, KDGAN consists of a classifier, a teacher, and a discriminator. In addition to the distillation loss in KD and the adversarial losses in NaGAN mentioned above, we define a distillation loss from the classifier to the teacher and an adversarial loss between the teacher and the discriminator. Specifically, the classifier and the teacher, serving as generators, aim to fool the discriminator by generating pseudo labels that resemble the true labels. Meanwhile, the classifier and the teacher try to reach an agreement on what pseudo labels to generate by distilling their knowledge into each other. By formulating the distillation and adversarial losses as a minimax game, we enable the classifier to learn the true data distribution at the equilibrium (see Section 3.2). Besides, the classifier receives gradients from the teacher via the distillation loss and the discriminator via the adversarial loss. The gradients from the teacher often have low variance, which reduces the variance of gradients and thus speeds up the adversarial training (see Section 3.3).</p><p>We further consider reducing the variance of the gradients from the discriminator to accelerate the training of KDGAN. The gradients from the discriminator may have large variance when obtained through the widely used policy gradient methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52]</ref>. It is non-trivial to obtain low-variance gradients from the discriminator because the classifier and the teacher generate discrete samples, which are not differentiable w.r.t. their parameters. We propose to relax the discrete distributions learned by the classifier and the teacher into concrete distributions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> with the Gumbel-Max trick <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. We use the concrete distributions for generating continuous samples to enable endto-end differentiability and sufficient control over the variance of gradients. Given the continuous samples, we obtain low-variance gradients from the discriminator to accelerate the KDGAN training.</p><p>To summarize, our contributions are as follows:</p><p>‚Ä¢ We propose a novel framework named KDGAN for multi-label learning, which trains a lightweight classifier suitable for resource-constrained inference using resources available only for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review studies on knowledge distillation (KD) and generative adversarial networks (GAN).</p><p>KD aims to transfer the knowledge in a powerful teacher to a lightweight classifier <ref type="bibr" target="#b8">[9]</ref>. For example, Ba and Caruana <ref type="bibr" target="#b6">[7]</ref> train a shallow classifier network to mimic a deep teacher network by matching logits via the L2 loss. Hinton et al. <ref type="bibr" target="#b22">[23]</ref> generalize this work by training a classifier to predict soft labels provided by a teacher. Sau and Balasubramanian <ref type="bibr" target="#b38">[39]</ref> further add random perturbations into soft labels to simulate learning from multiple teachers. Instead of using soft labels, Romero et al. <ref type="bibr" target="#b35">[36]</ref> propose to use middle layers of a teacher to train a classifier. Unlike previous work on classification problems, Chen et al. <ref type="bibr" target="#b9">[10]</ref> apply KD and hint learning to object detection problems.</p><p>There also exists work that leverages KD to transfer knowledge between different domains <ref type="bibr" target="#b20">[21]</ref>, e.g., between high-quality and low-quality images <ref type="bibr" target="#b40">[41]</ref>. Lopez-Paz et al. <ref type="bibr" target="#b28">[29]</ref> unify KD with privileged information <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> as generalized distillation where a teacher is pretrained by taking as input privileged information. Compared to KD, the proposed KDGAN framework introduces a discriminator to guarantee that the classifier can learn the true data distribution at the equilibrium.</p><p>GAN is initially proposed to generate continuous data by training a generator and a discriminator adversarially in a minimax game <ref type="bibr" target="#b16">[17]</ref>. GAN has only recently been introduced to generate discrete data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> because discrete data makes it difficult to pass gradients from a discriminator backward to update a generator. For example, sequence GAN (SeqGAN) <ref type="bibr" target="#b51">[52]</ref> models the process of token sequence generation as a stochastic policy and adopts Monte Carlo search to update a generator. Different from these GANs with two players, Li et al. propose a GAN with three players called Triple-GAN <ref type="bibr" target="#b12">[13]</ref>. Our KDGAN also consists of three players including two generators and a discriminator, but differs from Triple-GAN in that: (1) Both generators in KDGAN learn a conditional distribution over labels given features. However, the generators in Triple-GAN learn a conditional distribution over labels given features and a conditional distribution over features given labels, respectively. (2) The samples from both generators in KDGAN are all discrete data while the samples from the generators in Triple-GAN include both discrete and continuous data. These differences lead to different objective functions and training techniques, e.g., KDGAN can use the Gumbel-Max trick <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref> to generate samples from both generators while Triple-GAN cannot do this. There is also a rich body of studies on improving the training of GAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b55">56]</ref> such as feature matching <ref type="bibr" target="#b37">[38]</ref>, which are orthogonal to our work and can be used to improve the training of KDGAN.</p><p>We explore the idea of integrating KD and GAN. A similar idea has been studied in <ref type="bibr" target="#b50">[51]</ref> where a discriminator is introduced to train a classifier. This previous study <ref type="bibr" target="#b50">[51]</ref> differs from ours in that their discriminator trains the classifier to learn the data distribution produced by the teacher, while our discriminator trains the classifier to learn the true data distribution.</p><p>We apply the proposed KDGAN to address the problem of deep model compression and image tag recommendation. We can also apply KDGAN to address the other problems where privileged provision is available <ref type="bibr" target="#b43">[44]</ref>. For example, we can consider contextual signals in the intent tracking problem <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> or user reviews in the movie recommendation problem <ref type="bibr" target="#b49">[50]</ref> as privileged provision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We study the problem of training a lightweight classifier from a teacher that is trained with privileged provision (denoted by ) to satisfy stringent inference requirements. The inference requirements may include (1) running in real time with limited computational resources, where privileged provision is computational resources <ref type="bibr" target="#b22">[23]</ref>;</p><p>(2) lacking a certain type of input features, where privileged provision is privileged information <ref type="bibr" target="#b46">[47]</ref>. Following existing work <ref type="bibr" target="#b28">[29]</ref>, we use multi-label learning problems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b52">53]</ref> as the target application scenarios of our methods for illustration purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier Teacher</head><p>Discriminator</p><formula xml:id="formula_0">s c = pc(y|x) L c DS x s t = p t (y|x) L t DS x y c ‚àº qc(y|x) L n AD y t ‚àº q t (y|x) L n AD y ‚àº pu(y|x) L p AD x KD NaGAN KDGAN Figure 2:</formula><p>Comparison among KD, NaGAN, and KDGAN. The classifier (C) and the teacher (T ) learn discrete categorical distributions p c (y|x) and p t (y|x); y is a true label generated from the true data distribution p u (y|x); y c and y t are continuous samples generated from concrete distributions q c (y|x) and q t (y|x); s c and s t are soft labels produced by C and T ; L c DS and L t DS are distillation losses for C and T ; L p AD and L n AD are adversarial losses for positive and negative feature-label pairs.</p><p>Since privileged provision is only available at the training stage, the goal of the problem is to train a lightweight classifier that does not use privileged provision for effective inference.</p><p>To achieve this goal, we start with NaGAN, a naive adaptation of the two-player framework proposed by Wang et al. in information retrieval (Section 3.1). Similar to other two-player frameworks <ref type="bibr" target="#b48">[49]</ref>, the naive adaptation requires a large number of training instances and epochs <ref type="bibr" target="#b14">[15]</ref>, which is difficult to satisfy in practice <ref type="bibr" target="#b3">[4]</ref>. To address the limitation, we propose a three-player framework named KDGAN that can speed up the training while preserving the equilibrium (Sections 3.2 and 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NaGAN Formulation</head><p>We begin with NaGAN that combines a classifier C with a discriminator D in a minimax game. Since D is not meant for inference, it can leverage privileged provision. For example, D may have a larger model capacity than C or take as input more features than those available to C. In NaGAN, C generates pseudo labels y given features x following a categorical distribution p c (y|x), while D computes the probability p d (x, y) of a label y being from the true data distribution p u (y|x) given features x. With a slight abuse of notation, we also use x to refer to features including privileged information when the context is clear. Following the value function of IRGAN <ref type="bibr" target="#b48">[49]</ref>, we define the value function V (c, d) for the minimax game in NaGAN as</p><formula xml:id="formula_1">min c max d V (c, d) = E y‚àºpu [log p d (x, y)] + E y‚àºpc [log(1 -p d (x, y))].<label>(1)</label></formula><p>Let h(x, y) and g(x, y) be the scoring functions for C and D. We define p c (y|x) and p d (x, y) as p c (y|x) = softmax(h(x, y)) and p d (x, y) = sigmoid(g(x, y)).</p><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The scoring functions can be implemented in various ways, e.g., h(x, y) can be a multilayer perceptron <ref type="bibr" target="#b26">[27]</ref>. We will detail the scoring functions for specific applications in Section 4. Such a two-player framework is trained by updating C and D alternatively <ref type="bibr" target="#b48">[49]</ref>. The training will proceed until the equilibrium is reached, where C learns the true data distribution. At that point, D can do no better than random guesses at deciding whether a given label is generated by C or not <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our key observation is that the advantages and the disadvantages of KD and NaGAN are complementary: (1) KD usually requires a small number of training instances and epochs but cannot ensure the equilibrium where p c (y|x) = p u (y|x).</p><p>(2) NaGAN ensures the equilibrium where p c (y|x) = p u (y|x) <ref type="bibr" target="#b48">[49]</ref> but normally requires a large number of training instances and epochs. We aim to retain the advantages and avoid the disadvantages of both methods in a single framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KDGAN Formulation</head><p>We formulate KDGAN as a minimax game with a classifier C, a teacher T , and a discriminator D. Similar to the classifier C, the teacher T generates pseudo labels based on a categorical distribution p t (y|x) = softmax(f (x, y)) where f (x, y) is also a scoring function. Both T and D use privileged provision, e.g., by having a large model capacity or taking privileged information as input. In KDGAN, D aims to maximize the probability of correctly distinguishing the true and pseudo labels, whereas C and T aim to minimize the probability that D rejects their generated pseudo labels. Meanwhile, C learns from T by mimicking the learned distribution of T . To build a general framework, we also enable T to learn from C because, in reality, a teacher's ability can also be enhanced by interacting with students (see Figure <ref type="figure">6</ref> in Appendix D for empirical evidence that T benefits from learning from Sample labels {y1, ..., y k }, {y c 1 , ..., y c k }, and {y t 1 , ..., y t k } from pu(y|x), qc(y|x), and q t (y|x).</p><p>Update D by ascending along its gradients</p><formula xml:id="formula_3">1 k k i=1 ‚àá d log p d (x, yi) + Œ±‚àá d log(1 -p d (x, z c i )) + (1 -Œ±)‚àá d log(1 -p d (x, z t i )) .</formula><p>for the number of training steps for the teacher do Sample labels {y t 1 , ..., y t k } from q t (y|x) and update the teacher by descending along its gradients</p><formula xml:id="formula_4">1 k k i=1 (1 -Œ±)‚àát log q t (y t i |x) log(1 -p d (x, z t i )) + Œ≥‚àátL t DS (p t (y|x), pc(y|x)).</formula><p>for the number of training steps for the classifier do Sample labels {y c 1 , ..., y c k } from qc(y|x) and update C by descending along its gradients</p><formula xml:id="formula_5">1 k k i=1 Œ±‚àác log qc(y c i |x) log(1 -p d (x, z c i )) + Œ≤‚àácL c DS (pc(y|x), p t (y|x)).</formula><p>C). Such a mutual learning helps C and T reduce their probability of generating different pseudo labels. Formally, we define the value function U (c, t, d) for the minimax game in KDGAN as</p><formula xml:id="formula_6">min c,t max d U (c, t, d) = E y‚àºpu [log p d (x, y)] + Œ±E y‚àºpc [log(1 -p d (x, y))] + (1 -Œ±)E y‚àºp t [log(1 -p d (x, y))] + Œ≤L c DS (p c (y|x), p t (y|x)) + Œ≥L t DS (p t (y|x), p c (y|x)),<label>(3)</label></formula><p>where Œ± ‚àà (0, 1), Œ≤ ‚àà (0, +‚àû), and Œ≥ ‚àà (0, +‚àû) are hyperparameters. We collectively refer to the expectation terms as the adversarial losses and refer to L c DS and L t DS as the distillation losses. The distillation losses can be defined in several ways <ref type="bibr" target="#b38">[39]</ref>, e.g., the L2 loss <ref type="bibr" target="#b6">[7]</ref> or Kullback-Leibler divergence <ref type="bibr" target="#b22">[23]</ref>. Note that L c DS and L t DS are used to train the classifier and the teacher, respectively. Theoretical Analysis. We show that the classifier perfectly learns the true data distribution at the equilibrium of KDGAN. To see this, let p Œ± (y|x) = Œ±p c (y|x) + (1 -Œ±)p t (y|x). It can be shown that the adversarial losses w.r.t. p c (y|x) and p t (y|x) are equal to an adversarial loss w.r.t. p Œ± (y|x):</p><formula xml:id="formula_7">Œ±E y‚àºpc [log(1 -p d (x, y))] + (1 -Œ±)E y‚àºp t [log(1 -p d (x, y))] = Œ± y p c (y|x) log(1 -p d (x, y)) + (1 -Œ±) y p t (y|x) log(1 -p d (x, y)) = y Œ±p c (y|x) + (1 -Œ±)p t (y|x) log(1 -p d (x, y)) = E y‚àºp Œ± [log(1 -p d (x, y))].<label>(4)</label></formula><p>Therefore, let L MD = Œ≤L c DS (p c (y|x), p t (y|x)) + Œ≥L t DS (p t (y|x), p c (y|x)) and L JS be the Jensen-Shannon divergence, the value function U (c, t, d) of the minimax game can be rewritten as</p><formula xml:id="formula_8">min Œ± max d E y‚àºpu [log p d (x, y)] + E y‚àºp Œ± [log(1 -p d (x, y))] + L MD = min Œ± 2L JS (p u (y|x)||p Œ± (y|x)) + Œ≤L c DS (p c (y|x), p t (y|x)) + Œ≥L t DS (p t (y|x), p c (y|x)) -log(4).<label>(5)</label></formula><p>Here, L JS reaches the minimum if and only if p Œ± (y|x) = p u (y|x) and L c DS (or L t DS ) reaches the minimum if and only if p c (y|x) = p t (y|x). Hence, the KDGAN equilibrium is reached if and only if p c (y|x) = p t (y|x) = p u (y|x) where the classifier learns the true data distribution. We summarize the above discussions in Lemma 4.1 (the necessary and sufficient conditions of maximizing the value function) and Theorem 4.2 (achieving the equilibrium), respectively (see Appendix A for proofs). Lemma 4.1. For any fixed classifier and teacher, the value function U (c, t, d) is maximized if and only if the distribution of the discriminator is given by p d (x, y) = pu(y|x) /(pu(y|x)+p Œ± (y|x)). Theorem 4.2. The equilibrium of the minimax game min c,t max d U (c, t, d) is achieved if and only if p c (y|x) = p t (y|x) = p u (y|x). At that point, U (c, t, d) reaches the value -log(4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">KDGAN Training</head><p>In this section, we detail techniques for accelerating the training speed of KDGAN via reducing the number of training epochs needed. As discussed in earlier studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46]</ref>, the training speed is closely related to the variance of gradients. Comparing with NaGAN, the KDGAN framework by design can reduce the variance of gradients. This is because the high variance of a random variable can be reduced by a low-variance random variable (detailed in Lemma 4.3) and as we will discuss, T provides gradients of lower variance than D does. To reduce the variance of gradients from D and attain sufficient control over the variance, we further propose to obtain gradients from a continuous space by relaxing the discrete samples, i.e., pseudo labels, propagated between the classifier (or the teacher) and the discriminator into continuous samples with a reparameterization trick <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>First, we show how KDGAN reduces the variance of gradients. As discussed above, C only receives gradients ‚àá c V from D in NaGAN while it receives gradients ‚àá c U from both D and T in KDGAN:</p><formula xml:id="formula_9">‚àá c V = ‚àá c L n AD , ‚àá c U = Œª‚àá c L n AD + (1 -Œª)‚àá c L c DS ,<label>(6)</label></formula><p>where Œª ‚àà (0, 1), ‚àá c L n AD and ‚àá c L c DS are gradients from D and T , respectively. Consistent with the findings in existing work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>, we also observe that ‚àá c L c DS usually has a lower variance than ‚àá c L n AD (see Figure <ref type="figure">7</ref> in Appendix D for empirical evidence that the variance of ‚àá c L c DS is smaller than that of ‚àá c L n AD during the training process). Hence, it can be easily shown that the gradients w.r.t. C in KDGAN have a lower variance than that in NaGAN (refer to Lemma 4.3):</p><formula xml:id="formula_10">Var(‚àá c L c DS ) ‚â§ Var(‚àá c L n AD ) ‚áí Var(‚àá c U ) ‚â§ Var(‚àá c V ).<label>(7)</label></formula><p>Next, we further reduce the variance of gradients with a reparameterization trick, in particular, the Gumbel-Max trick <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. The essence of the Gumbel-Max trick is to reparameterize generating discrete samples into a differentiable function of its parameters and an additional random variable of a Gumbel distribution. To perform the Gumbel-Max trick on generating discrete samples from the categorical distribution p c (y|x), a concrete distribution <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> can be used. We use a concrete distribution q c (y|x) to generate continuous samples and use the continuous samples to compute the gradients ‚àá c L n AD of the adversarial loss w.r.t. the classifier as</p><formula xml:id="formula_11">‚àá c L n AD = ‚àá c E y‚àºpc [log(1 -p d (x, y))] = E y‚àºqc [‚àá c log q c (y|x) log(1 -p d (x, z))].<label>(8)</label></formula><p>Here, z = onehot(argmax y) is a discrete pseudo label where y ‚àº q c (y|x). We define q c (y|x) as</p><formula xml:id="formula_12">q c (y|x) = softmax log p c (y|x) + g œÑ , g ‚àº Gumbel(0, 1).<label>(9)</label></formula><p>Here, œÑ ‚àà (0, +‚àû) is a temperature parameter and Gumbel(0, 1) is the Gumbel distribution<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b30">[31]</ref>. We leverage the temperature parameter œÑ to control the variance of gradients over the training. With a high temperature, the samples from the concrete distribution are smooth, which give lowvariance gradient estimates. Note that a disadvantage of the concrete distribution is that with a high temperature, it becomes a less accurate approximation to the original categorical distribution, which causes biased gradient estimates. We will discuss how to tune the temperature parameter in Section 4.</p><p>In addition to improving the training of C, we also apply the same techniques to improve the training of T . We update D with the back-propagation algorithm <ref type="bibr" target="#b36">[37]</ref> (detailed in Appendix B). The overall logic of the KDGAN training is summarized in Algorithm 1. The three players can be first pretrained separately and then trained alternatively via minibatch stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The proposed KDGAN framework can be applied to a wide range of multi-label learning tasks where privileged provision is available. To show the applicability of KDGAN, we conduct experiments with the tasks of deep model compression (Section 4.1) and image tag recommendation (Section 4.2). Note that privileged provision is referred to as computational resources in deep model compression and privileged information in image tag recommendation, respectively.</p><p>We implement KDGAN based on Tensorflow <ref type="bibr" target="#b0">[1]</ref> and here we briefly describe our experimental setup <ref type="foot" target="#foot_1">3</ref> . We use two formulations of the distillation losses including the L2 loss <ref type="bibr" target="#b6">[7]</ref> and the Kullback-Leibler divergence <ref type="bibr" target="#b22">[23]</ref>. The two formulations exhibit comparable results and the results presented are based on the L2 loss <ref type="bibr" target="#b6">[7]</ref>. Since both T and D can use privileged provision, we implement their scoring functions f (x, y) and g(x, y) using the same function s(x, y) but with different sets of parameters. We search for the optimal values for the hyperparameters Œ± in [0.0, 1.0], Œ≤ in [0.001, 1000], and Œ≥ in [0.0001, 100] based on validation performance. We find that a reasonable annealing schedule for the temperature parameter œÑ is to start with a large value (1.0) and exponentially decay it to a small value (0.1). We leave the exploration of the optimal schedule for future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Model Compression</head><p>Deep model compression aims to reduce the storage and runtime complexity of deep models and to improve the deployability of such models on portable devices such as smart phones. Extensive computational resources available for training are considered privileged provision in this task.</p><p>Dataset and Setup. We use the widely adopted MNIST <ref type="bibr" target="#b26">[27]</ref> and CIFAR-10 <ref type="bibr" target="#b25">[26]</ref> datasets. The MNIST dataset has 60,000 grayscale images (50,000 for training and 10,000 for testing) with 10 different label classes. Following an earlier work <ref type="bibr" target="#b38">[39]</ref>, we do not preprocess the images on MNIST. The CIFAR-10 dataset has 60,000 colored images (50,000 for training and 10,000 for testing) with 10 different label classes. We preprocess the images by subtracting per-pixel mean, and we augment the training data by mirrored images. We vary the number of training instances in [100, 10000] on MNIST and in [500, 50000] on CIFAR-10. The scoring functions h(x, y) and s(x, y) are implemented as an MLP (1.2M parameters) and a LeNet (3.1M parameters) on MNIST; while h(x, y) and s(x, y) are implemented as a LeNet (0.5M parameters) and a ResNet (1.7M parameters) on CIFAR-10 (detailed in Appendix C). We evaluate various methods over 10 runs with different initialization of C and report the mean accuracy and the standard deviation. Since the focus of this paper is to achieve a better accuracy for a given architecture of the classifier, we defer the discussion on the classifier's ratio of compression and loss of accuracy w.r.t. the teacher to Table <ref type="table">3</ref> in Appendix D.</p><p>Results and Discussions. First, we compare the proposed NaGAN and KDGAN with KD-based methods including MIMIC <ref type="bibr" target="#b6">[7]</ref>, DISTN <ref type="bibr" target="#b22">[23]</ref>, NOISY <ref type="bibr" target="#b38">[39]</ref>, and CODIS <ref type="bibr" target="#b1">[2]</ref>. The results obtained by varying the number of training images on MNIST and CIFAR-10 are summarized in Table <ref type="table" target="#tab_1">1</ref>. On both datasets, KDGAN consistently outperforms the KD-based methods by a large margin. For example, KDGAN achieves as much as 5.31% performance gain with 100 training images on MNIST. We further compare NaGAN with the KD-based methods. We observe that NaGAN performs better when a large amount of training data are available (e.g., 50,000 training images on CIFAR-10) while KD-based methods perform better when a small number of training images are available (e.g., 500 training images on CIFAR-10). This is consistent with our analysis in Section 3.1 that NaGAN can learn the true data distribution better, although this requires a large amount of training data.</p><p>Then, we compare NaGAN with KDGAN. As shown in ). One possible reason is that the Gumbel-Max trick effectively reduces the gradient variance from the discriminator as discussed in Section 3.3. This is also observed in our experiments, e.g., by comparing the gradient variance from the adversarial loss not using the Gumbel-Max trick in Figure <ref type="figure">7a</ref> with the one using the Gumbel-Max trick in Figure <ref type="figure">7b</ref> (see Appendix D for details).</p><p>Next, we study the reasons for the higher accuracy of KDGAN. We present how the accuracy of KDGAN varies against the hyperparameters on the MNIST dataset in Figure <ref type="figure">4</ref> (Note the logarithmic scale of the x-axis in Figures <ref type="figure">4b</ref> and<ref type="figure">4c</ref>). We find that Œ± and Œ≤ have a relatively small effect on the accuracy, which suggests that KDGAN is a robust framework. Besides, if we set Œ≤ to a small value (0.0001), we get more than 2% accuracy drop when KDGAN is trained with 100 training instances. This shows that T is important in training C when the number of training instances is small. We further find that a large value of Œ≥ causes the accuracy to deteriorate rapidly. This is because the soft labels provided by C are usually noisy. Emphasizing on training T to predict the noisy labels decreases the accuracy of T , which in turn decreases the accuracy of C. We obtain similar results for the effects of the hyperparameters on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Tag Recommendation</head><p>Image tag recommendation aims to recommend relevant tags (i.e., labels) after a user uploads an image to image-hosting websites such as Flickr <ref type="foot" target="#foot_2">4</ref> . As discussed before, we aim to recommend relevant tags right after a user uploads an image. This way, the user can just select from the recommended tags instead of inputting tags. Users may continue to add additional text for an uploaded image such as image titles and descriptions. We only use such additional text at the training stage as privileged information used by the teacher and the discriminator only. At the inference stage, our trained model (i.e., the classifier) only takes an image as input to make tag recommendations.</p><p>Dataset and Setup. We use the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset <ref type="foot" target="#foot_3">5</ref>in the experiments <ref type="bibr" target="#b44">[45]</ref>. To simulate the case where additional text about images is available for training, we randomly sample 20,000 images with titles or descriptions for training and another 2,000 images for testing. We create a dataset of images labeled with the 200 most popular tags and another dataset of images labeled with 200 randomly sampled tags. Following an earlier study <ref type="bibr" target="#b2">[3]</ref>, we use a VGGNet <ref type="bibr" target="#b39">[40]</ref> pretrained on ImageNet <ref type="bibr" target="#b13">[14]</ref> to extract image features and a LSTM <ref type="bibr" target="#b23">[24]</ref> with pretrained word embeddings <ref type="bibr" target="#b33">[34]</ref> to learn text features. We implement h(x, y) as an MLP with image features as input and implement s(x, y) as an MLP with the element-wise product of image and text features as input (detailed in Appendix C). We use precision (P@N), F-score (F@N), mean average precision (MAP), and mean reciprocal ranking (MRR) to evaluate performance. Method Most Popular Tags Randomly Sampled Tags P@3 P@5 F@3 F@5 MAP MRR P@3 P@5 F@3 F@5 MAP MRR KNN .2320 .  Results and Discussions. First, we compare C in KDGAN with KNN <ref type="bibr" target="#b31">[32]</ref>, TPROP <ref type="bibr" target="#b18">[19]</ref>, TFEAT <ref type="bibr" target="#b10">[11]</ref>,</p><p>and REXMP <ref type="bibr" target="#b27">[28]</ref>. The overall results are presented in Table <ref type="table" target="#tab_3">2</ref>. We find that KDGAN achieves significant improvements over the other methods across all the measures. Although KDGAN does not explicitly model the semantic similarity between two labels like what REXMP does, it still makes better recommendations than REXMP does. The reason is that in KDGAN, T provides C with soft labels at training. The soft labels contain a rich similarity structure over tags which cannot be modeled well by any pairwise similarity between tags used in REXMP. For example, an image labeled with a tag volleyball is supplied with a soft label assigning a probability of 10 -2 to basketball, 10 -4 to baseball, and 10 -8 to dragonfly. The reason that T generalizes is reflected in the relative probabilities over tags, which can be used for guiding C to generalize better.</p><p>Next, we compare the training curves of NaGAN, KDGAN-WO-GM, and KDGAN. We only plot the performance measured by P@3 in Figure <ref type="figure" target="#fig_3">3b</ref> because the other measures exhibit similar training curves. We find that KDGAN learns a more accurate classifier with a smaller number of training epochs (about 100 epochs) than NaGAN (about 220 epochs) and KDGAN-WO-GM (about 150 epochs). After convergence, KDGAN consistently outperforms the best baseline REXMP.</p><p>Last, we investigate how the performance of KDGAN varies against the hyperparameters over the YFCC100M dataset. The results are summarized in Figure <ref type="figure" target="#fig_4">5</ref>, which are consistent with our observations in the task of deep model compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a framework named KDGAN to distill knowledge with generative adversarial networks for multi-label learning with privileged provision. We have defined the KDGAN framework as a minimax game where a classifier, a teacher, and a discriminator are trained adversarially. We have proved that the minimax game has an equilibrium where the classifier perfectly models the true data distribution. We use the concrete distribution to control the variance of gradients during the adversarial training and obtained low-variance gradient estimates to accelerate the training. We have shown that KDGAN outperforms the state-of-the-art methods in two important applications, image tag recommendation and deep model compression. We show that KDGAN learns a more accurate classifier at a faster speed than a naive GAN (NaGAN) does. For future work, we will explore adaptive methods for determining model hyperparameters to achieve better training dynamics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Image tag recommendation where the additional text is only available for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Minibatch stochastic gradient descent training of KDGAN. Pretrain a classifier C, a teacher T , and a discriminator D with the training data {(x1, y1), ..., (xn, yn)}. for the number of training epochs do for the number of training steps for the discriminator do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Deep model compression over MNIST. Image tag recommendation on YFCC100M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training curves of the classifier in the proposed NaGAN and KDGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of hyperparameters in KDGAN on YFCC100M for image tag recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>‚Ä¢</head><label></label><figDesc>We reduce the number of training epochs required to converge by decreasing the variance of gradients, which is achieved by the design of KDGAN and the Gumbel-Max trick.‚Ä¢ We conduct extensive experiments in two applications, image tag recommendation and deep model compression. The experiments validate the superiority of KDGAN over state-of-the-art methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average accuracy over 10 runs in model compression (n is the number of training instances). ¬± 0.13 95.77 ¬± 0.10 98.89 ¬± 0.08 54.17 ¬± 0.20 77.82 ¬± 0.14 85.12 ¬± 0.11 DISTN 68.34 ¬± 0.06 93.97 ¬± 0.08 98.79 ¬± 0.07 50.92 ¬± 0.18 76.59 ¬± 0.15 83.32 ¬± 0.08 NOISY 66.53 ¬± 0.18 93.45 ¬± 0.11 98.58 ¬± 0.11 50.18 ¬± 0.28 75.42 ¬± 0.19 82.99 ¬± 0.12 MIMIC 67.35 ¬± 0.15 93.78 ¬± 0.13 98.65 ¬± 0.05 51.74 ¬± 0.23 75.66 ¬± 0.17 84.33 ¬± 0.10 NaGAN 64.90 ¬± 0.31 93.60 ¬± 0.22 98.95 ¬± 0.19 46.29 ¬± 0.32 76.11 ¬± 0.24 85.34 ¬± 0.27 KDGAN 77.95 ¬± 0.05 96.42 ¬± 0.05 99.25 ¬± 0.02 57.56 ¬± 0.13 79.36 ¬± 0.04 86.50 ¬± 0.04</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell>CIFAR-10</cell></row><row><cell cols="2">n = 100</cell><cell cols="2">n = 1, 000</cell><cell>n = 10, 000</cell><cell>n = 500</cell><cell>n = 5, 000</cell><cell>n = 50, 000</cell></row><row><cell>CODIS 0.2 0.4 0.6 0.8 Accuracy 74.02 0 40 0.0</cell><cell cols="2">80 Training Epochs 120</cell><cell cols="2">DISTN CODIS NaGAN KDGAN-WO-GM 160 200 KDGAN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 ,</head><label>1</label><figDesc>KDGAN achieves a larger performance gain over NaGAN with fewer training instances. This indicates that KDGAN requires a smaller number of training instances than NaGAN does to reach the same level of accuracy. This can be explained by that KDGAN introduces T to provide soft labels for training C. The soft labels generally have high entropy and reveal much useful information about each training instance. Hence, the soft labels impose much more constraint on the parameters of C than the true labels, which can reduce the number of training instances required to train C. We further investigate the training speedFigure 4: Effects of hyperparameters in KDGAN on MNIST for deep model compression. of NaGAN and KDGAN by the number of training epochs. Typical learning curves of C in NaGAN and KDGAN are shown in Figure3a. Due to the page limit, we only show the results using 100 training images on MNIST. We find that KDGAN converges to a better accuracy with a smaller number of training epochs (about 25 epochs) than NaGAN (about 135 epochs). After convergence, the training curve in KDGAN is more stable than that in NaGAN. Moreover, we investigate the benefit provided by the Gumbel-Max trick for the KDGAN training. We perform the KDGAN training without using the Gumbel-Max trick (referred to as KDGAN-WO-GM) and also plot the accuracy against training epochs in Figure3a. By comparing KDGAN with KDGAN-WO-GM, we can see that the Gumbel-Max trick speeds up the training process by around 45% in terms of training epochs. The Gumbel-Max trick also helps improve the accuracy from 0.7605 to 0.7795 (by around 2.5%</figDesc><table><row><cell></cell><cell>.99</cell><cell>n=100</cell><cell>n=1000</cell><cell>n=10000</cell><cell></cell><cell>.99</cell><cell>n=100</cell><cell>n=1000</cell><cell></cell><cell cols="2">n=10000</cell><cell></cell><cell>1.0</cell><cell>n=100</cell><cell>n=1000</cell><cell>n=10000</cell></row><row><cell>Accuracy</cell><cell>.95 .97 .73 .75 .77</cell><cell cols="3">0.0 0.2 0.4 0.6 0.8 1.0 Œ±</cell><cell>Accuracy</cell><cell>.95 .97 .73 .75 .77</cell><cell>-3 -2 -1</cell><cell>0 log 10 Œ≤</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>Accuracy</cell><cell>0.6 0.7 0.8 0.9</cell><cell cols="2">-4 -3 -2 -1 log 10 Œ≥</cell><cell>0</cell><cell>1</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) Effect of varying Œ±</cell><cell></cell><cell></cell><cell cols="4">(b) Effect of varying Œ≤</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) Effect of varying Œ≥</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of various methods on the YFCC100M dataset in tag recommendation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The Gumbel distribution can be sampled by drawing u ‚àº Uniform(0, 1) and computing g = -log(-log u).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The code and the data are made available at https://github.com/xiaojiew1/KDGAN/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://www.flickr.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Yahoo Webscope Program. http://webscope.sandbox.yahoo.com/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by Australian Research Council Future Fellowship Project FT120100832 and Discovery Project DP180102050. We thank the anonymous reviewers for their feedback on the paper. We have incorporated responses to reviewers' comments in the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04838</idno>
		<title level="m">Optimization methods for large-scale machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucilu«é</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tag-based image retrieval improved by augmented features and group-based refinement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Label ranking methods based on the plackett-luce model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>H√ºllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dembczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chongxuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10793</idno>
		<title level="m">Understanding gans: the lqg setting</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Triangle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised clustering of label ranking data using label preference information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical theory of extreme values and some practical applications: A series of lectures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gumbel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>US Government Printing Office, Washington</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifying tag relevance with relevant positive and negative examples</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Baselines for image annotation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the theory of learnining with privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pechyony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Sau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09650</idno>
		<title level="m">Deep model compression: Distilling knowledge from noisy teachers</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00433</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Cross quality distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual intent tracking for personal assistants</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Collaborative nowcasting for contextual recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative intent prediction with real-time contextual data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: the new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning using privileged information: similarity control and knowledge transfer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A joint optimization approach for personalized recommendation diversification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<editor>PAKDD</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning loss for knowledge distillation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00513</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generating text via adversarial training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adversarial feature matching for text generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
