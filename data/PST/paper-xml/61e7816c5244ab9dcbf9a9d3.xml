<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recommendation Unlearning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-18">18 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
							<email>bolin.ding@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recommendation Unlearning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-18">18 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2201.06820v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Unlearning</term>
					<term>Selective Deletion</term>
					<term>Recommender Systems</term>
					<term>Collaborative Filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender systems provide essential web services by learning users' personal preferences from collected data. However, in many cases, systems also need to forget some training data. From the perspective of privacy, several privacy regulations have recently been enacted, requiring systems to eliminate any impact of the data whose owner requests to forget. From the perspective of utility, if a system's utility is damaged by some bad data, the system needs to forget such data to regain utility. While unlearning is very important, it has not been well-considered in existing recommender systems. Although there are some researches have studied the problem of machine unlearning in the domains of image and text data, existing methods can not be directly applied to recommendation as they are unable to consider the collaborative information.</p><p>In this paper, we propose RecEraser, a general and efficient machine unlearning framework tailored to recommendation tasks. The main idea of RecEraser is to partition the training set into multiple shards and train a constituent model for each shard. Specifically, to keep the collaborative information of the data, we first design three novel data partition algorithms to divide training data into balanced groups based on their similarity. Then, considering that different shard models do not uniformly contribute to the final prediction, we further propose an adaptive aggregation method to improve the global model utility. Experimental results on three public benchmarks show that RecEraser can not only achieve efficient unlearning but also outperform the state-of-the-art unlearning methods in terms of model utility. The source code can be found at https://github.com/chenchongthu/Recommendation-Unlearning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems provide personalized service for users to alleviate the information overload problem, playing a more and more important role in a wide range of applications, such as ecommerce <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>, social media <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46]</ref>, and news portal <ref type="bibr" target="#b39">[40]</ref>. The key technology of personalized recommender systems is known as collaborative filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>, which learns users' preference based on their historical records (e.g., views, clicks, and ratings).</p><p>Once a recommender system is built, it has potentially memorized the training data. However, in many cases, a recommender system also needs to forget certain sensitive data and its complete lineage, which is called Recommendation Unlearning in this paper. Consider privacy first. Recently several regulations have been proposed to protect the privacy of users, such as the General Data Protection Regulation (GDPR) in the European Union <ref type="foot" target="#foot_0">1</ref> and the California Consumer Privacy Act (CCPA) in the United States <ref type="foot" target="#foot_1">2</ref> . One of the most important articles in these regulations is "the right to be forgotten", which entitles data subjects the right to delete their data from an entity storing them <ref type="bibr" target="#b0">[1]</ref>. For recommender systems, "the right to be forgotten" requires systems to eliminate any impact of the data (e.g., the learned preference from the data) whose owner requests to forget. The second reason is utility. Nowadays, data pipelines are often not static. New data is collected regularly and incrementally used to further refine existing models <ref type="bibr" target="#b46">[47]</ref>. However, bad data (or called dirty data), e.g., polluted data in poisoning attacks <ref type="bibr" target="#b31">[32]</ref> or out-of-distribution (OOD) data <ref type="bibr" target="#b2">[3]</ref>, will seriously degrade the performance of recommendation. Once these data are identified, the system needs to forget them to regain utility.</p><p>The most straightforward and naive unlearning method is to remove the revoked sample and retrain the model from scratch. However, this method presents strong challenges in learning efficiency, especially when the training data is large. To address this, some machine unlearning methods have been proposed in the domains of computer vision and natural language processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Among them, the SISA method <ref type="bibr" target="#b0">[1]</ref> is the most representative one. The basic idea of SISA is to randomly split the training data into several disjoint shards and train each shard model separately. After that, the final prediction results are obtained from the aggregation of shard models through majority voting or average. When receiving an unlearning request, only the corresponding shard model needs to retrain. Although existing machine unlearning methods are effective to handle image and text data, they cannot be directly applied to recommendation. Since recommender systems rely on collaborative information across users and items, randomly partitioning the data into shards could severely damage the resulting model utility. Moreover, the aggregation methods used in existing unlearning methods often assign a static weight to each shard model, which is not suitable for recommender systems that usually face a variety of users and items.</p><p>Motivated by the above observations, in this paper we propose a novel and efficient erasable recommendation framework, namely RecEraser, to achieve high unlearning efficiency and keep high model utility in the context of recommendation. The general pipeline of RecEraser is to partition the training set into multiple shards and train a constituent model for each shard. To permit efficient retraining while keeping the collaborative information of the data, we design three data partition strategies based on the similarities of users, items, and interactions, respectively. Different from traditional community detection and clustering methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>, our data partition strategies are designed to achieve balanced partition so that the unlearning efficiency will not be affected by the unbalanced shard size. In addition, considering that the different shard models do not uniformly contribute to the final prediction, we further propose an attention-based adaptive aggregation method to improve the global model utility. It first transfers shard models into the same space, then automatically learns weights of shard models for the prediction of each individual.</p><p>To evaluate the unlearning efficiency and model utility of our method, we apply RecEraser on three real-world datasets with extensive experiments. Since the architecture of RecEraser is modelagnostic for base models, we utilize three representative recommendation models BPR <ref type="bibr" target="#b40">[41]</ref>, WMF <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>, and LightGCN <ref type="bibr" target="#b26">[27]</ref> as its base models and present the performance of different base models respectively. The experimental results show that RecEraser can not only achieve efficient unlearning, but also outperform the state-ofthe-art unlearning frameworks like SISA <ref type="bibr" target="#b0">[1]</ref> and GraphEraser <ref type="bibr" target="#b11">[12]</ref> in terms of model utility. Further ablation studies also show the effectiveness of our proposed data partition strategies and adaptive aggregation method on improving the recommendation utility. The main contributions of this work are summarized as follows.</p><p>(1) To the best of our knowledge, this is the first work that addresses the machine unlearning problem for recommendation models. A general erasable recommendation framework named RecEraser is proposed to achieve both high unlearning efficiency and high model utility. (2) We design three data partition strategies to split recommendation data into balanced shards, and propose an attention-based adaptive aggregation method to further improve the model utility of RecEraser. (3) We conduct extensive experiments on three real-world datasets and three representative recommendation models. The results show that RecEraser can not only achieve efficient unlearning, but also outperform the state-of-the-art unlearning framework in terms of model utility. We will release our implementation to facilitate further developments on recommendation unlearning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Item Recommendation</head><p>Popularized by the Netflix Challenge, early recommendation methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> were designed to model users' explicit feedback by mapping users and items to a latent factor space like matrix factorization. Later on, researchers found that users interact with items mainly through implicit feedback, such as purchases on Ecommerce sites and watches on online video platforms. Then a surge of recommendation methods were proposed for learning from implicit feedback <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>. Specifically, Hu et al. <ref type="bibr" target="#b29">[30]</ref> proposed a non-sampling based method WMF, which assumes that all unobserved items are negative samples. Some recent studies have also been made to resolving the inefficiency issue of nonsampling learning. For instance, Chen et al. <ref type="bibr" target="#b9">[10]</ref> derive a flexible non-sampling loss for recommendation models, which achieves both effective and efficient performance. On another line of research, Rendle et al. <ref type="bibr" target="#b40">[41]</ref> proposed a pair-wise learning method BPR, which is a sampling-based method that optimizes the model based on the relative preference of a user over pairs of items. Due to the popularity of deep learning, there is a large literature exploiting different neural networks for recommender systems. In <ref type="bibr" target="#b28">[29]</ref>, He et al. presented a Neural Collaborative Filtering (NCF) framework to address implicit feedback data by jointly learning a matrix factorization and a feedforward neural network. The NCF framework has been widely extended to adapt to different recommendation scenarios <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. Recently, it has become a trend to explore the application of newly proposed deep learning architectures in recommendation. Such as attention mechanisms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref>, Convolutional Neural Network <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48]</ref>, Recurrent Neural Network <ref type="bibr" target="#b39">[40]</ref>, and Graph Neural Networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44]</ref>. Specifically, Wang et al. <ref type="bibr" target="#b43">[44]</ref> proposed NGCF to exploit high-order proximity by propagating embeddings on the user-item interaction graph. NGCF is then further extended to LightGCN <ref type="bibr" target="#b26">[27]</ref> by removing the non-linear activation function and feature transformation. LightGCN is more efficient than vanilla GCN modes and has achieved state-of-the-art performance in Top-𝐾 recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Unlearning</head><p>Machine unlearning <ref type="foot" target="#foot_2">3</ref> , also known as selective forgetting <ref type="bibr" target="#b22">[23]</ref> or data removal/deletion <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> in machine learning, refers to a process that aims to remove the influence of a specified subset of training data upon request from a trained model. Previous studies on machine unlearning can be divided into two groups: approximate unlearning and exact unlearning.</p><p>Approximate unlearning offers statistical guarantee about data deletion, thus a.k.a statistical unlearning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>. The essential idea is to relax the requirements for exact deletion, i.e., it only provides a statistical guarantee that an unlearned model cannot be distinguished from a model that was never trained on the removed data <ref type="bibr" target="#b24">[25]</ref>. They usually employ gradient based update The aggregated embeddings of user 𝑢 and item 𝑣, respectively strategies to quickly eliminate the influence of samples that are requested to be deleted <ref type="bibr" target="#b38">[39]</ref>. For example, Guo et al. <ref type="bibr" target="#b24">[25]</ref>, Golatkar et al. <ref type="bibr" target="#b22">[23]</ref>, and Golatkar et al. <ref type="bibr" target="#b23">[24]</ref> proposed different Newton's methods to approximate retraining for convex models, e.g., linear regression, logistic regression, and the last fully connected layer of a neural network. An alternative is to eliminate the influence of the samples that need to be deleted to the learned model based on influence functions <ref type="bibr" target="#b30">[31]</ref>. Compared with exact unlearning, approximate unlearning methods are usually more efficient. However, their guarantees are probabilistic and are hard to apply on non-convex models like deep neural networks. It makes them not very suitable for the applications of recommender systems, which are strictly regulated by the laws, e.g., GDPR and CCPA.</p><p>Exact unlearning aims to ensure that the request data is completely removed from the learned model. Early works usually aim to speed up the exact unlearning for simple models or under some specific conditions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42]</ref>, like leave-one-out cross-validation for SVMs (Support Vector Machines) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>, provably efficient data deletion in 𝑘-means clustering <ref type="bibr" target="#b19">[20]</ref>, and fast data deletion for Naïve Bayes based on statistical query learning which assumes the training data is in a decided order <ref type="bibr" target="#b1">[2]</ref>. More recently, the representative work is SISA (Sharded, Isolated, Sliced, and Aggregated) <ref type="bibr" target="#b0">[1]</ref>. SISA is a quite general framework, and its key idea can be abstracted into three steps: (1) divide the training data into several disjoint shards; (2) train sub-models independently (i.e., without communication) on each of these data shards; (3) aggregate the results from all shards for final prediction. In this way, unlearning can be effectively achieved by only retraining the affected sub-model. Subsequently, Chen et al. <ref type="bibr" target="#b11">[12]</ref> applied this idea to the unlearning of graph with an improved sharding algorithm. Our RecEraser is different from existing methods in the following aspects: (1) we design new data partition methods to keep the collaborative information of the data; <ref type="bibr" target="#b1">(2)</ref> we propose an adaptive aggregation method to improve the global model utility. These designs make our RecEraser more suitable for recommendation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECOMMENDATION UNLEARNING</head><p>In this section we first introduce the key notation, then define the machine unlearning problem of recommendation and discuss its differences and challenges compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Formulation</head><p>Table <ref type="table" target="#tab_0">1</ref> depicts the notations and key concepts used in this paper. We denote the user and item sets as U and V, respectively. The useritem interaction matrix is denoted as Y = [𝑦 𝑢𝑣 ] ∈ {0, 1}, indicating whether 𝑢 has an interaction with item 𝑣. Given a target user 𝑢, the recommendation task is to recommend a list of items that 𝑢 may be interested in.</p><p>For Recommendation Unlearning, if user 𝑢 wants to revoke one record to item 𝑣 (i.e., 𝑦 𝑢𝑣 ), a recommender system needs to obtain an unlearned model trained on Y\𝑦 𝑢𝑣 . Formally, the task of recommended unlearning is to achieve three general objectives.</p><p>• Provable Guarantees: It is the basic requirement of unlearning which demands the revoked data must be really unlearned and do not influence model parameters.</p><p>• High Unlearning Efficiency: The Unlearning time taken when receiving an unlearning request should be as short as possible.</p><p>• Comparable Model Utility: The performance of the unlearned model's prediction should be comparable to that of retraining from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Challenges of Recommendation Unlearning</head><p>As we have mentioned in Section 1, existing machine unlearning methods are mainly designed for models whose inputs reside in the Euclidean space, such as images and texts. For example, the state-of-the-art unlearning method SISA <ref type="bibr" target="#b0">[1]</ref> randomly partitions the training data into multiple shards, trains a constituent model for each shard, and then gets the final prediction results through majority voting or average. However, the inputs of recommendation are user-item interactions, which contain rich collaborative information and are not independent identically distributed. Directly applying SISA for a recommendation task will destroy the training data's structure which could severely damage the resulting model utility. One solution is to leverage community detection methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> to partition the training data, so that the collaborative information of the data can be preserved to a great extent. The issue of classical community detection methods is that they may lead to highly unbalanced shard sizes <ref type="bibr" target="#b11">[12]</ref>. Therefore, the efficiency of the unlearning process will be affected if a revoked record belongs to a large shard with a long retraining time. To address the above problems, we need to design new approaches to achieve balanced data partition while keeping the collaborative information. Moreover, existing unlearning methods typically use a static aggregation strategy at the inference phase, which is not suitable for recommender systems that usually face a variety of users and items. To further improves the model utility of a recommendation unlearning framework, the importance score of a shard model should depend on how well it learns the feature of an individual user (or an item).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RECERASER METHOD</head><p>In this section, we first present a general overview of the RecEraser framework, then introduce the two key ingredients of our proposed model in detail, which are: 1) balanced data partition and 2) attention-based adaptive aggregation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The overall framework of RecEraser is described in Figure <ref type="figure" target="#fig_0">1</ref>. From the figure, we first make a simple overview of our method:</p><p>(1) RecEraser consists of three phases: balanced data partition, shard model training, and attention-based adaptive aggregation.</p><p>(2) The balanced data partition part is designed to divide the training data while preserving the collaborative information. After the data is partitioned, a shard model (𝑀 𝑖 ) is trained for each of the shard data (𝑆 𝑖 ). All shard models share the same model architecture and can be trained in parallel to speed up the training process. At the prediction phase, for each individual prediction, an attention-based adaptive aggregation strategy is applied to find the optimal weight allocation for different shard models. (3) When data needs to be unlearned, only one of the constituent models whose shard contains the point to be unlearned and the aggregation part need to be retrained, which is much more efficient than retraining the whole model from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Balanced Data Partition</head><p>As we have mentioned before, the data used for recommendation tasks usually contains rich collaborative information. To preserve the collaborative information as much as possible, one promising approach is to rely on community detection and clustering methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. However, due to the underlying structure of real-world data, direct application of them may lead to highly unbalanced data partition. for each 𝑎 𝑖 in A do</p><formula xml:id="formula_0">4:</formula><p>for each 𝑢 in U do 5:</p><p>Calculate the distance 𝐸 (𝑎 𝑖 , 𝑢) = dist(𝑎 𝑖 , 𝑢) (Eq.( <ref type="formula">1</ref>)) Updating A by Eq.(4) 16: end while 17: return S Item-based Balanced Partition (IBP), and Interaction-based Balanced Partition (InBP). The general idea to achieve balanced partitions is to set the maximum number of shards with size 𝑡, and only allocate data points to shards whose size does not exceed 𝑡.</p><p>The UBP method divides training data based on the similarities of users, which is shown in Algorithm 1. Given an entire user set U, 𝐾 anchor users are first randomly selected as the centers of each shard model, denoted by A= {𝑎 1 , 𝑎 2 , . . . , 𝑎 𝐾 }. For each anchor user, the corresponding shard is discovered by estimating the distance with respect to all other users. Following the setting of 𝑘-means <ref type="bibr" target="#b32">[33]</ref>, the Euclidean distance is employed in this work, which is defined as follows:</p><formula xml:id="formula_1">dist(𝑎 𝑖 , 𝑢) = p𝑎 𝑖 − p𝑢 2 = 𝑛 ∑︁ 𝑗=1 p𝑎 𝑖 ,𝑗 − p𝑢,𝑗 2 (1)</formula><p>where p𝑎 𝑖 and p𝑢 are the embedding vectors for the anchor user 𝑎 𝑖 and the user 𝑢. The embedding vectors are computed by a pretrained model (WMF <ref type="bibr" target="#b29">[30]</ref> is used in this work, but any other embedding vectors can be applied).</p><p>After that, we get 𝑚 × 𝐾 distance scores, where 𝑚 denotes the number of users. These scores are sorted in ascending order to get 𝐸 𝑠 . For each user-anchor pair in 𝐸 𝑠 , if the size of 𝑆 𝑖 is smaller than the maximum number of shard 𝑡, then the training interaction Y 𝑢 is allocated to shard 𝑆 𝑖 . Finally, the new anchors are calculated as the average of all the users in their corresponding shards:</p><formula xml:id="formula_2">𝑎 𝑖 = 𝑗 ∈𝑆 𝑖 p𝑗 |𝑆 𝑖 |<label>(2)</label></formula><p>The UBP method repeats the above steps until it meets the stopping criteria (reaches the maximum iteration or the anchors do not change). The IBP method is based on the similarities of pre-trained item embeddings, which is similar to UBP with little variations. To avoid repetition, we do not introduce it step by step. UBP and IBP use users and items to model the local property respectively, which may ignore the global feature of training interaction (user-item pairs). So we further propose an Interaction-based Balanced Partition method (InBP), which considers the similarities of user-item pairs to divide the training data. As shown in Algorithm 2, the inputs of InBP include the pre-trained user embeddings P= { p1 , p2 , . . . , p𝑚 }, item embeddings Q= { q1 , q2 , . . . , q𝑛 }, useritem interactions Y, the number of shards 𝐾, and the maximum number of shard 𝑡. The main differences between InBP and UBP is as follows: (1) InBP randomly select 𝐾 anchors A= {𝑎 1 , 𝑎 2 , . . . , 𝑎 𝐾 } from user-item interactions Y, where 𝑎 𝑖 = (p 𝑖 , q 𝑖 ), p 𝑖 and q 𝑖 are the embedding vectors for the data point 𝑎 𝑖 ; <ref type="bibr" target="#b1">(2)</ref> The distance between anchor 𝑎 𝑖 and interaction 𝑦 𝑢𝑣 is calculated as follows:</p><formula xml:id="formula_3">dist(𝑎 𝑖 , 𝑦 𝑢𝑣 ) = p𝑖 − p𝑢 2 × q𝑖 − q𝑣 2 = 𝑛 ∑︁ 𝑗=1 p𝑖,𝑗 − p𝑢,𝑗 2 × 𝑛 ∑︁ 𝑗=1 q𝑖,𝑗 − q𝑣,𝑗 2<label>(3)</label></formula><p>where p𝑢 and q𝑣 are the pre-trained embedding vectors for the data point 𝑦 𝑢𝑣 ; (3) For each interaction pair in 𝐸 𝑠 , if the size of 𝑆 𝑖 is smaller than the maximum number of shard 𝑡, then the training interaction 𝑦 𝑢𝑣 is allocated to shard 𝑆 𝑖 ; (4) The new anchors are updated as follows:</p><formula xml:id="formula_4">𝑎 𝑖 = 𝑗 ∈𝑆 𝑖 p𝑗 |𝑆 𝑖 | , 𝑗 ∈𝑆 𝑖 q𝑗 |𝑆 𝑖 |<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attention-based Adaptive Aggregation</head><p>After the training data is partitioned, a shard model 𝑀 𝑖 is trained on each shard data 𝑆 𝑖 . Generally, various shard models should have different contributions with regard to the prediction of different user-item pairs. For example, if shard 𝑆 𝑖 contains more training interactions of user 𝑢 than shard 𝑆 𝑗 , then the weight of 𝑀 𝑖 is more likely to be greater than that of 𝑀 𝑗 when predicting 𝑢's preference.</p><p>To further improves the recommendation performance, we propose an attention-based adaptive aggregation method.</p><p>Considering that the representations of users and items learned by different shard models may be embedded in different spaces, we first transfer them into the same representation space. Specifically, for P 𝑖 and Q 𝑖 learned by shard model 𝑆 𝑖 , the transfer scheme is defined as:</p><formula xml:id="formula_5">P 𝑖 tr = W 𝑖 P 𝑖 + b 𝑖 ; Q 𝑖 tr = W 𝑖 Q 𝑖 + b 𝑖<label>(5)</label></formula><p>where W 𝑖 ∈ R 𝑑×𝑑 is a transfer matrix which projects P 𝑖 and Q 𝑖 to the global representation space, b 𝑖 ∈ R 𝑑 is the bias vector, and 𝑑 denotes the embedding size.</p><p>Then the aggregated embeddings of users and items are calculated as follows:</p><formula xml:id="formula_6">P = 𝐾 ∑︁ 𝑖=1 𝛼 𝑖 P 𝑖 tr ; Q = 𝐾 ∑︁ 𝑖=1 𝛽 𝑖 Q 𝑖 tr (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where 𝛼 𝑖 and 𝛽 𝑖 are the attention weights for aggregating the embeddings of users and items, indicating how much information being propagated from shard model 𝑆 𝑖 . More precisely, 𝛼 𝑖 and 𝛽 𝑖 are defined as:</p><formula xml:id="formula_8">𝛼 * 𝑖 = h ⊤ 1 𝜎 (W 1 p 𝑖 tr + b 1 ); 𝛼 𝑖 = exp(𝛼 * 𝑖 ) 𝐾 𝑗=1 exp(𝛼 * 𝑗 ) 𝛽 * 𝑖 = h ⊤ 2 𝜎 (W 2 p 𝑖 tr + b 2 ); 𝛽 𝑖 = exp(𝛽 * 𝑖 ) 𝐾 𝑗=1 exp(𝛽 * 𝑗 )<label>(7)</label></formula><p>where where P and Q are the aggregated embeddings of users and items, Y is the true lable of user-item interaction, L represents the loss function of base model (e.g., Eq. 9 and Eq. 10), and Θ denotes the parameters of attention network. ℓ 2 regularization parameterized by 𝜆 on Θ is conducted to prevent overfitting. Note that only the parameters of attention network are optimized through Eq.( <ref type="formula">8</ref>) while the embeddings P 𝑖 and Q 𝑖 learned by shard models are fixed.</p><formula xml:id="formula_9">W 1 ∈ R 𝑘×𝑑 , b 1 ∈ R 𝑘 ,</formula><p>The whole training data Y is used for training, which can also be requested to be revoked. Therefore, our aggregation method needs to be retrained when receiving an unlearning request of data, and this training time is also part of the unlearning time. Fortunately, the proposed aggregation method only needs a few epochs (about 10) to learn the optimal attention scores, and can effectively improve the model utility compared to other aggregation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>The architecture of our RecEraser is model-agnostic for base models. In this work we instantiate RecEraser with three representative recommendation models: BPR <ref type="bibr" target="#b40">[41]</ref>, WMF <ref type="bibr" target="#b29">[30]</ref>, and LightGCN <ref type="bibr" target="#b26">[27]</ref>.</p><p>The scoring functions of the above methods are all based on vector inner product. Formally, for each shard model 𝑀 𝑖 , the scoring function is defined as ŷ𝑢𝑣 = p 𝑖 𝑢 ⊤ q 𝑖 𝑣 where p 𝑖 𝑢 and q 𝑖 𝑣 denote the embeddings of user 𝑢 and item 𝑣 learned by 𝑀 𝑖 , respectively.</p><p>BPR <ref type="bibr" target="#b40">[41]</ref> and LightGCN <ref type="bibr" target="#b26">[27]</ref> use the same negative sampling strategy. The objective function of shard model 𝑀 𝑖 is:</p><formula xml:id="formula_10">L (P 𝑖 , Q 𝑖 , 𝑆 𝑖 ) = − ∑︁ (𝑢,𝑣) ∈𝑆 𝑖 ln 𝜎 ( ŷ𝑢𝑣 − ŷ𝑢 𝑗 )<label>(9)</label></formula><p>where 𝜎 (𝑥) = 1/(1 + exp(−𝑥)) is a sigmoid function. For each positive user-item pair (𝑢, 𝑣), a negative instance (𝑢, 𝑗) is sampled from the unobserved items of user 𝑢. The difference between BPR and LightGCN is that BPR simply takes the user/item vectors (P 𝑖 , Q 𝑖 ) from ID embeddings while LightGCN computes P 𝑖 and Q 𝑖 through the light weight neighbor augmentation technique. WMF <ref type="bibr" target="#b29">[30]</ref> is a non-sampling based method, which computes the following objective function for shard model 𝑀 𝑖 :</p><formula xml:id="formula_11">L (P 𝑖 , Q 𝑖 , 𝑆 𝑖 ) = ∑︁ 𝑢 ∈U 𝑖 ∑︁ 𝑣 ∈V 𝑖 𝑐 𝑢𝑣 (𝑦 𝑢𝑣 − ŷ𝑢𝑣 ) 2<label>(10)</label></formula><p>where 𝑐 𝑢𝑣 denotes the weight of entry 𝑦 𝑢𝑣 , U 𝑖 and V 𝑖 denote the users and items in Shard 𝑆 𝑖 . Recently, Chen et al. <ref type="bibr" target="#b9">[10]</ref> have proposed an efficient method to compute the above loss, which is utilized in our work to achieve fast training of WMF.</p><p>To optimize the objective function, we use mini-batch Adagrad <ref type="bibr" target="#b15">[16]</ref> as the optimizer. Its main advantage is that the learning rate can be self-adaptive during model learning. RecEraser adopts a two-step training process. The first step is training shard models by minimizing Eq.( <ref type="formula" target="#formula_10">9</ref>) or Eq. <ref type="bibr" target="#b9">(10)</ref>. To speed up the training process, the model owner can train different shard models in parallel. The second step is fixing shard models and training the attention-based aggregation part through Eq.( <ref type="formula">8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setup</head><p>5.1.1 Datasets. We conduct our experiments on three real-world and publicly available datasets, which have been widely used for evaluating the performance of recommendation models: Yelp2018<ref type="foot" target="#foot_3">4</ref> , Movielens-1m<ref type="foot" target="#foot_4">5</ref> , and Movielens-10m<ref type="foot" target="#foot_5">6</ref> . Since we focus on the Top-𝑁 recommendation task, we follow the widely used pre-processing method to convert these datasets into implicit feedbacks. Specifically, the detailed rating is transformed into a value of 0 or 1 indicating whether a user has interacted with an item. The statistical details of these datasets are summarized in Table <ref type="table" target="#tab_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Compared Methods.</head><p>We compare with the state-of-theart machine unlearning methods on different recommendation models. The compared recommendation models include:</p><p>• BPR <ref type="bibr" target="#b40">[41]</ref>: This is a widely used recommendation model, which optimizes matrix factorization with the Bayesian Personalized Ranking objective function.</p><p>• WMF <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>: This is a non-sampling recommendation model, which treats all missing interactions as negative instances and weighting them uniformly. In our experiment, it is trained through the efficient non-sampling method proposed by <ref type="bibr" target="#b9">[10]</ref>.</p><p>• LightGCN <ref type="bibr" target="#b26">[27]</ref>: This is a state-of-the-art graph neural network model which simplifies the design of GNN to make it more appropriate for recommendation.</p><p>The compared machine unlearning methods include:</p><p>• Retrain: The most straightforward machine unlearning method, which removes the revoked sample and retrains the model. It is added as the basic benchmark.</p><p>• SISA <ref type="bibr" target="#b0">[1]</ref>: This is a state-of-the-art machine unlearning method, which randomly splits the training data into disjoint shards, trains each shard model, and aggregates the results from all shards through average for final prediction. • GraphEraser <ref type="bibr" target="#b11">[12]</ref>: This is a state-of-the-art machine unlearning method tailored to graph data, which uses node clustering methods for graph partition and a static weighted aggregation method for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Methodology.</head><p>For each dataset, we randomly select 80% interactions to construct the training set and treat the remaining as the test set. From the training set, we randomly select 10% interactions as the validation set to tune hyper-parameters. For each user, our evaluation protocol ranks all the items except the positive ones in the training set. To evaluate the model utility, we apply two widely-used evaluation protocols: Recall@𝑁 and NDCG@𝑁 . Recall@𝑁 measures whether the ground truth is ranked among the top 𝑁 items, while NDCG@𝑁 is a position-aware ranking metric, which assigns higher scores to higher positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.4</head><p>Hyper-parameter Settings. The hype-parameters are carefully tuned using a grid search to achieve optimal performances.</p><p>In the case of model-specific hyper-parameters, we tune them in the ranges suggested by their original papers. After the tuning process, the batch size is set to 512, the learning rate is set to 0.05, the embedding size 𝑑 is set to 64, and the attention size 𝑘 is set to 32. The maximum number of epochs is set to 1000. The early stopping strategy is also adopted in our experiment, which terminates the training when Recall@10 on the validation set does not increase for 10 successive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Model Utility</head><p>We first evaluate the model utility of different unlearning methods. In this section, the proposed InBP method is used for RecEraser and the number of shards is set to 10 for all unlearning methods. To evaluate different Top-𝑁 recommendation results, we set the length 𝑁 = 10, 20, and 50 in our experiments. The results are shown in Table <ref type="table" target="#tab_4">2</ref>. From the table, the following observations can be made: First, the retrain method generally can achieve the best performance since it uses the original dataset without the deleted sample for training. However, as shown in the next section, its unlearning efficiency is low especially when the model is complex and the original training dataset is large.</p><p>Second, our proposed RecEraser achieves significantly better performance (𝑝 &lt;0.01) than the state-of-the-art unlearning baselines on the three datasets. Specifically, on Yelp2018 dataset, RecEraser exhibits average improvements of 59%, 41%, and 46% compared to SISA when using base models BPR, WMF, and LightGCN, respectively. Compared to GraphEraser, the above improvements are 11%, 32%, and 19%, respectively. On other datasets, the results of RecEraser are also remarkable. The substantial improvement could be attributed to two reasons: (1) RecEraser partitions data through the proposed InBP method, which can enhance performance by preserving collaborative information; (2) RecEraser uses the proposed attention-based aggregation method to adaptively aggregate shard models, which could further improve the global model utility.</p><p>Third, considering the performance on each dataset, we observe that the performances of unlearning methods depend on the sparsity of datasets. For example, the performances of RecEraser are more comparable with the retraining method on movielens-1m and movielens-10m since the two datasets are relatively dense than yelp2018. User preferences are more difficult to learn from sparse user-item interactions, thus the number of shards should be carefully set to balance the unlearning efficiency and model utility. We make further discussions in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Unlearning Efficiency</head><p>In this section, we evaluate the unlearning efficiency of our proposed RecEraser. Following the settings of Chen et al. <ref type="bibr" target="#b11">[12]</ref>, we randomly sample 100 user-item interactions from the training data, record the retraining time of their corresponding shard models and aggregation part, and calculate the average retraining time. The shard number is set to 10 in our experiment and all experiments in this section are run on the same machine (Intel Xeon 8-Core CPU of 2.4 GHz and single NVIDIA GeForce GTX TITAN X GPU). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movielens-1m</head><p>Random UBP IBP InBP The results are shown in Table <ref type="table" target="#tab_6">4</ref>. The experimental results show that our proposed RecEraser method can significantly improve the unlearning efficiency compared to the retrain method. For example, on movielens-10m dataset, our RecEraser only needs 320 minutes, 16 minutes, and 920 minutes to achieve the optimal performance for BPR, WMF, and LightGCN respectively, while the retrain method needs about 3250 minutes, 292 minutes, and 25330 minutes respectively. This acceleration is over 10 times, which is highly valuable in practice and is difficult to achieve with simple engineering efforts.</p><p>For retrain method, training on a large data can cost a large amount of time. For our RecEraser, only the corresponding shard model and the aggregation part need to be retrained when receiving an unlearning request of data. Moreover, we can tolerate more shards for larger datasets to further improve the unlearning efficiency while preserving the model utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>5.4.1 Effect of proposed data partition methods. In this paper, we propose three balanced data partition methods based on the similarities of users, items, and interactions. We first conduct ablation study to understand their effect. Figure <ref type="figure" target="#fig_3">2</ref> shows the results w.r.t. Recall@20 of different methods on Yelp2018 and Movielens-1m datasets. The results of random partition are also shown as baselines. From Figure <ref type="figure" target="#fig_3">2</ref>, two observations can be made: First, we can see that our proposed methods, UBP, IBP, and InBP, can achieve a much better recommendation performance compared to the Random method. For instance, on the LightGCN model trained on the Yelp2018 dataset, the Recsall@20 score for InBP is 0.0568, while the corresponding result is 0.0472 for Random. Second, from an overall view, InBP generally performs better than UBP and IBP, which indicates that it is more effective to partition the recommended dataset based on user-item interactions than based on users or items only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Effect of proposed attention-based aggregation method.</head><p>To validate the effectiveness of our proposed attention-based aggregation method (AttAgg), we compare with MeanAgg and StaticAgg by conducting experiments on two datasets and three recommendation models. MeanAgg has been utilized in SISA <ref type="bibr" target="#b0">[1]</ref>, which obtains the aggregated score by averaging the prediction of all shard models. StaticAgg has been utilized in GraphEraser <ref type="bibr" target="#b11">[12]</ref>, which is a learning-based method to assign weights for different shard models. However, the weights are not changed for predicting different user-item interactions. Figure <ref type="figure">3</ref> shows the performance of the above methods. We can see that the proposed AttAgg method can  effectively improve the recommendation performance compared to MeanAgg and StaticAgg. For example, on yelp2018 with WMF method, AttAgg achieves 0.0572 of Recall@20 score, while the corresponding result is 0.0475 for MeanAgg and 0.0482 for StaticAgg, respectively. This make sense since recommender systems usually face a variety of users and items, the learned features by each shard model should contribute differently for different predictions of users and items. Using AttAgg helps better capture the global collaborative information by assigning different importance scores to shard models when predicting different user-item scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impact of the Number of Shards</head><p>In this section, we investigate the impact of the number of shards on the unlearning efficiency and model utility. We conduct the experiments on yelp2018 and movielens-1m datasets and vary the number of shards from 2 to 20. The experimental results in Figure <ref type="figure">4</ref> show that the average unlearning time decreases when the number of shards increases for all recommendation models. This is expected since a larger number of shards means smaller shard size, leading to higher unlearning efficiency. On the other hand, the recommendation performance of all the three recommendation models slightly decrease. This is because recommendation models require collaborative information for model learning, which generally decreases as the number of shards increases. The number of shards is an important hyper-parameter for RecEraser, in practice, it should be carefully selected to balance the unlearning efficiency and model utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a novel RecEraser framework, which is, to the best of our knowledge, the first machine unlearning method tailored to recommendation tasks. To permit efficient unlearning while keeping the collaborative information of the data, we first design three data partition strategies based on the similarities of users, items, and interactions, respectively. We then further propose an adaptive aggregation method to improve the global model utility. Extensive experiments have been made on three real-world datasets and three representative recommendation models. The proposed RecEraser can not only achieve efficient unlearning, but also outperform the state-of-the-art unlearning methods in terms of model utility. We believe the insights of RecEraser are inspirational to future developments of recommendation unlearning methods.</p><p>With the prevalence of data protection requirements in real applications, erasable models are becoming increasingly important in recommendation. Our RecEraser is general for addressing the unlearning task of recommendation systems. It also has the potential to benefit many other tasks where the collaborative information should be considered.</p><p>In this work, we focus on the setting where the unlearning requests arrive sequentially to study the feasibility of unlearning in recommender systems. Alternatively, the system can also aggregate several sequential unlearning requests into a batch and then unlearn this batch at once. In such a batch setting, the unlearning requests could occur in several shards. At present, how to efficiently unlearning using SISA framework in batch setting is still an open problem <ref type="bibr" target="#b0">[1]</ref>. In the future, we seek to improve the unlearning efficiency for recommendation in batch setting. In addition, we will also try to incorporate the content features of users and items, and exploring our method in other related tasks like social network embedding. Moreover, we are interested in extending RecEraser to jointly consider more factors, such as privacy preserving and membership inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our RecEraser framework, which consists of three phases: data partition, shard model training, and shard model aggregation. When receiving an unlearning request of data, only the corresponding shard model and the aggregation part need to be retrained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>and h 1 ∈</head><label>1</label><figDesc>R 𝑘 are parameters of user attention, W 2 ∈ R 𝑘×𝑑 , b 2 ∈ R 𝑘 , and h 2 ∈ R 𝑘 are parameters of item attention. 𝑘 is the dimension of attention size, and 𝜎 is the nonlinear activation function ReLU<ref type="bibr" target="#b37">[38]</ref>. Attention weights across all shards are normalized by the softmax function.The attention-based adaptive aggregation method assigns importance scores for different shards based on the following optimization problem:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance comparison of different data partition methods on Yelp2018 and Movielens-1m datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Performance comparison of different data aggregation methods on Yelp2018 and Movielens-1m datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of symbols and notations.</figDesc><table><row><cell cols="2">Symbol Description</cell></row><row><cell>U, V</cell><cell>Set of users and items, respectively</cell></row><row><cell>Y</cell><cell>User-item interaction matrix</cell></row><row><cell>𝐾</cell><cell>The number of shards</cell></row><row><cell>𝑆 𝑖</cell><cell>Shards 𝑖</cell></row><row><cell>𝑀 𝑖</cell><cell>The shard model trained on 𝑆 𝑖</cell></row><row><cell>p𝑢 , q𝑣</cell><cell>The pre-trained embeddings of user 𝑢 and item 𝑣,</cell></row><row><cell></cell><cell>respectively, used for data partition</cell></row><row><cell>p 𝑖 𝑢 , q 𝑖 𝑣</cell><cell>The embeddings of user 𝑢 and item 𝑣 learned by</cell></row><row><cell></cell><cell>𝑀 𝑖 , respectively</cell></row><row><cell>p 𝑢 , q 𝑣</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Pre-trained user embeddings P= { p1 , p2 , . . . , p𝑚 }; useritem interactions Y; number of shards 𝐾; maximum number of each shard 𝑡 Ensure: Shards S= {𝑆 1 , 𝑆 2 , . . . , 𝑆 𝐾 } 1: Randomly select 𝐾 anchors A= {𝑎 1 , 𝑎 2 , . . . , 𝑎 𝐾 } from U 2: while Stopping criteria is not met do</figDesc><table><row><cell>3:</cell></row></table><note>To address this issue, we propose three new balanced data partition methods, namely User-based Balanced Partition (UBP), Algorithm 1 User-based Balanced Partition algorithm (UBP) Require:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for each 𝑎 𝑖 and 𝑢 in 𝐸 𝑠 do 11:if |𝑆 𝑖 | &lt; 𝑡 and 𝑢 has not been assigned then𝑆 𝑖 ← 𝑆 𝑖 ∪ Y 𝑢 𝑆 𝑖 ← 𝑆 𝑖 ∪ 𝑦 𝑢𝑣</figDesc><table><row><cell>6:</cell><cell>end for</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell>8:</cell><cell>Sort 𝐸 in ascending order to get 𝐸 𝑠</cell></row><row><cell>9:</cell><cell>Empty S</cell></row><row><cell>10:</cell><cell></cell></row><row><cell>12:</cell><cell></cell></row><row><cell>13:</cell><cell>end if</cell></row><row><cell>14:</cell><cell>end for</cell></row><row><cell>15:</cell><cell>Updating A by Eq.(2)</cell></row><row><cell cols="2">16: end while</cell></row><row><cell cols="2">17: return S</cell></row><row><cell cols="2">Algorithm 2 Interaction-based Balanced Partition algorithm</cell></row><row><cell>(InBP)</cell><cell></cell></row><row><cell>6:</cell><cell>end for</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell>8:</cell><cell>Sort 𝐸 in ascending order to get 𝐸 𝑠</cell></row><row><cell>9:</cell><cell>Empty S</cell></row><row><cell>10:</cell><cell></cell></row><row><cell>13:</cell><cell>end if</cell></row><row><cell>14:</cell><cell>end for</cell></row><row><cell>15:</cell><cell></cell></row></table><note>Require: Pre-trained user embeddings P= { p1 , p2 , . . . , p𝑚 }; item embeddings Q= { q1 , q2 , . . . , q𝑛 }; user-item interactions Y; number of shards 𝐾; maximum number of each shard 𝑡 Ensure: Shards S= {𝑆 1 , 𝑆 2 , . . . , 𝑆 𝐾 } 1: Randomly select 𝐾 anchors A= {𝑎 1 , 𝑎 2 , . . . , 𝑎 𝐾 } from Y 2: while Stopping criteria is not met do 3: for each 𝑎 𝑖 in A do 4: for each 𝑦 𝑢𝑣 in Y do 5: Calculate 𝐸 (𝑎 𝑖 , 𝑦 𝑢𝑣 ) = dist(𝑎 𝑖 , 𝑦 𝑢𝑣 ) (Eq.(3)) for each 𝑎 𝑖 and 𝑦 𝑢𝑣 in 𝐸 𝑠 do 11: if |𝑆 𝑖 | &lt; 𝑡 and 𝑦 𝑢𝑣 has not been assigned then 12:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistical details of the evaluation datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#User #Item #Interaction Density</cell></row><row><cell>Yelp2018</cell><cell cols="2">31,668 38,048</cell><cell>1,561,406</cell><cell>0.13%</cell></row><row><cell>Movielens-1m</cell><cell>6,940</cell><cell>3,706</cell><cell>1,000,209</cell><cell>3.89%</cell></row><row><cell cols="3">Movielens-10m 71,567 10,681</cell><cell>10,000,054</cell><cell>1.31%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different unlearning methods for recommendation unlearning. The proposed InBP method is used for data partition of RecEraser in this Table. For efficient unlearning methods SISA, GraphEraser, and our RecEraser, best results are highlighted in bold and the superscript ** indicates 𝑝 &lt; 0.01 for the paired t-test of RecEraser vs. SISA and GraphEraser.</figDesc><table><row><cell>Yelp2018</cell><cell></cell><cell></cell><cell>BPR</cell><cell></cell><cell></cell><cell></cell><cell>WMF</cell><cell></cell><cell></cell><cell></cell><cell>LightGCN</cell><cell></cell></row><row><cell></cell><cell cols="12">Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser</cell></row><row><cell>Recall@10</cell><cell>0.0302</cell><cell>0.0151</cell><cell>0.0224</cell><cell>0.0249**</cell><cell>0.0368</cell><cell>0.0235</cell><cell>0.0250</cell><cell>0.0335**</cell><cell>0.0367</cell><cell>0.0221</cell><cell>0.0278</cell><cell>0.0328**</cell></row><row><cell>Recall@20</cell><cell>0.0521</cell><cell>0.0272</cell><cell>0.0397</cell><cell>0.0439**</cell><cell>0.0619</cell><cell>0.0401</cell><cell>0.0431</cell><cell>0.0572**</cell><cell>0.0637</cell><cell>0.0392</cell><cell>0.0479</cell><cell>0.0568**</cell></row><row><cell>Recall@50</cell><cell>0.1028</cell><cell>0.0566</cell><cell>0.0798</cell><cell>0.0889**</cell><cell>0.1189</cell><cell>0.0785</cell><cell>0.0825</cell><cell>0.1105**</cell><cell>0.1216</cell><cell>0.0768</cell><cell>0.0924</cell><cell>0.1112**</cell></row><row><cell>NDCG@10</cell><cell>0.0344</cell><cell>0.0181</cell><cell>0.0254</cell><cell>0.0282**</cell><cell>0.0422</cell><cell>0.0278</cell><cell>0.0295</cell><cell>0.0385**</cell><cell>0.0424</cell><cell>0.0259</cell><cell>0.0319</cell><cell>0.0377**</cell></row><row><cell>NDCG@20</cell><cell>0.0423</cell><cell>0.0224</cell><cell>0.0319</cell><cell>0.0353**</cell><cell>0.0512</cell><cell>0.0336</cell><cell>0.0360</cell><cell>0.0471**</cell><cell>0.0524</cell><cell>0.0321</cell><cell>0.0393</cell><cell>0.0465**</cell></row><row><cell>NDCG@50</cell><cell>0.0612</cell><cell>0.0334</cell><cell>0.0468</cell><cell>0.0523**</cell><cell>0.0723</cell><cell>0.0478</cell><cell>0.0508</cell><cell>0.0669**</cell><cell>0.0735</cell><cell>0.0461</cell><cell>0.0557</cell><cell>0.0669**</cell></row><row><cell>Movielens-1m</cell><cell></cell><cell></cell><cell>BPR</cell><cell></cell><cell></cell><cell></cell><cell>WMF</cell><cell></cell><cell></cell><cell></cell><cell>LightGCN</cell><cell></cell></row><row><cell></cell><cell cols="12">Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser</cell></row><row><cell>Recall@10</cell><cell>0.1510</cell><cell>0.1080</cell><cell>0.1190</cell><cell>0.1393**</cell><cell>0.1592</cell><cell>0.0759</cell><cell>0.1127</cell><cell>0.1435**</cell><cell>0.1544</cell><cell>0.1089</cell><cell>0.1174</cell><cell>0.1425**</cell></row><row><cell>Recall@20</cell><cell>0.2389</cell><cell>0.1737</cell><cell>0.1906</cell><cell>0.2254**</cell><cell>0.2507</cell><cell>0.1266</cell><cell>0.1823</cell><cell>0.2254**</cell><cell>0.2428</cell><cell>0.1743</cell><cell>0.1914</cell><cell>0.2266**</cell></row><row><cell>Recall@50</cell><cell>0.4005</cell><cell>0.2970</cell><cell>0.3271</cell><cell>0.3822**</cell><cell>0.4165</cell><cell>0.2309</cell><cell>0.3081</cell><cell>0.3818**</cell><cell>0.4076</cell><cell>0.3084</cell><cell>0.3322</cell><cell>0.3839**</cell></row><row><cell>NDCG@10</cell><cell>0.3412</cell><cell>0.2774</cell><cell>0.2844</cell><cell>0.3208**</cell><cell>0.3546</cell><cell>0.2058</cell><cell>0.2575</cell><cell>0.3317**</cell><cell>0.3509</cell><cell>0.2767</cell><cell>0.3832</cell><cell>0.3276**</cell></row><row><cell>NDCG@20</cell><cell>0.3368</cell><cell>0.2679</cell><cell>0.2827</cell><cell>0.3186**</cell><cell>0.3509</cell><cell>0.1998</cell><cell>0.2557</cell><cell>0.3253**</cell><cell>0.3447</cell><cell>0.2686</cell><cell>0.2791</cell><cell>0.3229**</cell></row><row><cell>NDCG@50</cell><cell>0.3669</cell><cell>0.2840</cell><cell>0.3105</cell><cell>0.3483**</cell><cell>0.3838</cell><cell>0.2174</cell><cell>0.2807</cell><cell>0.3542**</cell><cell>0.3754</cell><cell>0.2905</cell><cell>0.3043</cell><cell>0.3520**</cell></row><row><cell>Movielens-10m</cell><cell></cell><cell></cell><cell>BPR</cell><cell></cell><cell></cell><cell></cell><cell>WMF</cell><cell></cell><cell></cell><cell></cell><cell>LightGCN</cell><cell></cell></row><row><cell></cell><cell cols="12">Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser Retrain SISA GraphEraser RecEraser</cell></row><row><cell>Recall@10</cell><cell>0.1951</cell><cell>0.1620</cell><cell>0.1556</cell><cell>0.1798**</cell><cell>0.2094</cell><cell>0.1332</cell><cell>0.1135</cell><cell>0.1994**</cell><cell>0.1994</cell><cell>0.1221</cell><cell>0.1055</cell><cell>0.1881**</cell></row><row><cell>Recall@20</cell><cell>0.3016</cell><cell>0.2507</cell><cell>0.2470</cell><cell>0.2808**</cell><cell>0.3177</cell><cell>0.2142</cell><cell>0.1766</cell><cell>0.3031**</cell><cell>0.3059</cell><cell>0.2033</cell><cell>0.1676</cell><cell>0.2899**</cell></row><row><cell>Recall@50</cell><cell>0.4688</cell><cell>0.3906</cell><cell>0.3979</cell><cell>0.4436**</cell><cell>0.4838</cell><cell>0.3515</cell><cell>0.2838</cell><cell>0.4655**</cell><cell>0.4748</cell><cell>0.3314</cell><cell>0.2632</cell><cell>0.4537**</cell></row><row><cell>NDCG@10</cell><cell>0.3425</cell><cell>0.3002</cell><cell>0.2688</cell><cell>0.3223**</cell><cell>0.3704</cell><cell>0.2499</cell><cell>0.2209</cell><cell>0.3575**</cell><cell>0.3555</cell><cell>0.2289</cell><cell>0.2139</cell><cell>0.3369**</cell></row><row><cell>NDCG@20</cell><cell>0.3534</cell><cell>0.3053</cell><cell>0.2813</cell><cell>0.3319**</cell><cell>0.3784</cell><cell>0.2579</cell><cell>0.2243</cell><cell>0.3642**</cell><cell>0.3635</cell><cell>0.2476</cell><cell>0.2141</cell><cell>0.3352**</cell></row><row><cell>NDCG@50</cell><cell>0.3946</cell><cell>0.3366</cell><cell>0.3219</cell><cell>0.3719**</cell><cell>0.4169</cell><cell>0.2918</cell><cell>0.2484</cell><cell>0.4015**</cell><cell>0.4037</cell><cell>0.2808</cell><cell>0.2288</cell><cell>0.3844**</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of running time (minute [m]). For RecEraser, only the corresponding shard model and the aggregation part need to be retrained when receiving an unlearning request of data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Yelp2018</cell><cell></cell><cell></cell><cell cols="2">Movielens-1m</cell><cell cols="3">Movielens-10m</cell></row><row><cell></cell><cell></cell><cell cols="6">BPR WMF LightGCN BPR WMF LightGCN</cell><cell cols="3">BPR WMF LightGCN</cell></row><row><cell>Retrain</cell><cell></cell><cell>400m</cell><cell>75m</cell><cell>1090m</cell><cell>330m</cell><cell>13m</cell><cell>545m</cell><cell cols="2">3250m 292m</cell><cell>25330m</cell></row><row><cell></cell><cell>Shard Training</cell><cell>39m</cell><cell>2.5m</cell><cell>52m</cell><cell>12m</cell><cell>0.9m</cell><cell>17m</cell><cell>240m</cell><cell>14m</cell><cell>610m</cell></row><row><cell>RecEraser</cell><cell>Aggregation Training</cell><cell>13m</cell><cell>1.3m</cell><cell>34m</cell><cell>10m</cell><cell>0.2m</cell><cell>14m</cell><cell>80m</cell><cell>4m</cell><cell>330m</cell></row><row><cell></cell><cell>Total</cell><cell>52m</cell><cell>3.8m</cell><cell>86m</cell><cell>22m</cell><cell>1.1m</cell><cell>31m</cell><cell>320m</cell><cell>16m</cell><cell>940m</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://gdpr-info.eu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://oag.ca.gov/privacy/ccpa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">It is worth noting that the purpose of unlearning is different from differential privacy (DP) methods<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> which aim to protect users' privacy information instead of deleting them. Besides, unlearning usually requires more strict guarantee than DP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://www.yelp.com/dataset/challenge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://grouplens.org/datasets/movielens/1m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://grouplens.org/datasets/movielens/10m/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the Natural Science Foundation of China (Grant No. U21B2026) and Tsinghua University Guoqiang Research Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">David Lie, and Nicolas Papernot. 2021. Machine unlearning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Bourtoule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adelin</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiwu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="141" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards making systems forget with machine unlearning</title>
		<author>
			<persName><forename type="first">Yinzhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="463" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th USENIX Conference on Security Symposium</title>
				<meeting>the 28th USENIX Conference on Security Symposium<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association, USA</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental and Decremental Support Vector Machine Learning</title>
		<author>
			<persName><forename type="first">Gert</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="388" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph Heterogeneous Multi-Relational Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3958" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Attentional Rating Regression with Review-Level Explanations</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1583" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social Attentional Memory Network: Modeling Aspect-and Friend-level Differences in Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Non-Sampling Factorization Machines for Optimal Context-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2400" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Efficient Adaptive Transfer Neural Network for Social-aware Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Neural Matrix Factorization without Sampling for Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020-01">2020. Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient Heterogeneous Collaborative Filtering without Negative Sampling for Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14991</idno>
		<title level="m">Graph Unlearning</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemal</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems<address><addrLine>Boston, MA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local Collaborative Autoencoders</title>
		<author>
			<persName><forename type="first">Minjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonki</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="734" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. Jul (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07243</idno>
		<title level="m">Graph Neural Networks for Social Recommendation</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DPLCF: Differentially Private Local Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making AI Forget You: Data Deletion in Machine Learning</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ginart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Making ai forget you: Data deletion in machine learning</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ginart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05012</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eternal sunshine of the spotless net: Selective forgetting in deep networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9304" to="9312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="383" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Certified Data Removal from Machine Learning Models</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3832" to="3842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Outer Product-Based Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia; Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth IEEE International Conference on Data Mining</title>
				<meeting>Eighth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approximate Data Deletion from Machine Learning Models</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Izzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Anne</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="2008" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manipulating machine learning: Poisoning attacks and countermeasures for regression learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An efficient k-means clustering algorithm: Analysis and implementation</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="881" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiple Incremental Decremental Learning of Support Vector Machines</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SDM: Sequential Deep Matching Model for Online Large-Scale Recommender System</title>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management<address><addrLine>Beijing, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2635" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
				<meeting>the 27th International Conference on International Conference on Machine Learning<address><addrLine>Haifa, Israel; Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Descent-to-Delete: Gradient-Based Methods for Machine Unlearning</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 32nd International Conference on Algorithmic Learning Theory</title>
				<meeting>32nd International Conference on Algorithmic Learning Theory</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="931" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Embedding-based news recommendation for millions of users</title>
		<author>
			<persName><forename type="first">Shumpei</forename><surname>Okura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1933" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Montreal, Quebec, Canada; Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Amnesia&quot; -Machine Learning Models That Can Forget User Data Very Fast</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schelter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applied AI for Database Systems and Applications (AIDB) at VLDB</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Item Silk Road: Recommending Items from Information Domains to Social Users</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Item-based Collaborative Filtering for Top-N Recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How to Retrain Recommender System? A Sequential Meta-Learning Method</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1479" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint deep modeling of users and items using reviews for recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
