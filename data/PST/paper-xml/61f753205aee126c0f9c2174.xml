<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-28">28 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amazon</forename><forename type="middle">Web</forename><surname>Services</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University 5 Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-28">28 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.12023v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism. Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations, which does not suffice to scale out complex DL models on distributed compute devices. Alpa distributes the training of large DL models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms. Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans. Alpa designs a number of compilation passes to automatically derive the optimal parallel execution plan in each independent parallelism level and implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices. Our evaluation shows Alpa generates parallelization plans that match or outperform handtuned model-parallel training systems even on models they are designed for. Unlike specialized systems, Alpa also generalizes to models with heterogeneous architectures and models without manually-designed plans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Several of the recent advances <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref> in deep learning (DL) have been a direct result of significant increases in model size. For example, scaling language models, such as GPT-3, to hundreds of billions of parameters <ref type="bibr" target="#b6">[7]</ref> and training on much larger datasets enabled fundamentally new capabilities.</p><p>However, training these extremely large models on clusters of high-performance accelerators currently requires a significant engineering effort that is specific to both the model definition and training cluster environment. For example, training a large Transformer-based language model requires heavy tuning and careful selection of multiple parallelism dimensions <ref type="bibr" target="#b35">[36]</ref>. Training the large Mixture-of-Expert (MoE) trans-formers model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> on a cluster of TPUs required manually tuning the partitioning axis for each layer, whereas training the same model on distributed GPUs calls for new pipeline schemes that can depend on the choices of partitioning ( ?7.6).</p><p>More generally, efficient large-scale model training requires tuning a complex combination of data, operator, and pipeline parallelization approaches at the granularity of the individual operators in the network. Correctly tuning the parallelization strategy has been shown <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> to deliver an order of magnitude improvements in training performance, but depends on strong ML and system expertise.</p><p>Automating the parallelization of large-scale models would significantly accelerate machine learning (ML) research and production by enabling model developers to quickly explore new model designs without regard for the underlying system challenges. Unfortunately, it requires navigating a complex space of plans that grows exponentially with the dimensions of parallelism and the size of the model and cluster. For example, when all parallelism techniques are enabled, figuring out the execution plan involves answering a web of interdependent questions, such as how many data-parallel replicas to create, which axis to partition each operator along, how to split the model into pipeline stages, and how to map devices to the resulting parallel executables. The interplay of different parallelization methods and their strong dependence on model and cluster setups form a combinatorial space of plans to optimize. Recent efforts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50]</ref> to automatically parallelize model training are constrained to the space of a single model-parallelism approach, or rely on strong assumptions on the model and cluster specifications ( ?2.1).</p><p>Our key observation is that we can organize different parallelization techniques into a hierarchical space and map these parallelization techniques to the hierarchical structure of the compute cluster. Different parallelization techniques have different bandwidth requirements for communication, while a typical compute cluster has a corresponding structure: closely located devices can communicate with high bandwidth while distant devices have limited communication bandwidth.</p><p>With this observation in mind, in this paper, we take a differ-ent view from conventional data and model parallelisms, and re-categorize ML parallelization approaches as intra-operator and inter-operator parallelisms. Intra-operator parallelism partitions ML operators along any tensor axes (batch or nonbatch) and dispatches the partitions to distributed devices (Fig. <ref type="figure" target="#fig_0">1c</ref>); inter-operator parallelism, on the other hand, slices the model into disjoint stages and pipelines the execution of stages on different sets of devices (Fig. <ref type="figure" target="#fig_0">1d</ref>). They take place at two different granularities of the model computation, differentiated by whether to partition operators. Given that, a parallel execution plan can be expressed hierarchically by specifying the plan in each parallelism category, leading to a number of advantages. First, intra-and interoperator parallelisms feature distinct characteristics: intraoperator parallelism has better device utilization, but results in communicating at every split and merge of partitioned operators, per training iteration; whereas inter-operator parallelism only communicates between adjacent stages, which can be light if sliced properly, but incurs device idle time due to scheduling constraints. We can harness the asymmetric nature of communication bandwidth in a compute cluster, and map intra-operator parallelism to devices connected with high communication bandwidth, while orchestrating the interoperator parallelism between distant devices with relatively lower bandwidth in between. Second, this hierarchical design allows us to solve each level optimally as an individual tractable sub-problem. While the joint execution plan is not guaranteed globally optimal, they demonstrate strong performance empirically for training various large models.</p><p>Guided by this new problem formulation, we design and implement Alpa, the first compiler that automatically generates parallel execution plans covering all data, operator, and pipeline parallelisms. Given the model description and a cluster configuration, Alpa achieves this by partitioning the cluster into a number of device meshes, each of which contains devices with preferably high-bandwidth connections, and partitioning the computation graph of the model into stages. It assigns stages to device meshes, and automatically orchestrates intra-operator parallelisms on a device mesh, and inter-operator parallelisms between device meshes.</p><p>In summary, we make the following contributions: ? We construct a two-level parallel execution plan space (Fig. <ref type="figure" target="#fig_0">1e</ref>) where plans are specified hierarchically using interand intra-operator parallelisms.</p><p>? We design tractable optimization algorithms to derive optimal execution plans at each level.</p><p>? We implement Alpa, a compiler system for distributed DL on GPU clusters. Alpa features: (1) a set of compilation passes that generate execution plans using the hierarchical optimization algorithms, (2) a new runtime architecture that orchestrates the inter-op parallelism between stages and device meshes, and (3) a number of system optimizations that improve compilation and address cross-mesh communication. parameters. We compare Alpa with state-of-the-art distributed training systems on an AWS cluster of 8 p3.16xlarge instances with 64 GPUs. On GPT <ref type="bibr" target="#b6">[7]</ref> models, Alpa can match the specialized system Megatron-LM <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>. On GShard MoE models <ref type="bibr" target="#b26">[27]</ref>, compared to a hand-tuned system Deepspeed <ref type="bibr" target="#b40">[41]</ref>, Alpa achieves a 3.5? speedup on 2 nodes and a 9.7? speedup on 4 nodes. Unlike specialized systems, Alpa also generalizes to models without manual strategies and achieves an 80% linear scaling efficiency on Wide-ResNet <ref type="bibr" target="#b52">[53]</ref> with 4 nodes. This means developers can get efficient model-parallel execution of large DL models out-of-the-box using Alpa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? We evaluate Alpa on training large models with billions of</head><formula xml:id="formula_0">A B D C A B D C ? A B D C A B D C A B D C A B D C A B D C A A B D C B D C ? ? ? ?<label>(</label></formula><p>2 Background: Distributed Deep Learning DL computation is commonly represented by popular ML frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref> as a dataflow graph. Edges in the graph represent multi-dimensional tensors; nodes are computational operators, such as matrix multiplication (matmul), that transform input tensors into output tensors. Training a DL model for one iteration consists of computing a loss by forwarding a batch of data through the graph, deriving the updates via a reverse backward pass, and applying the updates to the parameters via weight update operations. In practice, model developers define the dataflow graph, an execution engine optimizes and executes it on a compute device.</p><p>When either the model or data is large that a single device cannot complete the training with a reasonable amount of time, we resort to ML parallelization approaches that parallelize the computation on distributed devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional View of ML Parallelism</head><p>Existing ML parallelization approaches are typically categorized as data, operator, and pipeline parallelisms. Data parallelism. In data parallelism, the training data is partitioned across distributed workers, but the model is replicated. Each worker computes the parameter updates on its independent data split, and synchronizes the updates with other workers before the weight update, so that all workers observe consistent model parameters throughout training.</p><p>Operator parallelism. When the model is too large to fit in one device, operator parallelism is an effective model parallelism option. Operator parallelism refers to approaches that partition the computation of a specific operator (abbreviated as op in the following text), such as matmul shown in Fig. <ref type="figure" target="#fig_1">2b</ref>, along non-batch axes, and compute each part of the operator in parallel across multiple devices.</p><p>Because input tensors are jointly partitioned, when a device computes its op partition, the required portions of input tensors may not reside in its local memory. Communication is thus required to fetch the input data from other devices. When the tensors are partitioned evenly, i.e., SPMD <ref type="bibr" target="#b51">[52]</ref>, all devices follow the same collective communication patterns such as all-reduce, all-gather, and all-to-all. Pipeline parallelism. Instead of partitioning ops, pipeline parallelism places different groups of ops from the model graph, referred as stages, on different workers; meanwhile, it splits the training batch as a number of microbatches, and pipelines the forward and backward passes across microbatches on distributed workers, as Fig. <ref type="figure" target="#fig_1">2d</ref> shows. Unlike operator parallelism, pipeline parallelism transfers intermediate activations at the forward and backward passes between different workers using point-to-point communication.</p><p>Manual combination of parallelisms. Recent development shows the approaches mentioned above need to be combined to scale out today's large DL models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref>. The state-of-theart training systems, such as Megatron-LM <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>, manually design a specialized execution plan that combines these parallelisms for transformer language models. By assuming the model has the same transformer layer repeated, it assigns an equal number of layers to each pipeline stage and applies a hand-designed operator and data parallelism configuration identically for all layers. Despite the requirement of strong expertise, the manual plan cannot generalize to different models or different clusters setups ( ?7.1). Automatic combination of parallelisms. The configurations of each individual parallelism, their interdependence, and their dependence on model and cluster setup form an intractable space, which prevents the trivial realization of automatically combining these parallelisms. For example, when coupled with operator parallelism, adding a data-parallel worker would require the optimal allocation of a set of devices performing operator parallelism therein. When including pipeline parallelism, the optimal pipeline scheme depends on the data and operator parallelisms choices of each stage and the devices allocated for each stage. With this conventional view, prior explorations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref> of auto-parallelization are limited to combining data parallelism with at most one model parallelism approach, which misses substantial performance opportunities. We next develop our view of ML parallelisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intra-and Inter-Operator Parallelisms</head><p>Different from the conventional view, in this paper, we recatalog existing parallelization approaches into two orthogonal categories: intra-operator and inter-operator parallelisms. They are distinguished by the fact that if they involve partitioning operators along any tensor axis. We next use the examples in Fig. <ref type="figure" target="#fig_1">2</ref> to introduce the two types of parallelisms. Intra-operator parallelism. An operator works on multidimensional tensors. We can partition the tensor along some dimensions, assign the resulting partitioned computations to multiple devices, and let them execute different portions of the operator at the same time. We define all parallelization approaches using this workflow as intra-operator parallelism.</p><p>Fig. <ref type="figure" target="#fig_1">2a-c</ref> illustrates the application of several typical instantiations of intra-op parallelism on an MLP. Data parallelism <ref type="bibr" target="#b24">[25]</ref>, by definition, belongs to intra-op parallelismthe input tensors and matmuls are partitioned along the batch dimension, and weight tensors are replicated. Alternatively, when the weights are very large, partitioning the weights (Fig. <ref type="figure" target="#fig_1">2b</ref>) leads to the aforementioned operator parallelism adopted in Megatron-LM. Besides operators in the forward or backward passes, one can partition the operators from the weight update phase along the batch dimension, yielding the weight update sharding or equivalently the ZeRO <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51]</ref> technique, commonly comprehended as an optimization of data parallelism.</p><p>Due to the partitioning, collective communication is required at the split and merge of the operator. Hence, a key characteristic of intra-operator parallelism is that it results in substantial communication among distributed devices. Inter-operator parallelism. We define inter-operator parallelism as the orthogonal class of approaches that do not perform operator partitioning, but instead, assign different operators of the graph to execute on distributed devices. Fig. <ref type="figure" target="#fig_1">2d</ref> illustrates the batch-splitting pipeline parallelism as a case of inter-operator parallelism. <ref type="foot" target="#foot_0">2</ref> The pipeline execution can follow different schedules, such as Gpipe <ref type="bibr" target="#b17">[18]</ref>, PipeDream <ref type="bibr" target="#b33">[34]</ref>, and 1F1B <ref type="bibr" target="#b34">[35]</ref>. We adopt the 1F1B schedule <ref type="bibr" target="#b35">[36]</ref> throughout this paper as it respects synchronous consistency, and has the same theoretical pipeline latency but lower peak memory usage compared to Gpipe.</p><p>In inter-operator parallelism, devices communicate only between pipeline stages, typically using point-to-point communication between paired devices. The required communication volume can be much less than the collective communication in intra-operator parallelism. Regardless of the schedule used, due to the data dependency between stages, inter-operator parallelism results in some devices being idle during the forward and backward computation.</p><p>By this categorization, the two parallelisms take place at different granularities of the DL computation and have distinct communication requirements, which happen to match the structure of today's typical compute clusters. We will leverage these properties to design hierarchical algorithms and compilation passes to auto-generate execution plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>Alpa is a compiler that generates model-parallel execution plans by hierarchically optimizing the plan at two different granularities: intra-op and inter-op parallelism. At the intraop level, Alpa minimizes the cost of executing a stage (i.e., subgraph) of the computational graph, with respect to its intraoperator parallelism plan, on a given device mesh, which is a set of devices that may have high bandwidth between each other (e.g., GPUs within a single server). Different meshes  might have different numbers of computing devices according to the workload assigned. At the inter-op level, Alpa minimizes the inter-op parallelization latency, with respect to how to slice the model and device cluster into stages and device meshes, and how to map them as stage-mesh pairs. The interop optimization depends on knowing the optimal execution cost of each stage-mesh pair reported by the intra-op optimizer. Through this hierarchical optimization process, Alpa generates the execution plan consisting of intra-op and interop plans which are locally optimal at their respective level of the hierarchy.</p><p>To achieve this, Alpa implements three novel compilation passes as Fig. <ref type="figure" target="#fig_2">3</ref> shows. Given a model description, in the form of a Jax <ref type="bibr" target="#b5">[6]</ref> intermediate representation (IR), and a cluster configuration, the inter-op compilation pass slices the IR into a number of stages, and the device cluster into a number of device meshes. The inter-op pass uses a Dynamic Programming (DP) algorithm to assign stages to meshes and invokes the intra-op compilation pass on each stage-mesh pair, to query the optimal execution cost of this assignment. Once invoked, the intra-op pass optimizes the intra-op parallel execution plan of the stage running on its assigned mesh, by minimizing its execution cost using an Integer Linear Programming (ILP) formulation, and reports the cost back to the inter-op pass. By repeatedly querying the intra-op pass for each allocation of a stage-mesh pair, the inter-op pass uses the DP to minimize the inter-op parallel execution latency, and obtains the best slicing scheme of stages and meshes.</p><p>Given the output hierarchical plan and a designated pipeline-parallel schedule, each stage is first compiled as a parallel executable on its located mesh. A runtime orchestration pass is invoked to fulfill the communication requirement between two adjacent stages that require communication between the two meshes they locate on. The runtime orchestration pass then generates static instructions specific to each mesh according to the pipeline-parallel schedule and invokes the execution on all meshes.</p><formula xml:id="formula_1">S 0 S 1 0 3 1 2 S 1 S 0 0 3 2 1 0 3 2 1 RS 0 0 3 1 2 RS 1 0 3 1 2 S 0 R 0 3 2 1 S 1 R S 01 R 0 3 1 2 0 3 12 RS 01 RR 0 3 1 2</formula><p>Figure <ref type="figure">5</ref>: Sharding specs of a 2D tensor on a 2x2 device mesh. Colors denote different partitions of the tensor. The numbers on a partition are indices of devices that hold the partition. Table <ref type="table">1</ref>: Several cases of resharding. all-gather(x, i) means an all-gather of x bytes along the i-th mesh axis. M is the size of the output tensor. (n 0 , n 1 ) is the mesh shape.</p><formula xml:id="formula_2"># Src Spec Dst Spec Communication Cost 1 RR S 0 S 1 0 2 S 0 R RR all-gather(M, 0) 3 S 0 S 1 S 0 R all-gather( M n 0 , 1) 4 S 0 R RS 0 all-to-all(M, 0) 5 S 0 S 1 S 01 R all-to-all( M n 0 , 1)</formula><p>API. Alpa has a simple API shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Alpa requires developers to annotate functions to be parallelized, such as the train_step(), using a Python decorator @parallelize.</p><p>Upon the first call to train_step(), Alpa traces the whole function to get the model IR, invokes the compilation, and converts the function to a parallel version. Since the inter-op pass depends on the intra-op pass, we first describe the intra-op pass, followed by the inter-op pass, and finally the runtime orchestration pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Intra-Operator Parallelism</head><p>Alpa optimizes the intra-operator parallelism plan within a device mesh. Alpa adopts the SPMD-style intra-op parallelism, similar to many existing work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, which partitions operators evenly and executes the same instructions across devices, by noticing the fact that devices within a single mesh often have equivalent compute capability. With this simplification, Alpa significantly reduces the space of intra-op parallelism plans, but conveniently expresses and unifies all important parallelism approaches such as data parallelism, ZeRO, Megatron-LM's operator parallelism, and their combinations, which are not covered by some existing automatic operator parallelism systems, such as Tofu <ref type="bibr" target="#b49">[50]</ref> and FlexFlow <ref type="bibr" target="#b20">[21]</ref>. Unlike systems that perform randomized search <ref type="bibr" target="#b20">[21]</ref> or assume linear graphs <ref type="bibr" target="#b49">[50]</ref>, Alpa formalizes the problem as an integer linear programming (ILP) and shows it can be solved optimally for DL computational graphs with tens of thousands of nodes. Next, we describe the space of intra-operator parallelism and our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Space of Intra-Operator Parallelism</head><p>Alpa works on primitive operators of arbitrary computational graphs. For an operator in the computational graph, there are multiple possible parallel algorithms to run it on a device mesh. For example, a matrix multiplication C i j = ? k A ik B k j corresponds to a three-level for-loop. To parallelize it, we can parallelize the loop i, loop j, loop k, or combinations of them across devices, which would have distinct computation and Table <ref type="table">2</ref>: Several parallel algorithms for a batched matmul C b,i, j = ? k A b,i,k B b,k, j . The notation all-reduce(x, i) means an all-reduce of x bytes along the i-th mesh axis. M is the size of the output tensor. (n 0 , n 1 ) is the mesh shape.</p><formula xml:id="formula_3"># Parallel Output Input Communication Mapping Spec Specs Cost 1 i ? 0, j ? 1 RS 0 S 1 RS 0 R, RRS 1 0 2 i ? 0, k ? 1 RS 0 R RS 0 S 1 , RS 1 R all-reduce( M n 0 , 1) 3 j ? 0, k ? 1 RRS 0 RRS 1 , RS 1 S 0 all-reduce( M n 0 , 1) 4 b ? 0, i ? 1 S 0 RS 1 S 0 RS 1 , S 0 RR 0 5 b ? 0, k ? 1 S 0 RR S 0 RS 1 , S 0 S 1 R all-reduce( M n 0 , 1) 6 i ? {0, 1} RS 01 R RS 01 R, RRR 0 7 k ? {0, 1} RRR RRS 01 , RS 01 R all-reduce( M n 0 , {0, 1})</formula><p>communication costs, require different layouts for the input tensors, and result in output tensors with different layouts.</p><p>If an input tensor does not satisfy the layout requirement, a layout conversion is required, which introduces extra communication costs. The goal of intra-operator pass is to pick one algorithm for every operator, in order to minimize the execution time of the entire computational graph. Next, we formally define the layout of a tensor and discuss the cost of layout conversion. We use sharding spec to formally define the layout of a tensor. For an N-dimensional tensor, its sharding spec is defined as</p><formula xml:id="formula_4">X 0 X 1 ? ? ? X n-1 , where X i ? {S, R}. If X i = S,</formula><p>it means the i-th axis of the tensor is partitioned. Otherwise, the i-th axis is replicated. For example, for a 2D tensor (i.e., a matrix), SR means it is row-partitioned, RS means it is column-partitioned, SS means it is both row-and column-partitioned. RR means it is replicated without any partitioning. After we define which tensor axes are partitioned, we then have to map the partitioned tensor axes to mesh axes. We only consider 2D device meshes, so a partitioned tensor axis can be mapped to either the first or the second axis of device mesh, or both. We added a superscript to S to denote the device assignment. For example, S 0 means the partitions are stored along the 0-th axis of the mesh, S 01 means the partitions are stored along both mesh axes. Fig. <ref type="figure">5</ref> shows all possible sharding specs of a 2D tensor on a 2 ? 2 mesh with 4 devices -RS 0 means the tensor is column-partitioned into two parts, the first part is replicated on device 0 and device 1, the second part is replicated on device 2 and device 3.</p><p>When the input tensors of an operator do not satisfy the sharding spec of the chosen algorithm for the operator, a layout conversion, namely resharding, is required, which might require cross-device communication. Table <ref type="table">1</ref> lists several cases of resharding. For instance, to convert a fully replicated tensor to any other sharding specs (case #1), we can slice the tensor locally without communication; to swap the partitioned axis (case #4), we perform an all-to-all.</p><p>With the definition above, consider parallelizing a batched matmul C b,i, j = ? k A b,i,k B b,k, j on a 2D mesh -Table <ref type="table">2</ref> lists several intra-op parallel algorithms for a batched matmul. Algorithm#1 maps loop i to the 0-th mesh axis and loop j to the 1-th mesh axis, resulting in the output tensor C with a sharding spec RS 0 S 1 . As the LHS operand A b,i,k and RHS operand B b,k, j both have only one parallelized index, their sharding specs are written as RS 0 R and RRS 1 , respectively. In this algorithm, each device has all its required input tiles (i.e., a partition of the tensor) stored locally to compute its output tile, so there is no communication cost. In Algorithm #2 in Table <ref type="table">2</ref>, when the reduction loop k is parallelized, all-reduce communication is needed to aggregate the partial sum. For other primitive operators such as convolution and reduction, we can get a list of possible parallel algorithms following a similar analysis of their math expressions. Since there are only less than 80 kinds of common primitive operators in DL graphs, we can manually enumerate the possible parallel algorithms for all primitive operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ILP Formulation</head><p>The total execution cost of a computational graph G = (V, E) is the sum of the compute and communication costs on all nodes v ? V and the resharding costs on all edges. We formulate the problem as an ILP and solve it optimally with an off-the-shelf solver <ref type="bibr" target="#b13">[14]</ref>.</p><p>For node v, the number of possible algorithms is</p><formula xml:id="formula_5">k v . It then has a communication cost vector c v of length k v , or c v ? R k v ,</formula><p>where c vi is the communication cost of the i-th algorithm. Similarly, node v has a compute cost vector d v ? R k v . For each node v, we define an one-hot decision vector s v ? {0, 1} k v to represent the algorithm it uses -s vi = 1 means we pick the i-th algorithm for node v. For the resharding cost between node v and node u, we define a resharding cost matrix R vu ? R k v ?k u , where R vui j is the resharding cost from the output of i-th strategy of node v to the input of j-th strategy of node u. The objective of the problem is</p><formula xml:id="formula_6">min s ? v?V s v (c v + d v ) + ? (v,u)?E s v Rs u ,<label>(1)</label></formula><p>where the first term is the compute and communication cost of node v, and the second is the resharding cost of the edge (v, u). In this formulation, s is the variable, and the rest are constant values. The term s v Rs u in Eq. 1 is quadratic, and cannot be fed into an ILP solver. We linearize <ref type="bibr" target="#b14">[15]</ref> the quadratic term by introducing a new decision vector e vu ? {0, 1} k v ?k u which represents the resharding decision between node v and u.</p><p>Although we can use profiling to get the accurate costs for c v , d v , and R vu , we use the following methods to estimate them for simplicity. For communication costs d v and R vu , we compute the number of communicated bytes and use an alphabeta model to compute the cost. For compute costs c v , we set all of them as zero following the same motivation in <ref type="bibr" target="#b49">[50]</ref>. This is reasonable because: (1) For heavy operators such as matmul, we do not allow replicated computation, so all parallel algorithms of one operator have the same arithmetic complexity; (2) For lightweight operators such as elementwise operators, we allow replicated computation of them, but their computation costs are negligible.</p><p>For unimportant operators such as element-wise operators, transpose, and reduction, we merge it to one of its operands and propagate the sharding spec from its operand. This greatly reduces the number of nodes in the graph, thus the ILP problem size. We do a breath-first-search and compute the depth of each node. The node is merged to the deepest operand.</p><p>Once the parallel plan is decided by ILP, we also apply a set of post-ILP communication optimizations, such as replacing all-reduce with reduce-scatter and all-gather, whenever applicable, because the latter reduces the number of replicated tensors and corresponding computations, while keeping the communication volume the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inter-Operator Parallelism</head><p>Given a stage and a device mesh, the ILP solver in ?4 generates an optimal intra-op parallelism plan for the stage to execute on the mesh. In this section, we develop methods at the model graph level to optimally slice the model and device cluster into stage-mesh pairs. Our optimization goal is to minimize the end-to-end latency for the entire computational graph. Previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref> have considered simplified problems, such as assuming the device for each stage is preassigned, and all stages have fixed data or operator parallelism plan. We develop formulations that jointly consider device mesh assignment and the existence of varying intra-op parallelism plans on each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Space for Inter-Operator Parallelism</head><p>Assume the computation graph contains a sequence of operators following the topology order of the graph<ref type="foot" target="#foot_1">3</ref> , notated as o 1 , . . . , o K , where the inputs of an operator o k are from operators o 1 , . . . , o k-1 . We slice the operators into S stages s 1 , . . . , s S , where each stage s i consists of operators (o l i , . . . , o r i ), and we assign each stage s i to a submesh of size n i ? m i , sliced from a computer cluster that contains devices, notated as the cluster mesh with shape N ? M. Let t i = t intra (s i , Mesh(n i , m i )) be the latency of executing stage s i on a submesh of n i ? m i , minimized by the ILP and reported back by the intra-op pass ( ?4). As visualized in Fig. <ref type="figure">6</ref>, assuming we have B different input microbatches for pipeline, the total minimum latency for the entire computation graph is written as:</p><formula xml:id="formula_7">T * = min s 1 ,...,s S ; (n 1 ,m 1 ),...,(n S ,m S ) S ? i=1 t i + (B -1) ? max 1? j?S {t j } . (2)</formula><p>The overall latency contains two terms: the first term is the total latency of all stages, interpreted as the latency of the first microbatch going through the pipeline; the second term is the Figure <ref type="figure">6</ref>: Illustration of the total latency of a pipeline, which is determined by two parts: the total latency of all stages (t 1 + t 2 + t 3 + t 4 ) and the latency of the slowest stage ((B -1) ? t 3 ). pipelined execution time for the rest of B -1 microbatches, which is bounded by the slowest stage (stage 3 in Fig. <ref type="figure">6</ref>).</p><p>We aim to solve Eq. 2 with two additional constraints. (1) For an operator in the forward pass of the graph, we want to colocate it with its corresponded backward operator on the same submesh. Since backward propagation usually uses the similar set of tensors during forward propagation, this effectively reduces the amount of communication to fetch the required tensors generated at forward pass to the backward pass. We use the sum of forward and backward latency for t intra , so Eq. 2 reflects the total latency, including both forward and backward propagation. <ref type="bibr" target="#b1">(2)</ref> We need the sliced submeshes (n 1 , m 1 ), . . . , (n S , m S ) to fully cover the N ? M cluster mesh -we do not waste any compute device resources. We next elaborate on our DP formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DP Formulation</head><p>To ensure all submeshes (n 1 , m 1 ), . . . , (n S , m S ) fully cover the N ? M cluster mesh, we reduce the available submesh shapes into two options: (1) one-dimensional submeshes of sizes (1, 1), (1, 2), (1, 4) . . . (1, 2 m ) and (2) two-dimensional submeshes of size (2, M), (3, M), . . . , (N, M) that fully use the second dimension of the cluster mesh (i.e., on a GPU cluster, this means using all compute devices in each physical machine). We include a theorem in the Appendix. A that proves these submesh shapes can always fully cover the cluster mesh. Note that this assumption holds for most available cloud deep learning setups: On AWS <ref type="bibr" target="#b1">[2]</ref>, the GPU instances have 1, 2, 4, or 8 GPUs; on GCP <ref type="bibr" target="#b15">[16]</ref>, the TPU instances have 8, 32, 128, 256 or 512 TPUs. The set of submesh shapes (n, m) excluded from the reduction is with n &gt; 1 and m &lt; M, which we observe leading to inferior result, since an alternative submesh with shape (n , M) where n ? M = n ? m has more devices that can communicate with high bandwidth. With this reduction, we only need to ensure that ? S i=1 n i ? m i = N ? M. To find T * , we develop a DP algorithm. Our algorithm builds on top of that in TeraPipe <ref type="bibr" target="#b28">[29]</ref>, but additionally considers device mesh slicing. The DP first enumerates the second term t max = max 1? j?S t j and minimizes the first term t total (t max ) = ? 1?i?S t i for each different t max . Specifically, we use the function F(s, k, d;t max ) to represent the minimal total latency when slicing operators o k to o K into s stages and putting them onto d devices so that the latency of each stage is less than t max . We start with F(0, K, 0;t max ) = 0, and derive the optimal substructure of F as follows:</p><formula xml:id="formula_8">F(s, k, d;t max ) (3) = min k?i?K; (n s ,m s ) ? ? ? ? ? t intra ((o k , . . . , o i ), Mesh(n s , m s )) + F(s -1, i + 1, d -n s ? m s ;t max ) | t intra ((o k , . . . , o i ), Mesh(n s , m s )) ? t max ? ? ? ? ?</formula><p>We also need to consider the memory constraint here. The stage needs memory to store intermediate tensors for computation of the current stage and s batches of activation tensors for backward computation. If the stage (o k , . . . , o i ) cannot fit onto the n s ? m s device mesh, we set the corresponding t intra to infinity. Finally, we can derive the optimal total latency by</p><formula xml:id="formula_9">t * total (t max ) = min s {F(s, 0, N ? M;t max )}.<label>(4)</label></formula><p>Complexity. Our DP algorithm computes the slicing in O(K 3 NM(N + log(M))) time for a fixed t max . t max has at most O(K 2 (N + log(M))) choices: t intra ((o i , . . . , o j ), Mesh(n s , m s )) for i, j = 1, . . . , K and all the submesh choices. The complexity of this DP algorithm is thus O(K 5 NM(N + log(M)) 2 ). This complexity is not feasible for a large computational graph that can contain more than ten thousand operators. To speed up this DP, we introduce a few practical optimizations. Performance optimization #1: early pruning. We use one optimization that is similar to that in TeraPipe <ref type="bibr" target="#b28">[29]</ref>. We enumerate t max from small to large. When B ? t max is larger than the current best T * , we immediately stop the enumeration. This is because larger t max can no longer provide a better solution. Also, during enumeration of t max , we only evaluate a choice of t max if it sufficiently larger than the last t max (by at least ?). This allows the gap between the solution found by the DP algorithm and the global optima to be at most B ? ?. We empirically choose ? = 10 -6 s, and we find that the solution output by our algorithm is the same as the real optimal solution (? = 0) for all our evaluated settings. Performance optimization #2: operator clustering. Many operators in a computational graph are not computationally intensive (e.g., ReLU layer), and the exact placement of these operators has little impact on the total execution time. We develop another DP algorithm <ref type="bibr" target="#b2">[3]</ref> to cluster neighboring operators to reduce the total size of the graph Eq. 2 works on. We cluster the operators (o 1 , . . . , o K ) into a series of layers <ref type="foot" target="#foot_2">4</ref>(l 1 , . . . , l L ), where L K. The goal of the algorithm is to merge two types of operators: (1) those that do not call for much computation but lengthen the computational graph and (2) neighboring operators that may cause substantial communication if put on different device meshes. We define function G(k, r) as the minimum of maximal amount of data received by a single layer when clustering operators (o 1 , . . . , o k ) into r layers. Note that G has the following optimal substructure:</p><formula xml:id="formula_10">G(k, r) (5) = min 1?i?k ? ? ? max{G(i -1, r -1),C(i, k)} | FLOP(o i , . . . , o k ) ? (1 + ?)FLOP total L ? ? ? ,</formula><p>where C(i, k) denotes the total size of inputs of (o i , . . . , o k ) received from (o 1 , . . . , o i-1 ) and FLOP total = FLOP(o 1 , . . . , o K ) is the total FLOP of the whole computation graph. We make sure that each clustered layer's FLOP is within 1 + ? times of the average FLOP per layer while minimizing the communication. For the solutions with the same communication cost, we choose the one with the most uniform structure by also minimizing the variance of per-layer FLOP. With our DP algorithm, we can compute the best layer clustering in O(K 2 L) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion on Optimality</head><p>Alpa uses hierarchical search to generate execution plans. Alpa's finds the optimal execution plan at every parallelism level. However, the combined execution plan is not globally optimal: The intra-operator pass only sees a single stage and does not consider the communication cost between different pipeline stages. Since the communication between stages is by design small, our results on weak scaling ( ?7) suggest that Alpa can already generate near-optimal execution plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parallelism Orchestration</head><p>After stages, device meshes, and their assignments are decided, at the intra-op level, Alpa compiles each stage against its assigned device mesh, respecting the optimal intra-op parallelism plan output by the ILP solver. The compilation depends on XLA <ref type="bibr" target="#b45">[46]</ref> and GSPMD <ref type="bibr" target="#b51">[52]</ref>, and generates parallel executables for each stage-mesh pair. When needed, the compilation automatically inserts collective communication primitives (see ?4) to address the within-mesh communication caused by intra-op parallelism. At the inter-op level, Alpa implements an additional parallelism orchestration pass to address the cross-mesh communication between stages, and generate static instructions for inter-op parallel execution. Cross-mesh resharding. Existing manual systems, such as Megatron-LM <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>, constraint all pipeline stages to have the same degrees of data and tensor model parallelism, so the communication between pipeline stages is trivially realized by P2P send/recv between corresponded devices of two equivalent device meshes (Fig. <ref type="figure">7a</ref>). In Alpa, the device meshes holding two adjacent stages might have different mesh shapes, and the tensor to communicate between two stages might have different sharding specs (Fig. <ref type="figure">7b</ref> and Fig. <ref type="figure">7c</ref>). We call this communication pattern as cross-mesh resharding, which is a many-to-many multicast problem.</p><p>Given the sharding specs of the tensor on the sender and receiver mesh, Alpa generates a communication plan to address cross-mesh sharding in two iterations. In the first iteration, Alpa calculates the correspondences between tensor partitions (a.k.a. tiles) on the source and destination mesh. Based on that, it generates P2P send/recv primitives between the source devices and destination devices to fulfill the communication.</p><p>It then takes a second iteration to identify opportunities where the destination tensor has a replication in its sharding spec. In this case, the tensor only needs to be transferred once between two meshes, then exchanged via all-gather across the devices on the destination mesh using its higher bandwidth (Fig. <ref type="figure">7</ref>) -it rewrites send/recv generated at the first iteration into all-gather to avoid repeated communication.</p><p>We call this approach as local all-gather cross-mesh resharding. Since the communication between stages is normally small by our design, our experiments show it performs satisfactorily well ( ?7.5). We defer the development for the optimal cross-mesh resharding plan to future work. Generating Pipeline Execution Instructions. As the final step, Alpa generates static execution instructions to launch the training on clusters. Since each stage has different sets of operators and may locate on meshes with different shapes, in contrast to many SPMD pipeline-parallel training systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref>, Alpa adopts an MPMD-style runtime to orchestrate the inter-op parallel execution -Alpa generates distinct static execution instructions for each device mesh.</p><p>Alpa develops a set of instructions for inter-op parallel execution, including instructions for allocating and deallocating memory for tensors in a stage, communicating tensors between stages following the cross-mesh resharding plan, synchronization, and computation, etc. According to a userselected pipeline schedule, Alpa uses a driver process to generate the instructions in advance and dispatches the whole instruction lists to each worker before execution, avoiding driver-worker coordination overheads during runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Alpa is implemented using about 16K LoC in Python and 6K LoC in C++. Alpa uses Jax as the frontend and XLA as the backend. The compiler passes are implemented on Jax's and XLA's intermediate representation (i.e., Jaxpr and HLO). The number of GPUs For the distributed runtime, we use Ray <ref type="bibr" target="#b32">[33]</ref> actor to implement the device mesh worker, XLA runtime for executing computation, and NCCL <ref type="bibr" target="#b36">[37]</ref> for communication.</p><p>We evaluate Alpa on training large-scale models with billions of parameters, including GPT-3, GShard Mixture-of-Experts (MoE), and Wide-ResNet. The testbed is a typical cluster consisting of 8 nodes and 64 GPUs. Each node is an AWS EC2 p3.16xlarge instance with 8 NVIDIA V100 16 GB GPUs, 64 vCPUs, and 488 GB memory. The 8 GPUs in a node are connected via NVLink. The 8 nodes are launched within one placement group with 25Gbps cross-node bandwidth.</p><p>We compare Alpa against two state-of-the-art distributed systems for training large-scale models on GPUs. We then isolate different compilation passes and perform ablation studies of our optimization algorithms. We also include a case study of the execution plans found by Alpa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">End-to-End Performance</head><p>Models and training workloads. We target three types of models listed in Table <ref type="table" target="#tab_1">3</ref>, covering models with both homogeneous and heterogeneous architectures. GPT-3 is a homogeneous transformer-based LM by stacking many transformer layers. GShard MoE is a mixed dense and sparse LM, where mixture-of-experts layers are used to replace the MLP at the end of a transformer, every two layers. Wide-ResNet is a variant of ResNet with larger channel sizes. To study the ability to train large models, we follow common ML practice to scale the model size along with the number of GPUs, with the parameter range reported in Table <ref type="table" target="#tab_1">3</ref>. More precisely, for GPT-3, we increase the hidden size and the number of layers together with the number of GPUs following <ref type="bibr" target="#b35">[36]</ref>, whereas for MoE we mainly increase the number of experts suggested by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52]</ref>. For Wide-ResNet, we increase the channel size and width factor in convolution layers. For each model, we adopt the suggested global batch size per ML practice <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref> to keep the same statistical behavior. We then tune a best microbatch size for each model and system configuration that maximizes the system performance. The gradients are accumulated across microbatches. The detailed model specification is provided in Appendix. B. Baselines. For each model, we compare Alpa against a strong baseline system. We use Megatron-LM v2 <ref type="bibr" target="#b35">[36]</ref> as the baseline system for GPT-3. Megatron-LM is a state-of-the-art system for training homogeneous transformer-based LMs on GPUs. It combines data parallelism, pipeline parallelism, and manuallydesigned operator parallelism (denoted as TMP later). The combination of these techniques is controlled by three integer parameters that specify the parallelism degrees assigned to each technique. We grid-search the three parameters following the guidance of their paper and report the results of the best configuration. Megatron-LM is specialized for GPT-like models, so it does not support other models in Table <ref type="table" target="#tab_1">3</ref>.</p><p>We use DeepSpeed <ref type="bibr" target="#b40">[41]</ref> as the baseline for MoE. Deep-Speed provides a state-of-the-art implementation for training MoE on GPUs. It combines handcrafted operator parallelism for MoE layers and ZeRO-based <ref type="bibr" target="#b39">[40]</ref> data parallelism. The combination of these techniques is controlled by several integer parameters that specify the parallelism degree assigned to each technique. We also grid-search them and report the best results. The performance of DeepSpeed on GPT-3 is similar to or worse than Megatron-LM, so we skip it on GPT-3.</p><p>For large Wide-ResNet, there is no specialized system or manually designed plan for it. We use Alpa to build a baseline "PP-DP" whose space only consists of data parallelism and pipeline parallelism, which mimics the parallelism space of PipeDream <ref type="bibr" target="#b33">[34]</ref> and Dapple <ref type="bibr" target="#b12">[13]</ref>.</p><p>For all models, we also include the results of using Alpa with only one of intra-and inter-operator parallelism. Evaluation Metrics. We evaluate weak scaling of the system when increasing the model size along with the number of GPUs. As the models are different for different numbers of GPUs, we cannot use throughput metrics such as tokens per second or images per second. We follow <ref type="bibr" target="#b35">[36]</ref> and use the aggregated peta floating-point operations per second (PFLOPS) of the whole cluster as the metric. All our results (including those in later sections) have a standard deviation within 0.5%, so we skip the error bars in our figures. GPT-3 Results. The parallelization plan for GPT-3 has been extensively studied <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. We observe in Fig. <ref type="figure">8a</ref> that this manual plan with the best grid-searched parameters enables Megatron-LM to achieve super-linear weak scaling on GPT-3. Nevertheless, compared to Megatron-LM, Alpa auto-</p><formula xml:id="formula_11">1 2 4 8</formula><p>The number of GPUs The number of GPUs The number of GPUs matically generates execution plans and even achieves slightly better scaling on several settings. If compared to methods that only use intra-operator parallelism, our results are consistent with recent studies -"Intra-op only" performs poorly on &gt;16 GPUs because even the best plan has to communicate tensors heavily on cross-node connections, making communication a bottleneck. Surprisingly, "Inter-op only" performs well and maintains linear scaling on up to 64 GPUs.</p><p>We investigate the grid-searched parameters of the manual plan on Megatron-LM, and compare it to the plan generated by Alpa. It reveals two major findings. First, in Megatron-LM, the best manual plan has TMP as 1, except in rare settings, such as fitting the 39B model on 64 GPUs, where pipeline parallelism alone is unable to fit the model (stage) in GPU memory; meanwhile, data parallelism is maximized whenever memory allows. In practice, gradient accumulation (GA) is turned on to achieve a desired global batch size (e.g., 1024 in our setting). GA amortizes the communication of data parallelism and reduces the bubbles of pipeline parallelism, but the communication of TMP grows linearly with GA steps, which puts TMP disadvantaged. Second, Alpa-generated plan closely resembles the best-performed ones in Megatron-LM, featuring (1) evenly-sized stages, (2) partitioning along the batch dimension in stages when memory is not stressed, but along non-batch dimensions when memory is stressed. One key difference between our plan and the manual plan is that Alpa also partitions the weight update operations when data parallelism exists, which contributes to the slight performance improvement over Megatron-LM. This attributes to the fact that Alpa, as a generic compiler system, can compose a wide range of parallelism approaches, while Megatron-LM, for now, misses weight update sharding support. MoE Results. DeepSpeed adopts a manual operator parallelism plan for MoE models, developed by GShard <ref type="bibr" target="#b26">[27]</ref>, called expert parallelism, which uses a simple rule: it partitions the expert axis for the operators in MoE layers, but switches back to data parallelism for non-expert layers. This expert parallelism is then combined with ZeRO data parallelism and TMP. All of these techniques belong to intra-operator parallelism. Unfortunately, DeepSpeed's specialized implementation is incompatible with any inter-operator parallelism approach, which is required for scaling across multiple nodes with low inter-node bandwidth. Therefore, Deepspeed only maintains a good performance within a node (? 8 GPUs) on this cluster. "Intra-op only" fails to scale across multiple nodes due to the same reason. "Inter-op only" runs out of memory on 32 GPUs and 64 GPUs because it is not easy to equally slice the model when the number of GPUs is larger than the number of layers of the model. The imbalanced slicing makes some memory-intensive stages run out of memory.</p><p>By contrast, Alpa automatically discovers the best execution plans that combine intra-and inter-operator parallelism. For intra-operator parallelism, Alpa finds a strategy similar to expert parallelism and combines it with ZeRO data parallelism, thanks to its ILP-based intra-op pass. Alpa then constructs stages and uses inter-operator parallelism to favor small communication volume on slow connections. Alpa maintains linear scaling on 16 GPUs and scales well to 64 GPUs. Compared to DeepSpeed, Alpa achieves 3.5? speedup on 2 nodes and a 9.7? speedup on 4 nodes. Wide-ResNet Results. Unlike the previous two models that stack the same layer, Wide-ResNet has a more heterogeneous architecture. As the data batch is forwarded across layers, the size of activation tensor shrinks while the size of weight tensor inflates. This leads to an imbalanced distribution of memory usage and compute intensity across layers. For this kind of model, it is difficult, if not impossible, to manually design a plan. However, Alpa still achieves a scalable performance on 32 GPUs with 80% scaling. The baselines "PP-DP" and "Inter-op only" run out of memory when training large models, because they cannot partition weights to reduce the memory usage and it is difficult the construct memory-balanced stages for them. "Intra-only" requires a lot of communication on slow connections, so it cannot scale across multiple nodes. A case study on the generated plan for Wide-ResNet is in ?7.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Intra-Op Parallelism Ablation Study</head><p>We study the effectiveness of our intra-operator parallelism optimization algorithm. We compare our ILP-based optimal solution against alternatives such as ZeRO optimizer and rulebased partitioning strategies. Experimental setup. We run a weak scaling benchmark in terms of model size similar to ?7.1, but disable pipeline parallelism and gradient accumulation to control variables. The benchmark is done on one Amazon EC2 instance with 8 GPUs. In order to simulate an execution environment of largescale training in one node, we use larger hidden sizes, smaller batch sizes, smaller numbers of layers, compared to the model The number of GPUs Baselines. We compare to automatic solutions for intraoperator parallelism. "Data" is vanilla data parallelism. "ZeRO-2" <ref type="bibr" target="#b39">[40]</ref> is a memory-efficient version of data parallelism which partitions gradients and optimizer states. "ZeRO-3" <ref type="bibr" target="#b39">[40]</ref> additionally partitions parameters on top of "ZeRO-2". "Heuristic" uses a rule combined with the sharding propagation in GSPMD. It marks the largest dimension of every input tensor as partitioned and runs sharding propagation to get the sharding specs for all nodes in the graph. "Auto-sharding" is our solution based on the ILP solver.</p><p>Results. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, "Data" runs out of memory quickly and cannot train large models. "ZeRO-2" and "ZeRO-3" resolve the memory problem of data parallelism. But they do not optimize for communication as they always communicate the gradients. When the gradients are much larger than activations, their performance degenerates. "Heuristic" solves the memory issue by partitioning all tensors, but can be slowed down by larger communication. "Auto-sharding" performs best in all cases and maintains a near linear-scaling, because it figures out the correct partition plan that always minimizes the communication overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Inter-Op Parallelism Ablation Study</head><p>We study the effectiveness of our inter-operator parallelism optimization algorithm. We use "DP" to denote our algorithm. Experimental setup. We report the performance of three variants of our DP algorithm on GPT and Wide-ResNet. The benchmark settings are the same as the settings in ?7.1.</p><p>Baselines. We compare our DP algorithm with two rule-based algorithms. "Equal operator" disables our DP-based operator clustering but assigns the same number of operators to each cluster. "Equal layer" restricts our DP algorithm to use the same number of layers for all stages.</p><p>Results. Fig. <ref type="figure" target="#fig_10">10</ref> shows the result. "DP" always outperforms "Equal operator". This is because "Equal operator" merges operator that should be put onto different device meshes. Alpa's  algorithm can cluster operators based on the communication cost and computation balance. Whether "DP" can outperform "Equal layer" depends on the model architecture. On homogeneous models like GPT, the solution of our DP algorithm uses the same number of layers for all stages, so "Equal layer" performs the same as "DP". On Wide-ResNet, the optimal solution can assign different layers to different stages, so "Equal layer" is worse than the full flexible DP algorithm. For Wide-ResNet on 32 GPUs, our algorithm outperforms "Equal operator" and "Equal layer" by 2.6? and 1.6?, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Compilation Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Cross-Mesh Resharding</head><p>We evaluate our generalized local all-gather optimization for cross-mesh resharding between meshes with different shapes on Wide-ResNet, as shown in Fig. <ref type="figure" target="#fig_12">12</ref>. "signal send/recv" is a synthetic case where we only send 1 signal byte between stages, which can be seen as the upper bound of the performance. "w/o local all-gather" disables our local all-gather optimization and uses only send/recv. "w/ local all-gather" enables our local all-gather optimization to move more communication from slow connections to fast local connections, which brings 2.0? speedup on 32 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Case Study: Wide-ResNet</head><p>We visualize the parallelization strategies Alpa finds for Wide-ResNet on 16 GPUs in Fig. <ref type="figure" target="#fig_14">13</ref>. We also include the visualization of results on 4 and 8 GPUs in Appendix C. On 4 GPUs, Alpa uses only intra-operator parallelism. The intra-operator solution partitions along the batch axis for the first dozens of layers and then switches to partitioning the channel axis for the last few layers. On 16 GPU, Alpa slices the model into 3 stages and assigns 4, 4, 8 GPUs to stage 1, 2, 3, respectively. Data parallelism is preferred in the first two stages because the activation tensors are larger than weight tensors. In the third stage, the ILP solver finds a non-trivial way to partition the convolution operators. This result shows that it can be opaque to manually create such a strategy for a heterogeneous model like Wide-ResNet, even for domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Systems for data-parallel training. Horovod <ref type="bibr" target="#b42">[43]</ref> and Py-TorchDDP <ref type="bibr" target="#b27">[28]</ref> are two commonly adopted data-parallel training systems that synchronize gradients using all-reduce. BytePS <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref> unifies all-reduce and parameter servers and utilizes heterogeneous resources in data center clusters. Au-toDist <ref type="bibr" target="#b53">[54]</ref> uses learning-based approaches to compose a data parallel training strategy. ZeRO <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51]</ref> improves the memory usage of data parallelism by reducing replicated tensors. In Alpa, data parallelism <ref type="bibr" target="#b22">[23]</ref> reduces to a special case of intra-operator parallelism -partitioned along the batch axis. Systems for model-parallel training. The two major classes of model parallelisms have been discussed in ?2. Mesh-TensorFlow <ref type="bibr" target="#b43">[44]</ref> and GSPMD <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52]</ref> are two systems for intra-operator parallelism. They rely on user annotations to get the partition plan. Tofu <ref type="bibr" target="#b49">[50]</ref> develops a DP algorithm to generate the optimal partition strategy for linear graphs on a single node. ColocRL <ref type="bibr" target="#b31">[32]</ref> puts disjoint model partitions on different devices without pipelining, thereby the concurrency happens only when there exist parallel branches in the model. PipeDream <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> improves GPipe by using asynchronous training algorithms, reducing memory usage, and integrating it with data parallelism, but it is asynchronous while Alpa is a synchronous training system. TeraPipe <ref type="bibr" target="#b28">[29]</ref> discovers a new pipeline parallelism dimension for transformer-based LMs. Despite Megatron-LM, the closest system to us is FlexFlow <ref type="bibr" target="#b20">[21]</ref>. FlexFlow proposes a "SOAP" formulation and develops an MCMC-based randomized search algorithm. However, it only supports device placement without pipeline parallelism. Its search algorithm cannot scale to large graphs or clusters and does not have optimality guarantees. Techniques for training large-scale models. In addition to parallelization, there are other complementary techniques for training large-scale models, such as memory optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>, communication compression <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>, and low-precision training <ref type="bibr" target="#b30">[31]</ref>. Alpa can incorporate many of these techniques. For example, Alpa uses rematerialization to reduce memory usage and uses mixed-precision training to accelerate computation. Compilers for deep learning. Compiler techniques have been introduced to optimize the execution of DL models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref>. Most of them focus on optimizing the computation for a single device. In contrast, Alpa is a compiler that supports a comprehensive space of execution plans for distributed training. Distributed tensor computation in other domains. Besides deep learning, libraries and compilers for distributed tensor computation have been developed for linear algebra <ref type="bibr" target="#b4">[5]</ref> and stencil computations <ref type="bibr" target="#b10">[11]</ref>. Unlike Alpa, they do not consider necessary parallelization techniques for DL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We present Alpa, a new architecture for automated modelparallel distributed training, built on top of a new view of machine learning parallelization approaches: intra-and interoperator parallelisms. Alpa constructs a hierarchical space and uses a set of compilation passes to derive the optimal parallel execution plan in each independent parallelism level. Alpa orchestrates the parallel execution on distributed compute devices on two different granularities. Coming up with an efficient parallelization plan for distributed model-parallel deep learning is historically a labor-intensive task, and we believe Alpa will democratize distributed model-parallel learning and accelerate the adoption of emerging large deep learning models. Alpa's source code will be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Submesh Shape Covering</head><p>We prove the following theorem which shows we can always find a solution that fully covers the cluster mesh (N, M) with our selected submesh shapes in ?5.2: (1) onedimensional submeshes of shape (1, 1), (1, 2), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref> . . . (1, 2 m ) where 2 m = M and (2) two-dimensional submeshes of shape (2, M), (3, M), . . . , (N, M) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem</head><p>1. For a list of submesh shapes (n 1 , m 1 ), . . . (n S , m S ), if ? i n i ? m i = N ? M and each (n i , m i ) satisfies either (1) n i = 1 and m i = 2 p i is a power of 2 or (2) m i = M, then we can always cover the full (N, M) mesh where M = 2 m with these submesh shapes.</p><p>Proof. We start with putting the second type submesh into the full mesh. In this case, because m i = M, these submeshes can cover the full second dimension of the full mesh. After putting all the second kind of submeshes into the mesh, we reduce the problem to fit a cluster mesh of shape (N, M) with submeshes with shape (1, 2 p 1 ), . . . , (1, 2 p S ) where all p i ? {0, 1, . . . , m -1}. Note that now we have</p><formula xml:id="formula_12">2 p 1 + ? ? ? + 2 p S = N ? 2 m .<label>(6)</label></formula><p>We start an induction on m. When m = 1, we have all p i = 0 and thus all the submeshes are of shape (1, 1), which means that all the submeshes can definitely cover the full mesh. Assume the above hold for all m = 1, 2, . . . , k -1. When m = k, note that in this case the number of submeshes with p i = 0 should be an even number, because otherwise the left hand side of Eq. 6 will be an odd number while the right hand side is always an even number. Then we can split all submeshes with shape p i = 0 into pairs, and we co-locate each pair to form a (1, 2) mesh. After this transformation, we have all p i &gt; 0, so we can subtract all p i and m by 1 and reduce to m = k -1 case. Therefore, the theorem holds by induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Specifications</head><p>For GPT-3 models, we use sequence length = 1024 and vocabulary size = 51200 for all models. Other parameters of the models are listed in Table . 5. The last column is the number of GPUs used to train the corresponding model. For GShard MoE models, we use sequence length = 1024 and vocabulary size = 32000 for all models. Other parameters of the models are listed in Table. 6. The last column is the number of GPUs used to train the corresponding model.</p><p>For Wide-ResNet models, we use input image size = (224, 224, 3) and #class = 1024 for all models. Other parameters of the models are listed in Table . 7. The last column is the number of GPUs used to train the corresponding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extra Case Study</head><p>We visualize the parallelization strategies Alpa finds for Wide-ResNet on 4 and 8 GPUs in Fig. <ref type="figure" target="#fig_3">14</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generation of parallelization plans for a computational graph shown in (a). Different colors represent different devices, dashed boxes represent stages. (b) creates the plan manually. (c) and (d) automatically generate plans using only one of intra-and inter-operator parallelisms. (e) shows our approach that creates a hierarchical space to combine intraand inter-operator parallelisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Common parallelization techniques for training a 2-layer Multi-layer Perceptron (MLP). Only the forward pass is shown. "x" is the input data. "w1" and "w2" are two weight matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Compiler passes and runtime architecture. A sharded stage is a stage annotated with the sharding specs generated by intra-op pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example to demonstrate Alpa's API. The developers uses a one-line Python decorator @parallelize to annotate functions that need to be parallelized. The rest of the program is kept intact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 7 :</head><label>17</label><figDesc>Figure 7: Cross-mesh resharding. Red arrows denote send/recv on slow connections. Green arrows denote allgather on fast connections. (a) The scatter-gather optimization for equal mesh shapes in Megatron-LM. (b) The naive send/recv for unequal mesh shapes. (c) The generalized local all-gather optimization for unequal mesh shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Intra-operator parallelism ablation study. "?" denotes out-of-memory. Black boxes represent linear scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Inter-operator parallelism ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Alpa's compilation time on all GPT models. The model size and #GPUs are simultaneously scaled. configurations in ?7.1.Baselines. We compare to automatic solutions for intraoperator parallelism. "Data" is vanilla data parallelism. "ZeRO-2"<ref type="bibr" target="#b39">[40]</ref> is a memory-efficient version of data parallelism which partitions gradients and optimizer states. "ZeRO-3"<ref type="bibr" target="#b39">[40]</ref> additionally partitions parameters on top of "ZeRO-2". "Heuristic" uses a rule combined with the sharding propagation in GSPMD. It marks the largest dimension of every input tensor as partitioned and runs sharding propagation to get the sharding specs for all nodes in the graph. "Auto-sharding" is our solution based on the ILP solver. Results. As shown in Fig.9, "Data" runs out of memory quickly and cannot train large models. "ZeRO-2" and "ZeRO-3" resolve the memory problem of data parallelism. But they do not optimize for communication as they always communicate the gradients. When the gradients are much larger than activations, their performance degenerates. "Heuristic" solves the memory issue by partitioning all tensors, but can be slowed down by larger communication. "Auto-sharding" performs best in all cases and maintains a near linear-scaling, because it figures out the correct partition plan that always minimizes the communication overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Cross-mesh resharding on Wide-ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 shows</head><label>11</label><figDesc>Fig.11shows Alpa's compilation time for all the GPT settings in ?7.1. According to the result, Alpa scales to large models or large clusters well, because compilation time grows linearly with the size of the model and the number of GPUs in the cluster. Table4reports the compilation time breakdown for the largest GPT model in our evaluation (39B, 64 GPUs). Most of the time is spent on enumerating stage-mesh pairs and profiling them. For the compilation part, we accelerate it by compiling different stages in parallel with distributed workers. For profiling, we accelerate it using a simple cost model built at XLA instruction level, which estimates the cost of matrix multiplication and communication primitives with a piece-wise linear model. With these optimizations, the compilation and search for a model take at most several hours, which is acceptable as it is much shorter than the actual training time, which can take several weeks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Visualization of the parallel strategy of Wide-ResNet on 16 GPUs. Different colors represents the devices a tensor is distributed on. Grey blocks indicate a tensor is replicated across the devices. The input data and resulting activation of each convolution and dense layer can be partitioned along the batch axis and the hidden axis. The weights can be partitioned along the input and output channel axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Models used in the end-to-end evaluation. LM = language model. IC = image classification.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Throughput (PFLOPS)</cell><cell>0.0 1.0 2.0 3.0</cell><cell>Deepspeed Inter-op only Intra-op only Alpa (ours) Linear-scaling</cell><cell>X</cell><cell>XX</cell><cell>Throughput (PFLOPS)</cell><cell>0.0 0.2 0.4 0.6</cell><cell>1 PP-DP The number of GPUs 4 8 16 32 64 XX XX Inter-op only Intra-op only Alpa (ours) Linear-scaling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) MoE</cell><cell></cell><cell></cell><cell>(c) Wide-ResNet</cell></row><row><cell cols="9">Figure 8: End-to-end evaluation results. "?" denotes out-of-memory. Black boxes represent linear scaling.</cell></row><row><cell>Model</cell><cell cols="2">Task Batch size</cell><cell cols="2">#params (billion)</cell><cell>Precision</cell><cell></cell><cell></cell></row><row><cell>GPT-3 [7]</cell><cell>LM</cell><cell>1024</cell><cell cols="2">0.35, 1.3, 2.6, 6.7, 15, 39</cell><cell>FP16</cell><cell></cell><cell></cell></row><row><cell>GShard MoE [27]</cell><cell>LM</cell><cell>1024</cell><cell cols="2">0.38, 1.3, 2.4, 10, 27, 70</cell><cell>FP16</cell><cell></cell><cell></cell></row><row><cell>Wide-ResNet [53]</cell><cell>IC</cell><cell>1536</cell><cell cols="2">0.25, 1.0, 2.0, 4.0, 6.7, 13</cell><cell>FP32</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Compilation time breakdown of GPT-39B.</figDesc><table><row><cell></cell><cell></cell><cell>Steps</cell><cell>Ours</cell><cell>w/o optimization</cell></row><row><cell></cell><cell cols="2">Compilation</cell><cell>1582.66 s</cell><cell>&gt; 16hr</cell></row><row><cell></cell><cell cols="2">Profiling</cell><cell>804.48 s</cell><cell>&gt; 24hr</cell></row><row><cell cols="3">Stage Construction DP</cell><cell>1.65 s</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell>Other</cell><cell>4.47 s</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell>Total</cell><cell>2393.26 s</cell><cell>&gt; 40hr</cell></row><row><cell>Throughput (PFLOPS)</cell><cell>0.05 0.1 0.15 0.2 0.25 0.3</cell><cell cols="2">16 Signal send/recv w/o local all-gather #GPUs w/ local all-gather</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>reports the compilation time breakdown for</cell></row><row><cell>the largest GPT model in our evaluation (39B, 64 GPUs).</cell></row><row><cell>Most of the time is spent on enumerating stage-mesh pairs</cell></row><row><cell>and profiling them. For the compilation part, we accelerate</cell></row><row><cell>it by compiling different stages in parallel with distributed</cell></row><row><cell>workers. For profiling, we accelerate it using a simple cost</cell></row><row><cell>model built at XLA instruction level, which estimates the</cell></row><row><cell>cost of matrix multiplication and communication primitives</cell></row><row><cell>with a piece-wise linear model. With these optimizations,</cell></row><row><cell>the compilation and search for a model take at most several</cell></row><row><cell>hours, which is acceptable as it is much shorter than the actual</cell></row><row><cell>training time, which can take several weeks.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>GPT-3 Model Specification</figDesc><table><row><cell cols="5">#params Hidden size #layers #heads #gpus</cell></row><row><cell>350M</cell><cell>1024</cell><cell>24</cell><cell>16</cell><cell>1</cell></row><row><cell>1.3B</cell><cell>2048</cell><cell>24</cell><cell>32</cell><cell>4</cell></row><row><cell>2.6B</cell><cell>2560</cell><cell>32</cell><cell>32</cell><cell>8</cell></row><row><cell>6.7B</cell><cell>4096</cell><cell>32</cell><cell>32</cell><cell>16</cell></row><row><cell>15B</cell><cell>5120</cell><cell>48</cell><cell>32</cell><cell>32</cell></row><row><cell>39B</cell><cell>8192</cell><cell>48</cell><cell>64</cell><cell>64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>GShard MoE Model Specification#params Hidden size #layers #heads #experts #gpus</figDesc><table><row><cell>380M</cell><cell>768</cell><cell>8</cell><cell>16</cell><cell>8</cell><cell>1</cell></row><row><cell>1.3B</cell><cell>768</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>4</cell></row><row><cell>2.4B</cell><cell>1024</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>8</cell></row><row><cell>10B</cell><cell>1536</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>16</cell></row><row><cell>27B</cell><cell>2048</cell><cell>16</cell><cell>32</cell><cell>48</cell><cell>32</cell></row><row><cell>70B</cell><cell>2048</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Wide-ResNet Model Specification #params #layers Base channel Width factor #gpus</figDesc><table><row><cell>250M</cell><cell>50</cell><cell>160</cell><cell>2</cell><cell>1</cell></row><row><cell>1B</cell><cell>50</cell><cell>320</cell><cell>2</cell><cell>4</cell></row><row><cell>2B</cell><cell>50</cell><cell>448</cell><cell>2</cell><cell>8</cell></row><row><cell>4B</cell><cell>50</cell><cell>640</cell><cell>2</cell><cell>16</cell></row><row><cell>6.8B</cell><cell>50</cell><cell>320</cell><cell>16</cell><cell>32</cell></row><row><cell>13B</cell><cell>101</cell><cell>320</cell><cell>16</cell><cell>64</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Device placement is another case of inter-operator parallelism, which partitions the model graph and executes them on different devices, but does not saturate pipelines using multiple microbatches. Hence pipeline parallelism is often seen as a better alternative to it because of less device idle time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We simply use the order of how users define each operator, reflected in the model IR, with the input operator as the origin. This allows us to leverage the inherent locality present in the user's program -closely related nodes in the graph will be more likely to be partitioned into the same stage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that the clustering does not exactly reproduce the layers with original machine learning semantics in the model definition.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://aws.amazon.com/ec2/instance-types/p3/" />
		<title level="m">AWS Cluster Configuratoins</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed balanced partitioning via linear embedding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadhossein</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahab</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient compression supercharged high-performance data parallel dnn training</title>
		<author>
			<persName><forename type="first">Youhui</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles CD-ROM</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles CD-ROM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="359" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ScaLAPACK users&apos; guide</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo D'</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Petitet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actnn: Reducing training memory footprint via 2-bit activation compressed training</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Tyler Denniston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Distributed halide</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Glam: Efficient scaling of language models with mixture-of-experts</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dapple: A pipelined data parallel approach for training large models</title>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cbc user guide</title>
		<author>
			<persName><forename type="first">John</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Lougee-Heimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging theory, methods, and applications</title>
		<imprint>
			<publisher>INFORMS</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="257" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational comparison of exact solution methods for 0-1 quadratic programs: Recommendations for practitioners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Forrester</surname></persName>
		</author>
		<author>
			<persName><surname>Hunt-Isaak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://cloud.google.com/tpu" />
		<title level="m">Google Clould TPU Cluster Configurations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping</title>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1341" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02653</idno>
		<title level="m">Checkmate: Breaking the memory wall with optimal tensor rematerialization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taso: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05358</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified architecture for accelerating distributed {DNN} training in heterogeneous gpu/cpu clusters</title>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="463" to="479" />
		</imprint>
	</monogr>
	<note>{OSDI} 20</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Soojeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeong-In</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hojin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunji</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonmin</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<title level="m">Parallax: Sparsity-aware data parallel training of deep neural networks. In Proceedings of the Fourteenth EuroSys Conference</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Altan</forename><surname>Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09616</idno>
		<title level="m">Dynamic tensor rematerialization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automating system configuration of distributed machine learning</title>
		<author>
			<persName><forename type="first">Yunseong</forename><surname>Woo-Yeon Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeong-In</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo Yeon</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beomyeol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonwook</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2057" to="2067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Damania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15704</idno>
		<title level="m">Experiences on accelerating data parallel training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pytorch distributed</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Terapipe: Token-level pipeline parallelism for training large-scale language models</title>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07988</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rammer: Enabling holistic deep learning compiler optimizations with rtasks</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="881" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Device placement optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2430" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ray: A distributed framework for emerging {AI} applications</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melih</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pipedream: generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memory-efficient pipelineparallel dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7937" to="7947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The nvidia collective communication library</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A generic communication scheduler for distributed dnn training acceleration</title>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06840</idno>
		<title level="m">Zero-offload: Democratizing billion-scale model training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balso</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xla: Optimizing compiler for machine learning</title>
		<author>
			<persName><forename type="first">Xla</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Theano: A python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">The</forename><surname>Theano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Development</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?d?ric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoly</forename><surname>Belikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Powersgd: Practical low-rank gradient compression for distributed optimization</title>
		<author>
			<persName><forename type="first">Thijs</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Praneeth Karinireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">{PET}: Optimizing tensor programs with partially equivalent transformations and automated corrections</title>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supporting very large models using automatic dataflow graph partitioning</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
		<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Automatic cross-replica sharding of weight update in data-parallel training</title>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13336</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04663</idno>
		<title level="m">General and scalable parallelization for ml computation graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Autosync: Learning to synchronize for data-parallel distributed deep learning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
	<note>{OSDI} 20</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
