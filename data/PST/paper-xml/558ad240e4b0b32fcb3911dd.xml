<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A mobile markerless AR system for maintenance and repair</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juri</forename><surname>Platonov</surname></persName>
							<email>juri.platonov@metaio.com</email>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Metaio</forename><surname>Gmbh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hauke</forename><surname>Heibel</surname></persName>
							<email>hauke.heibel@metaio.com</email>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Meier</surname></persName>
							<email>peter.meier@metaio.com</email>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bert</forename><surname>Gmbh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Grollmann</surname></persName>
							<email>bert.grollmann@metaio.com</email>
							<affiliation key="aff0">
								<orgName type="department">metaio GmbH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A mobile markerless AR system for maintenance and repair</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3033BA7F55F3B01F8A92F3C7CBEE7431</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.4.8 [Computing Methodologies]: Image Processing and Computer Vision-Scene Analysis; J.6 [Computer Applications]: Computer-Aided Engineering-Computeraided manufacturing (CAM) augmented reality</term>
					<term>markerless tracking</term>
					<term>maintenance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a solution for AR based repair guidance. This solution covers software as well as hardware related issues. In particular we developed a markerless CAD based tracking system which can deal with different illumination conditions during the tracking stage, partial occlusions and rapid motion. The system is also able to automatically recover from occasional tracking failures. On the hardware side the system is based on an off the shelf notebook, a wireless mobile setup consisting of a wide-angle video camera and an analog video transmission system. This setup has been tested with a monocular full-color video-see-through HMD and additionally with a monochrome optical-see-through HMD. Our system underwent several extensive test series under real industrial conditions and proved to be useful for different maintenance and repair scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Augmented Reality (AR) is still a young technology capable of enhancing (or augmenting) a user's view of reality with additional virtual 3D information. An up-and-coming field for Augmented Reality is industrial maintenance and repair. The basic idea is to support technicians with helpful instructions to save time that they may lose by needing to look up required information in paper based documentation. The same techniques can be applied for education purposes, i.e. flattening the learning curve and making supervision by an expert unnecessary.</p><p>In this work we introduce a complete AR system for maintenance and repair purposes. In the past there have been a few attempts to develop an AR system for industrial applications. The solution developed during the ARVIKA project <ref type="bibr" target="#b1">[2]</ref> used markerbased optical tracking in combination with a video-see-through setup worn by a technician. In some scenarios however this approach turned out to be not applicable because markers may occlude parts of the workspace and must be properly calibrated to the tracked reference frame. In addition, marker based tracking systems need a free line of sight between the camera and the marker which can not always be guaranteed in repair scenarios where partial occlusions by worker's hands and tools are common.</p><p>Former hardware solutions forced the user either to wear bulky computing devices or to be connected to them via a flexible cable. Our experience showed that both solutions are often not accepted in industry for ergonomic reasons and due to the risk of injuries.</p><p>To overcome these problems we developed a markerless tracking system combined with a light weight mobile setup. In the proposed solution both, the raw and the augmented video stream are wirelessly transmitted between a computer (e.g., a laptop) and the user.</p><p>Our markerless tracking algorithm has been inspired by <ref type="bibr" target="#b16">[17]</ref> though it differs in several aspects that are described later. The algorithm is based on 2D point features and uses a CAD model to establish 2D-3D correspondences which in turn are used to compute the camera pose. The tracking algorithm can be separated into two stages -the learning stage and the online tracking stage. A similar paradigm has already been proposed in <ref type="bibr" target="#b5">[6]</ref>. The result of the learning stage is a set of key frames. Each key frame consists of an image of the object to be tracked with a reliable set of 2D-3D correspondences. Such a learning stage has to be passed only once for each scenario. The online tracking consists of an initialization and a tracking phase. During initialization the system is in a state where no information about the camera pose is given. In this phase each camera frame is matched against the key frame set built during the learning stage. Throughout the second phase the system tracks 2D features frame-to-frame based on the KLT tracker <ref type="bibr" target="#b12">[13]</ref> that performs feature matching between successive images based on image pyramids and correlation techniques. After this the camera pose is computed using non-linear optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since the use of AR in industrial applications is a promising and at the same time challenging issue, several prototypical systems have already been developed. The system developed during the biggest German AR-Project -ARVIKA <ref type="bibr" target="#b1">[2]</ref> as well as system <ref type="bibr" target="#b0">[1]</ref> use markers for pose estimation which is not practical in many real industrial scenarios due to the line of sight problem. On the other hand there are numerous attempts to solve the pose estimation problem without the use of fiducials (e.g., <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>). Most of these attempts lack testing in real industrial applications. An overview of the AR technology in production can be found in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Most of the work related to our tracking approach has been described in <ref type="bibr" target="#b16">[17]</ref>. We however use different feature detection and tracking algorithms as will be described in successive sections. Furthermore we do not use the local bundle adjustment technique proposed in <ref type="bibr" target="#b16">[17]</ref> yet we do not experience a noticeable jitter. We use a restrictive feature rejection strategy which eliminates potential outliers during the tracking stage and abandons the need for RANSAC preprocessing of 2D-3D correspondences. In addition we use an enhanced algorithm for the training of key frames, which already allows the rejection of malign features during the learning stage making the initialization procedure more reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR MARKERLESS TRACKING APPROACH</head><p>In this chapter we describe the prototype we built to prove the concept of markerless tracking in an industrial context. The problem of markerless AR consists of two mainly independent parts -tracking and initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tracking</head><p>The main problem of model based tracking with a monocular camera can be formulated as follows:</p><p>Given a camera's pose P t-1 at some time t -1, video images I t-1 and I t taken at time t -1 and t, as well as a 3D work area model M, estimate the current camera pose P t .</p><p>To solve this problem we use a feature based approach, where features are points with high textural fluctuations. Having a camera pose P t-1 , which in our case is a 6-DOF vector consisting of three translation values (x, y, z) T and three Euler angles (α, β , θ ) T , a corresponding image I t-1 as well as a 3D CAD model M of the object to be tracked we establish a set of 2D-3D correspondences ν t-1 . First we extract a set of 2D features x t-1 i from the image I t-1 by using the Shi-Tomasi operator <ref type="bibr" target="#b15">[16]</ref>. For efficiency purposes the set is limited to a maximum of 100 features. The camera's center of projection C connected with found features x t-1 i in the image plane defines a pencil of 3D rays r t-1 i . Now we use the camera pose P t-1 and the CAD model M to assign to every 2D feature x t-1 i a corresponding 3D point X i . To perform this step efficiently we assign to each face of the CAD model an unique color and render the CAD model using the pose P t-1 . After this we use the color of the rendered CAD model at every 2D feature position x t-1 i to index the corresponding face of the CAD model. This face spans a plane π that the ray r t-1 i can be intersected with. The 3D intersection point X i is then added to the set ν t-1 together with the 2D feature x t-1 i as a 2D-3D correspondence. Figure <ref type="figure" target="#fig_0">1</ref> presents a schematic view of the whole procedure. After the set ν t-1 of 2D-3D correspondences has been established, the frame-to-frame tracking of 2D features x t-1 i is performed from image I t-1 into the image I t . We use the Kanade-Lucas-Tracker <ref type="bibr" target="#b12">[13]</ref> in combination with image pyramids <ref type="bibr" target="#b4">[5]</ref> to accomplish this task. Thus a set of 2D-3D correspondences ν t-1 is transformed into a new set ν t , which corresponds to the Image I t . The camera pose P t can now be calculated in a linear or non-linear manner. We minimize the vector of reprojection errors δ i using the Levenberg-Marquardt algorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To be more robust to outliers we do not use the reprojection error itself but the pseudo-Huber robust cost function <ref type="bibr" target="#b7">[8]</ref> </p><formula xml:id="formula_0">g(||δ i ||) = 2b 2 ( 1 + (||δ i || /b) 2 -1) (1)</formula><p>which diminishes the influence of outliers on the result. After features have been extracted they are tracked with KLT between successive frames. During this tracking stage more and more features get lost, either because they cross the image border and thus cannot be tracked any more or because their matching error exceeds a certain threshold value. In addition, after each pose estimation step all 3D points are projected into the image plane using the estimated pose P t , and their distances to the corresponding 2D features are calculated. For every distance exceeding a certain threshold the according 2D-3D pair becomes rejected and thus classified as an outlier. If the number of tracked features falls below a threshold value (≈ 20) all features are deleted and the whole procedure is repeated by using the last estimated pose.</p><p>With the hardware setup described in chapter 4 our system is capable of running at a frequency of 60 Hz and currently limited only by the frame rate of the video camera (30 Hz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initialization</head><p>The phase of initialization copes with the problem of determining a camera's projection matrix without any constraints to the pose and without any knowledge of the previous poses. The only constraint given in our case is that we are working with a calibrated camera and thus a known intrinsic matrix K.</p><p>This problem is solved when a sufficiently large set of 2D-3D correspondences can be established. Given such correspondences, one can compute the camera's extrinsic parameters and refine them by applying non-linear optimization (e.g., <ref type="bibr">Levenberg-Marquardt)</ref>.</p><p>The main problem during the initialization phase lies in establishing these 2D-3D correspondences.</p><p>In the section 3.1 we assumed that the first camera pose for tracking initialization was available. Now we discard this assumption and describe the semiautomatic approach we developed for markerless initialization and reinitialization.</p><p>Our initialization approach is similar to the one described in <ref type="bibr" target="#b9">[10]</ref> and is based upon key frames -stored snapshots of the tracked object along with 2D-3D point pairs and the corresponding camera pose. The initialization consists of two stages: The offline learning procedure and its application. During the learning stage key frames are created and then applied for initialization purposes during the application stage.</p><p>Learning key frames. Key frames are created using an additional robust tracking system. Such a system could be any outsidein tracker which provides reliable pose data. Since the use of the outside-in tracking systems is not practicable for mobile industrial applications, we used an inside-out marker based tracking system <ref type="bibr" target="#b2">[3]</ref>. To acquire a key frame, the user must place the marker at a known position in the world coordinate system (Fig. <ref type="figure" target="#fig_1">2</ref>). The camera pose provided by the marker is used to establish the 2D-3D correspondences as described in the section 3.1. The video frame (without the marker), 2D-3D correspondences and the camera pose provided by the marker are stored in a key frame structure. The whole procedure is then repeated to create a number of key frames (we used about 10-15). Key frames should, if possible, cover the whole work space to allow reinitialization from nearly every position of the user's head. Markerless initialization with key frames. After a set of key frames has been created, it can be used for initialization purposes as follows. When the application has started, every incoming video frame is, initially, normalized using histogram equalization to reduce the effect of different lighting conditions and then compared to all key frames (which are also normalized) using the SSD similarity metric. If the similarity is above some empirically estimated threshold then the camera pose associated with this key frame is assumed to be close to the camera pose of the real camera. In the next step the KLT tracker is used to track 2D features from the found key frame into the current video frame. If the percentage of successfully tracked 2D features exceeds a threshold (≈ 70%) the camera pose for the current video frame is calculated, as described in section 3.1, and tracking begins. The reinitialization, after the frame-to-frame tracking fails, works in the same manner.</p><p>Problems. Two main problems arise when using key frames for initialization. First, key frames introduce illumination dependency since they are snapshots of a particular scene under particular lighting conditions. Histogram equalization lessens the problem slightly but does not solve it. So, if the application domain is exposed to natural lighting, and the interval between key frame creation and application is significant, the initialization does not work well because 2D point features cannot be reliably tracked by KLT. Second, the initialization area for a key frame is small, and therefore the camera pose must be close to the pose associated with a key frame to allow for initialization. The reason for this behavior is the limited tracking radius of the KLT tracker.</p><p>To improve the performance of the initialization procedure, we introduced a training procedure to eliminate unreliable features during the key frame learning stage. After the user adds a key frame to the storage, he is asked to move the camera a little bit in the vicinity of the pose used to create the key frame. As the user moves the camera, 2D features extracted from the key frame are tracked with KLT into every video frame. All features for which the tracking fails are rejected and not saved in the key frame structure. As a consequence we achieve a more robust initialization, as the probability of a successful tracking of a feature, that was saved with the key frame, increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HARDWARE SETUP</head><p>The hardware we use at the moment consists of a standard notebook ( Pentium 1.7 GHz Centrino, 500 Mb RAM, NVIDIA FX 5650 Go graphics card ), a HMD ( both video -see-through and optical-seethrough have been tested ), a wide-angle micro camera ( Toshiba IK-CU50 with a 2.1 mm lens ) and an analog wireless bidirectional transmission system. The video camera is mounted rigidly on the HMD (Fig. <ref type="figure" target="#fig_2">3</ref>). The wireless transmitter and receiver as well as the power supply are built in a waistcoat worn by the user and the weight of the transmission system is thus spread over the user's body. The signal of the video camera is transmitted to the notebook, where both tracking and augmentation happen and the complete augmented video stream is then sent back to the mobile setup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We tested the presented AR system both with synthetic and real data. For evaluation purposes we created five synthetic video sequences (Fig. <ref type="figure" target="#fig_3">4</ref>). All sequences were created by rendering the uncompressed CAD model from BMW using a virtual camera with a 90 o field of view. The length of each video sequence is 100 frames at a resolution of 640 × 480 pixels. All sequences differ in their complexity regarding movement strength, occlusions and illumination changes. The distance between the camera and the car varies from about 500mm to 1600mm. The RMS Error in all sequences was ≈ 5mm for translation components and ≈ 0.8deg for orientation components of the pose. For evaluation with real data the maintenance of a BMW 7 Series engine was chosen as a test scenario. In particular the so called Valvetronic actuator had to be changed. Steps which are necessary to complete this task contain all types of problems that a robust tracking system has to deal with, such as strong partial occlusions by worker's hands, rapid movements, strong variations in the distance to the work area, changes applied to the work area and the need for reinitialization after the tracking has failed (Fig. <ref type="figure" target="#fig_4">5</ref>). The user is guided through the scenario by a set of animated 3D work instructions which are superimposed on the car engine. The test was performed by BMW staff who were not involved in the development of the tracking software. The whole scenario was repeatedly successfully performed many times by different persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We developed a prototypical AR system for industrial maintenance and repair purposes which utilizes markerless tracking with a monocular camera setup as well as a light weight mobile hardware platform for visualization. The developed AR system has been evaluated in numerous tests in a real industrial context and demonstrated robust and stable behaviour. Our system is based upon well known concepts and algorithms and it is our opinion that it is the right mixture of algorithms that leed to a successful AR system.</p><p>It is also of interest to mention the advantages of wide-angle cameras for feature-based optical markerless tracking. They allow us to use a larger part of the work space for tracking purposes which is important in presence of partial occlusions like in our test scenario. An extension of this idea to fish-eye cameras is presented in <ref type="bibr" target="#b8">[9]</ref>.</p><p>For the moment the main weaknesses of our system is the theoretically unbound error accumulation during the tracking stage, the small reinitialization radius of key frames as well as their dependency on lighting conditions. The last problem has been diminished using histogram equalization, but is not solved completely. During numerous tests on real camera data, we noticed that even though the pose error accumulates without theoretical bound, it does so very slowly and, since users often turn their heads away from the work area and thus trigger the reinitialization procedure, this error becomes negligible. A possible solution for the unbound error accumulation problem could incorporate the use of key frames during the tracking stage to correct for drift as proposed in <ref type="bibr" target="#b9">[10]</ref>. Another idea envisions on-line learning of feature descriptors (e.g., SIFT <ref type="bibr" target="#b11">[12]</ref>) for feature recognition and then using these recognizable features for drift correction. The same idea could be used for initialization purposes: Instead of using KLT, feature descriptors can be used to achieve stronger scale invariance as well as a broader recognition radius.</p><p>Although we used key frames in our prototypical implementation, we regard the use of key frames based upon reality photographs as a wrong direction if significant geometrical information is available. By "significant geometrical information" we mean a nontrivial CAD model. It is probably difficult to detect and track a textured box using only a CAD model, because the model is trivial and similar to many other objects in the scene. In such a case the use of real photographs is necessary. If, however, the object to be tracked, has a nontrivial topology like, e.g., a car engine, an appropriate CAD model should be used to extract meaningful features, in an off-line process, which in turn would be used for on-line initialization. Features extracted from a CAD model should probably use edge information to be robust regarding lighting conditions and possibly different object surfaces. The information obtained through analyzing the CAD model should be used primarily for the first initialization. After the first initialization succeeded, the system should start to learn from real images, adopting to real illumination and the object surface. Such on-line learning could incorporate both topological and appearance learning, the former being the extraction and triangulation of some features (e.g. using approaches similar to SfM), while the latter would assign descriptors for recog-nition purposes to a 3D entity (e.g., point, line, curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMNTS</head><p>This research was funded by metaio GmbH. We would also like to underline the uncomplicated and enjoyable cooperation with collegues from the V-S-L1 group at BMW: Nikola Recknagel, Laurent Chauvigne, Morten-Bo Jensen and Ingo Stock.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Extracted 2D features are projected onto the CAD model to establish 2D-3D correspondences.</figDesc><graphic coords="2,103.07,318.65,141.92,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Key frames should be spread over the whole workspace to allow for robust reinitialization.</figDesc><graphic coords="2,367.07,523.25,141.92,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The setup used for markerless tracking evaluation.</figDesc><graphic coords="3,87.83,582.53,85.18,89.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Snapshots from a synthetic video sequence. The sequence consists of a motion and rotation around all three coordinate axes along with heavy occlusions and illumination changes. Not only the color of the illumination is changed but also the direction.</figDesc><graphic coords="3,329.39,268.25,71.09,53.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Snapshots from a three minute real video sequence.</figDesc><graphic coords="4,87.83,224.69,85.18,68.18" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1-4244-0651-X/06/$20.00 ©2006 IEEE</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.hitl.washington.edu/artoolkit/" />
		<title level="m">ARToolKit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Arvika</surname></persName>
		</author>
		<ptr target="www.arvika.de" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gmbh</forename><surname>Metaio</surname></persName>
		</author>
		<ptr target="http://www.metaio.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Augmented Reality in der Produktion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Alt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pyramidal Implementation of the Lucas Kanade Feature Tracker. Description of the algorithm. OpenCV Documentation</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Bouguet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Microprocessor Research Labs, Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marker-less tracking for ar: A learning-based approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yakup Genc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Souvannavong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three dimensional model-based tracking using texture learning and matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gagalowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13-14</biblScope>
			<biblScope unit="page" from="1095" to="1103" />
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Robust Statistics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Markerless image-based 3d tracking for real-time augmented reality applications</title>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Koeser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birger</forename><surname>Streckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Friso</forename><surname>Evers-Senne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIAMIS</title>
		<imprint>
			<date type="published" when="2005-04">2005. April 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully automated and stable registration for augmented reality applications</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Vacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method for the solution of certain problems in least squares</title>
		<author>
			<persName><forename type="first">K</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quart. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981-07-22">July 22 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for least-squares estimation of nonlinear parameters</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="1963-06">June 1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A natural feature-based 3d object tracking method for wearable augmented reality</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Okuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhiko</forename><surname>Sakaue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IEEE International Workshop on Advanced Motion Control (AMC&apos;04)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="451" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;94)</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stable real-time 3d tracking using online and offline information</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Vacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1385" to="1391" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
