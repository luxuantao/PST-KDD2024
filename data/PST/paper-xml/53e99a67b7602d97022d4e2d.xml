<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Max-Margin Markov Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
							<email>btaskar@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
							<email>guestrin@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
							<email>koller@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Max-Margin Markov Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB066AF17E7F958AB281E44A0E8A32CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M 3 ) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M 3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In supervised classification, our goal is to classify instances into some set of discrete categories. Recently, support vector machines (SVMs) have demonstrated impressive successes on a broad range of tasks, including document categorization, character recognition, image classification, and many more. SVMs owe a great part of their success to their ability to use kernels, allowing the classifier to exploit a very high-dimensional (possibly even infinite-dimensional) feature space. In addition to their empirical success, SVMs are also appealing due to the existence of strong generalization guarantees, derived from the margin-maximizing properties of the learning algorithm.</p><p>However, many supervised learning tasks exhibit much richer structure than a simple categorization of instances into one of a small number of classes. In some cases, we might need to label a set of inter-related instances. For example: optical character recognition (OCR) or part-of-speech tagging both involve labeling an entire sequence of elements into some number of classes; image segmentation involves labeling all of the pixels in an image; and collective webpage classification involves labeling an entire set of interlinked webpages. In other cases, we might want to label an instance (e.g., a news article) with multiple non-exclusive labels. In both of these cases, we need to assign multiple labels simultaneously, leading to a classification problem that has an exponentially large set of joint labels. A common solution is to treat such problems as a set of independent classification tasks, dealing with each instance in isolation. However, it is well-known that this approach fails to exploit significant amounts of correlation information <ref type="bibr" target="#b6">[7]</ref>.</p><p>An alternative approach is offered by the probabilistic framework, and specifically by probabilistic graphical models. In this case, we can define and learn a joint probabilistic model over the set of label variables. For example, we can learn a hidden Markov model, or a conditional random field (CRF) <ref type="bibr" target="#b6">[7]</ref> over the labels and features of a sequence, and then use a probabilistic inference algorithm (such as the Viterbi algorithm) to classify these instances collectively, finding the most likely joint assignment to all of the labels simultaneously. This approach has the advantage of exploiting the correlations between the different labels, often resulting in significant improvements in accuracy over approaches that classify instances independently <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. The use of graphical models also allows problem structure to be exploited very effectively. Unfortunately, even probabilistic graphical models that are trained discriminatively do not usually achieve the same level of generalization accuracy as SVMs, especially when kernel features are used. Moreover, they are not (yet) associated with generalization bounds comparable to those of margin-based classifiers.</p><p>Clearly, the frameworks of kernel-based and probabilistic classifiers offer complementary strengths and weaknesses. In this paper, we present maximum margin Markov (M 3 ) networks, which unify the two frameworks, and combine the advantages of both. Our approach defines a log-linear Markov network over a set of label variables (e.g., the labels of the letters in an OCR problem); this network allows us to represent the correlations between these label variables. We then define a margin-based optimization problem for the parameters of this model. For Markov networks that can be triangulated tractably, the resulting quadratic program (QP) has an equivalent polynomial-size formulation (e.g., linear for sequences) that allows a very effective solution. By contrast, previous margin-based formulations for sequence labeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> require an exponential number of constraints. For non-triangulated networks, we provide an approximate reformulation based on the relaxation used by belief propagation algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. Importantly, the resulting QP supports the same kernel trick as do SVMs, allowing probabilistic graphical models to inherit the important benefits of kernels. We also show a generalization bound for such margin-based classifiers. Unlike previous results <ref type="bibr" target="#b2">[3]</ref>, our bound grows logarithmically rather than linearly with the number of label variables. Our experimental results on character recognition and on hypertext classification, demonstrate dramatic improvements in accuracy over both kernel-based instance-by-instance classification and probabilistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structure in classification problems</head><p>In supervised classification, the task is to learn a function h : X → Y from a set of m i.i.d. instances S = {(x (i) , y (i) = t(x (i) ))} m i=1 , drawn from a fixed distribution D X ×Y . The classification function h is typically selected from some parametric family H. A common choice is the linear family: Given n real-valued basis functions f j : X × Y → IR, a hypothesis h w ∈ H is defined by a set of n coefficients w j such that:</p><formula xml:id="formula_0">h w (x) = arg max y n i=1 w j f j (x, y) = arg max y w f (x, y) ,<label>(1)</label></formula><p>where the f (x, y) are features or basis functions.</p><p>The most common classification setting -single-label classification -takes Y = {y 1 , . . . , y k }. In this paper, we consider the much more general setting of multi-label classification, where Y = Y 1 × . . . × Y l with Y i = {y 1 , . . . , y k }. In an OCR task, for example, each Y i is a character, while Y is a full word. In a webpage collective classification task <ref type="bibr" target="#b9">[10]</ref>, each Y i is a webpage label, whereas Y is a joint label for an entire website. In these cases, the number of possible assignments to Y is exponential in the number of labels l. Thus, both representing the basis functions f j (x, y) in (1) and computing the maximization arg max y are infeasible.</p><p>An alternative approach is based on the framework of probabilistic graphical models. In this case, the model defines (directly or indirectly) a conditional distribution P (Y | X ). We can then select the label arg max y P (y | x). The advantage of the probabilistic framework is that it can exploit sparseness in the correlations between labels Y i . For example, in the OCR task, we might use a Markov model, where Y i is conditionally independent of the rest of the labels given Y i-1 , Y i+1 . We can encode this structure using a Markov network. In this paper, purely for simplicity of presentation, we focus on the case of pairwise interactions between labels. We emphasize that our results extend easily to the general case. A pairwise Markov network is defined as a graph G = (Y, E), where each edge (i, j) is associated with a potential function ψ ij (x, y i , y j ). The network encodes a joint conditional probability distribution as P (y | x) ∝ (i,j)∈E ψ ij (x, y i , y j ). These networks exploit the interaction structure to parameterize a classifier very compactly. In many cases (e.g., tree-structured networks), we can use effective dynamic programming algorithms (such as the Viterbi algorithm) to find the highest probability label y; in others, we can use approximate inference algorithms that also exploit the structure <ref type="bibr" target="#b11">[12]</ref>.</p><p>The Markov network distribution is simply a log-linear model, with the pairwise potential ψ ij (x, y i , y j ) representing (in log-space) a sum of basis functions over x, y i , y j . We can therefore parameterize such a model using a set of pairwise basis functions f (x, y i , y j ) for (i, j) ∈ E. We assume for simplicity of notation that all edges in the graph denote the same type of interaction, so that we can define a set of features</p><formula xml:id="formula_1">f k (x, y) = (i,j)∈E f k (x, y i , y j ).<label>(2)</label></formula><p>The network potentials are then</p><formula xml:id="formula_2">ψ ij (x, y i , y j ) = exp [ n k=1 w k f k (x, y i , y j )] = exp w f (x, y i , y j ) .</formula><p>The parameters w in a log-linear model can be trained to fit the data, typically by maximizing the likelihood or conditional likelihood (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>). This paper presents an algorithm for selecting w that maximize the margin, gaining all of the advantages of SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Margin-based structured classification</head><p>For a single-label binary classification problem, support vector machines (SVMs) <ref type="bibr" target="#b10">[11]</ref> provide an effective method of learning a maximum-margin decision boundary. For singlelabel multi-class classification, Crammer and Singer <ref type="bibr" target="#b4">[5]</ref> provide a natural extension of this framework by maximizing the margin γ subject to constraints:</p><formula xml:id="formula_3">maximize γ s.t. ||w|| ≤ 1; w ∆f x (y) ≥ γ, ∀ x ∈ S, ∀y = t(x);<label>(3)</label></formula><p>where ∆f x (y) = f (x, t(x))f (x, y). The constraints in this formulation ensure that arg max y w f (x, y) = t(x). Maximizing γ magnifies the difference between the value of the true label and the best runner-up, increasing the "confidence" of the classification.</p><p>In structured problems, where we are predicting multiple labels, the loss function is usually not simple 0-1 loss I(arg max y w f x (y) = t(x)), but per-label loss, such as the proportion of incorrect labels predicted. In order to extend the margin-based framework to the multi-label setting, we must generalize the notion of margin to take into account the number of labels in y that are misclassified. In particular, we would like the margin between t(x) and y to scale linearly with the number of wrong labels in y, ∆t x (y):</p><formula xml:id="formula_4">maximize γ s.t. ||w|| ≤ 1; w ∆f x (y) ≥ γ ∆t x (y), ∀x ∈ S, ∀ y;<label>(4)</label></formula><p>where ∆t x (y) = l i=1 ∆t x (y i ) and ∆t x (y i ) ≡ I(y i = (t(x)) i ). Now, using a standard transformation to eliminate γ, we get a quadratic program (QP):</p><formula xml:id="formula_5">minimize 1 2 ||w|| 2 s.t. w ∆f x (y) ≥ ∆t x (y), ∀x ∈ S, ∀ y.<label>(5)</label></formula><p>Unfortunately, the data is often not separable by a hyperplane defined over the space of the given set of features. In such cases, we need to introduce slack variables ξ x to allow some constraints to be violated. We can now present the complete form of our optimization problem, as well as the equivalent dual problem <ref type="bibr" target="#b1">[2]</ref>: (Note: for each x, we add an extra dual variable α x (t(x)), with no effect on the solution.)</p><p>4 Exploiting structure in M 3 networks</p><p>Unfortunately, both the number of constraints in the primal QP in <ref type="bibr" target="#b5">(6)</ref>, and the number of variables in the dual QP in <ref type="bibr" target="#b6">(7)</ref> are exponential in the number of labels l. In this section, we present an equivalent, polynomially-sized, formulation.</p><p>Our main insight is that the variables α x (y) in the dual formulation <ref type="bibr" target="#b6">(7)</ref> can be interpreted as a density function over y conditional on x, as y α x (y) = C and α x (y) ≥ 0. The dual objective is a function of expectations of ∆t x (y) and ∆f x (y) with respect to α x (y). Since both ∆t x (y) = i ∆t x (y i ) and ∆f x (y) = (i,j) ∆f x (y i , y j ) are sums of functions over nodes and edges, we only need node and edge marginals of the measure α x (y) to compute their expectations. We define the marginal dual variables as follows:</p><formula xml:id="formula_6">µ x (y i , y j ) = y∼[yi,yj ] α x (y), ∀ (i, j) ∈ E, ∀y i , y j , ∀ x; µ x (y i ) = y∼[yi] α x (y), ∀ i, ∀y i , ∀ x;<label>(8)</label></formula><p>where y ∼ [y i , y j ] denotes a full assignment y consistent with partial assignment y i , y j . Now we can reformulate our entire QP <ref type="bibr" target="#b6">(7)</ref> in terms of these dual variables. Consider, for example, the first term in the objective function:</p><formula xml:id="formula_7">X y αx(y)∆tx(y) = X y X i αx(y)∆tx(yi) = X i,y i ∆tx(yi) X y∼[y i ] αx(y) = X i,y i µx(yi)∆tx(yi).</formula><p>The decomposition of the second term in the objective uses edge marginals µ x (y i , y j ).</p><p>In order to produce an equivalent QP, however, we must also ensure that the dual variables µ x (y i , y j ), µ x (y i ) are the marginals resulting from a legal density α(y); that is, that they belong to the marginal polytope <ref type="bibr" target="#b3">[4]</ref>. In particular, we must enforce consistency between the pairwise and singleton marginals (and hence between overlapping pairwise marginals): yi µ x (y i , y j ) = µ x (y j ), ∀y j , ∀(i, j) ∈ E, ∀x.</p><p>If the Markov network for our basis functions is a forest (singly connected), these constraints are equivalent to the requirement that the µ variables arise from a density. Therefore, the following factored dual QP is equivalent to the original dual QP:</p><formula xml:id="formula_9">max X x X i,y i µx(yi)∆tx(yi) - 1 2 X x,x X (i,j) y i ,y j X (r,s)</formula><p>yr ,ys µx(yi, yj)µ x(yr , ys)fx(yi, yj) f x(yr , ys);</p><formula xml:id="formula_10">s.t. X y i µx(yi, yj) = µx(yj); X y i µx(yi) = C; µx(yi, yj) ≥ 0.<label>(10)</label></formula><p>Similarly, the original primal can be factored as follows:</p><formula xml:id="formula_11">min 1 2 ||w|| 2 + C X x X i ξx,i + C X x X (i,j) ξx,ij; s.t. w ∆fx(yi, yj) + X (i ,j):i =i m x,i (yj) + X (j ,i):j =j m x,j (yi) ≥ -ξx,ij; X (i,j) mx,j(yi) ≥ ∆tx(yi) -ξx,i; ξx,ij ≥ 0, ξx,i ≥ 0.<label>(11)</label></formula><p>The solution to the factored dual gives us: w = x (i,j) yi,yj µ x (y i , y j )∆f x (y i , y j ).</p><p>Theorem 4.1 If for each x the edges E form a forest, then a set of weights w will be optimal for the QP in <ref type="bibr" target="#b5">(6)</ref> if and only if it is optimal for the factored QP in <ref type="bibr" target="#b10">(11)</ref>.</p><p>If the underlying Markov net is not a forest, then the constraints in <ref type="bibr" target="#b8">(9)</ref> are not sufficient to enforce the fact that the µ's are in the marginal polytope. We can address this problem by triangulating the graph, and introducing new η LP variables that now span larger subsets of Y i 's. For example, if our graph is a 4-cycle Y 1 -Y 2 -Y 3 -Y 4 -Y 1 , we might triangulate the graph by adding an arc Y 1 -Y 3 , and introducing η variables over joint instantiations of the cliques Y 1 , Y 2 , Y 3 and Y 1 , Y 3 , Y 4 . These new η variables are used in linear equalities that constrain the original µ variables to be consistent with a density. The η variables appear only in the constraints; they do not add any new basis functions nor change the objective function. The number of constraints introduced is exponential in the number of variables in the new cliques. Nevertheless, in many classification problems, such as sequences and other graphs with low tree-width <ref type="bibr" target="#b3">[4]</ref>, the extended QP can be solved efficiently.</p><p>Unfortunately, triangulation is not feasible in highly connected problems. However, we can still solve the QP in <ref type="bibr" target="#b9">(10)</ref> defined by an untriangulated graph with loops. Such a procedure, which enforces only local consistency of marginals, optimizes our objective only over a relaxation of the marginal polytope. In this way, our approximation is analogous to the approximate belief propagation (BP) algorithm for inference in graphical models <ref type="bibr" target="#b7">[8]</ref>. In fact, BP makes an additional approximation, using not only the relaxed marginal polytope but also an approximate objective (Bethe free-energy) <ref type="bibr" target="#b11">[12]</ref>. Although the approximate QP does not offer the theoretical guarantee in Theorem 4.1, the solutions are often very accurate in practice, as we demonstrate below.</p><p>As with SVMs <ref type="bibr" target="#b10">[11]</ref>, the factored dual formulation in <ref type="bibr" target="#b9">(10)</ref> uses only dot products between basis functions. This allows us to use a kernel to define very large (and even infinite) set of features. In particular, we define our basis functions by f x (y i , y j ) = ρ(y i , y j )φ ij (x), i.e., the product of a selector function ρ(y i , y j ) with a possibly infinite feature vector φ ij (x). For example, in the OCR task, ρ(y i , y j ) could be an indicator function over the class of two adjacent characters i and j, and φ ij (x) could be an RBF kernel on the images of these two characters. The operation f x (y i , y j ) f x(y r , y s ) used in the objective function of the factored dual QP is now ρ(y i , y j )ρ(y r , y s )K φ (x, i, j, x, r, s), where K φ (x, i, j, x, r, s) = φ ij (x) • φ rs (x) is the kernel function for the feature φ. Even for some very complex functions φ, the dot-product required to compute K φ can be executed efficiently <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SMO learning of M 3 networks</head><p>Although the number of variables and constraints in the factored dual in <ref type="bibr" target="#b9">(10)</ref> is polynomial in the size of the data, the number of coefficients in the quadratic term (kernel matrix) in the objective is quadratic in the number of examples and edges in the network. Unfortunately, this matrix is often too large for standard QP solvers. Instead, we use a coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs <ref type="bibr" target="#b8">[9]</ref>.</p><p>Let us begin by considering the original dual problem <ref type="bibr" target="#b6">(7)</ref>. The SMO approach solves this QP by analytically optimizing two-variable subproblems. Recall that y α x (y) = C. We can therefore take any two variables α x (y 1 ), α x (y 2 ) and "move weight" from one to the other, keeping the values of all other variables fixed. More precisely, we optimize for α x (y 1 ), α x (y 2 ) such that α x (y 1 ) + α x (y 2 ) = α x (y 1 ) + α x (y 2 ).</p><p>Clearly, however, we cannot perform this optimization in terms of the original dual, which is exponentially large. Fortunately, we can perform precisely the same optimization in terms of the marginal dual variables. Let λ = α x (y 1 )α x (y 1 ) = α x (y 2 )α x (y 2 ). Consider a dual variable µ x (y i , y j ). It is easy to see that a change from α x (y 1 ), α x (y 2 ) to α x (y 1 ), α x (y 2 ) has the following effect on µ x (y i , y j ):</p><formula xml:id="formula_12">µ x (y i , y j ) = µ x (y i , y j ) + λI(y i = y 1 i , y j = y 1 j ) -λI(y i = y 2 i , y j = y 2 j ). (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>We can solve the one-variable quadratic subproblem in λ analytically and update the appropriate µ variables. We use inference in the network to test for optimality of the current solution (the KKT conditions <ref type="bibr" target="#b1">[2]</ref>) and use violations from optimality as a heuristic to select the next pair y 1 , y 2 . We omit details for lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Generalization bound</head><p>In this section, we show a generalization bound for the task of multi-label classification that allows us to relate the error rate on the training set to the generalization error. As we shall see, this bound is significantly stronger than previous bounds for this problem.</p><p>Our goal in multi-label classification is to maximize the number of correctly classified labels. Thus an appropriate error function is the average per-label loss L(w, x) = 1 l ∆t x (arg max y w f x (y)). As in other generalization bounds for margin-based classifiers, we relate the generalization error to the margin of the classifier. In Sec. 3, we define the notion of per-label margin, which grows with the number of mistakes between the correct assignment and the best runner-up. We can now define a γ-margin per-label loss: L γ (w, x) = sup z: |z(y)-w fx(y)|≤γ∆tx(y); ∀y 1 l ∆t x (arg max y z(y)). This loss function measures the worst per-label loss on x made by any classifier z which is perturbed from w f x by at most a γ-margin per-label. We can now prove that the generalization accuracy of any classifier is bounded by its expected γ-margin per-label loss on the training data, plus a term that grows inversely with the margin.Intuitively, the first term corresponds to the "bias", as margin γ decreases the complexity of our hypothesis class by considering a γ-per-label margin ball around w f x and selecting one (the worst) classifier within this ball. As γ shrinks, our hypothesis class becomes more complex, and the first term becomes smaller, but at the cost of increasing the second term, which intuitively corresponds to the "variance". Thus, the result provides a bound to the generalization error that trades off the effective complexity of the hypothesis space with the training error. Theorem 6.1 If the edge features have bounded 2-norm, max (i,j),yi,yj f x (y i , y j ) 2 ≤ R edge , then for a family of hyperplanes parameterized by w, and any δ &gt; 0, there exists a constant K such that for any γ &gt; 0 per-label margin, and m &gt; 1 samples, the per-label loss is bounded by:</p><formula xml:id="formula_14">ExL(w, x) ≤ ESL γ (w, x) + v u u t K m " R 2 edge w 2 2 q 2 γ 2</formula><p>[ln m + ln l + ln q + ln k] + ln 1 δ # ;</p><p>with probability at least 1δ, where q = max i |{(i, j) ∈ E}| is the maximum edge degree in the network, k is the number of classes in a label, and l is the number of labels.</p><p>Unfortunately, we omit the proof due to lack of space. (See a longer version of the paper at http://cs.stanford.edu/˜btaskar/.) The proof uses a covering number argument analogous to previous results in SVMs <ref type="bibr" target="#b12">[13]</ref>. However we propose a novel method for covering structured problems by constructing a cover to the loss function from a cover of the individual edge basis function differences ∆f x (y i , y j ). This new type of cover is polynomial in the number of edges, yielding significant improvements in the bound. Specifically, our bound has a logarithmic dependence on the number of labels (ln l) and depends only on the 2-norm of the basis functions per-edge (R edge ). This is a significant gain over the previous result of Collins <ref type="bibr" target="#b2">[3]</ref> which has linear dependence on the number of labels (l), and depends on the joint 2-norm of all of the features (which is ∼ lR edge , unless each sequence is normalized separately, which is often ineffective in practice). Finally, note that if l m = O(1) (for example, in OCR, if the number of instances is at least a constant times the length of a word), then our bound is independent of the number of labels l. Such a result was, until now, an open problem for margin-based sequence classification <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluate our approach on two very different tasks: a sequence model for handwriting recognition and an arbitrary topology Markov network for hypertext classification. Handwriting Recognition. We selected a subset of ∼ 6100 handwritten words, with average length of ∼ 8 characters, from 150 human subjects, from the data set collected by Kassel <ref type="bibr" target="#b5">[6]</ref>. Each word was divided into characters, each character was rasterized into an image of 16 by 8 binary pixels. (See Fig. <ref type="figure" target="#fig_0">1(a)</ref>.) In our framework, the image for each word corresponds to x, a label of an individual character to Y i , and a labeling for a complete word to Y. Each label Y i takes values from one of 26 classes {a, . . . , z}.</p><p>The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples. The accuracy results, summarized in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, are averages over the 10 folds. We implemented a selection of state-of-the-art classification algorithms: independent label approaches, which do not consider the correlation between neighboring characters -logistic regression, multi-class SVMs as described in (3), and one-against-all SVMs (whose performance was slightly lower than multi-class SVMs); and sequence approaches -CRFs, and our proposed M 3 networks. Logistic regression and CRFs are both trained by maximizing the conditional likelihood of the labels given the features, using a zero-mean diagonal Gaussian prior over the parameters, with a standard deviation between 0.1 and 1. The other methods are trained by margin maximization. Our features for each label Y i are the corresponding image of ith character. For the sequence approaches (CRFs and M 3 ), we used an indicator basis function to represent the correlation between Y i and Y i+1 . For margin-based methods (SVMs and M 3 ), we were able to use kernels (both quadratic and cubic were evaluated) to increase the dimensionality of the feature space. Using these high-dimensional feature spaces in CRFs is not feasible because of the enormous number of parameters.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref>(b) shows two types of gains in accuracy: First, by using kernels, margin-based methods achieve a very significant gain over the respective likelihood maximizing methods. Second, by using sequences, we obtain another significant gain in accuracy. Interestingly, the error rate of our method using linear features is 16% lower than that of CRFs, and about the same as multi-class SVMs with cubic kernels. Once we use cubic kernels our error rate is 45% lower than CRFs and about 33% lower than the best previous approach. For comparison, the previously published results, although using a different setup (e.g., a larger training set), are about comparable to those of multiclass SVMs. Hypertext. We also tested our approach on collective hypertext classification, using the data set in <ref type="bibr" target="#b9">[10]</ref>, which contains web pages from four different Computer Science departments. Each page is labeled as one of course, faculty, student, project, other. In all of our experiments, we learn a model from three schools, and test on the remaining school. The text content of the web page and anchor text of incoming links is represented using a set of binary attributes that indicate the presence of different words. The baseline model is a simple linear multi-class SVM that uses only words to predict the category of the page. The second model is a relational Markov network (RMN) of Taskar et al. <ref type="bibr" target="#b9">[10]</ref>, which in addition to word-label dependence, has an edge with a potential over the labels of two pages that are hyper-linked to each other. This model defines a Markov network over each web site that was trained to maximize the conditional probability of the labels given the words and the links. The third model is a M 3 net with the same features but trained by maximizing the margin using the relaxed dual formulation and loopy BP for inference.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref>(c) shows a gain in accuracy from SVMs to RMNs by using the correlations between labels of linked web pages, and a very significant additional gain by using maximum margin training. The error rate of M3Ns is 40% lower than that of RMNs, and 51% lower than multi-class SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We present a discriminative framework for labeling and segmentation of structured data such as sequences, images, etc. Our approach seamlessly integrates state-of-the-art kernel methods developed for classification of independent instances with the rich language of graphical models that can exploit the structure of complex data. In our experiments with the OCR task, for example, our sequence model significantly outperforms other approaches by incorporating high-dimensional decision boundaries of polynomial kernels over character images while capturing correlations between consecutive characters. We construct our models by solving a convex quadratic program that maximizes the per-label margin. Although the number of variables and constraints of our QP formulation is polynomial in the example size (e.g., sequence length), we also address its quadratic growth using an effective optimization procedure inspired by SMO. We provide theoretical guarantees on the average per-label generalization error of our models in terms of the training set margin. Our generalization bound significantly tightens previous results of Collins <ref type="bibr" target="#b2">[3]</ref> and suggests possibilities for analyzing per-label generalization properties of graphical models.</p><p>For brevity, we simplified our presentation of graphical models to only pairwise Markov networks. Our formulation and generalization bound easily extend to interaction patterns involving more than two labels (e.g., higher-order Markov models). Overall, we believe that M 3 networks will significantly further the applicability of high accuracy margin-based methods to real-world structured data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) 3 example words from the OCR data set; (b) OCR: Average per-character test error for logistic regression, CRFs, multiclass SVMs, and M 3 Ns, using linear, quadratic, and cubic kernels; (c) Hypertext: Test error for multiclass SVMs, RMNs and M 3 Ns, by school and average.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work was supported by ONR Contract F3060-01-2-0564-P00002 under DARPA's EELD program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hidden markov support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Athena Scientific, Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IWPT</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probabilistic Networks and Expert Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernelbased vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Comparison of Approaches to On-line Handwritten Character Recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kassel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>MIT Spoken Language Systems Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML01</title>
		<meeting>ICML01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using sparseness and analytic QP to speed training of support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative probabilistic models for relational data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI02</title>
		<meeting>UAI02<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized belief propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Covering number bounds of certain regularized linear function classes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="527" to="550" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
