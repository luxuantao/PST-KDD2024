<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constructing Taxonomies from Pretrained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Catherine</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
							<email>k-lin@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constructing Taxonomies from Pretrained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for constructing taxonomic trees (e.g., WORDNET) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WORDNET, and test on nonoverlapping WORDNET subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WORDNET, the model achieves 66.7 ancestor F 1 , a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using OPEN MULTILINGUAL WORDNET and extend our results across these languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A variety of NLP tasks use taxonomic information, including question answering <ref type="bibr" target="#b22">(Miller, 1998)</ref> and information retrieval <ref type="bibr" target="#b38">(Yang and Wu, 2012)</ref>. Taxonomies are also used as a resource for building knowledge and systematicity into neural models <ref type="bibr" target="#b25">(Peters et al., 2019;</ref><ref type="bibr" target="#b12">Geiger et al., 2020;</ref><ref type="bibr" target="#b33">Talmor et al., 2020)</ref>. NLP systems often retrieve taxonomic information from lexical databases such as WORD-NET <ref type="bibr" target="#b22">(Miller, 1998)</ref>, which consists of taxonomies that contain semantic relations across many domains. While manually curated taxonomies provide useful information, they are incomplete and expensive to maintain <ref type="bibr" target="#b14">(Hovy et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* indicates equal contribution</head><p>Traditionally, methods for automatic taxonomy construction have relied on statistics of web-scale corpora. These models generally apply lexicosyntactic patterns <ref type="bibr" target="#b13">(Hearst, 1992)</ref> to large corpora, and use corpus statistics to construct taxonomic trees (e.g., <ref type="bibr" target="#b32">Snow et al., 2005;</ref><ref type="bibr" target="#b17">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b20">Mao et al., 2018;</ref><ref type="bibr" target="#b31">Shang et al., 2020)</ref>. In this work, we propose an approach that constructs taxonomic trees using pretrained language models (CTP). Our results show that direct access to corpus statistics at test time is not necessary. Indeed, the re-representation latent in largescale models of such corpora can be beneficial in constructing taxonomies. We focus on the task proposed by <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>, where the task is to organize a set of input terms into a taxonomic tree. We convert this dataset into nine other languages using synset alignments collected in OPEN MULTI-LINGUAL WORDNET and evaluate our approach in these languages.</p><p>CTP first finetunes pretrained language models to predict the likelihood of pairwise parentchild relations, producing a graph of parenthood scores. Then it reconciles these predictions with a maximum spanning tree algorithm, creating a tree-structured taxonomy. We further test CTP in a setting where models have access to web-retrieved glosses. We reorder the glosses and finetune the model on the reordered glosses in the parenthood prediction module.</p><p>We compare model performance on subtrees across semantic categories and subtree depth, provide examples of taxonomic ambiguities, describe conditions for which retrieved glosses produce greater increases in tree construction F 1 score, and evaluate generalization to large taxonomic trees <ref type="bibr" target="#b5">(Bordea et al., 2016a)</ref>. These analyses suggest specific avenues of future improvements to automatic taxonomy construction.</p><p>Even without glosses, CTP achieves a 7.9 point absolute improvement in F 1 score on the task of constructing WORDNET subtrees, compared to previous work. When given access to the glosses, CTP obtains an additional 3.2 point absolute improvement in F 1 score. Overall, the best model achieves a 11.1 point absolute increase (a 20.0% relative increase) in F 1 score over the previous best published results on this task.</p><p>Our paper is structured as follows. In Section 2 we describe CTP, our approach for taxonomy construction. In Section 3 we describe the experimental setup, and in Section 4 we present the results for various languages, pretrained models, and glosses. In Section 5 we analyze our approach and suggest specific avenues for future improvement. We discuss related work and conclude in Sections 6 and 7.</p><p>2 Constructing Taxonomies from Pretrained Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Taxonomy Construction</head><p>We define taxonomy construction as the task of creating a tree-structured hierarchy T = (V, E), where V is a set of terms and E is a set of directed edges representing hypernym relations. In this task, the model receives a set of terms V , where each term can be a single word or a short phrase, and it must construct the tree T given these terms. CTP performs taxonomy construction in two steps: parenthood prediction (Section 2.2) followed by graph reconciliation (Section 2.3). We provide a schematic description of CTP in Figure <ref type="figure">2</ref> and provide details in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parenthood Prediction</head><p>We use pretrained models (e.g., BERT) to predict the edge indicators I[parent(v i , v j )], which denote whether v i is a parent of v j , for all pairs (v i , v j ) in the set of terms V = {v 1 , ..., v n } for each subtree T .</p><p>To generate training data from a tree T with n nodes, we create a positive training example for each of the n − 1 parenthood edges and a negative training example for each of the n(n−<ref type="foot" target="#foot_0">1</ref>) 2 − (n − 1) pairs of nodes that are not connected by a parenthood edge.</p><p>We construct an input for each example using the template v i is a v j , e.g., "A dog is a mammal." Different templates (e.g., [TERM_A] is an example of [TERM_B] or [TERM_A] is a type of [TERM_B]) did not substantially affect model performance in initial experiments, so we use a single template. The inputs and outputs are modeled in the standard format <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>.</p><p>We fine-tune pretrained models to predict I[parent(v i , v j )], which indicates whether v i is the parent of v j , for each pair of terms using a sentencelevel classification task on the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree Reconciliation</head><p>We then reconcile the parenthood graph into a valid tree-structured taxonomy. We apply the Chu-Liu-Edmonds algorithm to the graph of pairwise parenthood predictions. This algorithm finds the maximum weight spanning arborescence of a directed graph. It is the analog of MST for directed graphs, and finds the highest scoring arborescence in O(n 2 ) time <ref type="bibr" target="#b10">(Chu, 1965)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Web-Retrieved Glosses</head><p>We perform experiments in two settings: with and without web-retrieved glosses. In the setting without glosses, the model performs taxonomy construction using only the set of terms V . In the setting with glosses, the model is provided with glosses retrieved from the web. For settings in which the model receives glosses, we retrieve a list of glosses d 1 v , ..., d n v for each term v ∈ V . 1 Many of the terms in our dataset are polysemous, and the glosses contain multiple senses of the word. For example, the term dish appears in the subtree we show in Figure <ref type="figure" target="#fig_0">1</ref>. The glosses for dish include (1) (telecommunications) A type of antenna with Figure <ref type="figure">2</ref>: A schematic depiction of CTP. We start with a set of terms (A). We fine-tune a pretrained language model to predict pairwise parenthood relations between pairs of terms (B), creating a graph of parenthood predictions (C) (Section 2.2). We then reconcile the edges of this graph into a taxonomic tree (E) (Section 2.3). Optionally, we provide the model ranked web-retrieved glosses (Section 2.4). We re-order the glosses based on relevance to the current subtree (Z).</p><p>a similar shape to a plate or bowl, (2) (metonymically) A specific type of prepared food, and (3) (mining) A trough in which ore is measured.</p><p>We reorder the glosses based on their relevance to the current subtree. We define relevance of a given context d i v to subtree T as the cosine similarity between the average of the GloVe embeddings <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref> of the words in d i v (with stopwords removed), to the average of the GloVe embeddings of all terms v 1 , ..., v n in the subtree. This produces a reordered list of glosses d</p><formula xml:id="formula_0">(1) v , ..., d (n) v .</formula><p>We then use the input sequence containing the reordered glosses "</p><formula xml:id="formula_1">[CLS] v i d (1) v i , ..., d (n) v i . [SEP] v j d (1) v j , ..., d<label>(n)</label></formula><p>v j " to fine-tune the pretrained models on pairs of terms (v i , v j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we describe the details of our datasets (Section 3.1), and describe our evaluation metrics (Section 3.2). We ran our experiments on a cluster with 10 Quadro RTX 6000 GPUs. Each training runs finishes within one day on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate CTP using the dataset of mediumsized WORDNET subtrees created by <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>. This dataset consists of bottomed-out full subtrees of height 3 (this corresponds to trees containing 4 nodes in the longest path from the root to any leaf) that contain between 10 and 50 terms. This dataset comprises 761 English trees, with 533/114/114 train/dev/test trees respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multilingual WORDNET</head><p>WORDNET was originally constructed in English, and has since been extended to many other languages such as Finnish <ref type="bibr" target="#b19">(Magnini et al., 1994)</ref>, Italian <ref type="bibr" target="#b18">(Lindén and Niemi, 2014)</ref>, and Chinese <ref type="bibr" target="#b36">(Wang and Bond, 2013)</ref>. Researchers have provided alignments from synsets in English WORD-NET to terms in other languages, using a mix of automatic and manual methods (e.g., <ref type="bibr" target="#b19">Magnini et al., 1994;</ref><ref type="bibr" target="#b18">Lindén and Niemi, 2014)</ref>. These multilingual wordnets are collected in the OPEN MULTILIN-GUAL WORDNET project <ref type="bibr" target="#b4">(Bond and Paik, 2012)</ref>. The coverage of synset alignments varies widely. For instance, the alignment of ALBANET (Albanian) to English WORDNET covers 3.6% of the synsets in the <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> dataset, while the FINNWORDNET (Finnish) alignment covers 99.6% of the synsets in the dataset.</p><p>We convert the original English dataset to nine other languages using the synset alignments. (We create datasets for Catalan <ref type="bibr" target="#b0">(Agirre et al., 2011)</ref>, Chinese <ref type="bibr" target="#b36">(Wang and Bond, 2013)</ref>, Finnish <ref type="bibr" target="#b18">(Lindén and Niemi, 2014)</ref>, French <ref type="bibr" target="#b30">(Sagot, 2008)</ref>, Italian <ref type="bibr" target="#b19">(Magnini et al., 1994)</ref>, Dutch <ref type="bibr" target="#b28">(Postma et al., 2016)</ref>, Polish <ref type="bibr" target="#b27">(Piasecki et al., 2009)</ref>, Portuguese (de Paiva and Rademaker, 2012), and Spanish <ref type="bibr" target="#b0">(Agirre et al., 2011)</ref>).</p><p>Since these wordnets do not include alignments to all of the synsets in the English dataset, we convert the English dataset to each target language using alignments specified in WORDNET as follows.</p><p>We first exclude all subtrees whose roots are not included in the alignment between the WORDNET of the target language and English WORDNET. For each remaining subtree, we remove any node that is not included in the alignment. Then we remove all remaining nodes that are no longer connected to the root of the corresponding subtrees. We describe the resulting dataset statistics in Table <ref type="table" target="#tab_6">8</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>As with previous work <ref type="bibr" target="#b1">(Bansal et al., 2014;</ref><ref type="bibr" target="#b20">Mao et al., 2018)</ref>, we report the ancestor F 1 score 2P R P +R , where</p><formula xml:id="formula_2">P = |IS_A PREDICTED ∩ IS_A GOLD | |IS_A PREDICTED | R = |IS_A PREDICTED ∩ IS_A GOLD | |IS_A GOLD |</formula><p>IS_A PREDICTED and IS_A GOLD denote the set of predicted and gold ancestor relations, respectively. We report the mean precision (P ), recall (R), and F 1 score, averaged across the subtrees in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Models</head><p>In our experiments, we use pretrained models from the Huggingface library <ref type="bibr" target="#b37">(Wolf et al., 2019)</ref>. For the English dataset we experiment with BERT, BERT-Large, and ROBERTA-Large in the parenthood prediction module. We experiment with multilingual BERT and language-specific pretrained models (detailed in Section 9 in the Appendix). We finetuned each model using three learning rates {1e-5, 1e-6, 1e-7}. For each model, we ran three trials using the learning rate that achieved the highest dev F 1 score. In Section 4, we report the average scores over three trials. We include full results in Tables <ref type="table" target="#tab_2">13 and 15</ref> in the Appendix. The code and datasets are available at https://github.com/cchen23/ctp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>Our approach, CTP, outperforms existing stateof-the-art models on the WORDNET subtree construction task. In Table <ref type="table">1</ref> we provide a comparison of our results to previous work. Even without retrieved glosses, CTP with ROBERTA-LARGE in the parenthood prediction module achieves higher F 1 than previously published work. CTP achieves additional improvements when provided with the web-retrieved glosses described in Section 2.4.</p><p>We compare different pretrained models for the parenthood prediction module, and provide these comparisons in Section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web-Retrieved Glosses</head><p>In Table <ref type="table">2</ref> we show the improvement in taxonomy construction with two types of glosses -glosses retrieved from the web (as described in Section 2.4), and those obtained directly from WORDNET. We consider using the glosses from WORDNET as an oracle setting since these glosses are directly generated from the gold taxonomies. Thus, we focus on the web-retrieved glosses as the main setting. Models produce additional improvements when given WORDNET glosses. These improvements suggest that reducing the noise from web-retrieved glosses could improve automated taxonomy construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison of Pretrained Models</head><p>For both settings (with and without web-retrieved glosses), CTP attains the highest F 1 score when ROBERTA-Large is used in the parenthood prediction step. As we show in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Aligned Wordnets</head><p>We extend our results to the nine non-English alignments to the <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> dataset that we created. In Table <ref type="table" target="#tab_3">4</ref> we compare our best model in each language to a random baseline. We detail the random baseline in Section 9 in the Appendix and provide results from all tested models in Section 17 in the Appendix. CTP's F 1 score non-English languages is substantially worse than its F 1 score on English trees. Lower F 1 scores in non-English languages are likely due to multiple factors. First, English pretrained language models generally perform better than models in other languages because of the additional resources devoted to the development of English models. (See e.g., <ref type="bibr" target="#b2">Bender, 2011;</ref><ref type="bibr" target="#b21">Mielke, 2016;</ref><ref type="bibr" target="#b16">Joshi et al., 2020)</ref>. Second, OPEN MULTI-LINGUAL WORDNET aligns wordnets to English WORDNET, but the subtrees contained in English WORDNET might not be the natural taxonomy in other languages. However, we note that scores across languages are not directly comparable as dataset size and coverage vary across languages (as we show in Table <ref type="table" target="#tab_6">8</ref>).</p><p>These results highlight the importance of evaluating on non-English languages, and the difference in available lexical resources between languages. Furthermore, they provide strong baselines for fu- ture work in constructing wordnets in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section we analyze the models both quantitatively and qualitatively. Unless stated otherwise, we analyze our model on the dev set and use ROBERTA-Large in the parenthood prediction step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Models Predict Flatter Trees</head><p>In many error cases, CTP predicts a tree with edges that connect terms to their non-parent ancestors, skipping the direct parents. We show an example of this error in Figure <ref type="figure" target="#fig_1">3</ref>. In this fragment (taken from one of the subtrees in the dev set), the model predicts a tree in which botfly and horsefly are direct children of fly, bypassing the correct parent gadfly. On the dev set, 38.8% of incorrect parenthood edges were cases of this type of error. Missing edges result in predicted trees that are generally flatter than the gold tree. While all the gold trees have a height of 3 (4 nodes in the longest path from the root to any leaf), the predicted dev trees have a mean height of 2.61. Our approach scores the edges independently, without considering the structure of the tree beyond local parenthood edges. One potential way to address the bias towards flat trees is to also model the global structure of the tree (e.g., ancestor and sibling relations). CTP generally makes more errors in predicting edges involving nodes that are farther from the root of each subtree. In Table <ref type="table">5</ref> we show the recall of ancestor edges, categorized by the number of parent edges d between the subtree root and the descendant of each edge, and the number of parent edges l between the ancestor and descendant of each edge. The model has lower recall for edges involving descendants that are farther from the root (higher d). In permutation tests of the correlation between edge recall and d conditioned on l, 0 out of 100,000 permutations yielded a correlation at least as extreme as the observed correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Struggle Near Leaf Nodes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subtrees Higher Up in WORDNET are</head><p>Harder, and Physical Entities are Easier than Abstractions</p><p>Subtree performance also corresponds to the depth of the subtree in the entire WORDNET hierarchy.</p><p>The F 1 score is positively correlated with the depth of the subtree in the full WORDNET hierarchy, with a correlation of 0.27 (significant at p=0.004 using a permutation test with 100,000 permutations).</p><p>The subtrees included in this task span many different domains, and can be broadly categorized into subtrees representing concrete entities (such as telephone) and those representing abstractions (such as sympathy). WORD-NET provides this categorization using the toplevel synsets physical_entity.n.01 and abstraction.n.06. These categories are direct children of the root of the full WORDNET hierarchy (entity.n.01), and split almost all WORDNET terms into two subsets. The model produces a mean F 1 score of 60.5 on subtrees in the abstraction subsection of WORDNET, and a mean F 1 score of 68.9 on subtrees in the physical_entity subsection. A one-sided Mann-Whitney rank test shows that the model performs systematically worse on abstraction subtrees (compared to physical entity subtrees) (p=0.01). With models pretrained on large web corpora, the distinction between the settings with and without access to the web at test time is less clear, since large pretrained models can be viewed as a com-pressed version of the web. To quantify the extent the evaluation setting measures model capability to generalize to taxonomies consisting of unseen words, we count the number of times each term in the WORDNET dataset occurs in the pretraining corpus. We note that the WORDNET glosses do not directly appear in the pretraining corpus. In Figure <ref type="figure" target="#fig_3">4</ref> we show the distribution of the frequency with which the terms in the <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> dataset occur in the BERT pretraining corpus. <ref type="foot" target="#foot_2">2</ref> We find that over 97% of the terms occur at least once in the pretraining corpus. However, the majority of the terms are not very common words, with over 80% of terms occurring less than 50k times. While this shows that the current setting does not measure model ability to generalize to completely unseen terms, we find that the model does not perform substantially worse on edges that contain terms that do not appear in the pretraining corpus. Furthermore, the model is able do well on rare terms. Future work can investigate model ability to construct taxonomies from terms that are not covered in pretraining corpora. Some trees in the gold WORDNET hierarchy contain ambiguous edges. Figure <ref type="figure" target="#fig_4">5</ref> shows one example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Pretraining Corpus Covers Most Terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">WORDNET Contains Ambiguous Subtrees</head><p>In this subtree, the model predicts arteriography as a sibling of arthrography rather than as its child. The definitions of these two terms suggest why the model may have considered these terms as siblings: arteriograms produce images of arteries while arthrograms produce images of the inside of joints. In Figure <ref type="figure" target="#fig_5">6</ref> we show a second example of an ambiguous tree. The model predicts good faith as a child of sincerity rather than as a child of honesty, but the correct hypernymy relation between these terms is unclear to the authors, even after referencing multiple dictionaries.</p><p>These examples point to the potential of augmenting or improving the relations listed in WORD-NET using semi-automatic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Web-Retrieved Glosses Are Beneficial When They Contain Lexical Overlap</head><p>We compare the predictions of ROBERTA-Large, with and without web glosses, to understand what kind of glosses help. We split the parenthood edges in the gold trees into two groups based on the glosses: (1) lexical overlap (the parent term appears in the child gloss and/or the child term appears in the parent gloss) and ( <ref type="formula">2</ref>) no lexical overlap (neither the parent term nor the child term appears in the other term's gloss). We find that for edges in the "lexical overlap" group, glosses increase the recall of the gold edges from 60.9 to 67.7. For edges in the "no lexical overlap" group, retrieval decreases the recall (edge recall changes from 32.1 to 27.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Pretraining and Tree Reconciliation Both Contribute to Taxonomy Construction</head><p>We performed an ablation study in which we ablated either the pretrained language models for the parenthood prediction step or we ablated the tree reconciliation step. We ablated the pretrained language models in two ways. First, we used a onelayer LSTM on top of GloVe vectors instead of a pretrained language model as the input to the finetuning step, and then performed tree reconciliation as before. Second, we used a randomly initialized ROBERTA-Large model in place of a pretrained network, and then performed tree reconciliation as before. We ablated the tree reconciliation step by substituting the graph-based reconciliation step with a simpler threshold step, where we output a parenthood-relation between all pairs of words with softmax score greater than 0.5. We used the parenthood prediction scores from the fine-tuned ROBERTA-Large model, and substituted tree reconciliation with thresholding.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, we show the results of our ablation experiments. These results show that both steps (using pretrained language models for parenthoodprediction and performing tree reconciliation) are  important for taxonomy construction. Moreover, these results show that the incorporation of a new information source (knowledge learned by pretrained language models) produces the majority of the performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Models Struggle to Generalize to Large Taxonomies</head><p>To test generalization to large subtrees, we tested our models on the English environment and science taxonomies from SemEval-2016 Task 13 <ref type="bibr" target="#b5">(Bordea et al., 2016a)</ref>. Each of these taxonomies consists of a single large taxonomic tree with between 125 and 452 terms. Following <ref type="bibr" target="#b20">Mao et al. (2018)</ref> and <ref type="bibr" target="#b31">Shang et al. (2020)</ref>, we used the medium-sized trees from <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> to train our models. During training, we excluded all medium-sized trees from the <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> dataset that overlapped with the terms in the SemEval-2016 Task 13 environment and science taxonomies.</p><p>In Table <ref type="table">7</ref> we show the performance of the ROBERTA-Large CTP model. We show the Edge-F1 score rather than the Ancestor-F1 score in order to compare to previous work. Although the CTP model outperforms previous work in constructing medium-sized taxonomies, this model is limited in its ability to generalize to large taxonomies. Future work can incorporate modeling of the global tree structure into CTP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Taxonomy induction has been studied extensively, with both pattern-based and distributional approaches. Typically, taxonomy induction involves hypernym detection, the task of extracting candidate terms from corpora, and hypernym organization, the task of organizing the terms into a hierarchy.</p><p>While we focus on hypernym organization, many systems have studied the related task of hypernym detection. Traditionally, systems have used patternbased features such as Hearst patterns to infer hypernym relations from large corpora (e.g. <ref type="bibr" target="#b13">Hearst, 1992;</ref><ref type="bibr" target="#b32">Snow et al., 2005;</ref><ref type="bibr" target="#b17">Kozareva and Hovy, 2010)</ref>. For example, <ref type="bibr" target="#b32">Snow et al. (2005)</ref> propose a system that extracts pattern-based features from a corpus to predict hypernymy relations between terms. <ref type="bibr" target="#b17">Kozareva and Hovy (2010)</ref> propose a system that similarly uses pattern-based features to predict hypernymy relations, in addition to harvesting relevant terms and using a graph-based longest-path approach to construct a legal taxonomic tree.</p><p>Later work suggests that, for hypernymy detection tasks, pattern-based approaches outperform those based on distributional models <ref type="bibr" target="#b29">(Roller et al., 2018)</ref>. Subsequent work pointed out the sparsity that exists in pattern-based features derived from corpora, and showed that combining distributional and pattern-based approaches can improve hypernymy detection by addressing this problem <ref type="bibr" target="#b39">(Yu et al., 2020)</ref>.</p><p>In this work we consider the task of organizing a set of terms into a medium-sized taxonomic tree. <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>   <ref type="bibr" target="#b20">Mao et al. (2018)</ref> 37.9 37.9 37.9 <ref type="bibr" target="#b31">Shang et al. (2020)</ref>  rate siblinghood information. <ref type="bibr" target="#b20">Mao et al. (2018)</ref> propose a reinforcement learning based approach that combines the stages of hypernym detection and hypernym organization. In addition to the task of constructing medium-sized WORDNET subtrees, they show that their approach can leverage global structure to construct much larger taxonomies from the SemEval-2016 Task 13 benchmark dataset, which contain hundreds of terms <ref type="bibr" target="#b6">(Bordea et al., 2016b)</ref>. <ref type="bibr" target="#b31">Shang et al. (2020)</ref> apply graph neural networks and show that they improve performance in constructing large taxonomies in the SemEval-2016 Task 13 dataset.</p><p>Another relevant line of work involves extracting structured declarative knowledge from pretrained language models. For instance, <ref type="bibr" target="#b8">Bouraoui et al. (2019)</ref> showed that a wide range of relations can be extracted from pretrained language models such as BERT. Our work differs in that we consider tree structures and incorporate web glosses. <ref type="bibr" target="#b7">Bosselut et al. (2019)</ref> use pretrained models to generate explicit open-text descriptions of commonsense knowledge. Other work has focused on extracting knowledge of relations between entities <ref type="bibr" target="#b26">(Petroni et al., 2019;</ref><ref type="bibr" target="#b15">Jiang et al., 2020)</ref>. Blevins and Zettlemoyer (2020) use a similar approach to ours for word sense disambiguation, and encode glosses with pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Our experiments show that pretrained language models can be used to construct taxonomic trees. Importantly, the knowledge encoded in these pretrained language models can be used to construct taxonomies without additional web-based information. This approach produces subtrees with higher mean F 1 scores than previous approaches, which used information from web queries.</p><p>When given web-retrieved glosses, pretrained language models can produce improved taxonomic trees. The gain from accessing web glosses shows that incorporating both implicit knowledge of input terms and explicit textual descriptions of knowledge is a promising way to extract relational knowledge from pretrained models. Error analyses suggest specific avenues of future work, such as improving predictions for subtrees corresponding to abstractions, or explicitly modeling the global structure of the subtrees.</p><p>Experiments on aligned multilingual WORD-NET datasets emphasize that more work is needed in investigating the differences between taxonomic relations in different languages, and in improving pretrained language models in non-English languages. Our results provide strong baselines for future work on constructing taxonomies for different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Considerations</head><p>While taxonomies (e.g., WORDNET) are often used as ground-truth data, they have been shown to contain offensive and discriminatory content (e.g., <ref type="bibr" target="#b9">Broughton, 2019)</ref>. Automatic systems created by pretrained language models can reflect and exacerbate the biases contained by their training corpora. More work is needed to detect and combat biases that arise when constructing and evaluating taxonomies.</p><p>Furthermore, we used previously constructed alignments to extend our results to wordnets in multiple languages. While considering English WORDNET as the basis for the alignments allows for convenient comparisons between languages and is the standard method for aligning wordnets across languages, continued use of these alignments to evaluate taxonomy construction imparts undue bias towards conceptual relations found in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual WORDNET Dataset Statistics</head><p>Table <ref type="table" target="#tab_6">8</ref> details the datasets we created by using synset alignments to the English dataset proposed in <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>. The data construction method is described in Section 3.1. For each language, we show the number of train, dev, and test subtrees that remain after the subsetting procedure described in Section 3.1.1. In addition, we show the mean number of nodes per tree in each language. We use ISO 639-1 language acronyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Num</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Results</head><p>Table <ref type="table" target="#tab_7">9</ref> shows the results for the learning rate trials for the ablation experiment.  Table <ref type="table" target="#tab_10">11</ref> shows the results for the test trials for the SemEval experiment. These results all use the ROBERTA-Large model in the parenthood prediction step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Baseline for Multilingual WORDNET Datasets</head><p>To compute the random baseline in each language, we randomly construct a tree containing the nodes in each test tree and compute the ancestor precision, recall and F 1 score on the randomly constructed</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example subtree from the WORDNET hierarchy.</figDesc><graphic url="image-1.png" coords="1,306.14,332.58,218.27,108.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A fragment of a subtree from the WORD-NET hierarchy. Orange indicates incorrectly predicted edges and blue indicates missed edges.</figDesc><graphic url="image-3.png" coords="6,77.14,70.87,202.99,136.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Ancestor Edge Recall, Categorized by Descendant Node Depth d and Parent Edge Length l. Ancestor edge prediction recall decreases with deeper descendant nodes and closer ancestor-descendant relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Frequency of terms in the WORDNET dataset in the pretraining corpus. Over 97% of terms in the Bansal et al. (2014) dataset occur at least once in the pretraining corpus. Over 80% of terms occur less than 50k times.</figDesc><graphic url="image-4.png" coords="6,312.42,496.38,202.99,152.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A fragment of a subtree from the WORD-NET hierarchy. Orange indicates incorrectly predicted edges and blue indicates edges that were missed.</figDesc><graphic url="image-5.png" coords="7,91.33,432.21,174.61,145.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A fragment of a subtree from the WORDNET hierarchy. Orange indicates incorrectly predicted edges and blue indicates edges that were missed.</figDesc><graphic url="image-6.png" coords="8,70.87,70.87,453.54,128.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Generalization to large taxonomic trees. Models trained on medium-sized taxonomies generalize poorly to large taxonomies. Future work can improve the usage of global tree structure with CTP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="3,92.18,70.87,408.18,214.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>CTP</cell><cell cols="3">67.3 62.0 63.5</cell></row><row><cell>+ web glosses</cell><cell cols="3">69.3 66.2 66.7</cell></row><row><cell cols="4">+ oracle glosses 84.0 83.8 83.2</cell></row><row><cell cols="4">Table 2: English Results, Gloss Comparison on Test</cell></row><row><cell cols="4">Set. Adding web glosses improves performance over</cell></row><row><cell cols="4">only using input terms. Models achieve additional im-</cell></row><row><cell cols="4">provements in subtree reconstruction when given ora-</cell></row><row><cell cols="4">cle glosses from WORDNET, showing possibilities for</cell></row><row><cell cols="3">improvement in retrieving web glosses.</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>CTP (BERT-Base)</cell><cell cols="3">57.9 51.8 53.4</cell></row><row><cell>CTP (BERT-Large)</cell><cell cols="3">65.5 59.8 61.4</cell></row><row><cell cols="4">CTP (ROBERTA-Large) 67.3 62.0 63.5</cell></row></table><note>, the average F 1 score improves with both increased model size and with switching from BERT to ROBERTA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>English Results, Comparison of Pretrained Models on Test Set. Larger models perform better and ROBERTA outperforms BERT.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Multilingual WORDNET Test Results. We extend our model to datasets in nine other languages, and evaluate our approach on these datasets. We use ISO 639-1 acronyms to indicate languages.</figDesc><table><row><cell></cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>ca</cell><cell>Random Baseline CTP (MBERT)</cell><cell cols="3">20.0 31.3 23.6 38.7 39.7 38.0</cell></row><row><cell>zh</cell><cell cols="4">Random Baseline CTP (CHINESE BERT) 62.2 57.3 58.7 25.8 35.9 29.0</cell></row><row><cell>en</cell><cell cols="4">Random Baseline CTP (ROBERTA-Large) 67.3 62.0 63.5 8.9 22.2 12.4</cell></row><row><cell>fi</cell><cell>Random Baseline CTP (FINBERT)</cell><cell cols="3">10.1 22.5 13.5 47.9 42.6 43.8</cell></row><row><cell>fr</cell><cell>Random Baseline CTP (FRENCH BERT)</cell><cell cols="3">22.1 34.4 25.9 51.3 49.1 49.1</cell></row><row><cell>it</cell><cell>Random Baseline CTP (ITALIAN BERT)</cell><cell cols="3">28.9 39.4 32.3 48.3 45.5 46.1</cell></row><row><cell>nl</cell><cell>Random Baseline CTP (BERTJE)</cell><cell cols="3">26.8 38.4 30.6 44.6 44.8 43.7</cell></row><row><cell>pl</cell><cell>Random Baseline CTP (POLBERT)</cell><cell cols="3">23.4 33.6 26.8 51.9 49.7 49.5</cell></row><row><cell>pt</cell><cell>Random Baseline CTP (BERTIMBAU)</cell><cell cols="3">26.1 37.6 29.8 59.3 57.1 56.9</cell></row><row><cell>es</cell><cell>Random Baseline CTP (BETO)</cell><cell cols="3">27.0 37.2 30.5 53.1 51.7 51.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Ablation study. Pretraining and tree reconciliation both contribute to taxonomy construction.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>treat this as a structured learning problem and use belief propagation to incorpo-</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>CTP</cell><cell cols="3">29.4 28.8 29.1</cell></row><row><cell>Science (Averaged)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Dataset Statisics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Trees</cell><cell></cell><cell cols="3">Nodes per Tree</cell></row><row><cell></cell><cell cols="6">Train Dev Test Train Dev Test</cell></row><row><cell>ca</cell><cell>391</cell><cell>94</cell><cell>90</cell><cell>9.2</cell><cell>9.3</cell><cell>8.7</cell></row><row><cell>zh</cell><cell>216</cell><cell>48</cell><cell cols="3">64 10.0 12.4</cell><cell>9.2</cell></row><row><cell>en</cell><cell cols="6">533 114 114 19.7 20.3 19.8</cell></row><row><cell>fi</cell><cell cols="6">532 114 114 17.8 18.8 18.1</cell></row><row><cell>fr</cell><cell>387</cell><cell>82</cell><cell>76</cell><cell>8.7</cell><cell>9.1</cell><cell>8.3</cell></row><row><cell>it</cell><cell>340</cell><cell>85</cell><cell>75</cell><cell>6.3</cell><cell>7.2</cell><cell>6.2</cell></row><row><cell>nl</cell><cell>308</cell><cell>58</cell><cell>64</cell><cell>6.6</cell><cell>6.7</cell><cell>6.3</cell></row><row><cell>pl</cell><cell>283</cell><cell>73</cell><cell>72</cell><cell>7.7</cell><cell>8.0</cell><cell>7.4</cell></row><row><cell>pt</cell><cell>347</cell><cell>68</cell><cell>77</cell><cell>7.1</cell><cell>8.2</cell><cell>7.2</cell></row><row><cell>es</cell><cell>280</cell><cell>60</cell><cell>60</cell><cell>6.5</cell><cell>6.1</cell><cell>5.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Dev F1 Scores for Different Learning Rates, Ablation Experiments .</figDesc><table><row><cell></cell><cell>1e-5 1e-6 1e-7</cell></row><row><cell>ROBERTA-Large</cell><cell>59.5 67.3 60.7</cell></row><row><cell>w/o tree reconciliation</cell><cell>38.6 51.2 18.2</cell></row><row><cell cols="2">ROBERTA-Random-Init 17.4 26.4 27.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10</head><label>10</label><figDesc>shows the results for the test trials for the ablation experiment.</figDesc><table><row><cell></cell><cell cols="3">Run 0 Run 1 Run 2</cell></row><row><cell>ROBERTA-Large</cell><cell>67.1</cell><cell>67.3</cell><cell>67.7</cell></row><row><cell>w/o tree reconciliation</cell><cell>51.2</cell><cell>51.4</cell><cell>50.6</cell></row><row><cell>ROBERTA-Random-Init</cell><cell>27.0</cell><cell>29.9</cell><cell>31.1</cell></row><row><cell>LSTM GloVe</cell><cell>24.6</cell><cell>27.7</cell><cell>27.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Dev F1 Scores for Three Trials, Ablation Experiments .</figDesc><table><row><cell>SemEval Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Run 0 Run 1 Run 2</cell></row><row><cell>Science (Combined)</cell><cell>28.6</cell><cell>31.7</cell><cell>25.1</cell></row><row><cell>Science (Eurovoc)</cell><cell>26.6</cell><cell>37.1</cell><cell>31.5</cell></row><row><cell>Science (WordNet)</cell><cell>26.5</cell><cell>28.8</cell><cell>25.8</cell></row><row><cell cols="2">Environment (Eurovoc) 23.4</cell><cell>21.5</cell><cell>24.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Test F1 Scores for Three Trials, Semeval. We show the Edge-F1 score rather than the Ancestor-F1 score in order to compare to previous work.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We scrape glosses from wiktionary.com, merriamwebster.com, and wikipedia.org. For wikitionary.com and merriam-webster.com we retrieve a list of glosses from each site. For wikipedia.org we treat the first paragraph of the page associated with the term as a single gloss. The glosses were scraped in</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2020" xml:id="foot_1">August 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Since the original pretraining corpus is not available, we follow<ref type="bibr" target="#b11">Devlin et al. (2019)</ref> and recreate the dataset by crawling http://smashwords.com and Wikipedia.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We thank the members of the Berkeley NLP group and the anonymous reviewers for their insightful feedback. CC and KL are supported by National Science Foundation Graduate Research Fellowships. This research has been supported by DARPA under agreement HR00112020054. The content does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Language-Specific Pretrained Models</head><p>We used pretrained models from the following sources: https://github.com/codegram/calbert, https://github.com/google-research/bert/ blob/master/multilingual.md (Devlin et al., 2019), http://turkunlp.org/FinBERT/ (Virtanen et al., 2019), https://github.com/dbmdz/berts, https://github.com/wietsedv/bertje (de Vries et al., 2019), https://huggingface.co/dkleczek/ bert-base-polish-uncased-v1, https://github.com/neuralmind-ai/ portuguese-bert, https://github.com/dccuchile/beto/blob/ master/README.md <ref type="bibr" target="#b9">(Cañete et al., 2020)</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>trees. We include the F 1 scores for three trials in Table <ref type="table">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Run  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtree Construction Results, English WordNet</head><p>Table <ref type="table">13</ref> shows the results for the learning rate trials for the English WORDNET experiment. Table <ref type="table">14</ref> shows the results for the test trials for the English WORDNET experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtree Construction Results, Multilingual WordNet</head><p>Table <ref type="table">15</ref> shows the results for the learning rate trials for the non-English WORDNET experiments.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egoitz</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<title level="m">Multilingual central repository version 3 . 0 : upgrading a very large lexical knowledge base</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On achieving and evaluating language-independence in nlp</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Issues in Language Technology</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Moving down the long tail of word sense disambiguation with gloss-informed biencoders</title>
		<author>
			<persName><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A survey of wordnets and their licenses</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyonghee</forename><surname>Paik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 13: Taxonomy extraction evaluation (TExEval-2)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="1081" to="1091" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Çelikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12753</idno>
		<title level="m">Inducing relational knowledge from bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The respective roles of intellectual creativity and automation in representing diversity: human and machine generated bias</title>
		<author>
			<persName><forename type="first">Vanda</forename><surname>Broughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; José</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jou-Hui</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hojin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Pérez</surname></persName>
		</author>
		<idno>PML4DC at ICLR 2020</idno>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note>Spanish pre-trained bert model and evaluation data</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural natural language inference models partially embed theories of lexical entailment and negation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BlackBoxNLP</title>
				<meeting>BlackBoxNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th international conference on computational linguistics</title>
				<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward completeness in concept extraction and classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The state and fate of linguistic diversity and inclusion in the nlp world</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastin</forename><surname>Santy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Budhiraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09095</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A semi-supervised method to learn and construct taxonomies using the web</title>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
				<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Is it possible to create a very large wordnet in 100 days? an evaluation. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Krister</forename><surname>Lindén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyrki</forename><surname>Niemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A project for the construction of an italian lexical knowledge base in the framework of wordnet</title>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciravegna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end reinforcement learning for automatic taxonomy induction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1229</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2462" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Language diversity in acl</title>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2016. 2004 -2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Revisiting a brazilian wordnet</title>
		<editor>Valeria de Paiva and Alexandre Rademaker</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Scopus</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Piasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Broda</surname></persName>
		</author>
		<title level="m">A wordnet from the ground up</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Open Dutch WordNet</title>
		<author>
			<persName><forename type="first">Marten</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxane</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anneleen</forename><surname>Segers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piek</forename><surname>Schoen</surname></persName>
		</author>
		<author>
			<persName><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight Global Wordnet Conference</title>
				<meeting>the Eight Global Wordnet Conference<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hearst patterns revisited: Automatic hypernym detection from large text corpora</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2057</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Building a free french wordnet from multilingual resources</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Taxonomy construction of unseen domains via graph-based cross-domain knowledge transfer</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahbub</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2198" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Teaching pre-trained models to systematically reason over implicit knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.06609</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multilingual is not enough: Bert for finnish</title>
		<author>
			<persName><forename type="first">A</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno>ArXiv, abs/1912.07076</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Wietse De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Van Cranenburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName><surname>Nissim</surname></persName>
		</author>
		<idno>ArXiv, abs/1912.09582</idno>
		<title level="m">Bertje: A dutch bert model</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Building the chinese open wordnet (cow): Starting from core synsets</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">1910</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic web information retrieval based on the wordnet</title>
		<author>
			<persName><forename type="first">Cheyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Jung</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Digital Content Technology and Its Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="294" to="302" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">When hearst is not enough: Improving hypernymy detection from corpus with distributional models</title>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2020 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
