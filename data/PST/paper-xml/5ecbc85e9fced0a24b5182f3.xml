<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Accuracy and High Fidelity Extraction of Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University * Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University * Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University * Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University * Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University * Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High Accuracy and High Fidelity Extraction of Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: accuracy, i.e., performing well on the underlying learning task, and fidelity, i.e., matching the predictions of the remote victim classifier on any input.</p><p>To extract a high-accuracy model, we develop a learningbased attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model-i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights.</p><p>We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning, and neural networks in particular, are widely deployed in industry settings. Models are often deployed as prediction services or otherwise exposed to potential adversaries. Despite this fact, the trained models themselves are often proprietary and are closely guarded.</p><p>There are two reasons models are often seen as sensitive. First, they are expensive to obtain. Not only is it expensive to train the final model <ref type="bibr" target="#b0">[1]</ref> (e.g., Google recently trained a model with 340 million parameters on hardware costing 61,000 USD per training run <ref type="bibr" target="#b1">[2]</ref>), performing the work to identify the optimal set of model architecture, training algorithm, and hyper-parameters often eclipses the cost of training the final model. Further, training these models also requires investing in expensive collection process to obtain the training datasets necessary to obtain an accurate classifier <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Second, there are security <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and privacy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> concerns for revealing trained models to potential adversaries.</p><p>Concerningly, prior work found that an adversary with query access to a model can steal the model to obtain a copy that largely agrees with the remote victim models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b16">[16]</ref>. These extraction attacks are therefore important to consider.</p><p>In this paper, we systematize the space of model extraction around two adversarial objectives: accuracy and fidelity. Accuracy measures the correctness of predictions made by the extracted model on the test distribution. Fidelity, in contrast, measures the general agreement between the extracted and victim models on any input. Both of these objectives are desirable, but they are in conflict for imperfect victim models: a high-fidelity extraction should replicate the errors of the victim, whereas a high-accuracy model should instead try to make an accurate prediction. At the high-fidelity limit is functionally-equivalent model extraction: the two models agree on all inputs, both on and off the underlying data distribution.</p><p>While most prior work considers accuracy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, we argue that fidelity is often equally important. When using model extraction to mount black-box adversarial example attacks <ref type="bibr" target="#b6">[7]</ref>, fidelity ensures the attack is more effective because more adversarial examples transfer from the extracted model to the victim. Membership inference <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> benefits from the extracted model closely replicating the confidence of predictions made by the victim. Finally, a functionally-equivalent extraction enables the adversary to inspect whether internal representations reveal unintended attributes of the input-that are statistically uncorrelated with the training objective, enabling the adversary to benefit from overlearning <ref type="bibr" target="#b17">[17]</ref>.</p><p>We design one attack for each objective. First, a learningbased attack, which uses the victim to generate labels for training the extracted model. While existing techniques already achieve high accuracy, our attacks are 16× more queryefficient and scale to larger models. We perform experiments that surface inherent limitations of learning-based extraction attacks and argue that learning-based strategies are ill-suited to achieve high-fidelity extraction. Then, we develop the first practical functionally-equivalent attack, which directly recovers a two-layer neural network's weights exactly given access to double-precision model inference. Compared to prior work, which required a high-precision power side-channel <ref type="bibr" target="#b18">[18]</ref> or access to model gradients <ref type="bibr" target="#b19">[19]</ref>, our attack only requires inputoutput access to the model, while simultaneously scaling to larger networks than either of the prior methods.</p><p>We make the following contributions:</p><p>• We taxonomize the space of model extraction attacks by exploring the objective of accuracy and fidelity.</p><p>• We improve the query efficiency of learning attacks for accuracy extraction and make them practical for millionsof-parameter models trained on billions of images.</p><p>• We achieve high-fidelity extraction by developing the first practical functionally-equivalent model extraction.</p><p>• We mix the proposed methods to obtain a hybrid method which improves both accuracy and fidelity extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We consider classifiers with domain X ⊆ R d and range Y ⊆ R K ; the output of the classifier is a distribution over K class labels. The class assigned to an input x by a classifier f is arg max i∈[K] f (x) i (for n ∈ Z, we write [n] = {1, 2, . . . n}). In order to satisfy the constraint that a classifier's output is a distribution, a softmax σ(•) is typically applied to the output of an arbitrary function f L : X → R K :</p><formula xml:id="formula_0">σ( f L (x)) i = exp( f L (x) i ) ∑ j exp( f L (x) j )</formula><p>.</p><p>We call the function f L (•) the logit function for a classifier f . To convert a class label into a probability vector, it is common to use one-hot encoding: for a value j ∈ [K], the one-hot encoding OH( j; K) is a vector in R K with OH( j; K) i = 1(i = j)-that is, it is 1 only at index j, and 0 elsewhere. Model extraction concerns reproducing a victim model, or oracle, which we write O : X → Y . The model extraction adversary will run an extraction algorithm A(O), which outputs the extracted model Ô. We will sometimes parameterize the oracle (resp. extracted model) as O θ (resp. Ôθ ) to denote that it has model parameters θ-we will omit this when unnecessary or apparent from context.</p><p>In this work, we consider O and Ô to both be neural networks. A neural network is a sequence of operationsalternatingly applying linear operations and non-linear operations-a pair of linear and non-linear operations is called a layer. Each linear operation projects onto some space R hthe dimensionality h of this space is referred to as the width of the layer. The number of layers is the depth of the network. The non-linear operations are typically fixed, while the linear operations have parameters which are learned during training. The function computed by layer i, f i (a), is therefore computed as f i (a) = g i (A (i) a + B (i) ), where g i is the ith non-linear function, and A (i) , B (i) are the parameters of layer i (A (i) is the weights, B (i) the biases). A common choice of activation is the rectified linear unit, or ReLU, which sets ReLU(x) = max(0, x). Introduced to improve the convergence of optimization when training neural networks, the ReLU activation has established itself as an effective default choice for practitioners <ref type="bibr" target="#b20">[20]</ref>. Thus, we consider primarily ReLU networks in this work.</p><p>The network structure described here is called fully connected because each linear operation "connects" every input node to every output node. In many domains, such as computer vision, this is more structure than necessary. A neuron computing edge detection, for example, only needs to use information from a small region of the image. Convolutional networks were developed to combat this inefficiency-the linear functions become filters, which are still linear, but are only applied to a small (e.g., 3x3 or 5x5) window of the input. They are applied to every window using the same weights, making convolutions require far fewer parameters than fully connected networks.</p><p>Neural networks are trained by empirical risk minimization. Given a dataset of n samples</p><formula xml:id="formula_1">D = {x i , y i } n i=1 ⊆ X × Y ,</formula><p>training involves minimizing a loss function L on the dataset with respect to the parameters of the network f . A common loss function is the cross-entropy loss H for a sample (x, y):</p><formula xml:id="formula_2">H(y, f (x)) = − ∑ k∈[K] y k log( f (x) k )</formula><p>, where y is the probability (or one-hot) vector for the true class. The cross-entropy loss on the full dataset is then</p><formula xml:id="formula_3">L(D; f ) = 1 n n ∑ i=1 H(y i , f (x i )) = − 1 n n ∑ i=1 ∑ k∈[K] y k log( f (x) k ).</formula><p>The loss is minimized with some form of gradient descent, often stochastic gradient descent (SGD). In SGD, gradients of parameters θ are computed over a randomly sampled batch B, averaged, and scaled by a learning rate η:</p><formula xml:id="formula_4">θ t+1 = θ t − η |B| ∑ i∈B ∇ θ H(y i , f (x i )).</formula><p>Other optimizers <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref> use gradient statistics to reduce the variance of updates which can result in better performance. A less common setting, but one which is important for our work, is when the target values y which are used to train the network are not one-hot values, but are probability vectors output by a different model g(x). When training using the dataset D g = {x i , g(x i ) 1/T } n i=1 , we say the trained model is distilled from g with temperature T , referring to the process of distillation introduced in Hinton et al. <ref type="bibr" target="#b24">[24]</ref>. Note that the values of g(x i ) 1/T are always scaled to sum to 1.</p><p>We now address the spectrum of adversaries interested in extracting neural networks. As illustrated in Table <ref type="table">1</ref>, we taxonomize the space of possible adversaries around two overarching goals-theft and reconnaissance. We detail why extraction is not always practically realizable by constructing models that are impossible to extract, or require a large number of queries to extract. We conclude our threat model with a discussion of how adversarial capabilities (e.g., prior knowledge of model architecture or information returned by queries) affect the strategies an adversary may consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adversarial Motivations</head><p>Model extraction attacks target the confidentiality of a victim model deployed on a remote service. A model refers here to both the architecture and its parameters. Architectural details include the learning hypothesis (i.e., neural network in our case) and corresponding details (e.g., number of layers and activation functions for neural networks). Parameter values are the result of training.</p><p>First, we consider theft adversaries, motivated by economic incentives. Generally, the defender went through an expensive process to design the model's architecture and train it to set parameter values. Here, the model can be viewed as intellectual property that the adversary is trying to steal. A line of work has in fact referred to this as "model stealing" <ref type="bibr" target="#b10">[11]</ref>.</p><p>In the latter class of attacks, the adversary is performing reconnaissance to later mount attacks targeting other security properties of the learning system: e.g., its integrity with adversarial examples <ref type="bibr" target="#b6">[7]</ref>, or privacy with training data membership inference <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Model extraction enables an adversary previously operating in a black-box threat model to mount attacks against the extracted model in a white-box threat model. The adversary has-by design-access to the extracted model's parameters. In the limit, this adversary would expect to extract an exact copy of the oracle.</p><p>The goal of exact extraction is to produce Ôθ = O θ , so that the model's architecture and all of its weights are identical to the oracle. This definition is purely a strawman-it is the strongest possible attack, but it is fundamentally impossible for many classes of neural networks, including ReLU networks, because any individual model belongs to a large equivalence class of networks which are indistinguishable from input-output behavior. For example, we can scale an arbitrary neuron's input weights and biases by some c &gt; 0, and scale its output weights and biases by c −1 ; the resulting model's behavior is unchanged. Alternatively, in any intermediate layer of a ReLU network, we may also add a dead neuron which never contributes to the output, or might permute the (arbitrary) order of neurons internally. Given access to input-output behavior, the best we can do is identify the equivalence class the oracle belongs to. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Goals</head><p>This perspective yields a natural spectrum of realistic adversarial goals characterizing decreasingly precise extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Functionally Equivalent Extraction</head><p>The goal of functionally equivalent extraction is to construct an Ô such that ∀x ∈ X , Ô(x) = O(x). This is a tractable weakening of the exact extraction definition from earlier-it is the hardest possible goal using only input-output pairs. The adversary obtains a member of the oracle's equivalence class. This goal enables a number of downstream attacks, including those involving inspection of the model's internal representations like overlearning <ref type="bibr" target="#b17">[17]</ref>, to operate in the white-box threat model. Learning LM, NN Task Accuracy Gradients Pal et al. <ref type="bibr" target="#b14">[15]</ref> Active learning NN Fidelity Probabilities, labels Chandrasekharan et al. <ref type="bibr" target="#b12">[13]</ref> Active learning LM Functionally Equivalent Labels Copycat CNN <ref type="bibr" target="#b16">[16]</ref> Learning CNN Task Accuracy, Fidelity Labels Papernot et al. <ref type="bibr" target="#b6">[7]</ref> Active learning NN Fidelity Labels CSI NN <ref type="bibr" target="#b25">[25]</ref> Direct Recovery NN Functionally Equivalent Power Side Channel Knockoff Nets <ref type="bibr" target="#b11">[12]</ref> Learning Existing Attacks In Table <ref type="table">1</ref>, we fit previous model extraction work into this taxonomy, as well as discuss their techniques. Functionally equivalent extraction has been considered for linear models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>, decision trees <ref type="bibr" target="#b10">[11]</ref>, both given probabilities, and neural networks <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b25">25]</ref>, given extra access. Task accuracy extraction has been considered for linear models <ref type="bibr" target="#b10">[11]</ref> and neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19]</ref>, and fidelity extraction has also been considered for linear models <ref type="bibr" target="#b10">[11]</ref> and neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. Notably, functionally equivalent attacks require model-specific techniques, while task accuracy and fidelity typically use generic learning-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fidelity Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Extraction is Hard</head><p>Before we consider adversarial capabilities in Section 3.4 and potential corresponding approaches to model extraction, we must understand how successful we can hope to be. Here, we present arguments that will serve to bound our expectations. First, we will identify some limitations of functionally equivalent extraction by constructing networks which require arbitrarily many queries to extract. Second, we will present another class of networks that cannot be extracted with fidelity without querying a number of times exponential in its depth. We provide intuition in this section and later prove these statements in Appendix A. Exponential hardness of functionally equivalent attacks. In order to show that functionally equivalent extraction is intractable in the worst case, we construct of a class of neural networks that are hard to extract without making exponentially many queries in the network's width.</p><p>Theorem 1. There exists a class of width 3k and depth 2 neural networks on domain [0, 1] d (with precision p numbers) with d ≥ k that require, given logit access to the networks, Θ(p k ) queries to extract.</p><p>The precision p is the number of possible values a feature can take from [0, 1]. In images with 8-bit pixels, we have p = 256. The intuition for this theorem is that a width 3k network can implement a function that returns a non-zero value on at most a p −k fraction of the space. In the worst case, p k queries are necessary to find this fraction of the space.</p><p>Note that this result assumes the adversary can only observe the input-output behavior of the oracle. If this assumption is broken then functionally equivalent extraction becomes practical. For example, Batina et al. <ref type="bibr" target="#b25">[25]</ref> perform functionally equivalent extraction by performing a side channel attack (specifically, differential power analysis <ref type="bibr" target="#b26">[26]</ref>) on a microprocessor evaluating the neural network.</p><p>We also observe in Theorem 2 that, given white-box access to two neural networks, it is NP-hard in general to test if they are functionally equivalent. We do this by constructing two networks that differ only in coordinates satisfying a subset sum instance. Then testing functional equivalence for these networks is as hard as finding the satisfying subset.</p><p>Theorem 2 (Informal). Given their weights, it is NP-hard to test whether two neural networks are functionally equivalent.</p><p>Any attack which can claim to perform functionally equivalent extraction efficiently (both in number of queries used and in running time) must make some assumptions to avoid these pathologies. In Section 6, we will present and discuss the assumptions of a functionally equivalent extraction attack for two-layer neural network models.</p><p>Learning approaches struggle with fidelity. A final difficulty for model extraction comes from recent work in learnability <ref type="bibr" target="#b27">[27]</ref>. <ref type="bibr">Das</ref>  A sample-efficient approach therefore must either make assumptions about the model to be extracted (to distinguish it from a deep random network), or must access its dataset without statistical queries.</p><p>Theorem 3 (Informal <ref type="bibr" target="#b27">[27]</ref>). Random networks with domain {0, 1} d and range {0, 1} and depth h require exp(O(h)) samples to learn in the SQ learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adversarial Capabilities</head><p>We organize an adversary's prior knowledge about the oracle and its training data into three categories-domain knowledge, deployment knowledge, and model access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Domain Knowledge</head><p>Domain knowledge describes what the adversary knows about the task the model is designed for. For example, if the model is an image classifier, then the model output should not change under standard image data augmentations, such as shifts, rotations, or crops. Usually, the adversary should be assumed to have as much domain knowledge as the oracle's designer.</p><p>In some domains, it is reasonable to assume the adversary has access to public task-relevant pretrained models or datasets. This is often the case for learning-based model extraction, which we develop in Section 4. We consider an adversary using part of a public dataset of 1.3 million images <ref type="bibr" target="#b3">[4]</ref> as unlabeled data to mount an attack against a model trained on a proprietary dataset of 1 billion labeled images <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning-based extraction is hard without natural data</head><p>In learning-based extraction, we assume that the adversary is able to collect public unlabeled data to mount their attack. This is a natural assumption for a theft-motivated adversary who wishes to steal the oracle for local use-the adversary has data they want to learn the labels of without querying the model! For other adversaries, progress in generative modeling is likely to offer ways to remove this assumption <ref type="bibr" target="#b29">[29]</ref>. We leave this to future work because our overarching aim in this paper is to characterize the model extraction attacker space around the notions of accuracy and fidelity. All progress achieved by our approaches is complementary to possible progress in synthetic data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Deployment Knowledge</head><p>Deployment knowledge describes what the adversary knows about the oracle itself, including the model architecture, training procedure, and training dataset. The adversary may have access to public artifacts of the oracle-a distilled version of the oracle may be available (such as for OpenAI GPT <ref type="bibr" target="#b30">[30]</ref>) or the oracle may be transfer learned from a public pretrained model (such as many image classifiers <ref type="bibr" target="#b31">[31]</ref> or language models like BERT <ref type="bibr" target="#b32">[32]</ref>).</p><p>In addition, the adversary may not even know the features (the exact inputs to the model) or the labels (the classes the model may output). While the latter can generally be inferred by interacting with the model (e.g., making queries and observing the labels predicted by the model), inferring the former is usually more difficult. Our preliminary investigations suggest that these are not limiting assumptions, but we leave proper treatment of these constraints to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Model Access</head><p>Model access describes the information the adversary obtains from the oracle, including bounds on how many queries the adversary may make as well as the oracle's response:</p><p>• label: only the label of the most-likely class is revealed.</p><p>• label and score: in addition to the most-likely label, the confidence score of the model in its prediction for this label is revealed.</p><p>• top-k scores: the labels and confidence scores for the k classes whose confidence are highest are revealed.</p><p>• scores: confidence scores for all labels are revealed.</p><p>• logits: raw logit values for all labels are revealed.</p><p>In general, the more access an adversary is given, the more effective they should be in accomplishing their goal. We instantiate practical attacks under several of these assumptions.</p><p>Limiting model access has also been discussed as a defensive measure, as we elaborate in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning-based Model Extraction</head><p>We present our first attack strategy where the victim model serves as a labeling oracle for the adversary. While many attack variants exist <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, they generally stage an iterative interaction between the adversary and the oracle, where the adversary collects labels for a set of points from the oracle and uses them as a training set for the extracted model. These algorithms are typically designed for accuracy extraction; in this section, we will demonstrate improved algorithms for accuracy extraction, using task-relevant unlabeled data. We realistically simulate large-scale model extraction by considering an oracle that was trained on 1 billion Instagram images <ref type="bibr" target="#b28">[28]</ref> to obtain (at the time of the experiment) stateof-the-art performance on the standard image classification benchmark, ImageNet <ref type="bibr" target="#b3">[4]</ref>. The oracle, with 193 million parameters, obtained 84.2% top-1 accuracy and 97.2% top-5 accuracy on the 1000-class benchmark-we refer to the model as the "WSL model", abbreviating the paper title. We give the adversary access to the public ImageNet dataset. The adversary's goal is to use the WSL model as a labeling oracle to train an ImageNet classifier that performs better than if we trained the model directly on ImageNet. The attack is successful if access to the WSL model-trained on 1 billion proprietary images inaccessible to the adversary-enables the adversary to extract a model that outperforms a baseline model trained directly with ImageNet labels. This is accuracy extraction for the ImageNet distribution, given unlabeled ImageNet training data.</p><p>We consider two variants of the attack: one where the adversary selects 10% of the training set (i.e., about 130,000 points) and the other where the adversary keeps the entire training set (i.e., about 1.3 million points). To put this number in perspective, recall that each image has a dimension of 224x224 pixels and 3 color channels, giving us 224 • 224 • 3 = 150, 528 total input features. Each image belongs to one of 1,000 classes. Although ImageNet data is labeled, we always treat it as unlabeled to simulate a realistic adversary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fully-supervised model extraction</head><p>The first attack is fully supervised, as proposed by prior work <ref type="bibr" target="#b10">[11]</ref>. It serves to compare our subsequent attacks to prior work, and to validate our hypothesis that labels from the oracle are more informative than dataset labels.</p><p>The adversary needs to obtain a label for each of the points it intends to train the extracted model with. Then it queries the oracle to label its training points with the oracle's predictions. The oracle reveals labels and scores (in the threat model from Section 3) when queried.</p><p>The adversary then trains its model to match these labels using the cross-entropy loss. We used a distillation temperature of T = 1.5 in our experiments after a random search. Our experiments use two architectures known to perform well on image classification: ResNet-v2-50 and ResNet-v2-200.</p><p>Results. We present results in Table <ref type="table" target="#tab_3">2</ref>. For instance, the adversary is able to improve the accuracy of their model by 1.0% for ResNetv2-50 and 1.9% for ResNet_v2_200 after having queried the oracle for 10% of the ImageNet data. Recall that the task has 1,000 labels, making these improvements significant. The gains we are able to achieve as an adversary are in line with progress that has been made by the computer vision community on the ImageNet benchmark over recent years, where the research community improved the state-of-the-art top-1 accuracy by about one percent point per year. 1   1 https://paperswithcode.com/sota/image-classification-on-imagenet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unlabeled data improves query efficiency</head><p>For adversaries interested in theft, a learning-based strategy should minimize the number of queries required to achieve a given level of accuracy. A natural approach towards this end is to take advantage of advances in label-efficient ML, including active learning <ref type="bibr" target="#b33">[33]</ref> and semi-supervised learning <ref type="bibr" target="#b34">[34]</ref>.</p><p>Active learning allows a learner to query the labels of arbitrary points-the goal is to query the best set of points to learn a model with. Semi-supervised learning considers a learner with some labeled data, but much more unlabeled data-the learner seeks to leverage the unlabeled data (for example, by training on guessed labels) to improve classification performance. Active and semi-supervised learning are complementary techniques <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>; it is possible to pick the best subset of data to train on, while also using the rest of the unlabeled data without labels.</p><p>The connection between label-efficient learning and learning-based model extraction attacks is not new <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15</ref>], but has focused on active learning. We show that, assuming access to unlabeled task-specific data, semi-supervised learning can be used to improve model extraction attacks. This could potentially be improved further by leveraging active learning, as in prior work, but our improvements are overall complementary to approaches considered in prior work. We explore two semi-supervised learning techniques: rotation loss <ref type="bibr" target="#b37">[37]</ref> and MixMatch <ref type="bibr" target="#b38">[38]</ref>.</p><p>Rotation loss. We leverage the current state-of-the-art semisupervised learning approach on ImageNet, which augments the model with a rotation loss <ref type="bibr" target="#b37">[37]</ref>. The model contains two linear classifiers from the second-to-last layer of the model: the classifier for the image classification task, and a rotation predictor. The goal of the rotation classifier is to predict the rotation applied to an input-each input is fed in four times per batch, rotated by {0 • , 90 • , 180 • , 270 • }. The classifier should output onehot encodings {OH(0; 4), OH(1; 4), OH(2; 4), OH(3; 4)}, respectively, for these rotated images. Then, the rotation loss is written:</p><formula xml:id="formula_5">L R (X; f θ ) = 1 4N N ∑ i=0 r ∑ j=1 H( f θ (R j (x i )), j)</formula><p>where R j is the jth rotation, H is cross-entropy loss, and f θ is the model's probability outputs for the rotation task. Inputs need not be labeled, hence we compute this loss on unlabeled data for which the adversary did not query the model. That is, we train the model on both unlabeled data (with rotation loss), and labeled data (with standard classification loss), and both contribute towards learning a good representation for all of the data, including the unlabeled data. We compare the accuracy of models trained with the rotation loss on data labeled by the oracle and data with ImageNet labels. Our best performing extracted model, with an accuracy  of 64.5%, is trained with the rotation loss on oracle labels whereas the baseline on ImageNet labels only achieves 62.5% accuracy with the rotation loss and 61.2% without the rotation loss. This demonstrates the cumulative benefit of adding a rotation loss to the objective and training on oracle labels for a theft-motivated adversary.</p><p>We expect that as semi-supervised learning techniques on ImageNet mature, further gains should be reflected in the performance of model extraction attacks.</p><p>MixMatch. To validate this hypothesis, we turn to smaller datasets where semi-supervised learning has made significant progress. We investigate a technique called MixMatch <ref type="bibr" target="#b38">[38]</ref> on two datasets: SVHN <ref type="bibr" target="#b39">[39]</ref> and CIFAR10 <ref type="bibr" target="#b40">[40]</ref>. MixMatch uses a combination of techniques, including training on "guessed" labels, regularization, and image augmentations.</p><p>For both datasets, inputs are color images of 32x32 pixels belonging to one of 10 classes. The training set of SVHN contains 73257 images and the test set contains 26032 images. The training set of CIFAR10 contains 50000 images and the test set contains 10000 images. We train the oracle with a WideResNet-28-2 architecture on the labeled training set. The oracles achieve 97.36% accuracy on SVHN and 95.75% accuracy on CIFAR10.</p><p>The adversary is given access to the same training set but without knowledge of the labels. Our goal is to validate the effectiveness of semi-supervised learning by demonstrating that the adversary only needs to query the oracle on a small subset of these training points to extract a model whose accuracy on the task is comparable to the oracle's. To this end, we run 5 trials of fully supervised extraction (no use of unlabeled data), and 5 trials of MixMatch, reporting for each trial the median accuracy of the 20 latest checkpoints, as done in <ref type="bibr" target="#b38">[38]</ref>.</p><p>Results. In Table <ref type="table" target="#tab_4">3</ref>, we find that with only 250 queries (293x smaller label set than the SVHN oracle and 200x smaller for CIFAR10), MixMatch reaches 95.82% test accuracy on SVHN and 87.98% accuracy on CIFAR10. This is higher than fully supervised training that uses 4000 queries. With 4000 queries, MixMatch is within 0.29% of the accuracy of the oracle on SVHN, and 2.46% on CIFAR10. The variance of MixMatch is slightly higher than that of fully supervised training, but is much smaller than the performance gap. These gains come from the prior MixMatch is able to build using the unlabeled data, making it effective at exploiting few labels. We observe similar gains in test set fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations of Learning-Based Extraction</head><p>Learning-based approaches have several sources of nondeterminism: the random initializations of the model parameters, the order in which data is assembled to form batches for SGD, and even non-determinism in GPU instructions <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref>. Non-determinism impacts the model parameter values obtained from training. Therefore, even an adversary with full access to the oracle's training data, hyperparameters, etc., would still need all of the learner's non-determinism to achieve the functionally equivalent extraction goal described in Section 3. In this section, we will attempt to quantify this: for a strong adversary, with access to the exact details of the training setup, we will present an experiment to determine the limits of learning-based algorithms to achieving fidelity extraction.</p><p>We perform the following experiment. We query an oracle to obtain a labeled substitute dataset D.  </p><formula xml:id="formula_6">(x) = f 2 θ (x)</formula><p>, then the prediction on x is dependent not on the oracle, but on the non-determinism of the learning-based attack strategy-we are unable to guarantee fidelity.</p><p>We independently control the initialization randomness and batch randomness during training on Fashion-MNIST <ref type="bibr" target="#b43">[43]</ref> with fully supervised SGD (we use Fashion-MNIST for training speed). We repeated each run 10 times and measure agreement between the ten obtained models on the test set, adversarial examples generated by running FGSM with ε = 25/255 with the oracle model and the test set, and uniformly random inputs. The oracle uses initialization seed 0 and SGD seed 0-we also use two different initialization and SGD seeds.</p><p>Even when both training and initialization randomness are fixed (so that only GPU non-determinism remains), fidelity peaks at 93.7% on the test set (see Table <ref type="table" target="#tab_6">4</ref>). With no randomness fixed, extraction achieves 93.4% fidelity on the test set. (Agreement on the test set should should be considered in reference to the base test accuracy of 90%.) Hence, even an adversary who has the victim model's exact training set will be unable to exceed ~93.4% fidelity. Using prototypicality metrics, as investigated in Carlini et al. <ref type="bibr" target="#b44">[44]</ref>, we notice that test points where fidelity is easiest to achieve are also the most prototypical (i.e., more representative of the class it is labeled as). This connection is explored further in Appendix B. The experiment of this section is also related to uncertainty estimation using deep ensembles <ref type="bibr" target="#b42">[42]</ref>; we believe a deeper connection may exist between the fidelity of learning-based approaches and uncertainty estimation. Also relevant is the work mentioned earlier in Section 3, that shows that random networks are hard for learning-based approaches to extract. Here, we find that learning-based approaches have limits even for trained networks, on some portion of the input space.</p><p>It follows from these arguments that non-determinism of both the victim and extracted model's learning procedures potentially compound, limiting the effectiveness of using a learning-based approach to reaching high fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Functionally Equivalent Extraction</head><p>Having identified fundamental limitations that prevent learning-based approaches from perfectly matching the oracle's mistakes, we now turn to a different approach where the adversary extracts the oracle's weights directly, seeking to achieve functionally-equivalent extraction.</p><p>This attack can be seen as an extension of two prior works.</p><p>• Milli et al. <ref type="bibr" target="#b19">[19]</ref> introduce an attack to extract neural network weights under the assumption that the adversary is able to make gradient queries. That is, each query the adversary makes reveals not only the prediction of the neural network, but also the gradient of the neural network with respect to the query. To the best of our knowledge this is the only functionally-equivalent extraction attack on neural networks with one hidden layer, although it was not actually implemented in practice.</p><p>• Batina et al. <ref type="bibr" target="#b25">[25]</ref>, at USENIX Security 2019, develop a side-channel attack that extracts neural network weights through monitoring the power use of a microprocessor evaluating the neural network. This is a much more powerful threat model than made by any of the other model extraction papers. To the best of our knowledge this is the only practical direct model extraction result-they manage to extract essentially arbitrary depth networks.</p><p>In this section we introduce an attack which only requires standard queries (i.e., that return the model's prediction instead of its gradients) and does not require any side-channel leakages, yet still manages to achieve higher fidelity extraction than the side-channel extraction work for two-layer networks, assuming double-precision inference.</p><p>Attack Algorithm Intuition. As in <ref type="bibr" target="#b19">[19]</ref>, our attack is tailored to work on neural networks with the ReLU activation function (the ReLU is an effective default choice of activation function <ref type="bibr" target="#b20">[20]</ref>). This makes the neural network a piecewise linear function. Two samples are within the same linear region if all ReLU units have the same sign, illustrated in Figure <ref type="figure">2</ref>.</p><p>By finding adjacent linear regions, and computing the difference between them, we force a single ReLU to change signs. Doing this, it is possible to almost completely determine the weight vector going into that ReLU unit. Repeating this attack for all ReLU units lets us recover the first weight matrix completely. (We say almost here, because we must do some work to recover the sign of the weight vector.) Once the first layer of the two-layer neural network has been determined, the second layer can be uniquely solved for algebraically through least squares. This attack is optimal up to a constant factor-the query complexity is discussed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Notation and Assumptions</head><p>As in <ref type="bibr" target="#b19">[19]</ref>, we only aim to extract neural networks with one hidden layer using the ReLU activation function. We denote the model weights by A (0) ∈ R d×h , A (1) ∈ R h×K and biases by B (0) ∈ R h , B  to the input dimensionality, the size of the hidden layer, and the number of classes. This is found in Table <ref type="table" target="#tab_8">6</ref>.1. We say that ReLU(x) is at a critical point if x = 0; this is the location at which the unit's gradient changes from 0 to 1. We assume the adversary is able to observe the raw logit outputs as 64-bit floating point values. We will use the notation O L to denote the logit oracle. Our attack implicitly assumes that the rows of A (0) are linearly independent. Because the dimension of the input space is larger than the hidden space by at least 100, it is exceedingly unlikely for the rows to be linearly dependent (and we find this holds true in practice).</p><p>Note that our attack is not an SQ algorithm, which would only allow us to look at aggregate statistics of our dataset. Instead, our algorithm is very particular in its analysis of the network, computing the differences between linear regions, for example, cannot done with aggregate statistics. This structure allows us to avoid the pathologies of Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Attack Overview</head><p>The algorithm is broken into four phases:</p><p>• Critical point search identifies inputs {x i } n i=1 to the neural network so that exactly one of the ReLU units is at a critical point (i.e., has input identically 0).</p><p>• Weight recovery takes an input x which causes the ith neuron to be at a critical point. We use this point x to compute the difference between the two adjacent linear regions induced by the critical point, and thus the weight vector row A (0)</p><p>i . By repeating this process for each ReLU we obtain the complete matrix A (0) . Due to technical reasons discussed below, we can only recover the rowvector up to sign.</p><p>• Sign recovery determines the sign of each row-vector A (0) j for all j using global information about A (0) .</p><p>• Final layer extraction uses algebraic techniques (least squares) to solve for the second layer of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Critical Point Search</head><p>For a two layer network, observe that the logit function is</p><p>given by the equation O L (x) = A (1) ReLU(A (0) x+B (0) )+B (1) .</p><p>To find a critical point for every ReLU, we sample two random vectors u, v ∈ R d , and consider the function</p><formula xml:id="formula_7">L(t; u, v, O L ) = O L (u + tv).</formula><p>for t varying between a small and large appropriately selected value (discussed below). This amounts to drawing a line in the inputs of the network; passed through ReLUs, this line becomes the piecewise linear function L(•). The points t where L(t) is non-differentiable are exactly locations where some ReLU i is changing signs (i.e., some ReLU is at a critical point). Figure <ref type="figure">3</ref> shows an example of what this sweep looks like on a trained MNIST model. Furthermore, notice that given a pair u, v, there is exactly one value t for which each ReLU is at a critical point, and if t is allowed to grow arbitrarily large or small that every ReLU unit will switch sign exactly once. Intuitively, the reason this is true is that each ReLU's input, (say wx + b for some w, b), is a monotone function of t (w T ut + w T v + b). Thus, by varying t, we can identify an input x i that sets the ith ReLU to 0 for every relu i in the network. This assumes we are not moving parallel to any of the rows (where w T u = 0), and that we vary t within a sufficiently large interval (so the w T ut term may overpower the constant term). The analysis of <ref type="bibr" target="#b19">[19]</ref> suggests that these concerns can be resolved with high probability by varying t ∈ −h 2 , h 2 .</p><p>While in theory it would be possible to sweep all values of t to identify the critical points, this would require a large number of queries. Thus, to efficiently search for the locations 0.0 0.2 0.4 0.6 0.8 1.0 t and there is exactly one discontinuity in this range, we can precisely identify the location of that discontinuity efficiently. An intuitive diagram for this algorithm can be found in Figure <ref type="figure">4</ref> and the algorithm can be found in Algorithm 1. The property this leverages is that the function is piecewise linearif we know the range is composed of two linear segments, we can identify the linear segments and compute their intersection. In Algorithm 1, lines 1-3 describe computing the two linear regions' slopes and intercepts. Lines 4 and 5 compute the intersection of the two lines (also shown in the red dotted line of Figure <ref type="figure">4</ref>). The remainder of the algorithm performs Algorithm 1 Algorithm for 2-linearity testing. Computes the location of the only critical point in a given range or rejects if there is more than one.</p><p>Function</p><formula xml:id="formula_8">f , range [t 1 ,t 2 ], ε m 1 = f (t 1 +ε)− f (t 1 ) ε Gradient at t 1 m 2 = f (t 2 )− f (t 2 −ε) ε Gradient at t 2 y 1 = f (a), y 2 = f (b) x = a + y 2 −y 1 −(b−a)m 2 m 1 −m 2 Candidate critical point ŷ = y 1 + m 1 y 2 −y 1 −(b−a)m 2 m 1 −m 2 Expected value at candidate y = f (x)</formula><p>True value at candidate if ŷ = y then return x else return "More than one critical point" end if the correctness check, also illustrated in Figure <ref type="figure">4</ref>; if there are more than 2 linear components, it is unlikely that the true function value will match the function value computed in line 5, and we can detect that the algorithm has failed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Weight Recovery</head><p>After running critical point search we obtain a set {x i } h i=1 , where each critical point corresponds to a point where a single ReLU flips sign. In order to use this information to learn the weight matrix A (0) we measure the second derivative of O L in each input direction at the points x i . Taking the second derivative here corresponds to measuring the difference between the linear regions on either side of the ReLU. Recall that prior work assumed direct access to gradient queries, and thus did not require any of the analysis in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Absolute Value Recovery</head><p>To formalize the intuition of comparing adjacent hyperplanes, observe that for the oracle O L and for a critical point x i (corresponding to ReLU i being zero) and for a random input-space direction e j we have</p><formula xml:id="formula_9">∂ 2 O L ∂e 2 j x i = ∂O L ∂e j x i +c•e j − ∂O L ∂e j x i −c•e j = ∑ k A (1) k 1(A (0) k (x i + c • e j ) + B (0) k &gt; 0)A (0) k j − ∑ k A (1) k 1(A (0) k (x i − c • e j ) + B (0) k &gt; 0)A (0) k j = A (1) i 1(A (0) i • e j &gt; 0) − 1(−A (0) i • e j &gt; 0) A (0) ji = ±(A (0) ji A (1) i )</formula><p>for a c &gt; 0 small enough so that x i ± c • e j does not flip any other ReLU. Because x i is a critical point and c is small, the sums in the second line differ only in the contribution of ReLU i . However at this point we only have a product involving both weight matrices. We now show this information is useful.</p><p>If we compute |A (0)</p><p>1i A (1) | and |A (0)</p><p>2i A (1) | by querying along directions e 1 and e 2 , we can divide these quantities to obtain the value</p><formula xml:id="formula_10">|A (0) 1i /A (0)</formula><p>2i |, the ratio of the two weights. By repeating the above process for each input direction we can, for all k, obtain the pairwise ratios</p><formula xml:id="formula_11">|A (0) 1i /A (0) ki |.</formula><p>Recall from Section 3 that obtaining the ratios of weights is the theoretically optimal result we could hope to achieve. It is always possible to multiply all of the weights into a ReLU by a constant c &gt; 0 and then multiply all of the weights out of the ReLU by c −1 . Thus, without loss of generality, we can assign A (0) 1i = 1 and scale the remaining entries accordingly. Unfortunately, we have lost a small amount of information here. We have only learned the absolute value of the ratio, and not the value itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Weight Sign Recovery</head><p>Once we reconstruct the values</p><formula xml:id="formula_12">|A (0) ji /A (0)</formula><p>1i | for all j we need to recover the sign of these values. To do this we consider the following quantity:</p><formula xml:id="formula_13">∂ 2 O L ∂(e j + e k ) 2 x i = ±(A (0) ji A (1) i ± A (0) ki A (1) i ).</formula><p>That is, we consider what would happen if we take the second partial derivative in the direction (e j +e k ). Their contributions to the gradient will either cancel out, indicating A 0) ji and A (0) ki are of opposite sign, or they will compound on each other, indicating they have the same sign. Thus, to recover signs, we can perform this comparison along each direction (e 1 + e j ).</p><p>Here we encounter one final difficulty. There are a total of n signs we need to recover, but because we compute the signs by comparing ratios along different directions, we only obtain n − 1 relations. That is, we now know the correct signed value of A (0) i up to a single sign for the entire row. It turns out this is to be expected. What we have computed is the normal direction to the hyperplane, but because any given hyperplane can be described by an infinite number of normal vectors differing by a constant scalar, we can not hope to use local information to recover this final sign bit.</p><p>Put differently, while it is possible to push a constant c &gt; 0 through from the first layer to the second layer, it is not possible to do this for negative constants, because the ReLU function is not symmetric. Therefore, it is necessary to learn the sign of this row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Global Sign Recovery</head><p>Once we have recovered the input vector's weights, we still don't know the sign for the given inputs-we only measure the difference between linear functions at each critical point, but do not know which side is the positive side of the ReLU <ref type="bibr" target="#b19">[19]</ref>. Now, we need to leverage global information in order to reconcile all of inputs' signs.</p><p>Notice that recovering Â(0) i allows us to obtain B (0) i by using the fact that A (0)</p><formula xml:id="formula_14">i • x i + B (0) i = 0. Then we can compute B(0)</formula><p>i up to the same global sign as is applied to Â(0) i . Now, to begin recovering sign, we search for a vector z that is in the null space of Â(0) , that is, Â(0) z = 0. Because the neural network has h &lt; d, the null-space is non-zero, and we can find many such vectors using least squares. Then, for each ReLU i , we search for a vector v i such that v i A (0) = e i where here e i is the ith basis vector in the hidden space. That is, moving along the v i direction only changes ReLU i 's input value. Again we can search for this through least squares.</p><p>Given z and these v i we query the neural network for the</p><formula xml:id="formula_15">values of O L (z), O L (z + v i ), and O L (z − v i ).</formula><p>On each of these three queries, all hidden units are 0 except for ReLU i which recieves as input either 0, 1, or −1 by the construction of v i . However, notice that the output of ReLU i can only be either 0 or 1, and the two {−1, 0} cases collapse to just output 0.</p><formula xml:id="formula_16">Therefore, if O L (z + v i ) = O L (z), we know that A (0) i • v i &lt; 0.</formula><p>Otherwise, we will find O L (z</p><formula xml:id="formula_17">− v i ) = O L (z) and A (0) i • v i &gt; 0.</formula><p>This allows us to recover the sign bit for ReLU i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Last Layer Extraction</head><p>Given the completely extracted first layer, the logit function of the network is just a linear transformation which we can recover with least squares, through making h queries where each ReLU is active at least once. In practice, we use the critical points discovered in the previous section so that we do not need to make additional neural network queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Results</head><p>Setup. We train several one-layer fully-connected neural networks with between 16 and 512 hidden units (for 12,000 and 100,000 trainable parameters, respectively) on the MNIST <ref type="bibr" target="#b45">[45]</ref> and CIFAR-10 datasets <ref type="bibr" target="#b40">[40]</ref>. We train the models with the Adam <ref type="bibr" target="#b23">[23]</ref> optimizer for 20 epochs at batch size 128 until they converge. We train five networks of each size to obtain higher statistical significance. Accuracies of these networks can be found in the supplement in Appendix C. In Section 4, we used 140,000≈ 2 17 queries for ImageNet model extraction. This is comparable to the number of queries used to extract the smallest MNIST model in this section, highlighting the advantages of both approaches. MNIST Extraction. We implement the functionallyequivalent extraction attack in JAX <ref type="bibr" target="#b46">[46]</ref> and run it on each trained oracle. We measure the fidelity of the extracted model, comparing predicted labels, on the MNIST test set.</p><p>Results are summarized in Table <ref type="table" target="#tab_8">6</ref>. For smaller networks, we achieve 100% fidelity on the test set: every single one of the 10, 000 test examples is predicted the same. As the network size increases, low-probability errors we encounter become more common, but the extracted neural network still disagrees with the oracle on only 2 of the 10, 000 examples.</p><p>Inspecting the weight matrix that we extract and comparing it to the weight matrix of the oracle classifier, we find that we manage to reconstruct the first weight matrix to an average precision of 23 bits-we provide more results in Appendix C. CIFAR-10 Extraction. Because this attack is dataindependent, the underlying task is unimportant for how well the attack works; only the number of parameters matter. The results for CIFAR-10 are thus identical to MNIST when controlling for model size: we achieve 100% test set agreement on models with fewer than 200, 000 parameters and and greater than 99% test set agreement on larger models. Comparison to Prior Work. To the best of our knowledge, this is by orders of magnitude the highest fidelity extraction of neural network weights.</p><p>The only fully-implemented neural network extraction attack we are aware of is the work of Batina et al. <ref type="bibr" target="#b25">[25]</ref>, who uses an electromagnetic side channels and differential power analysis to recover an MNIST neural network with neural network weights with an average error of 0.0025. In comparison, we are able to achieve an average error in the first weight matrix for a similarly sized neural network of just 0.0000009-over two thousand times more precise. To the best of our knowledge no functionally-equivalent CIFAR-10 models have been extracted in the past.</p><p>We are unable to make a comparison between the fidelity of our extraction attack and the fidelity of the attack presented in Batina et al. because they do not report on this number: they only report the accuracy of the extracted model and show it is similar to the original model. We believe this strengthens our observation that comparing across accuracy and fidelity is not currently widely accepted as best practice. Investigating Errors. We observe that as the number of parameters that must be extracted increases, the fidelity of the model decreases. We investigate why this happens and discovered that a small fraction of the time (roughly 1 in 10,000) the gradient estimation procedure obtains an incorrect estimate of the gradient and therefore one of the extracted weights Â(0</p><formula xml:id="formula_18">) i j</formula><p>is incorrect by a non-insignificant margin.</p><p>Introducing an error into just one of the weights of the first matrix Â(0) should not induce significant further errors. However, because of this error, when we solve for the bias vector, the extracted bias B(0) i will have error proportional to the error of Â(0) i j . And when the bias is wrong, it impacts every calculation, even those where this edge is not in use.</p><p>Resolving this issue completely either requires reducing the failure rate of gradient estimation from 1 in 10,000 to practically 0, or would require a complex error-recovery procedure. Instead, we will introduce in the following section an improvement which almost completely solves this issue. of what layer a critical point is on, the inputs of any neuron past layer 1 are the outputs of other neurons, so we only have indirect control over their inputs. Finally, even with the ability to recover these weights, small numerical errors occur in the first layer extraction. These cause errors in every finite differences computation in further layers, causing the second layer to have even larger numerical errors than the first (and so on). Therefore, extending the attack to deeper networks will require at least solving each of the following: producing critical points belonging to a specific layer, recovering weights for those neurons without direct control of their inputs, and significantly reducing numerical errors in these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Hybrid Strategies</head><p>Until now the strategies we have developed for extraction have been pure and focused entirely on learning or entirely on direct extraction. We now show that there is a continuous spectrum from which we can draw attack strategies, and these hybrid strategies can leverage both the query efficiency of learning extraction, and the fidelity of direct extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Learning-Based Extraction with Gradient Matching</head><p>Milli et al. demonstrate that gradient matching helps extraction by optimizing the objective function </p><formula xml:id="formula_19">n ∑ i=1 H(O(x i ), f (x i )) + α|∇ x O(x i ) − ∇ x f (x i )|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Error Recovery through Learning</head><p>Recall from earlier that the functionally-equivalent extraction attack fidelity degrades as the model size increases. This is a result of low-probability errors in the first weight matrix inducing incorrect biases on the first layer, which in turn propagates and causes worse errors in the second layer. We now introduce a method for performing a learningbased error recovery routine. While performing a fullylearning-based attack leaves too many free variables so that functionally-equivalent extraction is not possible, if we fix many of the variables to the values extracted through the direct recovery attack, we now show it is possible to learn the remainder of the variables.</p><p>Formally, let Â(0) be the extracted weight matrix for the first layer and B(0) be the extracted bias vector for the first layer. Previously, we used least squares to directly solve for Â(1) and B(1) assuming we had extracted the first layer perfectly. Here, we relax this assumption. Instead, we perform gradient descent optimizing for parameters W 0..2 that minimize</p><formula xml:id="formula_20">E x∈D f θ (x) −W 1 ReLU( Â(0) x + B(0) +W 0 ) +W 2</formula><p>That is, we use a single trainable parameter to adjust the bias term of the first layer, and then solve (via gradient descent with training data) for the remaining weights accordingly.</p><p>This hybrid strategy increases the fidelity of the extracted model substantially, detailed in Table <ref type="table">8</ref>. In the worstperforming example from earlier (with only direct extraction) the extracted 128-neuron network had 80% fidelity agreement with the victim model. When performing learning-based recovery, the fidelity agreement jumps all the way to 99.75%.</p><p># of Parameters 50,000 100,000 200,000 400,000</p><formula xml:id="formula_21">Transferability 100% 100% 100% 100%</formula><p>Table <ref type="table">8</ref>: Transferability rate of adversarial examples using the extracted neural network from our Section 7 attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Transferability</head><p>Adversarial examples transfer: an adversarial example <ref type="bibr" target="#b47">[47]</ref> generated on one model often fools different models, too.</p><p>Transferability is higher when the models are more similar <ref type="bibr" target="#b6">[7]</ref>.</p><p>We should therefore expect that we can generate adversarial examples on our extracted model, and that these will fool the remote oracle nearly always. In order to measure transferability, we run 20 iterations of PGD <ref type="bibr" target="#b48">[48]</ref> with ∞ distortion set to the value most often used in the literature: for MNIST: 0.1, and for CIFAR-10: 0.03.</p><p>The attack achieves functionally equivalent extraction (modulo floating point precision errors in the extracted weights), so we expect it to have high adversarial example transferability. Indeed, we find we achieve a 100% transferability success rate for all extracted models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Defenses for model extraction have fallen into two camps: limiting the information gained per query, and differentiating extraction adversaries from benign users. Approaches to limiting information include perturbing the probabilities returned by the model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">49]</ref>, removing the probabilities for some of the model's classes <ref type="bibr" target="#b10">[11]</ref>, or returning only the class output <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. Another proposal has considered sampling from a distribution over model parameters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">50]</ref>. The other camp, differentiating benign from malicious users, has focused on analyzing query patterns <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>. Non-adaptive attacks (such as supervised or MixMatch extraction) bypass query patternbased detection, and are weakened by information limiting. We demonstrate the impact of removing complete access to probability values by considering only access to top 5 probabilities from WSL in Table <ref type="table" target="#tab_3">2</ref>. Our functionally-equivalent attack is broken by all of these measures. We leave consideration of defense-aware attacks to future work.</p><p>Queries to a model can also reveal hyperparameters <ref type="bibr" target="#b53">[53]</ref> or architectural information <ref type="bibr" target="#b13">[14]</ref>. Adversaries can use side channel attacks to do the same <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b25">25]</ref>. These are orthogonal to, but compatible with, our work-information about a model, such as assumptions made in Section 6, empowers extraction.</p><p>Watermarking neural networks has been proposed <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b55">55]</ref> to identify extracted models. Model extraction calls into question the utility of cryptographic protocols used to protect model weights. One unrealized approach is obfuscation <ref type="bibr" target="#b56">[56]</ref>, where an equivalent program could be released and queried as many times as desired. A practical approach is secure multiparty computation, where each query is computed by running a protocol between the model owner and querier <ref type="bibr" target="#b57">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This paper characterizes and explores the space of model extraction attacks on neural networks. We focus this paper specifically around the objectives of accuracy, to measure the success of a theft-motivated adversary, and fidelity, an oftenoverlooked measure which compares the agreement between models to reflect the success of a recon-motivated adversary.</p><p>Our learning-based methods can effectively attack a model with several millions of parameters trained on a billion images, and allows the attacker to reduce the error rate of their model by 10%. This attack does not match perfect fidelity with the victim model due to what we show are inherent limitations of learning-based approaches: nondeterminism (including only the nondeterminism on the GPU) prohibits training identical models. In contrast, our direct functionally-equivalent extraction returns a neural network agreeing with the victim model on 100% of the test samples and having 100% fidelity on transfered adversarial examples.</p><p>We then propose a hybrid method which unifies these two attacks, using learning-based approaches to recover from numerical instability errors when performing the functionallyequivalent extraction attack.</p><p>Our work highlights many remaining open problems in model extraction, such as reducing the capabilities required by our attacks and scaling functionally-equivalent extraction.</p><p>A Formal Statements for Section 3.3</p><p>Here, we give the formal arguments for the difficulty of model extraction to support informal statements from Section 3.3.</p><p>Theorem 1. There exists a class of width 3k and depth 2 neural networks on domain [0, 1] d (with precision p numbers) with d ≥ k that require, given logit access to the networks, Θ(p k ) queries to extract.</p><p>In order to prove Theorem 1, we introduce a family of functions we call k-rectangle bounded functions, which we will show satisfies this property. Proof. We will start by constructing a 3-ReLU gadget with output ≥ 1 only when a i ≤ x i ≤ b i . We will then show how to compose k of these gadgets, one for each index of the k-rectangle, to construct the k-rectangle bounded function.</p><p>The 3-ReLU gadget only depends on x i , so weights for all other ReLUs will be set to 0. Observe that the function T</p><formula xml:id="formula_22">i (x; a, b) = ReLU(x−a)+ReLU(x i −b i )−2ReLU(x i − (a i + b i )/2</formula><p>) is nonzero only on the interval (a i , b i ). This is easier to see when it is written as</p><formula xml:id="formula_23">ReLU(x i − a i ) − ReLU(x i − (a i + b i )/2) − (ReLU(x i − (a i + b i )/2) − ReLU(x i − b i )).</formula><p>The function ReLU(x − x 1 ) − ReLU(x − x 2 ) with x 1 &lt; x 2 looks like a sigmoid, and has the following form:</p><formula xml:id="formula_24">ReLU(x − x 1 ) − ReLU(x − x 2 ) =      0 x ≤ x 1 x − x 1 x 1 ≤ x ≤ x 2 x 2 − x 1 x ≥ x Now, T i (x; a i , b i ) • 1/(b i − a i ) has range [0, 1] for any value of a i , b i . Then the function f a,b (x) = ReLU( ∑ i (T i (x; a i , b i )/(b i − a i )) − (k − 1))</formula><p>is k-rectangle bounded for vectors a, b. To see why, we need that no input x not satisfying a x b has</p><formula xml:id="formula_25">∑ i (T i (x; a i , b i )/(b i − a i )) &gt; k − 1.</formula><p>This is simply because each term T i (x; a i , b i ) ≤ 1, so unless all k such terms are &gt; 0, the inequality cannot hold. Now that we know how to construct a k-rectangle bounded function, we will introduce a set of p k disjoint k-rectangle bounded functions, and then show that any one requires p k queries to extract when the others are also possible functions.</p><p>Lemma 2. There exists a family of k-rectangle bounded functions F such that extracting an element of F requires p k queries in the worst case.</p><p>Here, p is the feature precision; images with 8-bit pixels have p = 256.  Proof. We prove this by reduction to subset sum. A similar reduction (reducing to 3-SAT instead of Subset Sum) for a different statement appears in <ref type="bibr" target="#b58">[58]</ref>.</p><p>Suppose we receive a subset sum instance T, p, [v 1 , v 2 , • • • , v d ] -the set is v, the target sum is T , and the problem's precision is p. We will construct networks f 1 and f 2 such that checking if f 1 and f 2 are functionally equivalent is equivalent to solving the subset sum instance. We start by setting f 1 = 0 -it never returns a non-zero value. We now construct a network f 2 that has nonzero output only if the subset sum instance can be solved (and finding an input with nonzero output reveals the satisfying subset).</p><p>The network f 2 has three hidden units in the first layer with incoming weight for the ith feature equal to v i . This means the dot product of the input x with weights will be the sum of the subset {i|x i = 1}. We want to force this to accept iff there is an input where this sum is T . To do so, we use the same 3-ReLU gadget as in the proof of Theorem 1:</p><formula xml:id="formula_26">f 2 (x; T, p, v) = ReLU(x • v − (T − p/2)) + ReLU(x • v − (T + p/2)) − 2ReLU(x • v − T ).</formula><p>As before, this will only be nonzero in the range [T − p/2, T + p/2], and we are done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Prototypicality and Fidelity</head><p>We know from Section 5 that learning strategies struggle to achieve perfect fidelity due to non-determinism inherent in learning. What remains to be understood is whether some samples are more difficult than others to achieve fidelity on.</p><p>We investigate using recent work on identifying prototypical data points. Using each metric developed in Carlini et al. <ref type="bibr" target="#b44">[44]</ref>, we can rank the Fashion-MNIST test set in order of increasing prototypicality. Binning the prototypicality ranking into percentiles, we can measure how many of the 90 models we trained for Section 5 agree with the oracle's prediction. The intuition here is that more prototypical examples should be more consistently learnable, whereas more outlying points may be harder to consistently classify. Indeed, we find that this is the case -all metrics find a correlation between prototypicality and model agreement (fidelity), as seen in Figure <ref type="figure" target="#fig_5">5</ref>. Interestingly, the metrics which do not use ensembles of models (adversarial distance and holdout-retraining) have the best correlation with the model agreement metric-roughly the top 50% of prototypical examples by these metrics are classified the same by nearly all 90 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Supplement for Section 6</head><p>Accuracies for the oracles in Section 6 are found in Table <ref type="table" target="#tab_10">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST CIFAR-10</head><p>Parameters Accuracy Parameters Accuracy Figure <ref type="figure" target="#fig_6">6</ref> shows a distribution over the bits of precision in the difference between the logits (i.e., pre-softmax prediction) of the 16 neuron oracle neural network and the extracted network. Formally, we measure the magnitude of the gap | f θ (x) − f θ(x)|. Notice that this is a different (and typically stronger) measure of fidelity than used elsewhere in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Query Complexity of Functionally Equivalent Extraction</head><p>In this section, we briefly analyze the query complexity of the attack from Section 6. We assume that a simulated partial derivative requires O(1) queries using finite differences.</p><p>1. Critical Point Search. This step is the most nontrivial to analyze, but fortunately this was addressed in <ref type="bibr" target="#b19">[19]</ref>. They found this step requires O(h log(h)) gradient queries, which we simulate with O(h log(h)) model queries.</p><p>2. Weight Recovery. This piece is significantly complicated by not having access to gradient queries. For each 3. Global Sign Recovery. For each ReLU, we require only three queries. Then this step is O(h).</p><p>4. Last Layer Extraction. This step requires h queries to make the system of linear equations full rank (although in practice we reuse previous queries here, making this step require 0 queries).</p><p>Overall, the algorithm requires O(h log(h) + dh + h) = O(dh) queries. Extraction requires Ω(dh) queries without auxillary information, as there are dh parameters in the model. Then the algorithm is query-optimal up to a constant factor, removing logarithmic factors from Milli et al. <ref type="bibr" target="#b19">[19]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrating fidelity vs. accuracy. The solid blue line is the oracle; functionally equivalent extraction recovers this exactly. The green dash-dot line achieves high fidelity: it matches the oracle on all data points. The orange dashed line achieves perfect accuracy: it classifies all points correctly.</figDesc><graphic url="image-1.png" coords="3,340.65,72.00,192.09,187.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(1) ∈ R K . Here, d, h, and K respectively refer Symbol Definition d Input dimensionality h Hidden layer dimensionality (h &lt; d) K Number of classesA (0) ∈ R d×hInput layer weightsB (0) ∈ R h Input layer bias A (1) ∈ R h×KLogit layer weights B(1) ∈ R K Logit layer bias Table5: Parameters for the functionally-equivalent attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 : 2 -</head><label>22</label><figDesc>Figure 2: 2-dimension intuition for the functionally equivalent extraction attack.</figDesc><graphic url="image-2.png" coords="9,54.00,222.99,240.12,222.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition A. 1 .</head><label>1</label><figDesc>A function f on domain [0, 1] d with range R is a rectangle bounded function if there exists two vectors a, b such that f (x) = 0 =⇒ a x b, where denotes elementwise comparison. The function f is a k-rectangle bounded function if there are k indices i such that a i = 0 or b i = 1.Intuitively, a k-rectangle function only outputs a non-zero value on a multidimensional rectangle that is constrained in only k coordinates. We begin by showing that we can implement k-rectangle functions for any a, b using a ReLU network of width k and depth 2. Lemma 1. For any a, b with k indices i such that a i = 0 or b i = 1, we can construct a k-rectangle bounded function for a, b with a ReLU network of width 3k and depth 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proof.</head><label></label><figDesc>We begin by constructing F . The following p ranges are clearly pairwise disjoint: {( i−1 p , i p )} p i=1 . Then pick any k indices, and we can construct p k distinct k-rectangle bounded functions -one for each element in the Cartesian product of each index's set of ranges. Call this set F . The set of inputs with non-zero output is distinct for each function, because their rectangles are distinct. Now consider the information gained from any query. If the query returns a non-zero value, the function is learned. If not, at most one function from F is ruled out -the function whose rectangle was queried. Then any sequence of n queries to an oracle can rule out at most n of the functions of F , so that at least |F | = p k queries are required in the worst case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fidelity is easier on more prototypical examples.</figDesc><graphic url="image-3.png" coords="17,317.88,72.00,240.12,169.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: For a 16-neuron MNIST model the attack works.Plotted here is number of bits of precision on the logits normalized by the value of the lot as done in the prior figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Given some target distribution D F over X , and goal similarity function S(p 1 , p 2 ), the goal</figDesc><table><row><cell>Attack</cell><cell>Type</cell><cell>Model type</cell><cell>Goal</cell><cell>Query Output</cell></row><row><cell>Lowd &amp; Meek [8]</cell><cell>Direct Recovery</cell><cell>LM</cell><cell cols="2">Functionally Equivalent Labels</cell></row><row><cell>Tramer et al. [11]</cell><cell cols="2">(Active) Learning LM, NN</cell><cell>Task Accuracy, Fidelity</cell><cell>Probabilities, labels</cell></row><row><cell>Tramer et al. [11]</cell><cell>Path finding</cell><cell>DT</cell><cell cols="2">Functionally Equivalent Probabilities, labels</cell></row><row><cell>Milli et al. [19] (theoretical)</cell><cell>Direct Recovery</cell><cell cols="3">NN (2 layer) Functionally Equivalent Gradients, logits</cell></row><row><cell>Milli et al. [19]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">of fidelity extraction is to construct an Ô that maxi-</cell></row><row><cell></cell><cell></cell><cell cols="3">mizes Pr x∼D F S( Ô(x), O(x)) . In this work, we consider only label agreement, where S(p 1 , p 2 ) = 1(argmax(p 1 ) =</cell></row><row><cell></cell><cell></cell><cell cols="3">arg max(p 2 )); we leave exploration of other similarity func-</cell></row><row><cell></cell><cell></cell><cell cols="2">tions to future work.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">A natural distribution of interest D F is the data distribution</cell></row><row><cell></cell><cell></cell><cell cols="3">itself-the adversary wants to make sure the mistakes and</cell></row><row><cell></cell><cell></cell><cell cols="3">correct labels are the same between the two models. A recon-</cell></row><row><cell></cell><cell></cell><cell cols="3">naissance attack for constructing adversarial examples would</cell></row><row><cell></cell><cell></cell><cell cols="3">care about a perturbed data distribution; mistakes might be</cell></row><row><cell></cell><cell></cell><cell cols="3">more important to the adversary in this setting. Membership</cell></row><row><cell></cell><cell></cell><cell cols="3">inference would use the natural data distribution, including</cell></row><row><cell></cell><cell></cell><cell cols="3">any outliers. These distributions tend to be concentrated on</cell></row><row><cell></cell><cell></cell><cell cols="3">a low-dimension manifold of X , making fidelity extraction</cell></row><row><cell></cell><cell></cell><cell cols="3">significantly easier than functionally equivalent extraction.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>et al. prove that, for deep random networks with input dimension d and depth h, model extraction approaches that can be written as Statistical Query (SQ) learning algorithms require exp(O(h)) samples for fidelity extraction. SQ algorithms are a restricted form of learning algorithm which only access the data with noisy aggregate statistics; many learning algorithms, such as (stochastic) gradient descent and PCA, are examples. As a result, most learning-based approaches to model extraction will inherit this inefficiency.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Extraction attack (top-5 accuracy/top-5 fidelity) of the WSL model<ref type="bibr" target="#b28">[28]</ref>. Each row contains an architecture and fraction of public ImageNet data used by the adversary. ImageNet is a baseline using only ImageNet labels. WSL is an oracle returning WSL model probabilities. WSL-5 is an oracle returning only the top 5 probabilities. Columns with (+ Rot) use rotation loss on unlabeled data (rotation loss was not run when all data is labeled). An adversary able to query WSL always improves over ImageNet labels, even when given only top 5 probabilities. Rotation loss does not significantly improve the performance on ResNet_v2_50, but provides a (1.36/1.80) improvement for ResNet_v2_200, comparable to the performance boost given by WSL labels on 10% data. In the high-data regime, where we observe a (0.56/1.13) improvement using WSL labels.</figDesc><table><row><cell cols="2">Architecture</cell><cell>Data Fraction</cell><cell>ImageNet</cell><cell>WSL</cell><cell>WSL-5</cell><cell>ImageNet + Rot</cell><cell>WSL + Rot</cell><cell>WSL-5 + Rot</cell></row><row><cell cols="2">Resnet_v2_50</cell><cell>10%</cell><cell cols="3">(81.86/82.95) (82.71/84.18) (82.97/84.52)</cell><cell>(82.27/84.14)</cell><cell>(82.76/84.73)</cell><cell>(82.84/84.59)</cell></row><row><cell cols="2">Resnet_v2_200</cell><cell>10%</cell><cell cols="3">(83.50/84.96) (84.81/86.36) (85.00/86.67)</cell><cell>(85.10/86.29)</cell><cell>(86.17/88.16)</cell><cell>(86.11/87.54)</cell></row><row><cell cols="2">Resnet_v2_50</cell><cell>100%</cell><cell cols="3">(92.45/93.93) (93.00/94.64) (93.12/94.87)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Resnet_v2_200</cell><cell>100%</cell><cell cols="3">(93.70/95.11) (94.26/96.24) (94.21/95.85)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Dataset</cell><cell>Algorithm</cell><cell>250 Queries</cell><cell>1000 Queries</cell><cell>4000 Queries</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVHN</cell><cell>FS</cell><cell cols="3">(79.25/79.48) (89.47/89.87) (94.25/94.71)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVHN</cell><cell>MM</cell><cell cols="3">(95.82/96.38) (96.87/97.45) (97.07/97.61)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR10</cell><cell>FS</cell><cell cols="3">(53.35/53.61) (73.47/73.96) (86.51/87.37)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR10</cell><cell>MM</cell><cell cols="3">(87.98/88.79) (90.63/91.39) (93.29/93.99)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance (accuracy/fidelity) of fully supervised (FS) and MixMatch (MM) extraction on SVHN and CIFAR10. MixMatch with 4000 labels performs nearly as well as the oracle for both datasets, and MixMatch at 250 queries beats fully supervised training at 4000 queries for both datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Impact of non-determinism on extraction fidelity. Even models extracted using the same SGD and initialization randomness as the oracle do not reach 100% fidelity.</figDesc><table><row><cell>a new set of parameters f 2 θ (x). If there are points x such that f 1 θ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Standard binary search requires O(n) model queries to obtain n bits of precision. Therefore, we propose a refined technique which does not have this restriction and requires just O(1) queries to obtain high (20+ bits) precision. The key observation we make is that if we are searching between two values [t 1 ,t 2 ]</figDesc><table><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">t O L (u +tv) 1.6 1.8 2.0 2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>/</cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>O(x) = exp. Ô(x)</cell><cell></cell><cell cols="2">exp. Ô(x)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O(x)</cell></row><row><cell></cell><cell>t 1</cell><cell>x</cell><cell>t 2</cell><cell>t 1</cell><cell>x</cell><cell>t 2</cell></row></table><note>Figure 3: An example sweep for critical point search. Here we plot the partial derivative across t and see that O L (u + tv) is piecewise linear, enabling a binary search.Figure 4: Efficient and accurate 2-linear testing subroutine in Algorithm 1. Left shows a successful case where the algorithm succeeds; right shows a potential failure case, where there are multiple nonlinearities. We detect this by observing the expected value of O(x) is not the observed (queried) value.of critical points, we introduce a refined search algorithm which improves on the binary search as used in<ref type="bibr" target="#b19">[19]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc><ref type="bibr" target="#b11">12</ref>,500 25,000 50,000 100,000 Fidelity of the functionally-equivalent extraction attack across different test distributions on an MNIST victim model. Results are averaged over five extraction attacks. For small models, we achieve perfect fidelity extraction; larger models have near-perfect fidelity on the test data distribution, but begins to lose accuracy at 100, 000 parameters.Difficulties Extending the Attack. The attack is specific to two layer neural networks; deeper networks pose multiple difficulties. In deep networks, the critical point search step of Section 6.3 will result in critical points from many different layers, and determining which layer a critical point is on is nontrivial. Without knowing which layer a critical point is on, we cannot control inputs to the neuron, which we need to do to recover the weights in Section 6.4. Even given knowledge</figDesc><table><row><cell>Fidelity</cell><cell>100%</cell><cell>100%</cell><cell cols="2">100% 99.98%</cell></row><row><cell>Queries</cell><cell>2 17.2</cell><cell>2 18.2</cell><cell>2 19.2</cell><cell>2 20.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Fidelity of extracted MNIST model is improved with the hybrid strategy. Note when comparing to Table6the model sizes are 4× larger.</figDesc><table><row><cell>2 2 ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Statistics for the oracle models we train to extract.</figDesc><table><row><cell>12,500</cell><cell>94.3%</cell><cell>49,000</cell><cell>29.2%</cell></row><row><cell>25,000</cell><cell>95.6%</cell><cell>98,000</cell><cell>34.2%</cell></row><row><cell>50,000</cell><cell>97.2%</cell><cell>196,000</cell><cell>40.3%</cell></row><row><cell>100,000</cell><cell>97.7%</cell><cell>393,000</cell><cell>42.6%</cell></row><row><cell>200,000</cell><cell>98.0%</cell><cell>786,000</cell><cell>43.1%</cell></row><row><cell>400,000</cell><cell>98.3%</cell><cell>1,572,000</cell><cell>45.9%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Ilya Mironov for lengthy and fruitful discussions regarding the functionally equivalent extraction attack. We also thank Úlfar Erlingsson for helpful discussions on positioning the work, and Florian Tramèr for his comments on an early draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSW</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
				<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
				<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berrang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01246</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th {USENIX} Security Symposium ({USENIX} Security 16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knockoff nets: Stealing functionality of black-box models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4954" to="4963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model extraction and active learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1811.02054</idno>
		<ptr target="http://arxiv.org/abs/1811.02054" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards reverse-engineering black-box neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01768</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A framework for the extraction of deep neural networks by leveraging public data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapathy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.09165" />
		<imprint>
			<date type="published" when="1905">1905.09165. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Correia-Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Overlearning reveals sensitive attributes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11742</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Security analysis of deep neural networks operating in the presence of cache side-channel attacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davinroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rackow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dachman-Soled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dumitraş</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03487</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Model reconstruction from model explanations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05185</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
				<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/kˆ2)</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Csi neural network: Using side-channels to recover your artificial neural network information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Picek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09076</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Differential power analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Cryptology Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the learnability of deep random networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904">1904.03866, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09768</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Queries and concept learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="342" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
				<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Combining mixmatch and active learning for better accuracy with fewer labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxWl0NKPB" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking deep active learning: Using unlabeled data at model training</title>
		<author>
			<persName><forename type="first">O</forename><surname>Siméoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJehllrtDS" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hidden technical debt in machine learning systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2503" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Prototypical examples in deep learning: Metrics, characteristics, and utility</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xyx3R9tQ" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Jax</surname></persName>
		</author>
		<ptr target="https://github.com/google/jax" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Defending against model stealing attacks using deceptive perturbations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00054</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adding robustness to support vector machines against adversarial reverse engineering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Prada: protecting against dnn model stealing attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szyller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dmitrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asokan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02628</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Model extraction warning in mlaas paradigm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kesarwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mukhoty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Computer Security Applications Conference</title>
				<meeting>the 34th Annual Computer Security Applications Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stealing hyperparameters in machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Protecting intellectual property of deep neural networks with watermarking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Stoecklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2018 on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Embedding watermarks into deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</title>
				<meeting>the 2017 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the (im) possibility of obfuscating programs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rudich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual international cryptology conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A privacy-preserving protocol for neural-network-based computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th workshop on Multimedia and security</title>
				<meeting>the 8th workshop on Multimedia and security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="146" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
