<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A FAIR COMPARISON OF GRAPH NEURAL NETWORKS FOR GRAPH CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
							<email>federico.errica@phd.unipi.it</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Podda</surname></persName>
							<email>marco.podda@di.unipi.it</email>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
							<email>bacciu@di.unipi.it</email>
						</author>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
							<email>micheli@di.unipi.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A FAIR COMPARISON OF GRAPH NEURAL NETWORKS FOR GRAPH CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the years, researchers have raised concerns about several flaws in scholarship, such as experimental reproducibility and replicability in machine learning <ref type="bibr" target="#b13">(McDermott, 1976;</ref><ref type="bibr" target="#b11">Lipton &amp; Steinhardt, 2018)</ref> and science in general (National Academies of Sciences &amp; Medicine, 2019). These issues are not easy to address, as a collective effort is required to avoid bad practices. Examples include the ambiguity of experimental procedures, the impossibility of reproducing results and the improper comparison of machine learning models. As a result, it can be difficult to uniformly assess the effectiveness of one method against another. This work investigates these issues for the graph representation learning field, by providing a uniform and rigorous benchmarking of state-of-the-art models.</p><p>Graph Neural Networks (GNNs) <ref type="bibr">(Micheli, 2009;</ref><ref type="bibr" target="#b16">Scarselli et al., 2008)</ref> have recently become the standard tool for machine learning on graphs. These architectures effectively combine node features and graph topology to build distributed node representations. GNNs can be used to solve node classification <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref> and link prediction <ref type="bibr">(Zhang &amp; Chen, 2018</ref>) tasks, or they can be applied to downstream graph classification <ref type="bibr" target="#b0">(Bacciu et al., 2018)</ref>. In literature, such models are usually evaluated on chemical and social domains <ref type="bibr" target="#b26">(Xu et al., 2019)</ref>. Given their appeal, an ever increasing number of GNNs is being developed <ref type="bibr" target="#b7">(Gilmer et al., 2017)</ref>. However, despite the theoretical advancements reached by the latest contributions in the field, we find that the experimental settings are in many cases ambiguous or not reproducible. Some of the most common reproducibility problems we encounter in this field concern hyperparameters selection and the correct usage of data splits for model selection versus model assessment. Moreover, the evaluation code is sometimes missing or incomplete, and experiments are not standardized across different works in terms of node and edge features.</p><p>These issues easily generate doubts and confusion among practitioners that need a fully transparent and reproducible experimental setting. As a matter of fact, the evaluation of a model goes through two different phases, namely model selection on the validation set and model assessment on the test set. Clearly, to fail in keeping these phases well separated could lead to over-optimistic and biased estimates of the true performance of a model, making it hard for other researchers to present competitive results without following the same ambiguous evaluation procedures.</p><p>With this premise, our primary contribution is to provide the graph learning community with a fair performance comparison among GNN architectures, using a standardized and reproducible experimental environment. More in detail, we performed a large number of experiments within a rigorous model selection and assessment framework, in which all models were compared using the same features and the same data splits.</p><p>Secondly, we investigate if and to what extent current GNN models can effectively exploit graph structure. To this end, we add two domain-specific and structure-agnostic baselines, whose purpose is to disentangle the contribution of structural information from node features. Much to our surprise, we found out that these baselines can even perform better than GNNs on some datasets; this calls for moderation when reporting improvements that do not clearly outperform structure-agnostic competitors.</p><p>Our last contribution is a study on the effect of node degrees as features in social datasets. Indeed, we show that providing the degree can be beneficial in terms of performances, and it has also implications in the number of GNN layers needed to reach good results. We publicly release code and dataset splits to reproduce our results, in order to allow other researchers to carry out rigorous evaluations with minimum additional effort<ref type="foot" target="#foot_0">1</ref> .</p><p>Disclaimer Before delving into the work, we would like to clarify that this work does not aim at pinpointing the best (or worst) performing GNN, nor it disavows the effort researchers have put in the development of these models. Rather, it is intended to be an attempt to set up a standardized and uniform evaluation framework for GNNs, such that future contributions can be compared fairly and objectively with existing architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph Neural Networks At the core of GNNs is the idea to compute a state for each node in a graph, which is iteratively updated according to the state of neighboring nodes. Thanks to layering <ref type="bibr">(Micheli, 2009)</ref> or recursive <ref type="bibr" target="#b16">(Scarselli et al., 2008)</ref> schemes, these models propagate information and construct node representations that can be "aware" of the broader graph structure. GNNs have recently gained popularity because they can efficiently and automatically extract relevant features from a graph; in the past, the most popular way to deal with complex structures was to use kernel functions <ref type="bibr" target="#b19">(Shervashidze et al., 2011)</ref> to compute task-agnostic features. However, such kernels are non-adaptive and typically computationally expensive, which makes GNNs even more appealing.</p><p>Even though in this work we specifically focus on architectures designed for graph classification, all GNNs share the notion of "convolution" over node neighborhoods, as a generalization of convolution on grids. For example, GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref> first performs sum, mean or max-pooling neighborhood aggregation, and then it updates the node representation applying a linear projection on top of the convolution. It also relies on a neighborhood sampling scheme to keep computational complexity constant. Instead, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> builds upon the limitations of GraphSAGE, extending it with arbitrary aggregation functions on multi-sets. The model is proven to be as theoretically powerful as the Weisfeiler-Lehman test of graph isomorphism. Very recently, <ref type="bibr" target="#b24">Wagstaff et al. (2019)</ref> gave an upper bound to the number of hidden units needed to learn permutation-invariant functions over sets and multi-sets. Differently from the above methods, Edge-Conditioned Convolution (ECC) <ref type="bibr" target="#b20">(Simonovsky &amp; Komodakis, 2017)</ref> learns a different parameter for each edge label. Therefore, neighbor aggregation is weighted according to specific edge parameters. Finally, Deep Graph Convolutional Neural Network (DGCNN) <ref type="bibr">(Zhang et al., 2018)</ref> proposes a convolutional layer similar to the formulation of <ref type="bibr" target="#b10">Kipf &amp; Welling (2017)</ref>. Some models also exploit a pooling scheme, which is applied after convolutional layers in order to reduce the size of a graph. For example, the pooling scheme of ECC coarsens graphs through a differentiable pooling map that can be pre-computed. Similarly, DiffPool <ref type="bibr" target="#b28">(Ying et al., 2018)</ref> proposes an adaptive pooling mechanism that collapses nodes on the basis of a supervised criterion. In practice, DiffPool combines a differentiable graph encoder with its pooling strategy, so that the architecture is end-to-end trainable. Lastly, DGCNN differs from other works in that nodes are sorted and aligned by a specific algorithm called SortPool <ref type="bibr">(Zhang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model evaluation</head><p>The work of <ref type="bibr" target="#b18">Shchur et al. (2018)</ref> shares a similar purpose with our contribution. In particular, the authors compare different GNNs on node classification tasks, showing that results are highly dependent on the particular train/validation/test split of choice, up to the point where changing splits leads to dramatically different performance rankings. Thus, they recommend to evaluate GNNs on multiple test splits to achieve a fair comparison. Even though we operate in a different setting (graph instead of node classification), we follow the authors' suggestions by evaluating models under a controlled and rigorous assessment framework. Finally, the work of Dacrema et al. ( <ref type="formula">2019</ref>) criticizes a large number of neural recommender systems, most of which are not reproducible, showing that only one of them truly improves against a simple baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RISK ASSESSMENT AND MODEL SELECTION</head><p>Here, we recap the risk assessment (also called model evaluation or model assessment) and model selection procedures, to clearly layout the experimental procedure followed in this paper. For space reasons, the overall procedure is visually summarized in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RISK ASSESSMENT</head><p>The goal of risk assessment is to provide an estimate of the performance of a class of models. When a test set is not explicitly given, a common way to proceed is to use k-fold Cross Validation (CV) <ref type="bibr" target="#b21">(Stone, 1974;</ref><ref type="bibr">Varma &amp; Simon, 2006;</ref><ref type="bibr" target="#b2">Cawley &amp; Talbot, 2010)</ref>. k-fold CV uses k different training/test splits to estimate the generalization performance of a model; for each partition, an internal model selection procedure selects the hyper-parameters using the training data only. This way, test data is never used for model selection. As model selection is performed independently for each training/test split, we obtain different "best" hyper-parameter configurations; this is why we refer to the performance of a class of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL SELECTION</head><p>The goal of model selection, or hyper-parameter tuning, is to choose among a set of candidate hyperparameter configurations the one that works best on a specific validation set. If a validation set is not given, one can rely on a holdout training/validation split or an inner k-fold. Nevertheless, the key point to remember is that validation performances are biased estimates of the true generalization capabilities. Consequently, model selection results are generally over-optimistic; this issue is thoroughly documented in <ref type="bibr" target="#b2">Cawley &amp; Talbot (2010)</ref>. This is why the main contribution of this work is to clearly separate model selection and model assessment estimates, something that is lacking or ambiguous in the literature under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OVERVIEW OF REPRODUCIBILITY ISSUES</head><p>To motivate our contribution, we follow the approach of Dacrema et al. ( <ref type="formula">2019</ref>) and briefly review recent papers describing five different GNN models, highlighting problems in the experimental setups as well as reproducibility of results. We emphasize that our observations are based solely on the contents of their paper and the available code 2 . Suitable GNN works were selected according to the following criteria: i) performances obtained with 10-fold CV; ii) peer reviewed; iii) strong architectural differences; iv) popularity. In particular, we selected DGCNN <ref type="bibr">(Zhang et al., 2018)</ref>, DiffPool <ref type="bibr" target="#b28">(Ying et al., 2018)</ref>, ECC <ref type="bibr" target="#b20">(Simonovsky &amp; Komodakis, 2017)</ref>, GIN <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> and GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>. For a detailed description of each model we refer to their respective papers. Our criteria to assess quality of evaluation and reproducibility are: i) code for data preprocessing, model selection and assessment is provided; ii) data splits are provided; iii) data is split by means of a stratification technique, to preserve class proportions across all partitions; iv) results of the 10-fold CV are reported correctly using standard deviations, and they refer to model evaluation (test sets) rather than model selection (validation sets). Table <ref type="table">1</ref> summarizes our findings.</p><p>Table <ref type="table">1</ref>: Criteria for reproducibility considered in this work and their compliance among considered models. (Y) indicates that the criterion is met, (N) indicates that the criterion is not satisfied, (A) indicates ambiguity (i.e. it is unclear whether the criteria is met or not), (-) indicates lack of information (i.e. no details are provided about the criteria). Note that GraphSAGE is excluded from this comparison, as it was not directly applied by authors to graph classification tasks.</p><p>DGCNN DiffPool ECC GIN</p><formula xml:id="formula_0">Data preprocessing code Y Y - Y Model selection code N N - N Model evaluation code Y Y - Y Data splits provided Y N N Y Label Stratification Y N - Y Report accuracy on test Y A A N Report standard deviations Y N N Y</formula><p>DGCNN The authors evaluate the model on 10-fold CV. While the architecture is fixed for all dataset, learning rate and epochs are tuned using only one random CV fold, and then reused on all the other folds. While this practice is still acceptable, it may lead to sub-optimal performances. Nonetheless, the code to reproduce model selection is not available. Moreover, the authors run CV 10 times, and they report the average of the 10 final scores. As a result, the variance of the provided estimates is reduced. However, the same procedure was not applied to the other competitors as well.</p><p>Finally, CV data splits are correctly stratified and publicly available, making it possible to reproduce at least the evaluation experiments.</p><p>DiffPool From both the paper and the provided code, it is unclear if reported results are obtained on a test set rather than a validation set. Although the authors state that 10-fold CV is used, standard deviations of DiffPool and its competitors are not reported. Moreover, the authors affirm to have applied early stopping on the validation set to prevent overfitting; unfortunately, neither model selection code nor validation splits are available. Furthermore, according to the code, data is randomly split (without stratification) and no random seed is set, hence splits are different each time the code is executed.</p><p>ECC The paper reports that ECC is evaluated on 10-fold CV, but results do not include standard deviations. Similarly to DGCNN, hyper-parameters are fixed in advance, hence it is not clear if and how model selection has been performed. Importantly, there are no references in the code repository to data pre-processing, data stratification, data splitting, and model selection.</p><p>GIN The authors correctly list all the hyper-parameters tuned. However, as stated explicitly in the paper and in the public review discussion, they report the validation accuracy of 10-fold CV. In other words, reported results refer to model selection and not to model evaluation. The code for model selection is not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphSAGE</head><p>The original paper does not test this model on graph classification datasets, but GraphSAGE is often used in other papers as a strong baseline. It follows that GraphSAGE results on graph classification should be accompanied by the code to reproduce the experiments. Despite that, the two works which report results of GraphSAGE (DiffPool and GIN) fail to do so.</p><p>Summary Our analysis reveals that GNN works rarely comply with good machine learning practices as regards the quality of evaluation and reproducibility of results. This motivates the need to re-evaluate all models within a rigorous, reproducible and fair environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section we detail our main experiment, in which we re-evaluate the above-mentioned models on 9 datasets (4 chemical, 5 social), using a model selection and assessment framework that closely follows the rigorous practices described in Section 3. In addition, we implement two baselines whose purpose is to understand the extent to which GNNs are able to exploit structural information.</p><p>All models have been implemented by means of the Pytorch Geometrics library <ref type="bibr" target="#b5">(Fey &amp; Lenssen, 2019)</ref>, which provides graph pre-processing routines and makes the definition of graph convolution easier to implement. We sometimes found discrepancies between papers and related code; in such cases, we complied with the specifications in the paper. Because GraphSAGE was not applied to graph classification in the original work, we opted for a max-pooling global aggregation function to classify graph instances; further, we do not use the sampled neighborhood aggregation scheme defined in <ref type="bibr" target="#b8">Hamilton et al. (2017)</ref>, in order to allow nodes to have access to their whole neighborhood.</p><p>Datasets All graph datasets are publicly available <ref type="bibr" target="#b9">(Kersting et al., 2016)</ref> and represent a relevant subset of those most frequently used in literature to compare GNNs. Some collect molecular graphs, while others contain social graphs. In particular, we used D&amp;D <ref type="bibr" target="#b4">(Dobson &amp; Doig, 2003)</ref>, PROTEINS <ref type="bibr" target="#b1">(Borgwardt et al., 2005)</ref>, NCI1 <ref type="bibr" target="#b25">(Wale et al., 2008)</ref> and ENZYMES <ref type="bibr" target="#b17">(Schomburg et al., 2004)</ref> for binary and multi-class classification of chemical compounds, whereas IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-5K and COLLAB <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref> are social datasets. Dataset statistics are reported in Table <ref type="table">A</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>In GNN literature, it is common practice to augment node descriptors with structural features. For example, DiffPool adds the degree and clustering coefficient to each node feature vector, whereas GIN adds a one-hot representation of node degrees. The latter choice trades off an improvement in performances (due to injectivity of the first sum) with the inability to generalize to graphs with arbitrary node degree.</p><p>In general, good experimental practices suggest that all models should be consistently compared to the same input representations. This is why we re-evaluate all models using the same node features.</p><p>In particular, we use one common setting for the chemical domain and two alternative settings as regards the social domain. As regards the chemical domain, nodes are labeled with a one-hot encoding of their atom type, though on ENZYMES we follow the literature and use 18 additional features available. As regards social graphs, whose nodes do not have features, we use either an uninformative feature for all nodes or the node degree. As such, we are able to reason about the effectiveness of the structural inductive bias imposed by the model; that is if the model is able to implicitly learn structural features or not. The effect of adding structural features to general machine learning models for graphs has been investigated in <ref type="bibr" target="#b6">Gallagher &amp; Eliassi-Rad (2008)</ref>; here, we focus on the impact of node degrees on performances for social datasets.</p><p>Baselines We adopt two distinct baselines, one for chemical and one for social datasets. On all chemical datasets but for ENZYMES, we follow <ref type="bibr" target="#b15">Ralaivola et al. (2005)</ref>; Luzhnica et al. ( <ref type="formula">2019</ref>) and implement the Molecular Fingerprint technique, which first applies global sum pooling (i.e., counts the occurrences of atom types in the graph by summing the features of all nodes in the graph together) and then applies a single-layer MLP with ReLU activations. On social domains and ENZYMES (due to the presence of additional features), we take inspiration from the work of <ref type="bibr" target="#b29">Zaheer et al. (2017)</ref> to learn permutation-invariant functions over sets of nodes: first, we apply a single-layer MLP on top of node features, followed by global sum pooling and another singlelayer MLP for classification. Note that both baselines do not leverage graph topology. Using these baselines as a reference is of fundamental importance for future works, as they can provide feedback on the effectiveness of GNNs on a specific dataset. As a matter of fact, if GNN performances are close to the ones of a structure-agnostic baseline, one can draw two possible conclusions: the task does not need topological information to be effectively solved, or the GNN is not exploiting graph structure adequately. While the former can be verified through domain-specific human expertise, the second is more difficult to assess, as multiple factors come into play such as the amount of training data, the structural inductive bias imposed by the architecture and the hyper-parameters used for model selection. Nevertheless, significant improvements with respect to these baselines are a strong indicator that graph topology has been exploited. Therefore, structure-agnostic baselines become vital to understand if and how a model can be improved. Experimental Setting Our experimental approach is to use a 10-fold CV for model assessment and an inner holdout technique with a 90%/10% training/validation split for model selection. After each model selection, we train three times on the whole training fold, holding out a random fraction (10%) of the data to perform early stopping. These three separate runs are needed to smooth the effect of unfavorable random weight initialization on test performances. The final test fold score is obtained as the mean of these three runs; Table <ref type="table" target="#tab_0">2</ref> reports the pseudo-code of the entire evaluation process. To be consistent with literature, we implement early stopping with patience parameter n, where training stops if n epochs have passed without improvement on the validation set. A high value of n can favor model selection by making it less sensitive to fluctuations in the validation score at the cost of additional computation. Importantly, all data partitions have been pre-computed, so that models are selected and evaluated on the same data splits. Moreover, all data splits are stratified, i.e., class proportions are preserved inside each k-fold split as well as in the holdout splits used for model selection.</p><p>Hyper-parameters Hyper-parameter tuning is performed via grid search. For the sake of conciseness, we list all hyper-parameters in Section A.4. Notice that we always include those used by other authors in their respective papers. We select the number of convolutional layers, the embedding space dimension, the learning rate, and the criterion for early stopping (either based on the validation accuracy or validation loss) for all models. Depending on the model, we also selected regularization terms, dropout, and other model-specific parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational considerations</head><p>Our experiments involve a large number of training runs. For all models, grid sizes range from 32 to 72 possible configurations, depending on the number of hyper-parameters to choose from. However, we tried to keep the upper bound on the number of parameters as similar as possible across models. The total effort required, in terms of the number of single training runs, to complete model assessment procedures exceeded 47000. Such a large number required extensive use of parallelism, both in CPU and GPU, to conduct the experiments in a reasonable amount of time. We emphasize that in some cases (e.g. ECC in social datasets), training on a single hyper-parameter configuration required more than 72 hours, which would have made the sequential exploration of one single grid last months. Therefore, due to the large amount of experiments to conduct and to the computational resources available, we limited the time to complete a single training to 72 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head><p>Tables <ref type="table" target="#tab_2">3 and 4</ref> show the results of our experiments. Overall, GIN seems to be effective on social datasets. Importantly, we discover that on D&amp;D, PROTEINS and ENZYMES none of the GNNs are able to improve over the baseline. On the contrary, on NCI1 the baseline is clearly outperformed: this result suggests that the GNNs we analyzed can actually exploit the topological information of the graphs in this dataset. Moreover, we observe that an overly-parameterized baseline is not able to overfit the NCI1 training data completely. To see this, consider that a baseline with 10000 hidden units and no regularization reaches around 67% training accuracy, while GIN can easily overfit (≈ 100%) the training data. This indicates that structural information hugely affects the ability to fit the training set. On social datasets, we observe that adding node degrees as features is beneficial, but such an effect is more noticeable for REDDIT-BINARY, REDDIT-5K and COLLAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">THE IMPORTANCE OF BASELINES</head><p>Our results also show that structure-agnostic baselines are an essential tool to understand the effectiveness of GNNs and extract useful insights. As an example, since none of the GNNs surpasses the baseline on D&amp;D, PROTEINS and ENZYMES, we argue that the state-of-the-art GNN models we analyzed are not able to fully exploit the structure on such datasets yet; indeed, in chemistry, structural features are known to correlate with molecular properties <ref type="bibr" target="#b22">(van Rossum, 1963)</ref>. For all these reasons, we suggest putting small performance gains on these datasets into the right perspective, at least until the baseline will clearly be outperformed. Currently, small average fluctuations on these datasets are likely to be caused by other factors, such as random initializations, rather than a successful exploitation of the structure. In conclusion, we warmly recommend GNN practitioners to include baseline comparisons in future works, in order to better characterize the extent of their contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">THE EFFECT OF NODE DEGREE</head><p>Based on our results, using node degrees as input features is almost always beneficial to increase performances on social datasets, sometimes by a large amount. As an example, degree information is sufficient for our baseline to improve performances of ≈ 15%, hence being competitive on many datasets; in particular, the baseline achieves the best performance on IMDB-BINARY. In contrast, adding node degrees is less relevant for most GNNs, since they can automatically infer such information from the structure. One notable exception is DGCNN, which explicitly needs node degrees to perform well on all datasets. Moreover, we observe that the ranking of all models, after the addition of the degrees, drastically changes; this raises the question about the impact of other structural features (such as clustering coefficient) on performances, which we leave to future works. However, one may also wonder whether the addition of the degree has an influence on the number of layers that are necessary to solve the task or not. We therefore investigated the matter by computing the median number of layers across the 10 different folds. We observed a general trend across models, with GraphSAGE being the only exception, where the addition of the degree reduces the number of layers needed by ≈ 1 as shown in Table <ref type="table">A</ref>.3. This may be due to the fact that most architectures find useful to compute the degree at the very first layer, as such information seems useful to the overall performances.  For each of them, we report validation and test accuracies of the evaluated models, together with published results if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">COMPARISON WITH PUBLISHED RESULTS</head><p>Figure <ref type="figure" target="#fig_0">1</ref> compares the average values of our test results with those reported in literature. In addition, we plot the average of our validation results across the 10 different model selections. The plots show how our test accuracies are in most cases different from what reported in the literature, and the gap between the two estimates is usually consistent. In contrast, our average validation accuracies are always higher or equal to our test results; this is expected, as discussed in Section 3.2. Finally, we emphasize once again that our results are i) obtained within the framework of a rigorous model selection and assessment protocol; ii) fair with respect of data splits and input features assigned to all competitors; iii) reproducible. In contrast, we saw in Section 4 how published results rely on unclear or poorly documented experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we wanted to show how a rigorous empirical evaluation of GNNs can help design future experiments and better reason about the effectiveness of different architectural choices. To this aim, we highlighted ambiguities in the experimental settings of different papers, and we proposed a clear and reproducible procedure for future comparisons. We then provided a complete re-evaluation of five GNNs on nine datasets, which required a significant amount of time and computational resources. This uniform environment helped us reason about the role of structure, as we found that structure-agnostic baselines outperform GNNs on some chemical datasets, thus suggesting that structural properties have not been exploited yet. Moreover, we objectively analyzed the effect of the degree feature on performances and model selection in social datasets, unveiling an effect on the depth of GNNs. Finally, we provide the graph learning community with reliable and reproducible results to which GNN practitioners can compare their architectures. We hope that this work, along with the library we release, will prove useful to researchers and practitioners that want to compare GNNs in a more rigorous way.</p><p>A.4 HYPER-PARAMETERS   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Chemical and social (with degree) benchmark results are shown together with published results (when available). For each of them, we report validation and test accuracies of the evaluated models, together with published results if available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Pseudo-code for model assessment (left) and model selection (right). In Algorithm 1, "Select" refers to Algorithm 2, whereas "Train" and "Eval" represent training and inference phases, respectively. After each model selection, the best configuration best k is used to evaluate the external test fold. Performances are averaged across R training runs, where R in our case is set to 3. : end for 8: best θ ← argmax θ p θ 9: return best θ</figDesc><table><row><cell>Algorithm 1 Model Assessment (k-fold CV)</cell><cell></cell></row><row><cell>1: Input: Dataset D, set of configurations Θ 2: Split D into k folds F 1 , . . . , F k 3: for i ← 1, . . . , k do 4: train k , test k ← j =i F j , F i 5: end for 10: perf k ← 12: return i=1 perf i /k k 11: end for r=1 p r /R R best 9:</cell><cell>Algorithm 2 Model Selection 1: Input: train k , Θ 2: Split train k into train and valid 3: p θ = ∅</cell></row></table><note>k ← Select(train k , Θ) 6: for r ← 1, . . . , R do 7: model r ← Train(train k , best k ) 8: p r ← Eval(model k , test k ) 4: for each θ ∈ Θ do 5: model ← Train(train k , θ) 6: p θ ← p θ ∪ Eval(model, valid) 7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results on chemical datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold.</figDesc><table><row><cell></cell><cell>D&amp;D</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>ENZYMES</cell></row><row><cell>Baseline</cell><cell>78.4 ± 4.5</cell><cell>69.8 ± 2.2</cell><cell>75.8 ± 3.7</cell><cell>65.2 ± 6.4</cell></row><row><cell>DGCNN</cell><cell>76.6 ± 4.3</cell><cell>76.4 ± 1.7</cell><cell>72.9 ± 3.5</cell><cell>38.9 ± 5.7</cell></row><row><cell>DiffPool</cell><cell>75.0 ± 3.5</cell><cell>76.9 ± 1.9</cell><cell>73.7 ± 3.5</cell><cell>59.5 ± 5.6</cell></row><row><cell>ECC</cell><cell>72.6 ± 4.1</cell><cell>76.2 ± 1.4</cell><cell>72.3 ± 3.4</cell><cell>29.5 ± 8.2</cell></row><row><cell>GIN</cell><cell>75.3 ± 2.9</cell><cell>80.0 ± 1.4</cell><cell>73.3 ± 4.0</cell><cell>59.6 ± 4.5</cell></row><row><cell>GraphSAGE</cell><cell>72.9 ± 2.0</cell><cell>76.0 ± 1.8</cell><cell>73.0 ± 4.5</cell><cell>58.2 ± 6.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results on social datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time (&gt; 72 hours for a single training) or GPU memory.</figDesc><table><row><cell></cell><cell></cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>REDDIT-B</cell><cell>REDDIT-5K</cell><cell>COLLAB</cell></row><row><cell>NO FEATURES</cell><cell>Baseline DGCNN DiffPool ECC GIN GraphSAGE</cell><cell>50.7 ± 2.4 53.3 ± 5.0 68.3 ± 6.1 67.8 ± 4.8 66.8 ± 3.9 69.9 ± 4.6</cell><cell>36.1 ± 3.0 38.6 ± 2.2 45.1 ± 3.2 44.8 ± 3.1 42.2 ± 4.6 47.2 ± 3.6</cell><cell>72.1 ± 7.8 77.1 ± 2.9 76.6 ± 2.4 OOR 87.0 ± 4.4 86.1 ± 2.0</cell><cell>35.1 ± 1.4 35.7 ± 1.8 34.6 ± 2.0 OOR 53.8 ± 5.9 49.9 ± 1.7</cell><cell>55.0 ± 1.9 57.4 ± 1.9 67.7 ± 1.9 OOR 75.9 ± 1.9 71.6 ± 1.5</cell></row><row><cell>WITH DEGREE</cell><cell>Baseline DGCNN DiffPool ECC GIN GraphSAGE</cell><cell>70.8 ± 5.0 69.2 ± 3.0 68.4 ± 3.3 67.7 ± 2.8 71.2 ± 3.9 68.8 ± 4.5</cell><cell>49.1 ± 3.5 45.6 ± 3.4 45.6 ± 3.4 43.5 ± 3.1 48.5 ± 3.3 47.6 ± 3.5</cell><cell>82.2 ± 3.0 87.8 ± 2.5 89.1 ± 1.6 OOR 89.9 ± 1.9 84.3 ± 1.9</cell><cell>52.2 ± 1.5 49.2 ± 1.2 53.8 ± 1.4 OOR 56.1 ± 1.7 50.0 ± 1.3</cell><cell>70.2 ± 1.5 71.2 ± 1.9 68.9 ± 2.0 OOR 75.6 ± 2.3 73.9 ± 1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters used for model selection.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code available at: https://github.com/diningphil/gnn-comparison</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">As of the date of this submission.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENTS D. Bacciu would like to acknowledge support from the Italian Ministry of Education, University, and Research (MIUR) under project SIR 2014 LIST-IT (grant n. RBSI14STDE).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 VISUALIZATION OF THE EVALUATION FRAMEWORK Figure <ref type="figure">2</ref>: We give a visual representation of the evaluation framework. We apply an external k outfold CV to get an estimate of the generalization performance of a model, and we use an hold-out technique (bottom-left) to select the best hyper-parametres. For completeness, we show that it is also possible to apply an inner k inn -fold CV (implementing a complete Nested Cross Validation), which obviously amounts to multiplying the computational costs of model selection by a factor k inn . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DATASET STATISTICS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual graph Markov model: A deep and generative approach to graph processing</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On over-fitting in model selection and subsequent selection bias in performance evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">L C</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2079" to="2107" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems (RecSys)</title>
				<meeting>the 13th ACM Conference on Recommender Systems (RecSys)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning on Graphs and Manifolds Workshop, International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging label-independent features for classification in sparsely labeled networks: An empirical study</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Social Network Mining and Analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Troubling trends in machine learning scholarship</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On graph classification networks, datasets and baselines</title>
		<author>
			<persName><forename type="first">Enxhell</forename><surname>Luzhnica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Learning and Reasoning with Graph-Structured Data, International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Alessio Micheli. Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName><forename type="first">Drew</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="1976">1976. 2009</date>
		</imprint>
	</monogr>
	<note>ACM SIGART Bulletin</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Engineering National Academies of Sciences and Medicine. Reproducibility and replicability in science</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>National Academies Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph kernels for chemical informatics</title>
		<author>
			<persName><forename type="first">Liva</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroto</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1110" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: updates and major new developments</title>
		<author>
			<persName><forename type="first">Ida</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antje</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="D431" to="D433" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop, Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictions</title>
		<author>
			<persName><forename type="first">Mervyn</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B</title>
		<imprint>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Relation Between Chemical Structure and Biological Activity</title>
		<author>
			<persName><surname>Jacques Van Rossum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pharmacy and Pharmacology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="285" to="316" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bias in error estimation when using cross-validation for model selection</title>
		<author>
			<persName><forename type="first">Sudhir</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2" to="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the limitations of representing functions on sets</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research</title>
				<meeting>the International Conference on Machine Learning (ICML), volume 97 of Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6487" to="6494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<meeting>the 21th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
