<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast Optimization Method for General Binary Code Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fumin</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
							<email>fumin.shen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">is School of Information Technology and Electrical Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Heng</roleName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
							<email>jingkuan.song@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
							<email>shenht@itee.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and En-gineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>Cheng-du</addrLine>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Centre for Quantum Computation and Intelligent Systems</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<addrLine>81 Broadway Street</addrLine>
									<postCode>2007</postCode>
									<settlement>Sydney, Ultimo</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">is School of Information Technology and Electrical Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fast Optimization Method for General Binary Code Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C27E5A58415F0873ED6A3FA58FD9A310</idno>
					<idno type="DOI">10.1109/TIP.2016.2612883</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612883, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Binary code learning</term>
					<term>Hashing</term>
					<term>Discrete optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hashing or binary code learning has been recognized to accomplish efficient near neighbor search, and has thus attracted broad interests in recent retrieval, vision and learning studies. One main challenge of learning to hash arises from the involvement of discrete variables in binary code optimization. While the widely-used continuous relaxation may achieve high learning efficiency, the pursued codes are typically less effective due to accumulated quantization error. In this work, we propose a novel binary code optimization method, dubbed Discrete Proximal Linearized Minimization (DPLM), which directly handles the discrete constraints during the learning process. Specifically, the discrete (thus nonsmooth nonconvex) problem is reformulated as minimizing the sum of a smooth loss term with a nonsmooth indicator function. The obtained problem is then efficiently solved by an iterative procedure with each iteration admitting an analytical discrete solution, which is thus shown to converge very fast. In addition, the proposed method supports a large family of empirical loss functions, which is particularly instantiated in this work by both a supervised and an unsupervised hashing losses, together with the bits uncorrelation and balance constraints. In particular, the proposed DPLM with a supervised 2 loss encodes the whole NUS-WIDE database into 64-bit binary codes within 10 seconds on a standard desktop computer. The proposed approach is extensively evaluated on several large-scale datasets and the generated binary codes are shown to achieve very promising results on both retrieval and classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Binary coding (also known as hashing) has recently become a very popular research subject in information retrieval <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b37">[38]</ref>, computer vision <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>, machine learning <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>, etc. By encoding high-dimensional feature vectors (e.g., of documents, images, videos, or other types of data) to short hash codes, an effective hashing method is expected to accomplish efficient similarity search while preserving the similarities among original data to some extent. As a result, using binary codes to represent and search in massive data is a promising solution to handle largescale tasks, owing to reduced storage space (typically several hundred binary bits per datum) and the low complexity of pairwise distance computations in a Hamming space.</p><p>The hashing techniques can be generally divided into two major categories: data-independent and data-dependent methods. Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b7">[8]</ref> represents one large family of data-independent methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref>, which generate hash functions via random projections. Although LSH is ensured to have high collision probability for similar data items, in practice LSH usually needs long hash bits and multiple hash tables to achieve both high precision and recall. The huge storage overhead may restrict its applications.</p><p>The other category, data-dependent or learning based hashing methods have witnessed a rapid development in the most recent years, due to the benefit that they can effectively and efficiently index and organize massive data with very compact binary codes. Different from LSH, data-dependent binary coding methods aim to generate short binary codes using the training data. A number of algorithms in this category have been proposed, including the unsupervised Spectral Hashing <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b34">[35]</ref>, Binary Reconstructive Embedding (BRE) <ref type="bibr" target="#b11">[12]</ref>, PCA Hashing <ref type="bibr" target="#b32">[33]</ref>, Iterative Quantization (ITQ) <ref type="bibr" target="#b8">[9]</ref>, Circulant Binary Embedding (CBE) <ref type="bibr" target="#b38">[39]</ref>, Anchor Graph Hashing (AGH) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, Isotropic Hashing (IsoHash) <ref type="bibr" target="#b10">[11]</ref>, Inductive Manifold Hashing <ref type="bibr" target="#b28">[29]</ref>, Neighborhood Discriminant Hashing (NDH) <ref type="bibr" target="#b31">[32]</ref>, Binary Projection Bank (BPB) <ref type="bibr" target="#b18">[19]</ref> etc., and the supervised Minimal Loss Hashing (MLH) <ref type="bibr" target="#b24">[25]</ref>, Semi-Supervised Hashing (SSH) <ref type="bibr" target="#b32">[33]</ref>, Kernel-Based Supervised Hashing (KSH) <ref type="bibr" target="#b20">[21]</ref>, FastHash <ref type="bibr" target="#b16">[17]</ref>, Graph Cut Coding (GCC <ref type="bibr" target="#b6">[7]</ref>), Supervised Discrete Hashing (SDH) <ref type="bibr" target="#b27">[28]</ref> etc. The literature is comprehensive reviewed in <ref type="bibr" target="#b33">[34]</ref> recently.</p><p>The binary constraints imposed on the target hash codes make the associated optimization problem very difficult to solve, which are generally NP-hard. To simplify the optimization, most of the methods in the literature adopt the following two-step way: first solve a relaxed problem by discarding the discrete constraints, and then quantize the obtained continuous solution to achieve the approximate binary solution. This two-step scheme significantly simplifies the original discrete optimization. Unfortunately, such an approximate solution is typically of low quality and often makes the resulting hash functions less effective. This is possibly due to the accumulated quantization error, which is especially the case when learning long-length codes. Iterative Quantization (ITQ) <ref type="bibr" target="#b8">[9]</ref> is an effective approach to decrease the quantization distortion by applying an orthogonal rotation to projected training data. One limitation of ITQ is that it learns orthogonal rotations over pre-computed mappings (e.g., PCA or CCA) and the separate learning procedure usually makes ITQ suboptimal.</p><p>It would help generate more effective hashes to directly optimize the binary codes without continuous relaxations. However, the importance of discrete optimization in hashing has been less taken into account by most existing hashing methods. Recent efforts in this direction either lead to intractable optimization or are restricted to specific losses thus not easy to generalize. Very recently, binary optimization was studied in the unsupervised discrete graph hashing (DGH <ref type="bibr" target="#b19">[20]</ref>) and promising results were obtained compared to previous relaxed methods. One disadvantage of DGH is that it suffers from an expensive optimization due to the involvement of singular value decomposition in each optimization iteration. In the meantime, supervised discrete hashing (SDH <ref type="bibr" target="#b27">[28]</ref>) formulated supervised hashing as a linear classification problem with binary codes, where the associated key binary quadratic program (BQP) was efficiently solved by the discrete cyclic coordinate descent (DCC). However, DCC is limited to solving the standard BQP problem and it is still unclear how to apply DCC to other hashing problems with different objectives. For instance, DCC is not ready to optimize with the uncorrelation and balance constraints, which are widely-used in the hashing literature <ref type="bibr" target="#b35">[36]</ref>.</p><p>To overcome these problems, in this work, we propose a fast discrete optimization method for the general binary code learning problem. Our main contributions are summarized as follows:</p><p>1) The general binary code learning problem with discrete constraints is rewritten as an unconstrained minimization problem with an objective comprising two parts: a smooth loss function and a nonsmooth indicator function.</p><p>The smooth function characterizes the learning loss of target binary codes with training data, while the nonsmooth one indicates the binary domain of the optimizing codes. The simple reformulation greatly simplifies the nonconvex nonsmooth binary code optimization problem. 2) We propose a novel discrete optimization method, termed Discrete Proximal Linearized Minimization (DPLM), to learn binary codes in an efficient iterative way. In each optimization iteration, The corresponding subproblem admits an analytical solution by directly investigating the binary code space. As such, a highquality discrete solution without resort to the continuous relaxation can eventually be obtained in an efficient computing manner, therefore enabling to tackle massive datasets. 3) Different from other discrete optimization solvers in the hashing literature, the proposed method supports a large family of empirical loss functions. In this work, this method is particularly instantiated by the supervised 2 loss and unsupervised graph hashing loss. The wellknown bits uncorrelation and balance constraints are also investigated in the proposed optimization framework. 4) Comprehensive evaluations are conducted on several representative retrieval benchmarks, and the results consistently validate the superiority of the proposed methods over the state-of-the-art in terms of both efficiency and efficacy. In addition, we also show that the binary codes generated by our algorithm perform very well on the image and scene classification problems. The rest of the paper is organized as follows. Section II elaborates the details of the proposed DPLM method, which is instantiated by both a supervised and an unsupervised hashing objective in Section III, followed by the exploration of bits uncorrelation and balance constraints. In Section IV, we analyze the proposed discrete algorithm with comparison to the relaxed method and other optimization approach. In Section V, we evaluate our algorithm on several real-world large-scale datasets for both retrieval and classification tasks, followed by the conclusion of this work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FAST BINARY OPTIMIZATION FOR HASHING</head><p>Let us first introduce some notations. We denote matrices as boldface uppercase letters like X, vectors as boldface lowercase letters like x and scalars as x. The r × r identity matrix is denoted as I r , and the vector with all ones and zeros as 1 and 0, respectively. We abbreviate the Frobenius norm || • || F as || • || in this paper. ∇f denotes the gradient of function f (•). sgn(•) is the sign function with output +1 for positive numbers and -1 otherwise. For binary codes, we use (1, -1) bits for mathematical derivations, and use (1, 0) bits for implementations of all referred binary coding and hashing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The binary code learning problem</head><p>Suppose we have n samples x i ∈ R d , i = 1, • • • , n, stored in matrix X ∈ R d×n . For each sample x, we aim to learn its r-bit binary code b ∈ {-1, 1} r . We consider the following general binary code learning problem</p><formula xml:id="formula_0">min B L(B)<label>(1)</label></formula><formula xml:id="formula_1">s.t. B ∈ {-1, 1} r×n .</formula><p>Here B is the target binary codes for X and L(•) is the smooth loss function. In this work, we aim at a scalable and computationally tractable method which can be applied to a large family of loss functions L(•).</p><p>The binary constraints make problem (1) a mixed-integer optimization problem, which is generally NP-hard. Most previous methods resort to the continuous relaxation by discarding the discrete constraints. As aforementioned, however, this relaxed solution may cause large error accumulation as the code length increases. This is mainly because the discrete constraints have not been treated adequately during the learning procedure, as shown in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discrete Proximal Linearized Minimization</head><p>In this section, we shown problem (1) can be solved in an efficient way while keeping the discrete variables in the optimization. To simplify the discrete optimization in problem <ref type="bibr" target="#b0">(1)</ref>, let us first introduce the following indicator function</p><formula xml:id="formula_2">δ C (B) = 0 if B ∈ C +∞ otherwise, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where C is a nonempty and closed set. Let B denotes the binary codes space {-1, 1} r×n . The function δ B (B) yields infinity as long as one entry of B does not belong to the binary domain {-1, 1}. With the indicator function, we are safe to rewrite problem (1) to an unconstrained minimization problem</p><formula xml:id="formula_4">min B L(B) + δ B (B).<label>(3)</label></formula><p>The simple reformulation of problem (1) to (3) greatly simplify the optimization therein, as shown below. The objective of (3) consists of two parts: a smooth function and a nonsmooth one. The smooth function L(B) models the hashing loss which can be chosen freely according to different problem scenarios, while the nonsmooth function δ B (B) indicates the domain of the optimizing hash codes.</p><p>Solving problem (3) is still nontrivial due to the involvement of nonsmooth indicator. Inspired by the recent advance in nonconvex and nonsmooth optimization <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, we solve problem (3) with the following iterative procedure. Denote Prox f λ the proximal operator with function f and parameter λ:</p><formula xml:id="formula_5">Prox f λ (x) = arg min y f (y) + λ 2 ||y -x|| 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Suppose we have obtained the code solution B (j) at the j th iteration for problem <ref type="bibr" target="#b2">(3)</ref>. At the (j + 1) th iteration, B is updated by</p><formula xml:id="formula_7">B (j+1) = Prox δ λ B (j) - 1 λ ∇L(B (j) )<label>(5)</label></formula><p>= arg min</p><formula xml:id="formula_8">B δ(B) + λ 2 ||B -B (j) + 1 λ ∇L(B (j) )|| 2 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>The optimization procedure with (5) is also known as the forward-backward splitting algorithm <ref type="bibr" target="#b0">[1]</ref>. The forwardbackward splitting scheme for minimizing the sum of a smooth function L(•) with a nonsmooth one can simply be viewed as the proximal regularization of L(•) linearized at a given point B (j) . By transforming the indicator function back to the binary constraints, solution (6) leads to the following problem</p><formula xml:id="formula_10">min B ||B -B (j) + 1 λ ∇L(B (j) )|| 2 ,<label>(7)</label></formula><formula xml:id="formula_11">s.t. B ∈ {-1, 1} r×n .</formula><p>Remark 1. The derivation of problem (5) and ( <ref type="formula" target="#formula_10">7</ref>) is the key step of our algorithm. By looking at <ref type="bibr" target="#b6">(7)</ref>, we can see that the problem actually seeks the projection of B (j) -1 λ ∇L(B (j) ) onto the binary code space. Indeed, for the indicator function δ(B) (of the nonempty and closed set B), its proximal map Prox δ λ (X) reduces to the projection operator:</p><formula xml:id="formula_12">P B (X) = arg min{||B -X|| 2 : B ∈ B}.<label>(8)</label></formula><p>It is clear that, problem <ref type="bibr" target="#b6">(7)</ref> has the analytical solution</p><formula xml:id="formula_13">B (j+1) = sgn(B (j) - 1 λ ∇L(B (j) )).<label>(9)</label></formula><p>We term this optimization method as Discrete Proximal Linearized Minimization (DPLM) due to the involvement of discrete variables compared to the linearized proximal method <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the following, we show that for problem (1) the algorithm (9) converges to a critical point. First we introduce the convergence theorem from <ref type="bibr" target="#b0">[1]</ref> for the nonconvex gradient projection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 ([1]</head><p>). Let f : R n → R be a differentiable function whose gradient is L-Lipschitz continuous, and C a nonempty closed subset of R n . Being given ∈ (0, 1 2L ) and a sequence of stepsizes γ k such that &lt; γ k &lt; 1 L -, we consider a sequence (x k ) that complies with</p><formula xml:id="formula_14">x k+1 ∈ P C (x k -γ k ∇f (x k )), with x 0 ∈ C. If the function f + δ C is a Kurdyka-Lojasiewicz (KL) function and if (x k ) is bounded, then the sequence (x k ) converges to a point x in C.</formula><p>Corollary 3. Assume the loss function L is a C 1 (continuously differentiable) semi-algebraic function whose gradient is L-Lipschitz continuous. By choosing a proper sequence of parameters of λ, the sequence B (j) generated by the proposed DPLM algorithm with (9) converges to a critical point B .</p><p>Proof: This assumption ensures that the objective of ( <ref type="formula" target="#formula_4">3</ref>) <ref type="bibr" target="#b0">[1]</ref>. It is obvious that the sequence B (j) generated by ( <ref type="formula" target="#formula_13">9</ref>) is bounded in B. Based on Theorem 2, by choosing the parameter λ greater than the Lipschitz constant L, the DPLM algorithm converges to some critical point.</p><formula xml:id="formula_15">L(•) + δ B (B) is a KL function</formula><p>Remark 4. The requirement of the presented DPLM method is only mild. The KL assumption of the objective function is very general that L being the smooth polynomial is a typical instance. The empirical convergence of DPLM is referred to Section IV-C.</p><p>Till now, we have presented the key optimization method for learning binary codes with a general loss function. The optimization procedure is outlined in Algorithm 1. Despite its simplicity, the proposed method can obtain very high-quality codes for the retrieval and classification tasks, as shown in our experiments.</p><p>In addition, due to the analytical solution at each iteration, this method enjoys very fast optimization. We note that the analytical solution does not depend on a specific loss function. In Section III, we will discuss the application of DPLM to different hashing losses, such as supervised 2 hashing and unsupervised graph hashing. We will also show the wellknown bits uncorrelation balance constraints can be easily incorporated in the binary code optimization with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hash function learning</head><p>The above method describes the learning procedure for generating binary codes B for training data X. For a novel query x ∈ R d , we need a hash function to efficiently encode x into binary code. We here adopt the simple linear hash function h(x) = sgn(P x), which is learned by solving a Algorithm 1 Discrete Proximal Linearized Minimization Input: Training data X; code length r; maximum iteration number t; parameters λ. Output: Binary codes B ∈ {-1, 1} r×n ; hash function h(x).</p><p>1) Initialize B by the sign of random Gaussian matrix; 2) Loop until converge or reach maximum iterations:</p><p>-Calculate the gradient ∇L; -Update B by sgn(B -1 λ ∇L(B)); 3) Compute hash function h(x) = sgn(P x) with obtained B by <ref type="bibr" target="#b9">(10)</ref>.</p><p>linear regression system with the available training data and codes. That is</p><formula xml:id="formula_16">min P ||B -P X|| 2 ,<label>(10)</label></formula><p>which is clearly solved by P = (XX ) -1 XB . This hash function learning scheme has been widely used, such as in <ref type="bibr">[28][42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CASE STUDIES OF THE HASHING PROBLEMS</head><p>In this section, we investigate different hashing problems with the proposed DPLM method, where both the supervised and unsupervised losses are studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised hashing</head><p>We adopt the 2 loss in the supervised setting, where the learned binary codes are assumed to be optimal for linear classification. The learning objective writes,</p><formula xml:id="formula_17">L(B, W) = 1 2 n i=1 ||y i -W b i || 2 + δ||W|| 2 = 1 2 ||Y -W B|| 2 + δ||W|| 2 . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>Here Y ∈ R k×n stores the labels of of training data X ∈ R d×n , with its (i, j)-th entry Y ij = 1 if the j-th sample x j belongs to the i-th of the total k classes and 0 otherwise. Matrix W is the classification matrix which is jointly learned with the binary codes. δ is the regularization parameter. The above simple objective has been shown to achieve very promising results recently <ref type="bibr" target="#b27">[28]</ref>.</p><p>With the 2 loss, the binary codes can be easily computed by the DPLM optimization method as shown in Section II-B. Given W, the key step is updating B by <ref type="bibr" target="#b8">(9)</ref> with the following gradient</p><formula xml:id="formula_19">∇L(B) = WW B -WY. (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>With B obtained, the classification matrix W is efficiently computed by W = (BB ) -1 BY . The whole optimization alternatively runs over variable B and W. In practice, we simply initialize B by the sign of random Gaussian matrix, and W and B are then updated accordingly. The optimization typically converges within 5 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised graph hashing</head><p>For the unsupervised setting, we investigate the well-known graph hashing problem <ref type="bibr" target="#b35">[36]</ref>, which has been extensively studied in the literature <ref type="bibr" target="#b35">[36]</ref>[22] <ref type="bibr" target="#b28">[29]</ref>. The unsupervised graph hashing optimizes the following objective</p><formula xml:id="formula_21">L(B) = 1 2 n i,j=1 ||b i -b j || 2 A ij (13) = 1 2 tr(BLB ), (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>where A is the affinity matrix computed with A ij = exp(-||x i -x j || 2 /σ 2 ) and σ is the bandwidth parameter. L is the associated Laplacian matrix L = diag(A1) -A.</p><p>To tackle this challenging problem, Spectral Hashing <ref type="bibr" target="#b35">[36]</ref> additionally assumes that data are sampled from a uniform distribution, which leads to a simple analytical eigenfunction solution of 1-D Laplacians. However, the strong assumption can hardly be true in practice. AGH <ref type="bibr" target="#b21">[22]</ref> employs the anchor graph to facilitate constructing affinity matrix and learning hash functions analytically. IMH <ref type="bibr" target="#b28">[29]</ref> learns Laplacian eigenmaps on a small data subset and the hash codes are thus inferred with a linear combination of the base points. All these methods apply spectral relaxation to simplify the optimization by discarding the binary constraints.</p><p>Different from these methods, we optimize the graph hashing problem by DPLM directly over the binary variables. With the gradient of <ref type="bibr" target="#b12">(13)</ref> as</p><formula xml:id="formula_23">∇L(B) = BL,<label>(15)</label></formula><p>the optimization is performed by updating variable B in each iteration with</p><formula xml:id="formula_24">B (j+1) = sgn(B (j) - 1 λ B (j) L).</formula><p>Note that the computation of affinity matrix A dominates the optimization and is O(dn 2 ) in time complexity. In practice, we adopt the anchor graph to compute A = ZZ with Z ∈ R n×m as in <ref type="bibr" target="#b21">[22]</ref>, which is O(dnm) with m anchors. The gradient <ref type="bibr" target="#b14">(15)</ref> is thus computed by Bdiag(A1)-BA which is O(rmn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bits uncorrelation and balance</head><p>The bits uncorrelation and balance constraints have been widely used in previous hashing methods. With these two constraints, problem (1) is rewritten as</p><formula xml:id="formula_25">min B L(B)<label>(16)</label></formula><formula xml:id="formula_26">s.t. BB = nI r , B1 = 0, B ∈ {-1, 1} r×n .</formula><p>The first two constraints force the binary codes to be uncorrelated and balanced, respectively. They are two key features of compact binary code learning <ref type="bibr" target="#b35">[36]</ref>. A large family of existing hashing algorithms can be seen as the instances of this general model, such as unsupervised graph hashing <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and supervised hashing <ref type="bibr" target="#b6">[7]</ref>. However, these two constraints often make the hashing problem computationally intractable, especially with the binary constraints. The recent proposed discrete optimization solver <ref type="bibr" target="#b27">[28]</ref> discards these constraints for algorithm feasibility. We rewrite <ref type="bibr" target="#b15">(16)</ref> as follows</p><formula xml:id="formula_27">min B L(B) + µ 4 ||BB || 2 + ρ 2 ||B1|| 2<label>(17)</label></formula><formula xml:id="formula_28">s.t. B ∈ {-1, 1} r×n . Note that ||BB -nI r || 2 = ||BB || 2 +const.</formula><p>With sufficiently large parameters µ &gt; 0 and ρ &gt; 0, problems ( <ref type="formula" target="#formula_25">16</ref>) and ( <ref type="formula" target="#formula_27">17</ref>) will be equivalent. Denoting the objective of (17) as g(B), its gradient is given by</p><formula xml:id="formula_29">∇g(B) = ∇L(B) + µBB B + ρB11 . (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>With this, the binary optimization is conducted by updating variable B in each iteration with</p><formula xml:id="formula_31">B (j+1) = sgn(B (j) - 1 λ ∇g(B (j) ).<label>(19)</label></formula><p>In Section IV, we will explore the impact of these two binary codes properties for both the supervised and unsupervised hashing problems studied in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Complexity study</head><p>In this part, we discuss the computational complexity of our algorithm. For the supervised method in Section III-A, the main step is updating by computing its gradient WW B -WY, for which the time complexity is O(r 2 k + r 2 n+rkn), thus making the total time complexity of updating B be O(t(r 2 k + r 2 n + rnk)), where t is the maximum iteration number during the B updating step. The complexity of updating W is O(r 3 +rkn+r 2 k). Therefore, the total time complexity of the supervised algorithm is O(T (tr 2 n + rkn) with T iterations (updating W and B).</p><p>The unsupervised algorithm in Section III-B comprises two components: the anchor graph construction and binary code learning. As mentioned in III-B, the first part costs O(dnm) time and the second part costs O(trmn) with t iterations.</p><p>The time complexity of computing the hash functions with equation ( <ref type="formula" target="#formula_16">10</ref>) is O(d 3 + 2d 2 n). As for these algorithms with bit uncorrelation and balance constraints, the additional computation is due to the computing of µBB B + ρB11 in <ref type="bibr" target="#b17">(18)</ref> in each iteration of updating B, which is O(r 2 n + 2rn) in time.</p><p>To summarize, the training time complexities for the proposed supervised and unsupervised algorithms are both linear as the data size n. For a novel query, the predicting time with hash function h(x) is O(dr) which is independent of n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ALGORITHM ANALYSIS</head><p>In this section, we evaluate the proposed method from the following aspects: the impact of bits uncorrelation and balance, the optimization performance of DPLM compared to other solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The impact of bits uncorrelation and balance</head><p>We first evaluate the impact of the two well-known constraints on binary codes: bits uncorrelation and balance. Both the supervised loss and unsupervised loss are evaluated. The performance of our method with or without each of the constraints is shown in Table <ref type="table">I</ref>. The database of NUS-WIDE and CIFAR-10 are used for evaluation. As we can see, the constraints play important roles in binary code learning. For the two hashing losses, better results are obtained by imposing both these constraints than discarding them or keeping only one in most cases. The ability to incorporate these two constraints into binary code optimization is one of the advantages of our method over other discrete methods such as DCC <ref type="bibr" target="#b27">[28]</ref>. We also show in details the impacts of parameters µ and ρ with both the 2 supervised loss <ref type="bibr" target="#b10">(11)</ref> and unsupervised graph hashing loss <ref type="bibr" target="#b12">(13)</ref>. The MAP results with varying and ρ on NUS-WIDE are are shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DPLM vs. the relaxed method</head><p>One may be interested in how the proposed DPLM method performs compared to the relaxed method: first relaxing the original discrete problem to a continuous one and then rounding the resultant solution. In this part, we compare DPLM and the relaxed method for both the supervised 2 loss and unsupervised graph hashing loss. The mAP and Precision500 results are shown in Figure <ref type="figure">2</ref> with code length varying from 32 to 128 bits.</p><p>From Figure <ref type="figure">2</ref>, large performance gains are clearly observed with DPLM over the relaxed approach for both of the two hashing losses. The superior results demonstrate the effectiveness of our optimization method and the importance of discrete optimization for binary code learning or hashing problems. That is, it would be preferred to directly pursue the discrete codes in the binary space without continuous relaxations, provided that scalable and tractable solvers are accessible. In the next part, we will evaluate the convergence speed of DPLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DPLM vs. DCC</head><p>One main contribution of this work is the fast binary optimization method for hashing. In this part, we compare the optimization speed between the proposed Discrete Proximal Linearized Minimization (DPLM) and the recent discrete cyclic coordinate descent (DCC) method <ref type="bibr" target="#b27">[28]</ref>. To be fair, we omit the uncorrelation and balance constraints which DCC cannot handle. That is, both DPLM and DCC both minimize the following objective function and are compared according to the obtained optimal solutions:</p><formula xml:id="formula_32">min B,W 1 2 ||Y -W B|| 2 + δ||W|| 2 s.t. B ∈ {-1, 1} r×n .</formula><p>The objective value as a function of optimization time is shown in Figure <ref type="figure">3</ref>. We can clearly see that both these two methods converge to similar objective value. However, DPLM obtains much faster convergence than DCC. DPLM only costs TABLE I: Evaluation of our method with/without the constraint of balance or uncorrelation on the image retrieval task. -: this operation is not applied; : applied. Both the supervised and unsupervised losses are tested. The results are reported in mAP and Precision of top 500 retrieved samples with 64 and 128 bits. The database of NUS-WIDE and CIFAR-10 are used, where the descriptions can be found in Section V-A and <ref type="bibr" target="#b27">[28]</ref>   about 1 second to achieve the convergence. This is mainly because DPLM updates all bits at the same time in each iteration. In contrast, DCC computes the codes in a bit-by-bit manner, where each bit is computed based on the previously updated bits. In the next section, we will extensively compare the two algorithms on both the retrieval and classification tasks.</p><p>The advantages of DPLM over DCC is summarized as the following points: 1) DPLM is much more efficient than DCC for the binary optimization problem, as shown in Figure <ref type="figure">3</ref>; 2) is developed to solve the general binary code learning problem while DCC can only solve the BQP problem and cannot handle the bit uncorrelation constraints; 3) By adopting the bit balance and uncorrelation constraints, DPLM can achieve better performance than DCC, as shown in Table <ref type="table" target="#tab_2">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, extensive experiments are conducted to evaluate the proposed hashing methods in both computational efficiency and retrieval or classification performance. We test our method on three large-scale public databases, i.e., SUN397 <ref type="bibr" target="#b36">[37]</ref>, NUS-WIDE <ref type="bibr" target="#b2">[3]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref>. The detailed descriptions of these databases are introduced in the corresponding subsection. Several state-of-art hashing methods are taken into comparison, including the supervised MLH <ref type="bibr" target="#b24">[25]</ref>, CCA-ITQ <ref type="bibr" target="#b8">[9]</ref>, KSH <ref type="bibr" target="#b20">[21]</ref>, FastHash <ref type="bibr" target="#b16">[17]</ref>, SDH <ref type="bibr" target="#b27">[28]</ref> and the unsupervised SH <ref type="bibr" target="#b35">[36]</ref>, AGH <ref type="bibr" target="#b21">[22]</ref>, PCA-ITQ <ref type="bibr" target="#b8">[9]</ref>, and IMH <ref type="bibr" target="#b28">[29]</ref>. For these we employ the implementations and suggested parameters provided by the authors. For our method, since the bits uncorrelation and balance constraints help produce better codes, we impose these two constraints in our algorithm. We empirically set λ = 0.1, µ = 1e3 and ρ = 1e2. Since CCA-ITQ, SDH and our methods can efficiently handle large data during training, we use all the available training data for training. For KSH, MLH and FastHash, we learn these models with 50k training samples due to the large computational costs of these methods.</p><p>For the retrieval experiments, we report the compared results in terms of mean average precision (mAP), mean precision of the top 500 retrieved neighbors (Precision@500) and the precision and recall curves. Note that we treat a query a false case if no point is returned when calculating precisions.</p><p>Ground truths are defined by the category information from the datasets. For computational efficiency, we compare the training and testing time of the evaluated methods. For the compared supervised hashing approaches, we also test the performance of these methods on the classification task, where the metric of classification accuracy is used. If not otherwise specified, the experiments are conducted with MATLAB implementations on a standard PC with an Intel 6-core 3.50GHz CPU and 64G RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NUS-WIDE: Retrieval with multi-labeled data</head><p>The NUS-WIDE database contains about 270,000 images collected from Flickr. The images in NUS-WIDE are associated with 81 concepts, with each image containing multiple semantic labels. We define the true neighbors of a query as the images sharing at least one labels with the query image. The provided 500-dimensional Bag-of-Words features are used. we collect the 21 most frequent label for test. For each label, 100 images are uniformly sampled for the query set and the remaining images are for the training set. The results in terms of retrieval performance (mAP and Precision@500) and training/testing time efficiency are reported in Table <ref type="table" target="#tab_2">II</ref>.</p><p>It is clear from Table II that our approach achieves the best results in terms of both mAP and precision among all the compared supervised methods. In particular, with 64 bits, our method outperforms the best of all other methods (obtained by SDH) by more than 6% and 15% in terms of mAP and precision, respectively. The precision-recall and precision curves of these compared methods with 32 to 128 bits are shown in Figure <ref type="figure" target="#fig_1">4</ref>. Our method consistently outperforms all other methods by large margins in all situations.</p><p>We also evaluate these methods in terms of training and testing efficiencies. We can clearly see from Table II that, our method costs less training time than all other compared methods. Specifically, DPLM only consumes only about 8.3 seconds to train the hashing model on the NUS-WIDE database. CCA-ITQ also has a high computational efficiency, which is much faster than other methods. In terms of testing time (encoding a query image into binary code), our method together with SDH and CCA-ITQ run very fast on the same scale while FastHash suffers from a slow encoding speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ImageNet: Retrieval with large-scale high dimensional features</head><p>As a subset of ImageNet <ref type="bibr" target="#b4">[5]</ref>, the large dataset ILSVRC 2012 contains over 1.2 million images of totally 1,000 categories. We use the provided training set as the retrieval database and 50,000 images from the validation set as the query set. We extract the feature for each image by the convolutional neural networks (CNN) model as a 4096D vector. The results are reported in the Table <ref type="table" target="#tab_3">III</ref>.</p><p>As in the last section, similar results are observed from Table III that our method obtains the best results. On this large dataset our method is slightly better than SDH, while both of them outperforms other methods by even larger gaps than on the relatively smaller NUS-WIDE database. These results  demonstrate the importance of discrete optimization for binary code learning.</p><p>In addition, our method demonstrates clearer advantages on ImageNet in training efficiency. For example, our method trains on the whole dataset with only about 5.5 minutes with 128 bits, while SDH costs more than 1 hour and KSH, FastHash and MLH runs even slower. From these experiments, it is clear that our discrete hashing method can generate more effective binary codes with much less learning time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SUN397: Scene classification with binary codes</head><p>In this part, we test the compared hashing methods on the classification task by feeding the classifier with the generated binary feature with these methods. The LIBLINEAR implementation of linear SVM is used here. The proposed approach is compared with several other supervised hash-ing methods including SDH, CCA-ITQ, KSH, FastHash and MLH. SUN397 is a widely-used scene classification benchmark, which contains about 108,000 images from 397 scene categories, where each image is represented by a 1,600dimensional feature vector extracted by PCA from 12,288dimensional Deep Convolutional Activation Features <ref type="bibr" target="#b9">[10]</ref>. In this experiment, 100 images are sampled uniformly randomly from each of the 18 largest scene categories to form a test set of 1,800 images and the rest for training set. The results are reported in Table <ref type="table" target="#tab_4">IV</ref>.</p><p>As can be clearly seen, the proposed approach obtains the highest classification accuracies on this dataset. Clear advantage of our method is shown over SDH especially with short code length. With long code lengths (128 bits), our method achieves very close results with SDH, while outperforms other methods by more than 5% accuracies.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ImageNet: Image classification with binary codes</head><p>In this subsection, we test the classification performance of the learned binary codes on the ImageNet benchmark <ref type="bibr" target="#b4">[5]</ref>. The same training/test setting is used as in Section V-B. The classification accuracies on this dataset are reported in Table V. Our method performs slightly better than SDH on this dataset (with much lower learning cost however), while much better than all other methods. The results in Table <ref type="table" target="#tab_4">IV</ref> and Table V clearly show that the binary codes generated by our methods work very well on the classification problem as well as the retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with unsupervised methods</head><p>In this part, we evaluate our method in the unsupervised setting by performing DPLM with the unsupervised graph hashing loss. Other representative methods of graph hashing including SH, AGH (with one or two layers), IMH with Laplacian Eigenmaps (denoted as IMH LE) and the well known LSH and ITQ are taken into comparison. We denote AGH with one and two layers by AGH 1 and AGH 2, respectively. For AGH, IMH and our method, we use k-means to generate 1,000 cluster centers for anchor or subset points.</p><p>The comparison is conducted on the ImageNet dataset, where we form the retrieval and training database by the 100 largest classes with total 128K images from the provided training set, and 50,000 images from the validation set as the query set. The retrieval results of these unsupervised methods are reported in Table VI with code lengths from 32 to 128 bits. Consistent with the supervised experiments, the proposed method outperforms all other methods in both mAP and precision. The advantage of our method is further illustrated by the detailed precision-recall curves and precision curves on top 2,000 retrieved images, as shown in Figure <ref type="figure" target="#fig_2">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND DISCUSSION</head><p>This paper investigated discrete optimization in the general binary code learning problem. To tackle the difficult optimization problem over binary variables, we proposed an effective discrete optimization algorithm, dubbed Discrete Proximal Linearized Minimization (DPLM). Profiting from the analytical solution at each iteration, DPLM led very fast optimization. Compared with existing discrete methods, the proposed method supported a large family of empirical loss functions and constraints, which was instantiated by the supervised 2 loss and unsupervised graph hashing loss. Several large benchmark datasets were used for evaluation and the results clearly demonstrated the superiority of both our supervised and unsupervised approaches over many other state-of-the-art methods, in terms of both retrieval precision and classification accuracy.</p><p>Deep learning based hashing. Deep learning (DL) has become one of the most effective feature learning approach for vision applications. For image hashing, DL also show its promising performance for the image retrieval task ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b43">[44]</ref>). We note that, however, in the test phase DL  based hashing methods need to forward an image through a deep neural network (usually with many layers of projections), which costs much more time than non-DL hashing algorithms including ours (with only one projection). Therefore, with the same input (e.g., raw intensity or GIST feature), the proposed approach can provide more efficient binary code encoding. Another advantage of this work is that, we derive an efficient algorithm for the general binary optimization problem, which effectively handles discrete constraints. It has been shown that directly optimizing with the discrete constraints can produce higher quality codes than the ones with continuous relaxations. In contrast, current DL based hashing algorithms usually resort to a continuous relaxation (e.g., by the sigmoid function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential applications</head><p>To remedy the aforementioned limitation of current DL based hashing methods, a reasonable solution for high-quality binary code learning will be incor-porating the proposed DPLM binary optimization technique into the deep hash function learning process. This will be a challenging yet valuable research direction deserving further studies.</p><p>The proposed DPLM method is developed for general bianry optimization, threrefore another potential application of DPLM will be its deployment with different hashing sennarios. For instance, DLPM could be applied to boost the performance of current hashing algorithms with pairwise supervised information (e.g., <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b6">[7]</ref>), multi-model hashing (e.g., <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>), where discrete optimization is supposed to produced higher quality hashes.</p><p>In addition to hashing, DPLM is also potentialy applied to other binary optimization problems, such as inner product binarizing <ref type="bibr" target="#b26">[27]</ref> and collaborative filtering <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: The function of MAP w.r.t. parameters µ and ρ with (a) the 2 supervised loss<ref type="bibr" target="#b10">(11)</ref> and (b) unsupervised graph hashing loss<ref type="bibr" target="#b12">(13)</ref>, respectively. The evaluation is performed on NUS-WIDE. 64 bits are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (Top) Precision-Recall and (Bottom) Precision curves with top 2000 retrieved images of the compared methods on NUS-WIDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: (Top) Precision-Recall and (Bottom) Precision curves with top 2000 retrieved images of the compared methods on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NUS-WIDE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell cols="2">Unsupervised</cell><cell></cell></row><row><cell>Balance</cell><cell>Uncorrelation</cell><cell cols="2">mAP</cell><cell cols="2">Precision@500</cell><cell cols="2">mAP</cell><cell cols="2">Precision@500</cell></row><row><cell></cell><cell></cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell></row><row><cell>-</cell><cell>-</cell><cell>0.6955</cell><cell>0.7167</cell><cell>0.6079</cell><cell>0.6041</cell><cell>0.3719</cell><cell>0.3750</cell><cell>0.4407</cell><cell>0.4466</cell></row><row><cell></cell><cell>-</cell><cell>0.7527</cell><cell>0.7600</cell><cell>0.7524</cell><cell>0.7543</cell><cell>0.3903</cell><cell>0.3849</cell><cell>0.4414</cell><cell>0.4506</cell></row><row><cell>-</cell><cell></cell><cell>0.734</cell><cell>0.7580</cell><cell>0.7523</cell><cell>0.7598</cell><cell>0.4022</cell><cell>0.4087</cell><cell>0.4596</cell><cell>0.4752</cell></row><row><cell></cell><cell></cell><cell>0.7603</cell><cell>0.7610</cell><cell>0.7545</cell><cell>0.7554</cell><cell>0.4055</cell><cell>0.4119</cell><cell>0.4803</cell><cell>0.4920</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell cols="2">Unsupervised</cell><cell></cell></row><row><cell>Balance</cell><cell>Uncorrelation</cell><cell cols="2">mAP</cell><cell cols="2">Precision@500</cell><cell cols="2">mAP</cell><cell cols="2">Precision@500</cell></row><row><cell></cell><cell></cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell></row><row><cell>-</cell><cell>-</cell><cell>0.6809</cell><cell>0.7029</cell><cell>0.6129</cell><cell>0.6286</cell><cell>0.1786</cell><cell>0.1920</cell><cell>0.2377</cell><cell>0.2670</cell></row><row><cell></cell><cell>-</cell><cell>0.6842</cell><cell>0.7012</cell><cell>0.6133</cell><cell>0.6298</cell><cell>0.1872</cell><cell>0.2112</cell><cell>0.2596</cell><cell>0.3285</cell></row><row><cell>-</cell><cell></cell><cell>0.6821</cell><cell>0.7039</cell><cell>0.6140</cell><cell>0.6302</cell><cell>0.1820</cell><cell>0.1927</cell><cell>0.2441</cell><cell>0.2738</cell></row><row><cell></cell><cell></cell><cell>0.6852</cell><cell>0.7046</cell><cell>0.6155</cell><cell>0.6365</cell><cell>0.1960</cell><cell>0.2117</cell><cell>0.2724</cell><cell>0.3303</cell></row><row><cell cols="2">0 1e1 1e2 1e3 1e4 1e5 1e6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>µ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Results in term of mAP and mean precision of the top 500 retrieved neighbors (precision@500) of the compared supervised methods on the NUS-WIDE database with 64 and 128 bits, respectively. The training and testing time are reported in seconds.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell cols="2">mAP</cell><cell></cell><cell></cell><cell>Precision@500</cell><cell>Training time (s)</cell><cell>Testing time (s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64 bits</cell><cell cols="3">128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>0.7603</cell><cell cols="3">0.7610</cell><cell>0.7545</cell><cell>0.7547</cell><cell>8.32</cell><cell>13.97</cell><cell>1.31e-6</cell><cell>2.47e-6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SDH</cell><cell>0.6955</cell><cell cols="3">0.7167</cell><cell>0.6079</cell><cell>0.6041</cell><cell>34.72</cell><cell>119.8</cell><cell>1.65e-6</cell><cell>2.55e-6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CCA-ITQ</cell><cell>0.6232</cell><cell cols="3">0.6239</cell><cell>0.5919</cell><cell>0.5962</cell><cell>8.93</cell><cell>19.71</cell><cell>1.08e-6</cell><cell>3.03e-6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>KSH</cell><cell>0.6091</cell><cell cols="3">0.6129</cell><cell>0.5638</cell><cell>0.5659</cell><cell>2092.3</cell><cell>4384.3</cell><cell>5.84e-6</cell><cell>1.33e-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FastHash</cell><cell>0.5346</cell><cell cols="3">0.5507</cell><cell>0.6013</cell><cell>0.6197</cell><cell>3486.16</cell><cell>7091.99</cell><cell>1.31e-2</cell><cell>2.86e-2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLH</cell><cell>0.4726</cell><cell cols="3">0.4689</cell><cell>0.5540</cell><cell>0.5583</cell><cell>8413.4</cell><cell>13791</cell><cell>2.23e-5</cell><cell>3.79e-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>Ours</cell></row><row><cell>Precision</cell><cell cols="2">0.5 0.6 0.7</cell><cell></cell><cell></cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell><cell>Precision</cell><cell cols="2">0.6 0.8</cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell><cell>Precision</cell><cell>0.6 0.8</cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell></row><row><cell></cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.4</cell><cell>0.4</cell></row><row><cell></cell><cell cols="2">0 0.3</cell><cell cols="3">0.2 0.4 0.6 0.8</cell><cell>1</cell><cell cols="3">0 0.2</cell><cell>0.2 0.4 0.6 0.8</cell><cell>1</cell><cell>0 0.2</cell><cell>0.2 0.4 0.6 0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) 32 bits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) 64 bits</cell><cell>(c) 128 bits</cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>Ours</cell></row><row><cell cols="2">Precision</cell><cell>0.6 0.7</cell><cell></cell><cell></cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell><cell cols="2">Precision</cell><cell>0.7 0.8</cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell><cell>Precision</cell><cell>0.7 0.8</cell><cell>SDH CCA_ITQ KSH FastHash MLH</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell>0 0.4</cell><cell>500</cell><cell cols="3">1000 1500 2000</cell><cell></cell><cell cols="2">0 0.5</cell><cell>500</cell><cell>1000 1500 2000</cell><cell>0 0.5</cell><cell>500</cell><cell>1000 1500 2000</cell></row><row><cell></cell><cell></cell><cell cols="4">Number of retrieved samples</cell><cell></cell><cell></cell><cell></cell><cell>Number of retrieved samples</cell><cell>Number of retrieved samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(d) 32 bits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(e) 64 bits</cell><cell>(f) 128 bits</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Results in term of mAP and mean precision of the top 500 retrieved neighbors (precision@500) of the compared methods on the ImageNet database with 64 and 128 bits, respectively. The training and testing time are reported in seconds. The experiments are conducted on a workstation with an Intel 6-core 2.10GHz CPU and 188G RAM.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP</cell><cell cols="2">Precision@500</cell><cell cols="2">Training time (s)</cell><cell cols="2">Testing time (s)</cell></row><row><cell></cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell><cell>64 bits</cell><cell>128 bits</cell></row><row><cell>Ours</cell><cell>0.3235</cell><cell>0.4310</cell><cell>0.3039</cell><cell>0.3996</cell><cell>258.82</cell><cell>336.84</cell><cell>2.80e-6</cell><cell>6.93e-6</cell></row><row><cell>SDH</cell><cell>0.3225</cell><cell>0.4261</cell><cell>0.3016</cell><cell>0.3987</cell><cell>1568.66</cell><cell>4015.20</cell><cell>3.01e-6</cell><cell>8.24e-6</cell></row><row><cell>CCA-ITQ</cell><cell>0.1086</cell><cell>0.1694</cell><cell>0.1651</cell><cell>0.2461</cell><cell>372.52</cell><cell>468.00</cell><cell>3.42e-6</cell><cell>7.64e-6</cell></row><row><cell>KSH</cell><cell>0.0897</cell><cell>0.1351</cell><cell>0.1702</cell><cell>0.2381</cell><cell>6953.48</cell><cell>13897.53</cell><cell>6.18e-5</cell><cell>9.67-05</cell></row><row><cell>FastHash</cell><cell>0.1062</cell><cell>0.1827</cell><cell>0.1944</cell><cell>0.2598</cell><cell>3486.16</cell><cell>7091.99</cell><cell>1.59e-2</cell><cell>1.31e-2</cell></row><row><cell>MLH</cell><cell>0.0739</cell><cell>0.1111</cell><cell>0.1493</cell><cell>0.2069</cell><cell>20864.12</cell><cell>43494.35</cell><cell>1.18e-5</cell><cell>2.74e-5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Classification accuracy (%) on SUN397 with the produced binary codes by different hashing methods. The code length varies from 32 to 128 bits.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell>32 bits</cell><cell>64 bits</cell><cell cols="2">96 bits 128 bits</cell></row><row><cell>Ours</cell><cell>66.83</cell><cell>76.72</cell><cell>77.67</cell><cell>78.72</cell></row><row><cell>SDH</cell><cell>63.00</cell><cell>75.28</cell><cell>77.44</cell><cell>78.22</cell></row><row><cell>CCA-ITQ</cell><cell>60.06</cell><cell></cell><cell>73.50</cell><cell>73.67</cell></row><row><cell>KSH</cell><cell>63.06</cell><cell>68.83</cell><cell>70.44</cell><cell>73.17</cell></row><row><cell>FastHash</cell><cell>64.95</cell><cell>71.04</cell><cell>74.17</cell><cell>75.38</cell></row><row><cell>MLH</cell><cell>56.56</cell><cell>65.67</cell><cell>68.78</cell><cell>71.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="5">: Classification accuracy (%) on ImageNet with</cell></row><row><cell cols="5">the produced binary codes by different hashing methods. The</cell></row><row><cell cols="5">code length varies from 32 to 128 bits. The experiments are</cell></row><row><cell cols="5">conducted on a workstation with an Intel 6-core 2.10GHz CPU</cell></row><row><cell>and 188G RAM.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell>32 bits</cell><cell>64 bits</cell><cell cols="2">96 bits 128 bits</cell></row><row><cell>Ours</cell><cell>18.28</cell><cell>30.50</cell><cell>36.59</cell><cell>39.89</cell></row><row><cell>SDH</cell><cell>18.19</cell><cell>30.12</cell><cell>36.03</cell><cell>39.85</cell></row><row><cell>CCA-ITQ</cell><cell>10.66</cell><cell>18.31</cell><cell>23.84</cell><cell>27.42</cell></row><row><cell>KSH</cell><cell>18.07</cell><cell>27.58</cell><cell>31.62</cell><cell>32.69</cell></row><row><cell>FastHash</cell><cell>17.69</cell><cell>28.04</cell><cell>34.36</cell><cell>36.72</cell></row><row><cell>MLH</cell><cell>18.00</cell><cell>27.83</cell><cell>33.02</cell><cell>35.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Results in term of mAP and mean precision of the top 500 retrieved neighbors (precision@500) of the compared unsupervised methods on the ImageNet database with 32 to 128 bits.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">mAP</cell><cell></cell><cell>Precision@500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>32 bits</cell><cell cols="2">64 bits</cell><cell cols="3">96 bits 128 bits</cell><cell>32 bits</cell><cell>64 bits</cell><cell>96 bits 128 bits</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>0.4985</cell><cell cols="2">0.5445</cell><cell>0.5720</cell><cell cols="2">0.5784</cell><cell>0.5854</cell><cell>0.6269</cell><cell>0.6450</cell><cell>0.6485</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IMH</cell><cell>0.4469</cell><cell cols="2">0.5172</cell><cell>0.5261</cell><cell cols="2">0.5163</cell><cell>0.5191</cell><cell>0.6121</cell><cell>0.6256</cell><cell>0.6311</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AGH-1</cell><cell>0.4587</cell><cell cols="2">0.5243</cell><cell>0.5176</cell><cell cols="2">0.4537</cell><cell>0.5325</cell><cell>0.6159</cell><cell>0.6346</cell><cell>0.6197</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AGH-2</cell><cell>0.3927</cell><cell cols="2">0.4645</cell><cell>0.5011</cell><cell cols="2">0.5222</cell><cell>0.4653</cell><cell>0.5525</cell><cell>0.5919</cell><cell>0.6114</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SH</cell><cell>0.2418</cell><cell cols="2">0.3066</cell><cell>0.3243</cell><cell cols="2">0.3310</cell><cell>0.3647</cell><cell>0.4532</cell><cell>0.4813</cell><cell>0.4956</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ITQ</cell><cell>0.3001</cell><cell cols="2">0.3839</cell><cell>0.4244</cell><cell cols="2">0.4412</cell><cell>0.4086</cell><cell>0.5063</cell><cell>0.5490</cell><cell>0.5683</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LSH</cell><cell>0.0485</cell><cell cols="2">0.1010</cell><cell>0.1462</cell><cell cols="2">0.1922</cell><cell>0.1050</cell><cell>0.2013</cell><cell>0.2731</cell><cell>0.3359</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell>Ours</cell><cell>0.8</cell><cell>Ours</cell></row><row><cell>Precision</cell><cell>0.2 0.4</cell><cell></cell><cell></cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell><cell>Precision</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell></cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell><cell>Precision</cell><cell>0.4 0.6 0.2</cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell></row><row><cell></cell><cell>0 0</cell><cell cols="3">0.2 0.4 0.6 0.8</cell><cell>1</cell><cell cols="2">0 0</cell><cell></cell><cell>0 0</cell><cell>0.2 0.4 0.6 0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) 32 bits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) 64 bits</cell><cell>(c) 128 bits</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>Ours</cell></row><row><cell>Precision</cell><cell>0.2 0.4</cell><cell></cell><cell></cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell><cell>Precision</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell></cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell><cell>Precision</cell><cell>0.4 0.6 0.2</cell><cell>AGH_1 AGH_2 IMH_LE SH ITQ LSH</cell></row><row><cell></cell><cell>0 0</cell><cell>500</cell><cell cols="3">1000 1500 2000</cell><cell cols="2">0 0</cell><cell>500</cell><cell>1000 1500 2000</cell><cell>0 0</cell><cell>500</cell><cell>1000 1500 2000</cell></row><row><cell></cell><cell cols="4">Number of retrieved samples</cell><cell></cell><cell></cell><cell cols="3">Number of retrieved samples</cell><cell>Number of retrieved samples</cell></row><row><cell></cell><cell></cell><cell cols="2">(d) 32 bits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(e) 64 bits</cell><cell>(f) 128 bits</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forwardbackward splitting, and regularized gauss-seidel methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proximal alternating linearized minimization for nonconvex and nonsmooth problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="459" to="494" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Conf. on Image and Video Retrieval</title>
		<meeting>of ACM Conf. on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localitysensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium Comput. geometry</title>
		<meeting>ACM Symposium Comput. geometry</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph cuts for supervised binary coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="250" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Very Large Databases</title>
		<meeting>Int. Conf. Very Large Databases</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Isotropic Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1646" to="1654" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast similarity search for learned metrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2143" to="2157" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3270" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning hash functions using column generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast supervised hashing with decision trees for high-dimensional data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiview alignment hashing for efficient image search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="956" to="966" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Projection bank: From high-dimensional data to medium-length binary codes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discrete graph hashing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3419" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Query-adaptive reciprocal hash tables for nearest neighbor search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="907" to="919" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cost-sensitive local binary feature learning for facial age estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5356" to="5368" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Locality-sensitive binary codes from shift-invariant kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1509" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning binary codes for maximum inner product search</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4148" to="4156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hashing on nonlinear manifolds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1839" to="1851" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective multiple feature hashing for large-scale near-duplicate video retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1997">1997-2008, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inter-media hashing for large-scale retrieval from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Management Of Data</title>
		<meeting>ACM Conf. Management Of Data</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neighborhood discriminant hashing for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2827" to="2840" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large scale search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<title level="m">Hashing for similarity search: A survey</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multidimensional spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust discrete spectral hashing for large-scale image semantic indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="162" to="171" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Circulant binary embedding</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="946" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discrete collaborative filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Inf. Ret</title>
		<meeting>ACM Conf. Inf. Ret</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparse hashing tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="840" to="849" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Inf. Ret</title>
		<meeting>ACM Conf. Inf. Ret</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning binary codes for collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Knowl. Disc. Data Min</title>
		<meeting>ACM Conf. Knowl. Disc. Data Min</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="498" to="506" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
