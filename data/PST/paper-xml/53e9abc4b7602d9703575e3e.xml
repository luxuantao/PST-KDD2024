<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wavelets on Graphs via Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raif</forename><forename type="middle">M</forename><surname>Rustamov</surname></persName>
							<email>rustamov@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
							<email>guibas@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Wavelets on Graphs via Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FBD8789316496567F69E7128C78E9340</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An increasing number of applications require processing of signals defined on weighted graphs. While wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible -they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing of signals on graphs is emerging as a fundamental problem in an increasing number of applications <ref type="bibr" target="#b21">[22]</ref>. Indeed, in addition to providing a direct representation of a variety of networks arising in practice, graphs serve as an overarching abstraction for many other types of data. Highdimensional data clouds such as a collection of handwritten digit images, volumetric and connectivity data in medical imaging, laser scanner acquired point clouds and triangle meshes in computer graphics -all can be abstracted using weighted graphs. Given this generality, it is desirable to extend the flexibility of classical tools such as wavelets to the processing of signals defined on weighted graphs.</p><p>A number of approaches for constructing wavelets on graphs have been proposed, including, but not limited to the CKWT <ref type="bibr" target="#b6">[7]</ref>, Haar-like wavelets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>, diffusion wavelets <ref type="bibr" target="#b5">[6]</ref>, spectral wavelets <ref type="bibr" target="#b11">[12]</ref>, tree-based wavelets <ref type="bibr" target="#b18">[19]</ref>, average-interpolating wavelets <ref type="bibr" target="#b20">[21]</ref>, and separable filterbank wavelets <ref type="bibr" target="#b16">[17]</ref>. However, all of these constructions are guided solely by the structure of the underlying graph, and do not take directly into consideration the particular class of signals to be processed. While this information can be incorporated indirectly when building the underlying graph (e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>), such an approach does not fully exploit the degrees of freedom inherent in wavelet design. In contrast, a variety of signal class specific and adaptive wavelet constructions exist on images and multidimensional regular domains, see <ref type="bibr" target="#b8">[9]</ref> and references therein. Bridging this gap is challenging because obtaining graph wavelets, let alone adaptive ones, is complicated by the irregularity of the underlying space. In addition, theoretical guidance for such adaptive constructions is lacking as it remains largely unknown how the properties of the graph wavelet transforms, such as sparsity, relate to the structural properties of graph signals and their underlying graphs <ref type="bibr" target="#b21">[22]</ref>.</p><p>The goal of our work is to provide a machine learning framework for constructing wavelets on weighted graphs that can sparsely represent a given class of signals. Our construction uses the lifting scheme as applied to the Haar wavelets, and is based on the observation that the update and predict steps of the lifting scheme are similar to the encode and decode steps of an auto-encoder. From this point of view, the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network.</p><p>Particular properties that the resulting wavelets must satisfy, such as sparse representation of signals, local support, and vanishing moments, determine the training objective and the structure of the involved neural networks. The goal of achieving sparsity translates into minimizing a sparsity surrogate of the auto-encoder reconstruction error. Vanishing moments and locality can be satisfied by tying the weights of the auto-encoder in a special way and by restricting receptive fields of neurons in a manner that incorporates the structure of the underlying graph. The training is unsupervised, and is conducted similarly to the greedy (pre-)training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref> of a stack of auto-encoders.</p><p>The advantages of our construction are three-fold. First, when no training functions are specified by the application, we can impose a smoothness prior and obtain a novel general-purpose wavelet construction on graphs. Second, our wavelets are adaptive to a class of signals and after training we obtain a linear transform; this is in contrast to adapting to the input signal (e.g. by modifying the underlying graph <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>) which effectively renders those transforms non-linear. Third, our construction provides efficient and exact analysis and synthesis operators and results in a critically sampled basis that respects the multiscale structure imposed on the underlying graph.</p><p>The paper is organized as follows: in §2 we briefly overview the lifting scheme. Next, in §3 we provide a general overview of our approach, and fill in the details in §4. Finally, we present a number of experiments in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lifting scheme</head><p>The goal of wavelet design is to obtain a multiresolution <ref type="bibr" target="#b15">[16]</ref> of L 2 (G) -the set of all functions/signals on graph G. Namely, a nested sequence of approximation spaces from coarse to fine of the form</p><formula xml:id="formula_0">V 1 ⊂ V 2 ⊂ ... ⊂ V max = L 2 (G) is constructed.</formula><p>Projecting a signal in the spaces V provides better and better approximations with increasing level . Associated wavelet/detail spaces W satisfying V +1 = V ⊕ W are also obtained.</p><p>Scaling functions {φ ,k } provide a basis for approximation space V , and similarly wavelet functions {ψ ,k } for W . As a result, for any signal f ∈ L 2 (G) on graph and any level 0 &lt; max , we have the wavelet decomposition</p><formula xml:id="formula_1">f = k a 0,k φ 0,k + max -1 = 0 k d ,k ψ ,k .<label>(1)</label></formula><p>The coefficients a ,k and d ,k appearing in this decomposition are called approximation (also, scaling) and detail (also, wavelet) coefficients respectively. For simplicity, we use a and d to denote the vectors of all approximation and detail coefficients at level .</p><p>Our construction of wavelets is based on the lifting scheme <ref type="bibr" target="#b22">[23]</ref>. Starting with a given wavelet transform, which in our case is the Haar transform (HT ), one can obtain lifted wavelets by applying the process illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(left) starting with = max -1, a max = f and iterating down until = 1. At every level the lifted coefficients a and d are computed by augmenting the Haar where update (U ) and predict (P ) are linear operators (matrices). Note that in adaptive wavelet designs the update and predict operators will vary from level to level, but for simplicity of notation we do not indicate this explicitly.</p><p>This process is always invertible -the backward transform is depicted, with IHT being the inverse Haar transform, in Figure <ref type="figure" target="#fig_0">1</ref>(right) and allows obtaining perfect reconstruction of the original signal.</p><p>While the wavelets and scaling functions are not explicitly computed during either forward or backward transform, it is possible to recover them using the expansion of Eq. (1). For example, to obtain a specific scaling function φ ,k , one simply sets all of approximation and detail coefficients to zero, except for a ,k = 1 and runs the backward transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>For a given class of signals, our objective is to design wavelets that yield approximately sparse expansions in Eq.(1) -i.e. the detail coefficients are mostly small with a tiny fraction of large coefficients. Therefore, we learn the update and predict operators that minimize some sparsity surrogate of the detail (wavelet) coefficients of given training functions {f n } nmax n=1 . For a fixed multiresolution level , and a training function f n , let ān and dn be the Haar approximation and detail coefficient vectors of f n received at level (i.e. applied to a n +1 as in Figure <ref type="figure" target="#fig_0">1</ref>(left)). Consider the minimization problem</p><formula xml:id="formula_2">{U, P } = arg min U,P n s(d n ) = arg min U,P n s( dn -P (ā n + U dn )),<label>(2)</label></formula><p>where s is some sparse penalty function. This can be seen as optimizing a linear auto-encoder with encoding step given by ān + U dn , and decoding step given by multiplication with the matrix P . Since we would like to obtain a linear wavelet transform, the linearity of the encode and decode steps is of crucial importance. In addition to linearity and the special form of bias terms, our auto-encoders differ from commonly used ones in that we enforce sparsity on the reconstruction error, rather than the hidden representation -in our setting, the reconstruction errors correspond to detail coefficients.</p><p>The optimization problem of Eq. 2 suffers from a trivial solution: by choosing update matrix to have large norm (e.g. a large coefficient times identity matrix), and predict operator equal to the inverse of update, one can practically cancel the contribution of the bias terms, obtaining almost perfect reconstruction. Trivial solutions are a well-known problem in the context of auto-encoders, and an effective solution is to tie the weights of the encode and decode steps by setting U = P t . This also has the benefit of decreasing the number of parameters to learn. We also follow a similar strategy and tie the weights of update and predict steps, but the specific form of tying is dictated by the wavelet properties and will be discussed in §4.2.</p><p>The training is conducted in a manner similar to the greedy pre-training of a stack of auto-encoders <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref>. Namely, we first train the the update and predict operators at the finest level: here the input to the lifting step are the original training functions -this corresponds to = max -1 and ∀n, a n +1 = f n in Figure <ref type="figure" target="#fig_0">1</ref>(left). After training of this finest level is completed, we obtain new approximation coefficients a n which are passed to the next level as the training functions, and this process is repeated until one reaches the coarsest level.</p><p>The use of tied auto-encoders is motivated by their success in deep learning revealing their capability to learn useful features from the data under a variety of circumstances. The choice of the lifting scheme as the backbone of our construction is motivated by several observations. First, every invertible 1D discrete wavelet transform can be factored into lifting steps <ref type="bibr" target="#b7">[8]</ref>, which makes lifting a universal tool for constructing multiresolutions. Second, lifting scheme is always invertible, and provides exact reconstruction of signals. Third, it affords fast (linear time) and memory efficient (in-place) implementation after the update and predict operators are specified. We choose to apply lifting to Haar wavelets specifically because Haar wavelets are easy to define on any underlying space provided that it can be hierarchically partitioned <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>. Our use of update-first scheme mirrors its common use for adaptive wavelet constructions in image processing literature, which is motivated by its stability; see <ref type="bibr" target="#b3">[4]</ref> for a thorough discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Construction details</head><p>We consider a simple connected weighted graph G with vertex set V of size N . A signal on the graph is represented by a vector f ∈ R N . Let W be the N × N edge weight matrix (since there are no self-loops, W ii = 0), and let S be the diagonal N × N matrix of vertex weights; if no vertex weights are given, we set S ii = j W ij . For a graph signal f , we define its integral over the graph as a weighted sum, ´G f = i S ii f (i). We define the volume of a subset R of vertices of the graph by V ol(R) = ´R 1 = i∈R S ii . We assume that a hierarchical partitioning (not necessarily dyadic) of the underlying graph into connected regions is provided. We denote the regions at level = 1, ..., max by R ,k ; see the inset where the three coarsest partition levels of a dataset are shown. For each region at levels = 1, ..., max -1, we designate arbitrarily all except one of its children (i.e. regions at level +1) as active regions. As will become clear, our wavelet construction yields one approximation coefficient a ,k for each region R ,k , and one detail coefficient d ,k for each active region R +1,k at level + 1. Note that if the partition is not dyadic, at a given level the number of scaling coefficients (equal to number of regions at level ) will not be the same as the number of detail coefficients (equal to number of active regions at level + 1). We collect all of the coefficients at the same level into vectors denoted by a and d ; to keep our notation lightweight, we refrain from using boldface for vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Haar wavelets</head><p>Usually, the (unnormalized) Haar approximation and detail coefficients of a signal f are computed as follows. The coefficient ā ,k corresponding to region R ,k equals to the average of the function f on that region:</p><formula xml:id="formula_3">ā ,k = V ol(R ,k ) -1 ´R</formula><p>,k f . The detail coefficient d ,k corresponding to an active region R +1,k is the difference between averages at the region R +1,k and its parent region R ,par(k) , namely d ,k = ā +1,k -ā ,par(k) . For perfect reconstruction there is no need to keep detail coefficients for inactive regions, because these can be recovered from the scaling coefficient of the parent region and the detail coefficients of the sibling regions.</p><p>In our setting, Haar wavelets are a part of the lifting scheme, and so the coefficient vectors ā and d at level need to be computed from the augmented coefficient vector a +1 at level + 1 (c.f. Figure <ref type="figure" target="#fig_0">1(left)</ref>). This is equivalent to computing a function's average at a given region from its averages at the children regions. As a result, we obtain the following formula:</p><formula xml:id="formula_4">ā ,k = V ol(R ,k ) -1 j,par(j)=k a +1,j V ol(R +1,j ),</formula><p>where the summation is over all the children regions of R ,k . As before, the detail coefficient corresponding to an active region R +1,k is given by d ,k = a +1,k -ā ,par(k) . The resulting Haar wavelets are not normalized; when sorting wavelet/scaling coefficients we will multiply coefficients coming from level by 2 -/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Auto-encoder setup</head><p>The choice of the update and predict operators and their tying scheme is guided by a number of properties that wavelets need to satisfy. We discuss these requirements under separate headings.</p><p>Vanishing moments: The wavelets should have vanishing dual and primal moments -two independent conditions due to biorthogonality of our wavelets. In terms of the approximation and detail coefficients these can be expressed as follows: a) all of the detail coefficients of a constant function should be zero and b) the integral of the approximation at any level of multiresolution should be the same as the integral of the original function.</p><p>Since these conditions are already satisfied by the Haar wavelets, we need to ensure that the update and predict operators preserve them. To be more precise, if a +1 is a constant vector, then we have for Haar coefficients that ā = c 1 and d = 0; here c is some constant and 1 is a column-vector of all ones. To satisfy a) after lifting, we need to ensure that d = d -P (ā +U d ) = -P ā = -cP 1 = 0. Therefore, the rows of predict operator should sum to zero: P 1 = 0.</p><p>To satisfy b), we need to preserve the first order moment at every level by requiring</p><formula xml:id="formula_5">k a +1,k V ol(R +1,k ) = k ā ,k V ol(R ,k ) = k a ,k V ol(R ,k ).</formula><p>The first equality is already satisfied (due to the use of Haar wavelets), so we need to constrain our update operator. Introducing the diagonal matrix A c of the region volumes at level , we can write 0 =</p><formula xml:id="formula_6">k a ,k V ol(R ,k ) -k ā ,k V ol(R ,k ) = k U d V ol(R ,k ) = 1 t A c U d .</formula><p>Since this should be satisfied for all d , we must have</p><formula xml:id="formula_7">1 t A c U = 0 t .</formula><p>Taking these two requirements into consideration, we impose the following constraints on predict and update weights: P 1 = 0 and U = A -<ref type="foot" target="#foot_0">1</ref> c P t A f where A f is the diagonal matrix of the active region volumes at level + 1. It is easy to check that</p><formula xml:id="formula_8">1 t A c U = 1 t A c A -1 c P t A f = 1 t P t A f = (P 1) t A f = 0 t A f = 0 t as required.</formula><p>We have introduced the volume matrix A f of regions at the finer level to make the update/predict matrices dimensionless (i.e. insensitive to whether the volume is measured in any particular units).</p><p>Locality: To make our wavelets and scaling functions localized on the graph, we need to constrain update and predict operators in a way that would disallow distant regions from updating or predicting the approximation/detail coefficients of each other.</p><p>Since the update is tied to the predict operator, we can limit ourselves to the latter operator. For a detail coefficient d ,k corresponding to the active region R +1,k , we only allow predictions that come from the parent region R ,par(k) and the immediate neighbors of this parent region. Two regions of graph are considered neighboring if their union is a connected graph. This can be seen as enforcing a sparsity structure on the matrix P or as limiting the interconnections between the layers of neurons.</p><p>As a result of this choice, it is not difficult to see that the resulting scaling functions φ ,k and wavelets ψ ,k will be supported in the vicinity of the region R ,k . Larger supports can be obtained by allowing the use of second and higher order neighbors of the parent for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>A variety of ways for optimizing auto-encoders are available, we refer the reader to the recent paper <ref type="bibr" target="#b14">[15]</ref> and references therein. In our setting, due to the relatively small size of the training set and sparse inter-connectivity between the layers, an off-the-shelf L-BFGS 1 unconstrained smooth optimization package works very well. In order to make our problem unconstrained, we avoid imposing the equation P 1 = 0 as a hard constraint, but in each row of P (which corresponds to some active region), the weight corresponding to the parent is eliminated. To obtain a smooth objective, we use L 1 norm with soft absolute value s(x) = √ + x 2 ≈ |x|, where we set = 10 -4 . The initialization is done by setting all of the weights equal to zero. This is meaningful, because it corresponds to no lifting at all, and would reproduce the original Haar wavelets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training functions</head><p>When training functions are available we directly use them. However, our construction can be applied even if training functions are not specified. In this case we choose smoothness as our prior, and train the wavelets with a set of smooth functions on the graph -namely, we use scaled eigenvectors of graph Laplacian corresponding to the smallest eigenvalues. More precisely, let D be the diagonal matrix with entries D ii = j W ij . The graph Laplacian L is defined as L = S -1 (D -W ). We solve the symmetric generalized eigenvalue problem (D -W )ξ = λSξ to compute the smallest eigen-pairs {λ n , ξ n } nmax n=0 .We discard the 0-th eigen-pair which corresponds to the constant eigenvector, and use functions {ξ n /λ n } nmax n=1 as our training set. The inverse scaling by the eigenvalue is included because eigenvectors corresponding to larger eigenvalues are less smooth (cf. <ref type="bibr" target="#b0">[1]</ref>), and so should be assigned smaller weights to achieve a smooth prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Partitioning</head><p>Since our construction is based on improving upon the Haar wavelets, their quality will have an effect on the final wavelets. As proved in <ref type="bibr" target="#b9">[10]</ref>, the quality of Haar wavelets depends on the quality (balance) of the graph partitioning. From practical standpoint, it is hard to achieve high quality partitions on all types of graphs using a single algorithm. However, for the datasets presented in this paper we find that the following approach based on spectral clustering algorithm of <ref type="bibr" target="#b17">[18]</ref> works well. Namely, we first embed the graph vertices into R nmax as follows: i → (ξ 1 (i)/λ 1 , ξ 2 (i)/λ 2 , ..., ξ nmax (i)/λ nmax ), ∀i ∈ V , where {λ n , ξ n } nmax n=0 are the eigen-pairs of the Laplacian as in §4.4, and ξ • (i) is the value of the eigenvector at the i-th vertex of the graph. To obtain a hierarchical tree of partitions, we start with the graph itself as the root. At every step, a given region (a subset of the vertex set) of graph G is split into two children partitions by running the 2-means clustering algorithm (k-means with k = 2) on the above embedding restricted to the vertices of the given partition <ref type="bibr" target="#b23">[24]</ref>. This process is continued in recursion at every obtained region. This results in a dyadic partitioning except at the finest level max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Graph construction for point clouds</head><p>Our problem setup started with a weighted graph and arrived to the Laplacian matrix L in §4.4. It is also possible o reverse this process whereby one starts with the Laplacian matrix L and infers from it the weighted graph. This is a natural way of dealing with point clouds sampled from low-dimensional manifolds, a setting common in manifold learning. There is a number of ways for computing Laplacians on point clouds, see <ref type="bibr" target="#b4">[5]</ref>; almost all of them fit into the above form L = S -1 (D -W ), and so, they can be used to infer a weighted graph that can be plugged into our construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our goal is to experimentally investigate the constructed wavelets for multiscale behavior, meaningful adaptation to training signals, and sparse representation that generalizes to testing signals. For the first two objectives we visualize the scaling functions at different levels because they provide insight about the signal approximation spaces V . The generalization performance can be deduced from comparison to Haar wavelets, because during training we modify Haar wavelets so as to achieve a sparser representation of training signals.</p><p>We start with the case of a periodic interval, which is discretized as a cycle graph; 32 scaled eigenvectors (sines and cosines) are used for training. Figure <ref type="figure" target="#fig_1">2</ref> shows the resulting scaling and wavelet functions at level = 4. Up to discretization errors, the wavelets and scaling functions at the same level are shifts of each other -showing that our construction is able to learn shift invariance from training functions. seen by the algorithm so far) of the vertices; Figure <ref type="figure" target="#fig_2">3</ref>(g) shows one of such functions. Figure <ref type="figure" target="#fig_2">3(h)</ref> shows the average error of reconstruction from expansion Eq. ( <ref type="formula" target="#formula_1">1</ref>) with 0 = 1 by keeping a specified fraction of largest detail coefficients. The improvement over the Haar wavelets shows that our model generalizes well to unseen signals.</p><p>Next, we apply our approach to real-world graph signals. We use a dataset of average daily temperature measurements<ref type="foot" target="#foot_1">2</ref> from meteorological stations located on the mainland US. The longitudes and latitudes of stations are treated as coordinates of a point cloud, from which a weighted Laplacian is constructed using <ref type="bibr" target="#b4">[5]</ref> with 5-nearest neighbors; the resulting graph is shown in Figure <ref type="figure" target="#fig_3">4(a)</ref>.</p><p>The daily temperature data for the year of 2012 gives us 366 signals on the graph; Figure <ref type="figure" target="#fig_3">4</ref>(b) depicts one such signal. We use the signals from the first half of the year to train the wavelets, and test for sparse reconstruction quality on the second half of the year (and vice versa). Figure <ref type="figure" target="#fig_3">4(c,</ref><ref type="figure">d,</ref><ref type="figure">e,</ref><ref type="figure">f,</ref><ref type="figure">g</ref>) depicts some of the scaling functions at a number of levels; note that the depicted scaling function at level = 2 captures the rough temperature distribution pattern of the US. The average reconstruction error from a specified fraction of largest detail coefficients is shown in Figure <ref type="figure" target="#fig_3">4(g)</ref>.</p><p>As an application, we employ our wavelets for semi-supervised learning of the temperature distribution for a day from the temperatures at a subset of labeled graph vertices. The sought temperature  distribution is expanded as in Eq. ( <ref type="formula" target="#formula_1">1</ref>) with 0 = 1, and the coefficients are found by solving a least squares problem using temperature values at labeled vertices. Since we expect the detail coefficients to be sparse, we impose a lasso penalty on them; to make the problem smaller, all detail coefficients for levels ≥ 7 are set to zero. We compare to the Laplacian regularized least squares <ref type="bibr" target="#b0">[1]</ref> and harmonic interpolation approach <ref type="bibr" target="#b25">[26]</ref>. A hold-out set of 25 random vertices is used to assign all the regularization parameters. The experiment is repeated for each of the days (not used to learn the wavelets) with the number of labeled vertices ranging from 10 to 200. Figure <ref type="figure" target="#fig_3">4</ref>(h) shows the errors averaged over all days; our approach achieves lower error rates than the competitors.</p><p>Our final example serves two purposes -showing the benefits of our construction in a standard image processing application and better demonstrating the nature of learned scaling functions. Images can be seen as signals on a graph -pixels are the vertices and each pixel is connected to its 8 nearest neighbors. We consider all of the Extended Yale Face Database B <ref type="bibr" target="#b10">[11]</ref> images (cropped and down-sampled to 32 × 32) as a collection of signals on a single underlying graph. We randomly split the collection into half for training our wavelets, and test their reconstruction quality on the remaining half. Figure <ref type="figure" target="#fig_4">5</ref>(a) depicts a number of obtained scaling functions at different levels (the rows correspond to levels = 4, 5, 6, 7, 8) in various locations (columns). The scaling functions have a face-like appearance at coarser levels, and capture more detailed facial features at finer levels. Note that the scaling functions show controllable multiscale spatial behavior.</p><p>The quality of reconstruction from a sparse set of detail coefficients is plotted in Figure <ref type="figure" target="#fig_4">5(b,</ref><ref type="figure">c</ref>). Here again we consider the expansion of Eq. ( <ref type="formula" target="#formula_1">1</ref>) with 0 = 1, and reconstruct using a specified proportion of largest detail coefficients. We also make a comparison to reconstruction using the standard separable CDF 9/7 wavelet filterbanks from bottom-most level; for both of quality metrics, our wavelets trained on data perform better than CDF 9/7. The smoothly trained wavelets do not improve over the Haar wavelets, because the smoothness assumption does not hold for face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced an approach to constructing wavelets that take into consideration structural properties of both graph signals and their underlying graphs. An interesting direction for future research would be to randomize the graph partitioning process or to use bagging over training functions in order to obtain a family of wavelet constructions on the same graph -leading to overcomplete dictionaries like in <ref type="bibr" target="#b24">[25]</ref>. One can also introduce multiple lifting steps at each level or even add non-linearities as common with neural networks. Our wavelets are obtained by training a structure similar to a deep neural network; interestingly, the recent work of Mallat and collaborators (e.g. <ref type="bibr" target="#b2">[3]</ref>) goes in the other direction and provides a wavelet interpretation of deep neural networks. Therefore, we believe that there are ample opportunities for future work in the interface between wavelets and deep neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lifting scheme: one step of forward (left) and backward (right) transform. Here, a and d denote the vectors of all approximation and detail coefficients of the lifted transform at level . U and P are linear update and predict operators. HT and IHT are the Haar transform and its inverse.</figDesc><graphic coords="2,131.10,626.31,151.05,58.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scaling (left) and wavelet (right) functions on periodic interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 (</head><label>3</label><figDesc>Figure3(a) depicts a graph representing the road network of Minnesota, with edges showing the major roads and vertices being their intersections. In our construction we employ unit weights on edges and use 32 scaled eigenvectors of graph Laplacian as training functions. The resulting scaling functions for regions containing the red vertex in Figure3(a) are shown at different levels in Figure3(b,c,d,e,f). The function values at graph vertices are color coded from smallest (dark blue) to largest (dark red). Note that the scaling functions are continuous and show multiscale spatial behavior.To test whether the learned wavelets provide a sparse representation of smooth signals, we synthetically generated 100 continuous functions using the xy-coordinates (the coordinates have not been</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>= 4 (</head><label>4</label><figDesc>e) Scaling = 6 (f) Scaling = 8 (g) Reconstruction error (h) Learning error Figure 4: Our construction on the station network (a) trained with daily temperature data (e.g. (b)), yields the scaling functions (c,d,e,f). Reconstruction results (g) using our wavelets trained on data (Wav-data) and with smooth prior (Wav-smooth). Results of semi-supervised learning (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The scaling functions (a) resulting from training on a face images dataset. These wavelets (Wav-data) provide better sparse reconstruction quality than the CDF9/7 wavelet filterbanks (b,c).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Mark Schmidt, http://www.di.ens.fr/˜mschmidt/Software/minFunc.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>National Climatic Data Center, ftp://ftp.ncdc.noaa.gov/pub/data/gsod/2012/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We thank Jonathan Huang for discussions and especially for his advice regarding the experimental section. The authors acknowledge the support of NSF grants FODAVA 808515 and DMS 1228304, AFOSR grant FA9550-12-1-0372, ONR grant N00014-13-1-0341, a Google research award, and the Max Plack Center for Visual Computing and Communications.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on riemannian manifolds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004-04-04">2004. 4.4, 5</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="209" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonlinear wavelet transforms for image coding via lifting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Claypoole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1449" to="1459" />
			<date type="published" when="2003-12">Dec. 2003. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="30" />
			<date type="published" when="2005-04-06">July 2006. 4.6, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diffusion wavelets. Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph wavelets for spatial traffic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crovella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factoring wavelet transforms into lifting steps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="267" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multidimensional filter banks and multiscale geometric representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="264" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gavish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005-04-03">2010. 1, 3, 4.5</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Wavelet Tour of Signal Processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sparse Way</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-dimensional separable critically sampled wavelet filterbanks on arbitrary graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3501" to="3504" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized tree-based wavelet transform</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4199" to="4209" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Average interpolating wavelets on point clouds and graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rustamov</surname></persName>
		</author>
		<idno>CoRR, abs/1110.2227</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The lifting scheme: A construction of second generation wavelets</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="546" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion-driven multiscale analysis on manifolds and graphs: top-down and bottom-up constructions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2005-04-03">2005. 1, 3, 4.5</date>
			<biblScope unit="volume">5914</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning of structured graph dictionaries</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3373" to="3376" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
