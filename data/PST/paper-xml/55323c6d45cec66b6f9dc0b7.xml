<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIRM: Fair and High-Performance Memory Control for Persistent Memory Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<email>yuanxie@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
									<settlement>Hewlett-Packard Labs</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FIRM: Fair and High-Performance Memory Control for Persistent Memory Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5256B06B20DFC8FA24D0011EC4CB52F3</idno>
					<idno type="DOI">10.1109/MICRO.2014.47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>memory scheduling</term>
					<term>persistent memory</term>
					<term>fairness</term>
					<term>memory interference</term>
					<term>nonvolatile memory</term>
					<term>data persistence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Byte-addressable nonvolatile memories promise a new technology, persistent memory, which incorporates desirable attributes from both traditional main memory (byte-addressability and fast interface) and traditional storage (data persistence). To support data persistence, a persistent memory system requires sophisticated data duplication and ordering control for write requests. As a result, applications that manipulate persistent memory (persistent applications) have very different memory access characteristics than traditional (non-persistent) applications, as shown in this paper. Persistent applications introduce heavy write traffic to contiguous memory regions at a memory channel, which cannot concurrently service read and write requests, leading to memory bandwidth underutilization due to low bank-level parallelism, frequent write queue drains, and frequent bus turnarounds between reads and writes. These characteristics undermine the high-performance and fairness offered by conventional memory scheduling schemes designed for non-persistent applications.</p><p>Our goal in this paper is to design a fair and high-performance memory control scheme for a persistent memory based system that runs both persistent and non-persistent applications. Our proposal, FIRM, consists of three key ideas. First, FIRM categorizes request sources as non-intensive, streaming, random and persistent, and forms batches of requests for each source. Second, FIRM strides persistent memory updates across multiple banks, thereby improving bank-level parallelism and hence memory bandwidth utilization of persistent memory accesses. Third, FIRM schedules read and write request batches from different sources in a manner that minimizes bus turnarounds and write queue drains. Our detailed evaluations show that, compared to five previous memory scheduler designs, FIRM provides significantly higher system performance and fairness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>For decades, computer systems have adopted a two-level storage model consisting of: 1) a fast, byte-addressable main memory that temporarily stores applications' working sets, which is lost on a system halt/reboot/crash, and 2) a slow, block-addressable storage device that permanently stores persistent data, which can survive across system boots/crashes. Recently, this traditional storage model is enriched by the new persistent memory technology -a new tier between traditional main memory and storage with attributes from both <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59]</ref>. Persistent memory allows applications to perform loads and stores to manipulate persistent data, as if they are accessing traditional main memory. Yet, persistent memory is the permanent home of persistent data, which is protected by versioning (e.g., logging and shadow updates) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90]</ref> and write-order control <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b65">66]</ref>, borrowed from databases and file systems to provide consistency of data, as if data is stored in traditional storage devices (i.e., hard disks or flash memory). By enabling data persistence in main memory, applications can directly access persistent data through a fast memory interface without paging data blocks in and out of slow storage devices or performing context switches for page faults. As such, persistent memory can dramatically boost the performance of applications that require high reliability demand, such as databases and file systems, and enable the design of more robust systems at high performance. As a result, persistent memory has recently drawn significant interest from both academia and industry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90]</ref>. Recent works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b91">92]</ref> even demonstrated a persistent memory system with performance close to that of a system without persistence support in memory.</p><p>Various types of physical devices can be used to build persistent memory, as long as they appear byte-addressable and nonvolatile to applications. Examples of such byte-addressable nonvolatile memories (BA-NVMs) include spin-transfer torque RAM (STT-MRAM) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b92">93]</ref>, phase-change memory (PCM) <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b80">81]</ref>, resistive random-access memory (ReRAM) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, battery-backed DRAM <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>, and nonvolatile dual in-line memory modules (NV-DIMMs) <ref type="bibr" target="#b88">[89]</ref>. <ref type="foot" target="#foot_0">1</ref>As it is in its early stages of development, persistent memory especially serves applications that can benefit from reducing storage (or, persistent data) access latency with relatively few or lightweight changes to application programs, system software, and hardware <ref type="bibr" target="#b9">[10]</ref>. Such applications include databases <ref type="bibr" target="#b89">[90]</ref>, file systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, keyvalue stores <ref type="bibr" target="#b15">[16]</ref>, and persistent file caches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Other types of applications may not directly benefit from persistent memory, but can still use BA-NVMs as their working memory (nonvolatile main memory without persistence) to leverage the benefits of large capacity and low stand-by power <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b72">73]</ref>. For example, a large number of recent works aim to fit BA-NVMs as part of main memory in the traditional two-level storage model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b93">94]</ref>. Several very recent works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref> envision that BA-NVMs can be simultaneously used as persistent memory and working memory. In this paper, we call applications leveraging BA-NVMs to manipulate persistent data as persistent applications, and those using BA-NVMs solely as working memory as non-persistent applications. <ref type="foot" target="#foot_1">2</ref>Most prior work focused on designing memory systems to accommodate either type of applications, persistent or non-persistent. Strikingly little attention has been paid to study the cases when these two types of applications concurrently run in a system. Persistent applications require the memory system to support crash consistency, or the persistence property, typically supported in traditional storage systems. This property guarantees that the system's data will be in a consistent state after a system or application crash, by ensuring that persistent memory updates are done carefully such that incomplete updates are recoverable. Doing so requires data duplication and careful control over the ordering of writes arriving at memory (Section 2.2). The sophisticated designs to support persistence lead to new memory access characteristics for persistent applications. In particular, we find that these applications have very high write intensity and very low memory bank parallelism due to frequent streaming writes to persistent data in memory (Section 3.1). These characteristics lead to substantial resource contention between reads and writes at the shared memory interface for a system that concurrently runs persistent and non-persistent applications, unfairly slowing down either or both types of applications. Previous memory scheduling schemes, designed solely for non-persistent applications, become inefficient and lowperformance under this new scenario (Section 3.2). We find that this is because the heavy write intensity and low bank parallelism of persistent applications lead to three key problems not handled well by past schemes: 1) frequent write queue drains in the memory controller, 2) frequent bus turnarounds between reads and writes, both of which lead to wasted cycles on the memory bus, and 3) low memory bandwidth utilization during writes to memory due to low memory bank parallelism, which leads to long periods during which memory reads are delayed (Section 3).</p><p>Our goal is to design a memory control scheme that achieves both fair memory access and high system throughput in a system concurrently running persistent and non-persistent applications. We propose FIRM, a fair and high-performance memory control scheme, which 1) improves the bandwidth utilization of persistent applications and 2) balances the bandwidth usage between persistent and non-persistent applications. FIRM achieves this using three components. First, it categorizes memory request sources as non-intensive, streaming, random and persistent, to ensure fair treatment across different sources, and forms batches of requests for each source in a manner that preserves row buffer locality. Second, FIRM strides persistent memory updates across multiple banks, thereby improving bank-level parallelism and hence memory bandwidth utilization of persistent memory accesses. Third, FIRM schedules read and write request batches from different sources in a manner that minimizes bus turnarounds and write queue drains. Compared to five previous memory scheduler designs, FIRM provides significantly higher system performance and fairness. This paper makes the following contributions:</p><p>• We identify new problems related to resource contention at the shared memory interface when persistent and non-persistent applications concurrently access memory. The key fundamental problems, caused by memory access characteristics of persistent applications, are: 1) frequent write queue drains, 2) frequent bus turnarounds, both due to high memory write intensity, and 3) memory bandwidth underutilization due to low memory write parallelism. We describe the ineffectiveness of prior memory scheduling designs in handling these problems. (Section 3) • We propose a new strided writing mechanism to improve the bank-level parallelism of persistent memory updates. This technique improves memory bandwidth utilization of memory writes and reduces the stall time of non-persistent applications' read requests. (Section 4.3) • We propose a new persistence-aware memory scheduling policy between read and write requests of persistent and non-persistent applications to minimize memory interference and reduce unfair application slowdowns. This technique reduces the overhead of switching the memory bus between reads and writes by reducing bus turnarounds and write queue drains. (Section 4.4) • We comprehensively compare the performance and fairness of our proposed persistent memory control mechanism, FIRM, to five prior memory schedulers across a variety of workloads and system configurations. Our results show that 1) FIRM provides the highest system performance and fairness on average and for all evaluated workloads, 2) FIRM's benefits are robust across system configurations, 3) FIRM minimizes the bus turnaround overhead present in prior scheduler designs. (Section 7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>In this section, we provide background on existing memory scheduling schemes, the principles and mechanics of persistent memory, and the memory requests generated by persistent applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conventional Memory Scheduling Mechanisms</head><p>A memory controller employs memory request buffers, physically or logically separated into a read and a write queue, to store the memory requests waiting to be scheduled for service. It also utilizes a memory scheduler to decide which memory request should be scheduled next. A large body of previous work developed various memory scheduling policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b94">95]</ref>. Traditional commodity systems employ a variant of the first-ready first-come-first-serve (FR-FCFS) scheduling policy <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b94">95]</ref>, which prioritizes memory requests that are row-buffer hits over others and, after that, older memory requests over others. Because of this, it can unfairly deprioritize applications that have low buffer hit rate and that are not memory intensive, hurting both fairness and overall system throughput <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b63">64]</ref>. Several designs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref> aim to improve either system performance or fairness, or both. PAR-BS <ref type="bibr" target="#b64">[65]</ref> provides fairness and starvation freedom by batching requests from different applications based on their arrival times and prioritizing the oldest batch over others. It also improves system throughput by preserving the bank-level parallelism of each application via the use of rankbased scheduling of applications. ATLAS <ref type="bibr" target="#b40">[41]</ref> improves system throughput by prioritizing applications that have received the least memory service. However, it may unfairly deprioritize and slow down memory-intensive applications due to the strict ranking it employs between memory-intensive applications <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. To address this issue, TCM <ref type="bibr" target="#b41">[42]</ref> dynamically classifies applications into two clusters, low and high memory-intensity, and employs heterogeneous scheduling policies across the clusters to optimize for both system throughput and fairness. TCM prioritizes the applications in the lowmemory-intensity cluster over others, improving system throughput, and shuffles thread ranking between applications in the high-memoryintensity cluster, improving fairness and system throughput. While shown to be effective in a system that executes only non-persistent applications, unfortunately, none of these scheduling schemes address the memory request scheduling challenges posed by concurrentlyrunning persistent and non-persistent applications, as we discuss in Section 3 and evaluate in detail in Section 7. <ref type="foot" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Persistent Memory</head><p>Most persistent applications stem from traditional storage system workloads (databases and file systems), which require persistent memory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref> to support crash consistency <ref type="bibr" target="#b5">[6]</ref>, i.e., the persistence property. The persistence property guarantees that the critical data (e.g., database records, files, and the corresponding metadata) stored in nonvolatile devices retains a consistent state in case of power loss or a program crash, even when all the data in volatile devices may be lost. Achieving persistence in BA-NVM is nontrivial, due to the presence of volatile processor caches and memory write reordering performed by the write-back caches and memory controllers. For instance, a power outage may occur while a persistent application is inserting a node to a linked list stored in BA-NVM. Processor caches and memory controllers may reorder the write requests, writing the pointer into BA-NVM before writing the values of the new node. The linked list can lose consistency with dangling pointers, if values of the new node remaining in processor caches are lost due to power outage, which may lead to unrecoverable data corruption. To avoid such inconsistency problems, most persistent memory designs borrow the ACID (atomicity, consistency, isolation, and durability) concepts from the database and file system communities <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref>. Enforcing these concepts, as explained below, leads to additional memory requests, which affect the memory access behavior of persistent applications. Versioning and Write Ordering. While durability can be guaranteed by BA-NVMs' non-volatile nature, atomicity and consistency are supported by storing multiple versions of the same piece of data and carefully controlling the order of writes into persistent memory (please refer to prior studies for details <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref>). Figure <ref type="figure" target="#fig_0">1</ref> shows a persistent tree data structure as an example to illustrate the different methods to maintain versions and ordering. Assume nodes N3 and N4 are updated. We discuss two commonly-used methods to maintain multiple versions and ordering. The first one is redo logging <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b89">90]</ref>. With this method, new values of the two nodes, along with their addresses, are written into a log (logN 3 and logN 4 ) before their original locations are updated in memory (Figure <ref type="figure" target="#fig_0">1(a)</ref>). If a system loses power before logging is completed, persistent memory can always recover, using the intact original data in memory. A memory barrier is employed between the writes to the log and writes to the original locations in memory. This ordering control, with enough information kept in the log, ensures that the system can recover to a consistent state even if it crashes before all original locations are updated. The second method, illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(b), is the notion of shadow updates (copy-on-write) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b87">88]</ref>. Instead of storing logs, a temporary data buffer is allocated to store new values (shadow copies) of the nodes. Note that the parent node N1 is also shadow-copied, with the new pointer N 1 pointing to the shadow copies N 3 and N 4 . Ordering control (shown as a memory barrier in Figure <ref type="figure" target="#fig_0">1(b)</ref>) ensures that the root pointer is not updated until writes to the shadow copies are completed in persistent memory. Relaxed Persistence. Strict persistence <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b69">70]</ref> requires maintaining the program order of every write request, even within a single log update. Pelley et al. recently introduced a relaxed persistence model to minimize the ordering control to buffer and coalesce writes to the same data <ref type="bibr" target="#b69">[70]</ref>. <ref type="foot" target="#foot_3">4</ref> Our design adopts their relaxed persistence model. For example, we only enforce the ordering between the writes to shadow copies and to the root pointer, as shown in Figure <ref type="figure" target="#fig_0">1(b)</ref>. Another recent work, Kiln <ref type="bibr" target="#b91">[92]</ref> relaxed versioning, eliminating the use of logging or shadow updates by implementing a nonvolatile last-level cache (NV cache). However, due to the limited capacity and associativity of the NV cache, the design cannot efficiently accommodate large-granularity persistent updates in database and file system applications. Consequently, we envision that logging, shadow updates, and Kiln-like designs will coexist in persistent memory designs in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Memory Requests of Persistent Applications</head><p>Persistent Writes. We define the writes to perform critical data updates that need to be persistent (including updates to original data locations, log updates, and shadow-copy updates), as persistent writes. Each critical data update may generate an arbitrary number of persistent writes depending on the granularity of the update. For example, in a key-value store, an update may be the addition of a new value of several bytes, several kilobytes, several megabytes, or larger. Note that persistent memory architectures either typically flush persistent writes (i.e., dirty blocks) out of processor caches at the point of memory barriers, or implement persistent writes as uncacheable (UC) writes <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref>. Non-persistent Writes. Non-critical data, such as stacks and data buffers, are not required to survive system failures. Typically, persistent memory does not need to perform versioning or ordering control over these writes. As such, persistent applications not only perform persistent writes but also non-persistent writes as well. Reads. Persistent applications also perform reads of in-flight persistent writes and other independent reads. Persistent memory can relax the ordering of independent reads without violating the persistence requirement. However, doing so can impose substantial performance penalties (Section 3.2). Reads of in-flight persistent updates need to wait until these persistent writes arrive at BA-NVMs. Conventional memory controller designs provide read-after-write ordering by servicing reads of in-flight writes from write buffers. With volatile memory, such a behavior does not affect memory consistency. With nonvolatile memory, however, power outages or program crashes can destroy in-flight persistent writes before they are written to persistent memory. Speculative reads of in-flight persistent updates can lead to incorrect ordering and potential resulting inconsistency, because if a read has already gotten the value of an in-flight write that would disappear on a crash, wrong data may eventually propagate to persistent memory as a result of the read.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MOTIVATION: HANDLING PERSISTENT MEMORY ACCESSES</head><p>Conventional memory scheduling schemes are designed based on the assumption that main memory is used as working memory, i.e., a file cache for storage systems. This assumption no longer holds when main memory also supports data persistence, by accommodating persistent applications that access memory differently from traditional non-persistent applications. This is because persistent memory writes have different consistency requirements than working memory writes, as we described in Sections 2.2 and 2.3. In this section, we study the performance implications caused by this different memory access behavior of persistent applications (Section 3.1), discuss the problems of directly adopting existing memory scheduling methods to handle persistent memory accesses (Section 3.2), and describe why naïvely extending past memory schedulers does not solve the problem (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Memory Access Characteristics of Persistent Applications</head><p>An application's memory access characteristics can be evaluated using four metrics: a) memory intensity, measured as the number of last-level cache misses per thousand instructions (MPKI) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref>; b) write intensity, measured as the portion of write misses (WR%) out of all cache misses; c) bank-level parallelism (BLP), measured as the average number of banks with outstanding memory requests, when at least one other outstanding request exists <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref>; d) row-buffer locality (RBL), measured as the average hit rate of the row buffer across all banks <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b76">77]</ref>.</p><p>To illustrate the different memory access characteristics of persistent and non-persistent applications, we studied the memory accesses of three representative micro-benchmarks, streaming, random, and KVStore. Streaming and random <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b60">61]</ref> are both memory-intensive, non-persistent applications, performing streaming and random accesses to a large array, respectively. They serve as the two extreme cases with dramatically different BLP and RBL. The persistent application KVStore performs inserts and deletes to key-value pairs (25-byte keys and 2K-byte values) of an in-memory B+ tree data structure. The sizes of keys and values were specifically chosen so that KVStore had the same memory intensity as the other two micro-benchmarks. We build this benchmark by implementing a redo logging (i.e., writing new updates to a log while keeping the original data intact) interface on top of STX B+ Tree <ref type="bibr" target="#b11">[12]</ref> to provide persistence support. Redo logging behaves very similarly to shadow updates (Section 2.2), which perform the updates in a shadow version of the data structure instead of logging them in a log space. Our experiments (not shown here) show that the performance implications of KVStore with shadow updates are similar to those of KVStore with redo logging, which we present here.</p><p>Table <ref type="table" target="#tab_0">1</ref> lists the memory access characteristics of the three microbenchmarks running separately. The persistent application KVStore, especially in its persistence phase when it performs persistent writes, has three major discrepant memory access characteristics in comparison to the two non-persistent applications. 1. High write intensity. While the three applications have the same memory intensity, KVStore has much higher write intensity than the other two. This is because each insert or delete operation triggers a redo log update, which appends a log entry containing the addresses and the data of the modified key-value pair. The log updates generate extra write traffic in addition to the original location updates.</p><p>2. Higher memory intensity with persistent writes. The last row of Table <ref type="table" target="#tab_0">1</ref> shows that while the KVStore application is in its persistence phase (i.e., when it is performing persistent writes and flushing these writes out of processor caches), it causes greatly higher memory traffic (MPKI is 675). During this phase, writes make up almost all (92%) the memory traffic.</p><p>3. Low BLP and high RBL with persistent writes. KVStore, especially while performing persistent writes, has low BLP and high RBL. KVStore's log is implemented as a circular buffer, similar to those used in prior persistent memory designs <ref type="bibr" target="#b89">[90]</ref>, by allocating (as much as possible) one or more contiguous regions in the physical address space. As a result, the log updates lead to consecutive writes to contiguous locations in the same bank, i.e., an access pattern that can be characterized as streaming writes. This makes KVStore's write behavior similar to that of streaming's reads: low BLP and high RBL.</p><p>However, the memory bus can only service either reads or writes (to any bank) at any given time because the bus can be driven in only one direction <ref type="bibr" target="#b48">[49]</ref>, which causes a fundamental difference (and conflict) between handling streaming reads and streaming writes. We conclude that the persistent writes cause persistent applications to have widely different memory access characteristics than nonpersistent applications. As we show next, the high write intensity and low bank-level parallelism of writes in persistent applications cause a fundamental challenge to existing memory scheduler designs for two reasons: 1) the high write intensity causes frequent switching of the memory bus between reads and writes, causing bus turnaround delays, 2) the low write BLP causes underutilization of memory bandwidth while writes are being serviced, which delays any reads in the memory request buffer. These two problems become exacerbated when persistent applications run together with non-persistent ones, a scenario where both reads and persistent writes are frequently present in the memory request buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inefficiency of Prior Memory Scheduling Schemes</head><p>As we mentioned above, the memory bus can service either reads or writes (to any bank) at any given time because the bus can be driven in only one direction <ref type="bibr" target="#b48">[49]</ref>. Prior memory controllers (e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b94">95]</ref>) buffer writes in a write queue to allow read requests to aggressively utilize the memory bus. When the write queue is full or is filled to a predefined level, the memory scheduler switches to a write drain mode where it drains the write queue either fully or to a predetermined level <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b82">83]</ref>, in order to prevent stalling the entire processor pipeline. During the write drain mode, the memory bus can service only writes. In addition, switching into and out of the write drain mode from the read mode induces additional penalty in the DRAM protocol (called read-to-write and write-to-read turnaround delays, tRT W and tWTR, approximately 7.5ns and 15ns, respectively <ref type="bibr" target="#b42">[43]</ref>) during which no read or write commands can be scheduled on the bus, causing valuable memory bus cycles to be wasted. Therefore, frequent switches into the write drain mode and long time spent in the write drain mode can significantly slow down reads and can harm the performance of read-intensive applications and the entire system <ref type="bibr" target="#b48">[49]</ref>.</p><p>This design of conventional memory schedulers is based on two assumptions, which are generally sound for non-persistent applications. First, reads are on the critical path of application execution whereas writes are usually not. This is sound when most nonpersistent applications abound with read-dependent arithmetic, logic, and control flow operations and writes can be serviced from write buffers in caches and in the memory controller. Therefore, most prior memory scheduling schemes prioritize reads over writes. Second, applications are usually read-intensive, and memory controllers can delay writes without frequently filling up the write queues. Therefore, optimizing the performance of writes is not as critical to performance in many workloads as the write queues are large enough for such read-intensive applications.</p><p>Unfortunately, these assumptions no longer hold when persistent writes need to go through the same shared memory interface as nonpersistent requests. First, the ordering control of persistent writes requires the serialization of the persistent write traffic to main memory (e.g., via the use of memory barriers, as described in Section 2.2). This causes the persistent writes, reads of in-flight persistent writes, and computations dependent on these writes (and potentially all computations after the persistent writes, depending on the implementation) to be serialized. As such, persistent writes are also on the critical execution path. As a result, simply prioritizing read requests over persistent write requests can hurt system performance. Second, persistent applications are write-intensive as opposed to read-intensive. This is due to not only the persistent nature of data manipulation, which might lead to more frequent memory updates, but also the way persistence is guaranteed using multiple persistent updates (i.e., to the original location as well as the alternate version of the data in a redo log or a shadow copy, as explained in Section 2.2).</p><p>Because of these characteristics of persistent applications, existing memory controllers are inefficient in handling them concurrently with non-persistent applications. Figure <ref type="figure">2</ref> illustrates this inefficiency in a system that concurrently runs KVStore with either the streaming or the random application. This figure shows the fraction of memory access cycles that are spent due to delays related to bus turnaround between reads and writes as a function of the number of write queue entries. <ref type="foot" target="#foot_4">5</ref> The figure shows that up to 17% of memory bus cycles are wasted due to frequent bus turnarounds, with a commonly-used 64entry write queue. We found that this is mainly because persistent writes frequently overflow the write queue and force the memory controller to drain the writes. Typical schedulers in modern processors have only 32 to 64 write queue entries to buffer memory requests <ref type="bibr" target="#b29">[30]</ref>. Simply increasing the number of write queue entries in the scheduler is not a scalable solution <ref type="bibr" target="#b6">[7]</ref>. In summary, conventional memory scheduling schemes, which prioritize reads over persistent writes, become inefficient when persistent and non-persistent applications share the memory interface. This causes relatively low performance and fairness (as we show next).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of Prior and Naïve Scheduling Policies</head><p>We have observed, in Section 2.3, that the persistent applications' (e.g., KVStore's) writes behave similarly to streaming reads. As such, a natural idea would be to assign these persistent writes the same priority as read requests, instead of deprioritizing them below read requests, to ensure that persistent applications are not unfairly penalized. This is a naïve (yet simple) method of extending past schedulers to potentially deal with persistent writes.</p><p>In this section, we provide a case study analysis of fairness and performance of both prior schedulers (FR-FCFS <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b94">95]</ref> and TCM <ref type="bibr" target="#b41">[42]</ref>) and naïve extensions of these prior schedulers (FRFCFS-modified and TCM-modified) that give equal priority to reads and persistent writes. <ref type="foot" target="#foot_5">6</ref> Figure <ref type="figure" target="#fig_1">3</ref> illustrates fairness and system performance of these schedulers for two workloads where KVStore is run together with streaming or random. To evaluate fairness, we consider both the individual slowdown of each application <ref type="bibr" target="#b47">[48]</ref> and the maximum slowdown <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b86">87]</ref> across both applications in a workload. We make several major observations. Case Study 1 (WL1 in Figure <ref type="figure" target="#fig_1">3</ref>(a) and (c)): When KVStore is run together with streaming, prior scheduling policies (FR-FCFS and TCM) unfairly slow down the persistent KVStore. Because these policies delay writes behind reads, and streaming's reads with high row-buffer locality capture a memory bank for a long time, KVStore's writes need to wait for long time periods even though they also have high row buffer locality. When the naïve policies are employed, the effect is reversed: FRFCFS-modified and TCM-modified reduce the slowdown of KVStore but increase the slowdown of streaming compared to FRFCFS and TCM. KVStore performance improves because, as persistent writes are the same priority as reads, its frequent writes are not delayed too long behind streaming's reads. Streaming slows down greatly due to two major reasons. First, its read requests are interfered much more frequently with the write requests of KVStore. Second, due to equal read and persistent write priorities, the memory bus has to be frequently switched between persistent writes and streaming reads, leading to high bus turnaround latencies where no request gets scheduled on the bus. These delays slow down both applications but affect streaming a lot more because almost all accesses of streaming are reads and are on the critical path, and are affected by both read-to-write and write-to-read turnaround delays whereas KVStore's writes are less affected by write-to-read turnaround delays. Figure <ref type="figure" target="#fig_1">3(c)</ref> shows that the naïve policies greatly degrade overall system performance on this workload, even though they improve KVStore's performance. We find this system performance degradation is mainly due to the frequent bus turnarounds.</p><p>Case Study 2 (WL2 in Figure <ref type="figure" target="#fig_1">3</ref>(b) and (c)): KVStore and random are two applications with almost exactly opposite BLP, RBL, and write intensity. When these two run together, random slows down the most with all of the four evaluated scheduling policies. This is because random is more vulnerable to interference than the mostly-streaming KVStore due to its high BLP, as also observed in previous studies <ref type="bibr" target="#b41">[42]</ref>. FRFCFS-modified slightly improves KVStore's performance while largely degrading random's performance due to the same reason described for WL1. TCMmodified does not significantly affect either application's performance because three competing effects end up canceling any benefits. First, TCM-modified ends up prioritizing the random-access random over streaming KVStore in some time intervals, as it is aware of the high vulnerability of random due to its high BLP and low RBL. Second, at other times, it prioritizes the frequent persistent write requests of KVStore over read requests of random due to the equal priority of reads and persistent writes. Third, frequent bus turnarounds (as discussed above for WL1) degrade both applications' performance. Figure <ref type="figure" target="#fig_1">3</ref>(c) shows that the naïve policies slightly degrade or not affect overall system performance on this workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Summary and Our Goal</head><p>In summary, neither conventional scheduling policies nor their naïve extensions that take into account persistent writes provide high fairness and high system performance. This is because they lead to 1) frequent entries into write drain mode due to high intensity of persistent writes, 2) resulting frequent bus turnarounds between read and write requests that cause wasted bus cycles, and 3) memory bandwidth underutilization during write drain mode due to low BLP of persistent writes. These three problems are pictorially illustrated in Figure <ref type="figure" target="#fig_2">4</ref>(a) and (b), which depict the service timeline of memory requests with conventional scheduling and its naïve extension. This illustration shows that 1) persistent writes heavily access Bank-1, leading to high bandwidth underutilization with both schedulers, 2) both schedulers lead to frequent switching between reads and writes, and 3) the naïve scheduler delays read requests significantly because it prioritizes persistent writes, and it does not reduce the bus turnarounds. Our evaluation of 18 workload combinations (in <ref type="bibr">Section 7)</ref> shows that various conventional and naïve scheduling schemes lead to low system performance and fairness, due to these three reasons. Therefore, a new memory scheduler design is needed to overcome these challenges and provide high performance and fairness in a system where the memory interface is shared between persistent and non-persistent applications. Our goal in this work is to design such a scheduler (Section 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FIRM DESIGN</head><p>Overview. We propose FIRM, a memory control scheme that aims to serve requests from persistent and non-persistent applications in a fair and high throughput manner at the shared memory interface. FIRM introduces two novel design principles to achieve this goal, which are illustrated conceptually in Figure <ref type="figure" target="#fig_2">4</ref>(c). First, persistent write striding ensures that persistent writes to memory have high BLP such that memory bandwidth is well-utilized during the write drain mode. It does so by ensuring that consecutively-issued groups of writes to the log or shadow copies in persistent memory are mapped to different memory banks. This reduces not only the duration of the write drain mode but also the frequency of entry into write drain mode compared to prior methods, as shown in Figure <ref type="figure" target="#fig_2">4</ref>(c). Second, persistence-aware memory scheduling minimizes the frequency of write queue drains and bus turnarounds by scheduling the queued up reads and writes in a fair manner. It does so by balancing the amount of time spent in write drain mode and read mode, while ensuring that the time spent in each mode is long enough such that the wasted cycles due to bus turnaround delays are minimized. Persistence-aware memory scheduling therefore reduces: 1) the latency of servicing the persistent writes, 2) the amount of time persistent writes block outstanding reads, and 3) the frequency of entry into write queue drain mode. The realization of these two principles leads to higher performance and efficiency than conventional and naïve scheduler designs, as shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>FIRM design consists of four components: 1) request batching, which forms separate batches of read and write requests that go to the same row, to maximize row buffer locality, 2) source categorization, which categorizes the request sources for effective scheduling by distinguishing various access patterns of applications, 3) persistent write striding, which maximizes BLP of persistent requests, and 4) persistence-aware memory scheduling, which maximizes performance and fairness by appropriately adjusting the number of read and write batches to be serviced at a time. Figure <ref type="figure" target="#fig_3">5</ref>(a) depicts an overview of the components, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Request Batching</head><p>The goal of request batching is to group together the set of requests to the same memory row from each source (i.e., process or hardware thread context, as described below in Section 4.2). Batches are formed per source, similarly to previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b64">65]</ref>, separately for reads and writes. If scheduled consecutively, all requests in a read or write batch (except for the first one) will hit in the row buffer, minimizing latency and maximizing memory data throughput. A batch is considered to be formed when the next memory request in the request buffer of a source is to a different row <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Source Categorization</head><p>To apply appropriate memory control over requests with various characteristics, FIRM dynamically classifies the sources of memory requests into four: non-intensive, streaming, random, persistent. A source is defined as a process or thread during a particular time period, when it is generating memory requests in a specific manner. For example, a persistent application is considered a persistent source when it is performing persistent writes. It may also be a non-intensive, a streaming, or a random source in other time periods.</p><p>FIRM categorizes sources on an interval basis. At the end of an interval, each source is categorized based on its memory intensity, RBL, BLP, and persistence characteristics during the interval, predicting that it will exhibit similar behavior in the next interval. <ref type="foot" target="#foot_6">7</ref>The main new feature of FIRM's source categorization is its detection of a persistent source (inspired by the discrepant characteristics of persistent applications described in Section 2.3). Table <ref type="table">2</ref> depicts the rules FIRM employs to categorize a source as persistent. FIRM uses program hints (with the software interface described in Section 5) to determine whether a hardware context belongs to a persistent application. This ensures that a non-persistent application does not get classified as a persistent source. If a hardware context belonging to such an application is generating write batches that are larger than a pre-defined threshold (i.e., has an average write batch size greater than 30 in the previous interval) and if it inserts memory barriers between memory requests (i.e., has inserted at least one memory barrier between write requests in the previous interval), FIRM categorizes it as a persistent source.  Table <ref type="table">2</ref>. Rules used to identify persistent sources.</p><p>A thread is identified as a persistent source, if it 1: belongs to a persistent application; 2: is generating write batches that are larger than a pre-defined threshold in the past interval; 3: inserts memory barriers between memory requests. Sources that are not persistent are classified into non-intensive, streaming, and random based on three metrics: MPKI (memory intensity), BLP, RBL. This categorization is inspired by previous studies <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b62">63]</ref>, showing varying characteristics of such sources. A non-intensive source has low memory intensity. We identify these sources to prioritize their batches over batches of other sources; this maximizes system performance as such sources are latency sensitive <ref type="bibr" target="#b41">[42]</ref>. Streaming and random sources are typically read intensive, having opposite BLP and RBL characteristics (Table <ref type="table" target="#tab_0">1</ref>). 8  This streaming and random source classification is used later by the underlying scheduling policy FIRM borrows from past works to maximize system performance and fairness (e.g., TCM <ref type="bibr" target="#b41">[42]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Persistent Write Striding</head><p>The goal of this mechanism is to reduce the latency of servicing consecutively-issued persistent writes by ensuring they have high BLP and thus fully utilize memory bandwidth. We achieve this goal by striding the persistent writes across multiple memory banks via hardware or software support.</p><p>The basic idea of persistent write striding is simple: instead of mapping consecutive groups of row-buffer-sized persistent writes to consecutive row-buffer-sized locations in a persistent data buffer (that is used for the redo log or shadow copies in a persistent application), which causes them to map to the same memory bank, change the mapping such that they are strided by an offset that ensures they map to different memory banks.</p><p>Figure <ref type="figure" target="#fig_3">5</ref>(b) illustrates this idea. A persistent application can still allocate a contiguous memory space for the persistent data buffer. Our method maps the accesses to the data buffer to different banks in a strided manner. Contiguous persistent writes of less than or equal to the row-buffer size are still mapped to contiguous data buffer space with of a row buffer size (called a "buffer group") to achieve high RBL. However, contiguous persistent writes beyond the size of the row-buffer are strided by an offset. The value of the offset is determined by the position of bank index bits used in the physical 8 In our experiments, a hardware context is classified as non-intensive if its MPKI&lt; 1. A hardware context is classified as streaming if its MPKI&gt;1, BLP&lt;4 and RBL&gt;70%. All other hardware contexts that are not persistent are classified as random.</p><p>address mapping scheme employed by the memory controller. For example, with the address mapping scheme in Figure <ref type="figure">6</ref>, the offset should be 128K bytes if we want to fully utilize all eight banks with persistent writes (because a contiguous memory chunk of 16KB gets mapped to the same bank with this address mapping scheme, i.e., the memory interleaving granularity is 16KB across banks). This persistent write striding mechanism can be implemented in either the memory controller hardware or a user-mode library, as we describe in Section 5.</p><p>Higher-order address bits 20 19 18 17 16 15 14   0 13 Fig. <ref type="figure">6</ref>. Physical address to bank mapping example.</p><p>Note that the persistent write striding mechanism provides a deterministic (re)mapping of persistent data buffer physical addresses to physical memory addresses in a strided manner. The remapped physical addresses will not exceed the boundary of the original data buffer. As a result, re-accessing or recovering data at any time from the persistent data buffer is not an issue: all accesses to the buffer go through this remapping.</p><p>Alternative Methods. Note that commodity memory controllers randomize higher-order address bits to minimize bank conflicts (Figure <ref type="figure">6</ref>). However, they can still fail to map persistent writes to different banks because as we showed in Section 3.1, persistent writes are usually streaming and hence they are likely to map to the same bank. It is impractical to improve the BLP of persistent writes by aggressively buffering them due to two reasons: 1) The large buffering capacity required. For example, we might need a write queue as large as 128KB to utilize all eight banks of a DDR3 channel with the address mapping shown in Figure <ref type="figure">6.</ref> 2) The region of concurrent contiguous writes may not be large enough to cover multiple banks (i.e., there may not be enough writes present to different banks). Alternatively, kernel-level memory access randomization <ref type="bibr" target="#b68">[69]</ref> may distribute writes to multiple banks during persistent application execution. However, the address mapping information can be lost when the system reboots, leaving the BA-NVM with unrecoverable data. Finally, it is also prohibitively complex to randomize the bank mapping of only persistent writes by choosing a different set of address bits as their bank indexes, i.e., maintaining multiple address mapping schemes in a single memory system. Doing so requires complex bookkeeping mechanisms to ensure correct mapping of memory addresses. For these very reasons, we have developed the persistent write striding technique we have described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Persistence-Aware Memory Scheduling</head><p>The goal of this component is to minimize write queue drains and bus turnarounds by intelligently partitioning memory service between reads and persistent writes while maximizing system performance and fairness. To achieve this multi-objective goal, FIRM operates at the batch granularity and forms a schedule of read and write batches of different source types: non-intensive, streaming, random, and persistent. To maximize system performance, FIRM prioritizes non-intensive read batches over all other batches.</p><p>For the remaining batches of requests, FIRM employs a new policy that determines 1) how to group read batches and write batches and 2) when to switch between servicing read batches and write batches. FIRM does this in a manner that balances the amount of time spent in write drain mode (servicing write batches) and read mode (servicing read batches) in a way that is proportional to the read and write demands, while ensuring that time spent in each mode is long enough such that the wasted cycles due to bus turnaround delays are minimized. When the memory scheduler is servicing read or persistent write batches, in read mode or write drain mode, the scheduling policy employed can be any of the previously-proposed memory request scheduling policies (e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b94">95]</ref>) and the ordering of persistent write batches is fixed by the ordering control of persistent applications. The key novelty of our proposal is not the particular prioritization policy between requests, but the mechanism that determines how to group batches and when to switch between write drain mode and read mode, which we describe in detail next. 9  To write queue drains, FIRM schedules reads and persistent writes within an interval in a round-robin manner with the memory bandwidth (i.e., the time interval) partitioned between them based on their demands. To prevent frequent bus turnarounds, FIRM schedules a group of batches in one bus transfer direction before scheduling another group of batches in the other direction. Figure <ref type="figure" target="#fig_3">5(c</ref>) illustrates an example of this persistence-aware memory scheduling policy. Assume, without loss of generality, that we have the following batches ready to be scheduled at the beginning of a time interval: a random read batch R1, two streaming read batches R2 and R3, and two (already-strided) persistent write batches W1 and W2. We define a batch group as a group of batches that will be scheduled together. As illustrated in Figure <ref type="figure" target="#fig_3">5</ref>(c), the memory controller has various options to compose the read and write batch groups. This figure shows three possible batch groups for reads and two possible batch groups for writes. These possibilities assume that the underlying memory request scheduling policy dictates the order of batches within a batch group. Our proposed technique thus boils down to determining how many read or write batches to be grouped together to be scheduled in the next read mode or write drain mode.</p><p>We design a new technique that aims to satisfy the following two goals: 1) servicing the two batch groups (read and write) consumes durations proportional to their demand, 2) the total time spent servicing the two batch groups is much longer than the bus turnaround time. The first goal is to prevent the starvation of either reads or persistent writes, by fairly partitioning the memory bandwidth between them. The second goal is to maximize performance by ensuring minimal amount of time is wasted on bus turnarounds.</p><p>Mathematically, we formulate these two goals as the following 9 Note that most previous memory scheduling schemes focus on read requests and do not discuss how to handle switching between read and write modes in the memory controller, implicitly assuming that reads are prioritized over writes until the write queue becomes full or close to full <ref type="bibr" target="#b48">[49]</ref>.</p><p>following two inequalities:</p><formula xml:id="formula_0">t r t w ≈ t r max t w max t RT W +t W T R t r +t w ≤ μ turnaround (1)</formula><p>where t r and t w are the times to service a read and a persistent write batch group, respectively (Figure <ref type="figure" target="#fig_3">5</ref>(c)). They are the maximum service time for the batch group at any bank i:</p><formula xml:id="formula_1">t r = maxi{H r i t rhit i + M r i t rmiss i }, t w = maxi{H w i t whit i + M w i t wmiss i } (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where t rhit , t whit , t rmiss , and t wmiss are the times to service a row buffer hit/miss read/write request; H r i and H w i are the number of row-buffer read/write hits; M r i and M w i are row-buffer read/write misses. t r max and t w max are the maximum times to service all the in-flight read and write write requests (illustrated in Figure <ref type="figure" target="#fig_3">5 (c)</ref>). μ turnaround is a user-defined parameter to represent the maximum tolerable fraction of bus turnaround time out of the total service time of memory requests.</p><p>The goal of our mechanism is to group read and write batches (i.e., form read and write batch groups) to be scheduled in the next read mode and write drain mode in a manner that satisfies Equation 1. Thus, the technique boils down to selecting from the set of possible read/write batch groups such that they satisfy t r next (the duration of the next read mode) and t w next (the duration of the next write drain mode) as indicated by the constraints in Equation <ref type="formula" target="#formula_3">3</ref>(which is obtained by solving the inequality in Equation <ref type="formula">1</ref>). Our technique, Algorithm 1, forms a batch group that has a minimum service duration that satisfies the constraint on the right hand side of Equation 3.<ref type="foot" target="#foot_7">10</ref> </p><formula xml:id="formula_3">⎧ ⎨ ⎩ t r next = min j t r j , t r j ≥ (t RT W +t W T R )/μ turnaround 1+t w max /t r max t w next = min j t w j , t w j ≥ (t RT W +t W T R )/μ turnaround 1+t r max /t w max<label>(3)</label></formula><p>Algorithm 1 Formation of read and write batch groups.</p><p>Input: t RT W , t W T R , and μ turnaround .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The read batch group to be scheduled, indicated by t r next ; The persistent write batch group to be scheduled, indicated by t w next . Initialization: k r ← number of read batch groups; k w ← number of persistent write batch groups; for j ← 0 to k r -1 do Calculate t r j with Equation <ref type="formula" target="#formula_1">2</ref>; end for for j ← 0 to k w -1 do Calculate t w j with Equation <ref type="formula" target="#formula_1">2</ref>; end for</p><formula xml:id="formula_4">t r max ← k r -1 max j=0 t r j ; t w max ← k w -1 max j=0 t w j ; Calculate t r</formula><p>next and t w next with Equation 3;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Software-Hardware Interface</head><p>FIRM software interface provides memory controllers with the required information to identify persistent sources during the source categorization stage. This includes 1) the identification of the persistent application, 2) the communication of the execution of memory barriers. We offer programmers two options to define persistent applications: 1) declaring an entire application as persistent, or 2) specifying a thread of an application to perform persistent updates, i.e., a persistent thread. With the first option, programmers can employ the following software interface (similar to Kiln <ref type="bibr" target="#b91">[92]</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#pragma persistent_memory</head><p>With the second option, programmers can annotate a particular thread with a persistent attribute when they create the thread. The software interface is translated to ISA instructions with simple modifications to compilers and the ISA. Similar ISA extensions have been employed by previous studies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b91">92]</ref>. Once a processor reads software's input of designating a persistent application/thread, it signals each memory controller by setting persistent thread registers to indicate the presence of such an application/thread. Each persistent thread register stores log2N hwthreads -bit hardware thread identifiers (IDs).</p><p>The mechanism to detect memory barriers depends on the CPU architecture and is already present in many processors <ref type="bibr" target="#b55">[56]</ref>. <ref type="foot" target="#foot_8">11</ref> The processor communicates the execution of a barrier needs to the memory controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementing Persistent Write Striding</head><p>Our persistent write striding mechanism can be implemented by modifying memory controller hardware or the logging/shadow update operations in a user-mode library (e.g., employed by Mnemosyne <ref type="bibr" target="#b89">[90]</ref>). The two methods trade off between hardware overhead and low-level software implementation effort.</p><p>To implement persistent write striding in the memory controller, we employ a pair of registers and a counter for each persistent hardware thread. The two registers, referred to as start address and end address, initially record the starting and the end addresses of a contiguous persistent memory region (a data buffer storing the redo log or shadow copies) allocated by a persistent application/thread and currently being accessed. With the two registers, the memory controller can identify the boundary of the data buffer when the persistent application starts writing to it. These registers are reset when a new data buffer is allocated. A 6-bit counter, referred to as the intra-group index, is used to determine when a buffer group (Figure <ref type="figure" target="#fig_3">5(b)</ref>) is fully occupied. It records the number of appended log or shadow updates within a group. When the value of the intra-group index reaches the end of a buffer group, the memory controller will map the coming write requests to the next group in the data buffer by striding the physical address with an offset. At this time point, we also update the start address register with the starting address of its neighboring buffer group. When the next strided physical address exceeds the end of the entire data buffer, the corresponding persistent update will start to write to the first empty buffer group indicated by the start address register.</p><p>We can avoid the above hardware overhead by implementing persistent write striding in a user-mode library. For example, with logging-based persistent applications, we can modify the log_append() function(s) <ref type="bibr" target="#b89">[90]</ref> to stride for an offset with each log append request defined by programmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Tracking Memory Access Characteristics</head><p>Table <ref type="table" target="#tab_3">3</ref> lists the parameters to be tracked for each executing thread at the beginning of each time interval. We employ a set of counters and logic in memory controllers to collect the parameters. Some of the counters have already been implemented in previous work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b64">65]</ref>, which also need to track MPKI, BLP, and RBL. Our design adds a set of request counters in each memory controller, each belonging to a hardware thread context, to track the read and write batch sizes of each source. We reset the counters at the beginning of each time interval, after the memory request scheduling decision is made. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Area Overhead</head><p>Each memory controller maintains its own per-thread hardware counters and registers. Thus, it can independently make scheduling decisions for its local requests. Consequently, our design does not require a centralized arbiter to coordinate all the controllers. Table <ref type="table" target="#tab_4">4</ref> lists the storage required by registers and counters described in this section, in each memory controller. FIRM requires adding 1192 (2400) bits in each memory controller, if the processor has 8 <ref type="bibr" target="#b15">(16)</ref> hardware threads. The complexity of the scheduling algorithm is comparable to those of prior designs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b64">65]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Simulation Framework</head><p>We conducted our experiments using McSimA+ <ref type="bibr" target="#b3">[4]</ref>, a Pinbased <ref type="bibr" target="#b54">[55]</ref> cycle-level multi-core simulator. Table <ref type="table" target="#tab_5">5</ref> lists the parameters of the processor and memory system used in our experiments. Each processor core is similar to one of the Intel Core i7 cores <ref type="bibr" target="#b2">[3]</ref>. The processor incorporates SRAM-based volatile private and shared caches. The cores and L3 cache banks communicate with each other through a crossbar interconnect. The processor employs a two-level hierarchical directory-based MESI protocol to maintain cache coherence. The BA-NVM is modeled as off-chip DIMMs compatible with DDR3. <ref type="foot" target="#foot_9">12</ref> Except for sensitivity studies, we conservatively employ the worst-case timing parameters of the BA-NVM generated by NVSim <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Benchmarks</head><p>Table <ref type="table" target="#tab_6">6</ref> lists the characterization results of our benchmarks when run alone. We select seven non-persistent applications with different memory intensity, BLP, and RBL: four single-threaded applications from SPEC CPU2006 <ref type="bibr" target="#b81">[82]</ref> (mcf, lbm, leslie3d, and povray) and three multithreaded benchmarks from PARSEC 3.0 <ref type="bibr" target="#b10">[11]</ref> (x264, ferret, and  Table <ref type="table" target="#tab_7">7</ref> shows the evaluated workload combinations. W1 trough W6 are non-persistent workloads with three threads. W1 through W3 consist of single-threaded workloads with different fractions of streaming and random memory accesses. W4 through W6 each runs four threads. We constructed 18 workloads mixed by persistent and non-persistent applications to be used in our experiments: W1 through W3 are combined with persistent applications running one thread (each application is mapped to one processor core). W4 through W6 are combined with persistent applications running four threads, leading to a total of eight threads mapped to four two-threaded cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Metrics</head><p>We evaluate system throughput with weighted speedup <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b79">80]</ref>:</p><formula xml:id="formula_5">W eightedSpeedup = i T hroughput shared i T hroughput alone i</formula><p>For SPEC CPU2006, T hroughputi is calculated as instruction throughput, i.e., number of executed instructions per cycle. The throughput of x264 is calculated by frame rate. The throughput of the rest of PARSEC benchmarks and persistent applications is calculated by operation throughput, e.g., the number of completed search, insert, and delete operations per cycle. We evaluate unfairness using maximum slowdown <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b86">87]</ref>:</p><formula xml:id="formula_6">M aximumSlowdown = max i T hroughput alone i T hroughput shared i 7. RESULTS</formula><p>We present our evaluation results and their analyses. We set μ turnaround = 2% in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Performance and Fairness</head><p>We demonstrate system throughput and fairness of FIRM by comparing it with various prior memory scheduling mechanisms in Figure <ref type="figure">7</ref>. FR-FCFS <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b94">95]</ref>, PAR-BS <ref type="bibr" target="#b64">[65]</ref> and TCM <ref type="bibr" target="#b41">[42]</ref> are conventional memory scheduling schemes. TCM-modified is a naïve extended scheduling policy discussed in Section 3.3. NVMDuet is a memory scheduling policy recently proposed for persistent memory systems <ref type="bibr" target="#b51">[52]</ref>. It resolves the contention between only persistent and non-persistent writes. To illustrate the performance and fairness benefits of FIRM's two components, persistent write striding and persistence-aware memory scheduling, we collect the results of applying only persistent write striding (FIRM-Strided) and both (FIRM-Strided-Scheduling) separately. In both cases, we assume the request prioritization policy in FIRM is TCM, which offers the best performance and fairness across various conventional and naïve schemes. We make three major observations. First, because writes are already deprioritized in our baseline schedulers, NVMDuet, which mainly tackles the contention between persistent and non-persistent writes, can improve system performance by only 1.1% (a maximum of 2.6%) and reduces unfairness by 1.7% on average compared to TCM (see Section 9 for more detail). It shifts the most slowed down application from non-persistent ones to the persistent ones in workload combinations Btree-W1, Btree-W2, SPS-W1, and SPS-W2. <ref type="foot" target="#foot_10">13</ref> Second, FIRM-Strided can effectively accelerate persistent writes by scheduling them to multiple banks. It increases system performance by 10.1% on average compared to TCM.Third, FIRM, with both of its components, provides the highest performance and fairness of all evaluated scheduling policies on all workloads. Hence, the benefits of FIRM are consistent. Overall, compared to TCM, FIRM increases average system performance and fairness by 17.9% and 23.1%, respectively. We conclude that FIRM is the most effective policy in mitigating the memory contention between persistent and non-persistent applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Bus Turnaround Overhead</head><p>Figure <ref type="figure">8</ref> compares the bus turnaround overhead (i.e., the fraction of total memory access time spent on bus turnarounds) of FIRM and TCM. With conventional and naïve scheduling schemes, including TCM, the frequency of bus turnarounds is determined by the read/write request batch sizes and the frequency of write queue drains. Both smaller request batches and frequent write queue drains lead to frequent bus turnarounds. We observe that FIRM's two components can effectively reduce the bus turnaround overhead. FIRM-Strided reduces the bus turnaround overhead of a subset of workloads, where the frequency of bus turnarounds is dictated by the frequency of write queue drain. On average, FIRM-Strided reduces bus turnaround overhead by 12% over TCM. Our persistence-aware memory scheduling policy can reduce the bus turnaround frequency even further by dividing bandwidth carefully between read and write batch groups. Therefore, FIRM-Strided-Scheduling, which combines the two FIRM components, reduces bus turnaround overhead by 84% over TCM. We conclude that FIRM can effectively minimize wasted cycles due to bus turnarounds caused by ineffective handling of persistent applications by past memory schedulers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Sensitivity to BA-NVM Latency</head><p>Recently published results on BA-NVM latencies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b85">86]</ref> suggest that the range of read latency is between 32ns and 120ns across various BA-NVM technologies, and the range of write latency is between 40ns and 150ns. Therefore, we sweep the read and write row-buffer conflict latencies from 0.5× to 2× of the parameters listed in Table <ref type="table" target="#tab_5">5</ref>. For example, R2W2 represents doubled read and write latencies (for row-buffer conflicts). As shown in Figure <ref type="figure">9</ref> and Figure <ref type="figure" target="#fig_0">10</ref>, FIRM provides the best average system performance and fairness of all studied scheduling schemes regardless of the BA-NVM latency. We conclude that FIRM can be effective with a wide variety of BA-NVM technologies and design points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Sensitivity to BA-NVM Row-buffer Size</head><p>BA-NVMs can adopt various row-buffer sizes <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref>. We perform a sensitivity study with 1KB, 2KB (our default), 4KB, and 8KB rowbuffer sizes. With each row-buffer size, we tune the granularity of persistent updates (the size of one log entry or one shadow update) to be 0.5×, 1×, 1.5×, and 2× of row-buffer size as shown in the x-axis of Figure <ref type="figure" target="#fig_5">11</ref>. We investigated persistent applications implemented with redo logging and shadow updates, respectively. Logging-based persistent applications allocate log entries in the contiguous physical address space. Therefore, persistent writes fill the BA-NVM row by row, regardless of the variation of value/element size. As a result, we observe that FIRM yields similar results with various row-buffer sizes for workloads that contain logging-based persistent applications (shown in Figure <ref type="figure" target="#fig_5">11</ref>). The large performance and fairness improvements of FIRM are consistent, regardless of the row buffer size for such workloads. FIRM's benefits with shadow update based persistent applications (not shown) depend on the relationship between the size of a persistent update and the row-buffer size. When the size of a persistent update is larger than the row-buffer size, the persistent update will fill the row buffer. Therefore, we observe similar results in such workloads as with logging-based persistent applications. We conclude that the benefits of FIRM are robust to variation in row buffer sizes, assuming persistent applications fully utilize the full row buffer during persistent updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Scalability with Number of Threads</head><p>Figure <ref type="figure" target="#fig_6">12</ref> demonstrates that FIRM outperforms all evaluated memory scheduling schemes on both system performance and fairness, when the number of threads varies from two to 16 (by employing one to eight processor cores). <ref type="foot" target="#foot_11">14</ref> In fact, FIRM benefits increase with more threads, due to increased interference between persistent and nonpersistent applications. FIRM effectively reduces such interference. We conclude that FIRM benefits are robust to thread count. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">DISCUSSION</head><p>BA-NVM Endurance. Among various BA-NVM technologies, STT-MRAM has near-SRAM endurance (&gt; 10<ref type="foot" target="#foot_12">15</ref> ) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>. Therefore, endurance is not an issue in our implementation. Furthermore, FIRM does not apply any memory relocation in BA-NVM. Persistent write striding remaps memory accesses within the range of the physical address space allocated by persistent applications. Consequently, FIRM can be integrated with address-relocation based wear leveling mechanisms <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b78">79]</ref> to address endurance issues, if the system adopts BA-NVMs with worse endurance, e.g., PCM (10 5 -10 9 ) <ref type="bibr" target="#b4">[5]</ref> and ReRAM (10 5 -10 11 ) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Use Cases. Many scenarios are possible in general-purpose computing where persistent and non-persistent applications run concurrently. We provide two examples. First, in a multithreaded memoryhungry web browser application, BA-NVM can accommodate two types of threads concurrently: a front-end browser tab that uses the BA-NVM as working memory; a backend browser thread that caches user data to a persistent database <ref type="bibr" target="#b37">[38]</ref>. Second, such mixing of applications can exist in consolidated cloud computing workloads, where various applications are packed in a limited number of servers. For example, one server can concurrently execute 1) a text search across terabyte-scale documents using in-memory indices and 2) a backend in-memory persistent file system. Many other examples abound in various domains of computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">RELATED WORK</head><p>Hardware resource contention in persistent memory systems received attention only in very recent works, which studied shared processor caches <ref type="bibr" target="#b37">[38]</ref> and shared memory interface <ref type="bibr" target="#b51">[52]</ref>.</p><p>Kannan et al. observed that concurrently-running persistent and non-persistent applications tend to compete for the shared processor caches due to the doubled write requests issued by persistent applications to maintain versioning <ref type="bibr" target="#b37">[38]</ref>. The study proposed a cache partitioning mechanism to address this issue. 15 Since FIRM addresses the contention at the memory interface, it can be combined with the mechanisms from <ref type="bibr" target="#b37">[38]</ref>. Since all persistent writes need to go through the shared memory interface at some point, contention in shared memory can be a more common case in future memory-bandwidthlimited persistent memory systems.</p><p>Liu et al. studied the contention between persistent writes and non-persistent writes at the shared memory interface <ref type="bibr" target="#b51">[52]</ref>. This study considers mainly writes and therefore does not take a comprehensive view of all memory scheduling problems in a system with persistent and non-persistent memory accesses. Because conventional memory scheduling policies deprioritize writes, manipulating the schedule of only writes may not effectively improve system performance (as we show in our evaluation of NVMDuet in Section 7.1). In contrast, we show that the contention between persistent writes and the traditionally-prioritized reads is critically important, leading to significant performance loss and unfairness with existing scheduling policies that do not handle this contention well. FIRM, which comprehensively considers the many problems caused by persistent application requests, therefore, provides higher system performance and fairness than NVMDuet <ref type="bibr" target="#b51">[52]</ref>, as shown in Section 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">CONCLUSIONS</head><p>Emerging byte-addressable nonvolatile memory technologies open up promising opportunities for the creation of persistent applications that can directly and quickly manipulate persistent data via load and store instructions. This paper showed that when such applications contend with traditional non-persistent applications at the memory interface, existing memory scheduler designs lead to low system performance and low fairness across applications, since they cannot efficiently handle the intensive streaming write traffic commonly caused by persistent memory updates. We devised FIRM, which solves this problem with two novel key mechanisms: 1) striding of persistent memory writes such that they more effectively utilize memory parallelism and thus become less disruptive, 2) intelligent balanced scheduling of read and write requests of different types such that memory bandwidth loss is minimized and system performance and fairness are maximized. Our experimental evaluations across a variety of workloads and systems show that FIRM provides significantly higher system performance and fairness than five previous memory schedulers. We conclude that FIRM can provide an effective substrate to efficiently support applications that manipulate persistent memory in general-purpose systems. We also hope that the problems discovered in this paper can inspire other new solutions to shared resource management in systems employing persistent memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example persistent writes with (a) redo logging and (b) shadow updates, when nodes N3 and N4 in a tree data structure are updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance and fairness of prior and naïve scheduling methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example comparing conventional, naïve, and proposed schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overview of the FIRM design and its two key techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. System performance with various BA-NVM latencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Effect of BA-NVM row-buffer size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Effect of different number of threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Memory access characteristics of three applications running individually. The last row shows the memory access characteristics ofKVStore when it performs persistent writes.</figDesc><table><row><cell></cell><cell>MPKI</cell><cell>WR%</cell><cell>BLP</cell><cell>RBL</cell></row><row><cell>Streaming</cell><cell>100/High</cell><cell>47%/Low</cell><cell cols="2">0.05/Low 96%/High</cell></row><row><cell>Random</cell><cell>100/High</cell><cell>46%/Low</cell><cell>6.3/High</cell><cell>0.4%/Low</cell></row><row><cell>KVStore</cell><cell cols="4">100/High 77%/High 0.05/Low 71%/High</cell></row><row><cell>Persistence</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Phase (KVStore) 675/High 92%/High 0.01/Low 97%/High</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>r j t w j Schedule 1 read batch group and 1 persistent write batch group in each time interval.</head><label></label><figDesc></figDesc><table><row><cell cols="2">Memory Requests</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Data Buffer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">(Log or Shadow Copies)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Request Batching Source Categorization intensive Random Streaming Persistent Non-</cell><cell>A Batch</cell><cell>W</cell><cell>W</cell><cell>W</cell><cell>W ... W</cell><cell>W</cell><cell>...</cell><cell>Persistent writes issued to a contiguous memory space</cell><cell>Time</cell><cell>Read Queue R1</cell><cell>R2 R2 R2 R2 R1 R2</cell><cell cols="2">R3 R1 R1 R3 R3 R3 max = t r t r 3 R3</cell><cell>R1</cell><cell>t r 2 t r 3 t r 1</cell><cell>Ready read batches: R1, R2, R3 Possible batch groups: 1. [ R1 ] 2. [ R1, R2 ] 3. [ R1, R2, R3 ]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bank 1</cell><cell cols="2">Bank 2</cell><cell>Bank 3</cell><cell>Bank 4</cell><cell>Which batch group to schedule?</cell></row><row><cell cols="2">Persistent Write Striding Persistence-aware Memory Scheduling</cell><cell>A Batch</cell><cell cols="4">W Offset W</cell><cell>W</cell><cell></cell><cell>Strided persistent writes scheduled to memory</cell><cell>Time</cell><cell>Bank 1 Write Queue W2 W2 W2 W1 W1 W1</cell><cell cols="3">Bank 2 W2 W2 W1 W1 t w max = t w Bank 3 W2 W1 W1 2</cell><cell>W2 W1 Bank 4</cell><cell>t w 2 t w 1</cell><cell>Ready persistent write batches: W1, W2 Possible batch groups: 1. [ W1 ] 2. [ W1, W2 ] Which batch group to schedule?</cell></row><row><cell>(a)</cell><cell>FIRM components.</cell><cell cols="8">(b) Persistent write striding.</cell><cell></cell><cell></cell><cell>(c)</cell><cell cols="4">Persistence-aware memory scheduling policy.</cell></row></table><note><p>Buffer Group (Row-buffer Size) ... W ... R Streaming Reads Random Reads W Persistent Writes (Strided) t</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Parameters tracked for each executing thread.</figDesc><table><row><cell>Parameters</cell><cell>Usage</cell></row><row><cell>MPKI, BLP, RBL</cell><cell>To categorize non-intensive,</cell></row><row><cell></cell><cell>streaming, and random sources</cell></row><row><cell cols="2">Size of each write batch To categorize persistent sources;</cell></row><row><cell></cell><cell>to calculate t w (Section 4.4)</cell></row><row><cell>Size of each read batch</cell><cell>To calculate t r (Section 4.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Storage required by FIRM in each memory controller. N hwthreads is the number of hardware contexts.</figDesc><table><row><cell>Register/Counter Name</cell><cell>Storage (bits)</cell></row><row><cell>Persistent thread registers</cell><cell>log 2 N hwthreads × N hwthreads</cell></row><row><cell>Start address registers</cell><cell>64 × N hwthreads</cell></row><row><cell>End address registers</cell><cell>64 × N hwthreads</cell></row><row><cell>Intra-group index counters</cell><cell>6 × N hwthreads</cell></row><row><cell>Request counters</cell><cell>2 × 6 × N hwthreads</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Parameters of the evaluated multi-core system.</figDesc><table><row><cell>Processor</cell><cell>Similar to Intel Core i7 / 22 nm</cell></row><row><cell>Cores</cell><cell>4 cores, 2.5GHz, 2 threads per core</cell></row><row><cell>L1 Cache (Private)</cell><cell>64KB, 4-way, 64B lines, 1.6ns latency</cell></row><row><cell>L2 Cache (Private)</cell><cell>256KB, 8-way, 64B lines, 4.4ns latency</cell></row><row><cell>L3 Cache (Shared)</cell><cell>Multi-banked, 2MB/core, 16-way,</cell></row><row><cell></cell><cell>64B lines, 10ns latency</cell></row><row><cell cols="2">Memory Controller 64-/64-entry read/write queues</cell></row><row><cell></cell><cell>STT-MRAM, 8GB, 8 banks,</cell></row><row><cell>BA-NVM DIMM</cell><cell>2KB row, 36ns row-buffer hit,</cell></row><row><cell></cell><cell>65/76ns read/write row-buffer conflict</cell></row><row><cell cols="2">dedup). Currently, persistent applications are not available public</cell></row><row><cell cols="2">benchmark suites. Similar to recent studies on persistent memory [16,</cell></row><row><cell cols="2">38, 52, 70, 92], we constructed three benchmarks with persistent</cell></row><row><cell cols="2">applications on common data structures used in file systems and</cell></row><row><cell cols="2">databases. Each persistent application searches for a node with a</cell></row><row><cell cols="2">randomly generated key in three different data structures (a B+ tree,</cell></row><row><cell cols="2">a hash table, and an array), deletes the node if it exists, and inserts</cell></row><row><cell cols="2">it otherwise. Redo logging is used to maintain versioning, although</cell></row><row><cell cols="2">FIRM is applicable to shadow copies, too, since its techniques are</cell></row><row><cell cols="2">orthogonal to versioning and write order control mechanisms.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Benchmarks. The second numbers in the last three rows belong to the persistence phase.</figDesc><table><row><cell></cell><cell>MPKI</cell><cell>WR%</cell><cell>BLP</cell><cell>RBL</cell></row><row><cell>mcf</cell><cell>32.0</cell><cell>25.6%</cell><cell>6.0</cell><cell>41.1%</cell></row><row><cell>lbm</cell><cell>28.2</cell><cell>42.0%</cell><cell>2.8</cell><cell>78.7%</cell></row><row><cell>leslie3d</cell><cell>15.7</cell><cell>4.0%</cell><cell>1.7</cell><cell>90.8%</cell></row><row><cell>povray</cell><cell>0.1</cell><cell>6.0%</cell><cell>1.2</cell><cell>77.6%</cell></row><row><cell>x264</cell><cell>22.5</cell><cell>26.2%</cell><cell>0.7</cell><cell>87%</cell></row><row><cell>ferret</cell><cell>12.6</cell><cell>13.9%</cell><cell>4.6</cell><cell>58.2%</cell></row><row><cell>dedup</cell><cell>20.2</cell><cell>20.8%</cell><cell>1.6</cell><cell>90.1%</cell></row><row><cell>Btree</cell><cell cols="4">26.2, 71.9 22.0%, 92.2% 0.4, 0.2 57.6%, 97.6%</cell></row><row><cell>Hash</cell><cell cols="4">37.1, 62.7 42.6%, 95.3% 0.2, 0.1 95.6%, 98.0%</cell></row><row><cell>SPS</cell><cell cols="4">11.0, 60.5 35.2%, 89.0% 0.2, 0.1 65.1%, 93.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Workloads mixed with various persistent and non-persistent applications.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Non-persistent Workloads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Mix of Persistent and Non-persistent Applications</cell></row><row><cell></cell><cell cols="3">Index</cell><cell></cell><cell cols="5">Benchmarks</cell><cell cols="5">Index Benchmark</cell><cell></cell><cell></cell><cell></cell><cell>W1</cell><cell>W2</cell><cell>W3</cell><cell>W4</cell><cell></cell><cell>W5</cell><cell>W6</cell></row><row><cell></cell><cell cols="2">W1</cell><cell></cell><cell cols="6">mcf, lbm, povray</cell><cell cols="2">W4</cell><cell></cell><cell>x264</cell><cell cols="10">Btree Btree-W1 Btree-W2 Btree-W3 Btree-W4 Btree-W5 Btree-W6</cell></row><row><cell></cell><cell cols="2">W2</cell><cell></cell><cell cols="8">mcf, lbm, leslie3d W5</cell><cell></cell><cell>ferret</cell><cell cols="3">Hash</cell><cell cols="2">Hash-W1</cell><cell>Hash-W2</cell><cell>Hash-W3</cell><cell>Hash-W4</cell><cell cols="2">Hash-W5</cell><cell>Hash-W6</cell></row><row><cell></cell><cell cols="2">W3</cell><cell></cell><cell cols="6">mcf(2), leslie3d</cell><cell cols="2">W6</cell><cell></cell><cell>dedup</cell><cell cols="2">SPS</cell><cell></cell><cell cols="2">SPS-W1</cell><cell>SPS-W2</cell><cell>SPS-W3</cell><cell>SPS-W4</cell><cell cols="2">SPS-W5</cell><cell>SPS-W6</cell></row><row><cell>Weighted Speedup</cell><cell>1.0 1.4 1.8 2.2 2.6 3.0 3.4 (a)</cell><cell></cell><cell>Btree-W1</cell><cell>Btree-W2</cell><cell>Btree-W3</cell><cell>Btree-W4</cell><cell cols="2">Btree-W5</cell><cell cols="2">Btree-W6 FR-FCFS Hash-W1 Hash-W2</cell><cell cols="2">Hash-W3 PAR-BS Hash-W4 Hash-W5</cell><cell>Hash-W6 TCM SPS-W1</cell><cell cols="3">SPS-W2 TCM-modified SPS-W3 SPS-W4 SPS-W5</cell><cell>SPS-W6</cell><cell>AVG NVMDuet (b) 1.0 1.2 1.4 1.6 1.8 2.0 Maximum Slowdown</cell><cell>FIRM-Strided Btree-W1 Btree-W2 Btree-W3 Btree-W4</cell><cell cols="2">FIRM-Strided-Scheduling Btree-W5 Btree-W6 Hash-W1 Hash-W2 Hash-W3 Hash-W4 Hash-W5</cell><cell>Hash-W6</cell><cell>SPS-W1</cell><cell>SPS-W2</cell><cell>SPS-W3</cell><cell>SPS-W4</cell><cell>SPS-W5</cell><cell>SPS-W6</cell><cell>AVG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">Fig. 7. System performance and fairness of FIRM versus various memory scheduling schemes.</cell><cell></cell></row><row><cell></cell><cell>rnaround Bus Tur</cell><cell>ead (%) Overh</cell><cell>6 14 12 10 8 0 2 4 6</cell><cell>e-W1 Btre</cell><cell>e-W2 Btre</cell><cell cols="2">TCM e-W3 e-W4 Btre Btre</cell><cell cols="4">FIRM-S Strided e-W5 e-W6 h-W1 Btre Btre Has h W1 h-W2 h-W3 Has Has Has</cell><cell cols="3">FIRM-Strided-Scheduling h-W4 h-W5 h-W6 S-W1 S-W2 S-W3 S-W4 S-W5 Has Has Has SPS SPS SPS SPS SPS</cell><cell>S-W6 SPS</cell><cell>AVG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Fig. 8. Bus turnaround overhead.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>STT-MRAM, PCM, and ReRAM are collectively called nonvolatile random-access memories (NVRAMs) or storage-class memories (SCMs) in recent studies<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b89">90]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A system with BA-NVMs may also employ volatile DRAM, controlled by a separate memory controller<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b90">91]</ref>. As we show in this paper, significant resource contention exists at the BA-NVM memory interface of persistent memory systems between persistent and non-persistent applications. We do not focus on the DRAM interface.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The recently developed BLISS scheduler<ref type="bibr" target="#b83">[84]</ref> was shown to be more effective than TCM while providing low cost. Even though we do not evaluate BLISS, it also does not take into account the nature of interference caused by persistent applications.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>More recently, Lu et al.<ref type="bibr" target="#b53">[54]</ref> proposed the notion of loose-ordering consistency, which relaxes the ordering of persistent memory writes even more by performing them speculatively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>explains our system setup and methodology.<ref type="bibr" target="#b5">6</ref> Note that we preserve all the other ordering rules of FR-FCFS and TCM in FRCFCS-modified and TCM-modified. Within each prioritization level, reads and persistent writes are prioritized over non-persistent writes. For example, with FRCFCS-modified, the highest priority requests are row-buffer-hit read and persistent write requests, second highest priority requests are row-bufferhit non-persistent write requests.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We use an interval size of one million cycles, which we empirically find to provide a good tradeoff between prediction accuracy, adaptivity to workload behavior, and overhead of categorization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>This algorithm can be invoked only once at the beginning of each interval to determine the (relative) durations of consecutive read and write drain modes for the interval.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>For example, x86 processors use the memory fence instruction mfence, which prevents progress of the thread until the store buffer is drained<ref type="bibr" target="#b55">[56]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>Everspin recently launched DDR3-compatible STT-MRAM components<ref type="bibr" target="#b35">[36]</ref>, which transfer data at a speed comparable to current DDR3-1600.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10"><p>The low performance and fairness benefits are consistent, so we do not show results of NVMDuet in the rest of this section.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11"><p>These average results are generated using multithreaded workloads, i.e., various persistent applications combined with W4, W5, and W6. Each persistent application and each of W4, W5, W6 executes the same number of threads as the number of cores when thread count changes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12"><p>Contention in shared caches may not exist in all persistent memory systems, however. Some persistent memory designs update data logs with uncacheable writes<ref type="bibr" target="#b89">[90]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their valuable feedback. Zhao  and Xie were supported in part by NSF grants 1218867, 1213052, 1409798, and Department of Energy under Award Number DESC0005026. Mutlu was supported in part by NSF grants 0953246, 1065112, 1212962, 1320531, SRC, and the Intel Science and Technology Center for Cloud Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Intel, persistent memory file system</title>
		<ptr target="https://github.com/linux-pmfs/pmfs" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel, a collection of linux persistent memory programming examples</title>
		<ptr target="https://github.com/pmem/linux-examples" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Intel Core i7</title>
		<ptr target="http://www.intel.com/content/www/us/en/processors/core/core-i7-processor.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">McSimA+: a manycore simulator with application-level+ simulation and detailed microarchitecture modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highly manufacturable high density phase change memory of 64Mb and beyond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEDM</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<title level="m">Operating Systems: Three Easy Pieces</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Staged memory scheduling: Achieving high performance and scalability in heterogeneous systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ausavarungnirun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How persistent memory will change software systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Operating system implications of fast, cheap, nonvolatile memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SNIA NVM programming model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Behren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SNIA Education</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Benchmarking modern multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">STX B+ Tree</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bingmann</surname></persName>
		</author>
		<ptr target="http://panthema.net/2007/stx-btree" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The design and use of persistent memory on the DNCP hardware fault-tolerant platform</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bressoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DSN</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characterization and modelling of electrode impact in HfO2-based RRAM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cagli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Memory Technologies</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A 20nm 1.8V 8Gb PRAM with 40MB/s program bandwidth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSCC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">NV-heaps: making persistent objects fast and safe with next-generation, non-volatile memories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Coburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better I/O through byte-addressable, persistent memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Condit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The case for safe RAM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Copeland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application-to-Core Mapping Policies to Reduce Memory System Interference in Multi-Core Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Application-aware prioritization mechanisms for on-chip networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic hourglass model for SET and RESET in HfO2 RRAM</title>
		<author>
			<persName><forename type="first">R</forename><surname>Degraeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLSI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PDRAM: A hybrid PRAM and DRAM main memory system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dhiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Simple but effective heterogeneous main memory with on-chip memory controller support</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<editor>SC</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NVSim: A circuit-level performance, energy, and area model for emerging nonvolatile memory</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCAD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Latest advances in STT-RAM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Driskill-Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>NVMW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prefetch-aware shared resource management for multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Parallel application memory scheduling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Software exploitation of a fault-tolerant computer with a large memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eskesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FTCS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An optimization guide for assembly programmers and compiler makers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Copenhagen University College of Engineering</orgName>
		</respStmt>
	</monogr>
	<note>The microarchitecture of Intel, AMD and VIA CPUs</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel nonvolatile memory with spin torque transfer magnetization switching: spin-RAM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hosomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEDM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intel architecture instruction set extensions programming reference, 319433-012 edition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dynamically replicated memory: Building reliable systems from nanoscale resistive memories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-optimizing memory controllers: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Transactional memory architecture and implementation for IBM System Z</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Device performance in a fully functional 800MHz DDR3 Spin Torque Magnetic Random Access Memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Janesky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMW</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memorage: Emerging persistent RAM based malleable main memory and storage architecture</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reducing the cost of persistence for nonvolatile heaps in end user devices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extended scalability of perpendicular STT-MRAM towards sub-20nm MTJ node</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEDM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bi-layered RRAM with unlimited endurance and extremely uniform switching</title>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLSI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ATLAS: A scalable and high-performance scheduling algorithm for multiple memory controllers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Thread cluster memory scheduling: Exploiting differences in memory access behavior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A case for exploiting subarray-level parallelism (SALP) in DRAM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluating STT-RAM as an energy-efficient main memory alternative</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kultursay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Architecting phase change memory as a scalable DRAM alternative</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Phase change memory architecture and the quest for scalability</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Phase-change technology and the future of main memory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Prefetch-Aware DRAM Controllers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DRAM-aware last-level cache writeback: Reducing write-caused interference in memory systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
			<affiliation>
				<orgName type="collaboration">HPS Tech</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improving memory bank-level parallelism in the presence of prefetching</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evidence and solution of over-RESET problem for HfOx based resistive memory with sub-ns switching speed and high endurance</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">NVM Duet: Unified working memory and persistent store architecture</title>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LightTx: A lightweight transactional design in flash-based SSDs to support flexible transactions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Loose-ordering consistency for persistent memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pin: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Memory barriers: a hardware view for software hackers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mckenney</surname></persName>
		</author>
		<ptr target="http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Enabling efficient and scalable hybrid memories using fine-granularity DRAM cache management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A case for small row buffers in non-volatile main memories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A case for efficient hardware/software cooperative management of storage and memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>WEED</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Consistent, durable, and safe memory management for byte-addressable non volatile main memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Moraru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRIOS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Memory performance attacks: Denial of memory service in multi-core systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Distributed Order Scheduling and its Application to Multi-Core DRAM Controllers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Reducing memory interference in multicore systems via application-aware memory channel partitioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Muralidhara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Stall-time fair memory access scheduling for chip multiprocessors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Parallelism-aware batch scheduling: Enhancing both performance and fairness of shared DRAM systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Whole-system persistence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Fair queuing memory systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A 4Mb conductive-bridge resistive memory with 2.3GB/s read-throughput and 216MB/s program-throughput</title>
		<author>
			<persName><forename type="first">W</forename><surname>Otsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSCC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Regularities considered harmful: Forcing randomness to memory accesses to reduce row buffer conflicts for multi-core, multibank systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Memory persistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Storage management in the NVRAM era</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Enhancing lifetime and security of PCM-based main memory with start-gap wear leveling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scalable high performance main memory system using phase-change memory technology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Page placement in hybrid memory systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Phase-change random access memory: A scalable technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM JRD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Memory controller optimizations for web servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<editor>MI-CRO</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Memory access scheduling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The dirty-block index</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A phase change memory as a secure main memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling for a simultaneous multithreaded processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Phase change materials engineering for RESET current reduction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sousa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Memory Technologies</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">SPEC CPU2006</title>
		<author>
			<persName><surname>Spec Cpu</surname></persName>
		</author>
		<ptr target="http://www.spec.org/cpu2006/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The virtual write queue: Coordinating DRAM and last-level cache policies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stuecheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The blacklisting memory scheduler: Achieving high performance and fairness at low cost</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">MISE: Providing performance predictability and improving fairness in shared main memory systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A 32-Mb SPRAM with 2T1R memory cell, localized bi-directional write driver and &apos;1&apos;/&apos;0&apos; dual-array equalized feference scheme</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Fairness metrics for multi-threaded processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vandierendonck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Consistent and durable data structures for nonvolatile byte-addressable memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Understanding non-volatile memory technology whitepaper</title>
		<ptr target="http://www.vikingtechnology.com/uploads/nvwhitepaper.pdf" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Viking Technology</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Mnemosyne: lightweight persistent memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Volos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP-LOS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Row buffer locality aware caching policies for hybrid memories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Kiln: Closing the performance gap between systems with and without persistence support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Macro-model of spin-transfer torque based magnetic tunnel junction device for hybrid magnetic-CMOS design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMAS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A durable and energy efficient main memory using phase change memory technology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Controller for a synchronous DRAM that maximizes throughput by allowing memory requests and commands to be issued out of order</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Zuravleff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent Number</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
