<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
							<email>jiongzhu@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
							<email>yujunyan@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
							<email>lingxia1@andrew.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
							<email>mheimann@umich.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
							<email>lakoglu@andrew.cmu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
							<email>dkoutra@umich.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the representation power of graph neural networks in the semisupervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs-ego-and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations-that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H 2 GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40% and 27% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We focus on the effectiveness of graph neural networks (GNNs) <ref type="bibr" target="#b41">[42]</ref> in tackling the semi-supervised node classification task in challenging settings: the goal of the task is to infer the unknown labels of the nodes by using the network structure <ref type="bibr" target="#b43">[44]</ref>, given partially labeled networks with node features (or attributes). Unlike most prior work that considers networks with strong homophily, we study the representation power of GNNs in settings with different levels of homophily or class label smoothness.</p><p>Homophily is a key principle of many real-world networks, whereby linked nodes often belong to the same class or have similar features ("birds of a feather flock together") <ref type="bibr" target="#b20">[21]</ref>. For example, friends are likely to have similar political beliefs or age, and papers tend to cite papers from the same research area <ref type="bibr" target="#b22">[23]</ref>. GNNs model the homophily principle by propagating features and aggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref>. However, in the real world, there are also settings where "opposites attract", leading to networks with heterophily: linked nodes are likely from different classes or have dissimilar features. For instance, the majority of people tend to connect with people of the opposite gender in dating networks, different amino acid types are more likely to connect in protein structures, fraudsters are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type="bibr" target="#b23">[24]</ref>. Since many existing GNNs assume strong homophily, they fail to generalize to networks with heterophily (or low/medium level of homophily). In such cases, we find that even models that ignore 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2006.11468v2 [cs.LG] 23 Oct 2020</head><p>the graph structure altogether, such as multilayer perceptrons or MLPs, can outperform a number of existing GNNs. Motivated by this limitation, we make the following contributions:</p><p>• Current Limitations: We reveal the limitation of GNNs to learn over networks with heterophily, which is ignored in the literature due to evaluation on few benchmarks with similar properties. § 3 • Key Designs for Heterophily &amp; New Model: We identify a set of key designs that can boost learning from the graph structure in heterophily without trading off accuracy in homophily: (D1) egoand neighbor-embedding separation, (D2) higher-order neighborhoods, and (D3) combination of intermediate representations. We justify the designs theoretically, and combine them into a model, H 2 GCN, that effectively adapts to both heterophily and homophily. We compare it to prior GNN models, and make our code and data available at https://github.com/GemsLab/H2GCN. § 3-4 • Extensive Empirical Evaluation: We empirically analyze our model and competitive existing GNN models on both synthetic and real networks covering the full spectrum of low-to-high homophily (besides the typically-used benchmarks with strong homophily only). In synthetic networks, our detailed ablation study of H 2 GCN (which is free of confounding designs) shows that the identified designs result in up to 40% performance gain in heterophily. In real networks, we observe that GNN models utilizing even a subset of our identified designs outperform popular models without them by up to 27% in heterophily, while being competitive in homophily. § 5</p><p>2 Notation and Preliminaries We summarize our notation in Table <ref type="table" target="#tab_6">A</ref>.1 (App. A). Let G = (V, E) be an undirected, unweighted graph with nodeset V and edgeset E. We denote a general neighborhood centered around v as N (v) (G may have self-loops), the corresponding neighborhood that does not include the ego (node v) as N (v), and the general neighbors of node v at exactly i hops/steps away (minimum distance) as N i (v). For example, N 1 (v) = {u : (u, v) ∈ E} are the immediate neighbors of v. Other examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We represent the graph by its adjacency matrix A ∈ {0, 1} n×n and its node feature matrix X ∈ R n×F , where the vector x v corresponds to the ego-feature of node v, and {x u : u ∈ N (v)} to its neighbor-features.</p><p>We further assume a class label vector y, which for each node v contains a unique class label y v . The goal of semi-supervised node classification is to learn a mapping : V → Y, where Y is the set of labels, given a set of labeled nodes T V = {(v 1 , y 1 ), (v 2 , y 2 ), ...} as training data.</p><p>Graph neural networks From a probabilistic perspective, most GNN models assume the following local Markov property on node features: for each node v ∈ V, there exists a neighborhood N (v) such that y v only depends on the ego-feature x v and neighbor-features {x u : u ∈ N (v)}. Most models derive the class label y v via the following representation learning approach:</p><formula xml:id="formula_0">r (k) v = f r (k−1) v , {r (k−1) u : u ∈ N (v)} , r (0) v = xv, and yv = arg max{softmax(r (K) v )W},<label>(1)</label></formula><p>where the embedding function f is applied repeatedly in K total rounds, node v's representation (or hidden state vector) at round k, r (k) v , is learned from its ego-and neighbor-representations in the previous round, and a softmax classifier with learnable weight matrix W is applied to the final representation of v. Most existing models differ in their definitions of neighborhoods N (v) and embedding function f . A typical definition of neighborhood is N 1 (v)-i.e., the 1-hop neighbors of v. As for f , in graph convolutional networks (GCN) <ref type="bibr" target="#b16">[17]</ref> each node repeatedly averages its own features and those of its neighbors to update its own feature representation. Using an attention mechanism, GAT <ref type="bibr" target="#b35">[36]</ref> models the influence of different neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type="bibr" target="#b10">[11]</ref> generalizes the aggregation beyond averaging, and models the ego-features distinctly from the neighbor-features in its subsampled neighborhood.</p><p>Homophily and heterophily In this work, we focus on heterophily in class labels. We first define the edge homophily ratio h as a measure of the graph homophily level, and use it to define graphs with strong homophily/heterophily: Definition 1 The edge homophily ratio h = |{(u,v):(u,v)∈E∧yu=yv}| |E| is the fraction of edges in a graph which connect nodes that have the same class label (i.e., intra-class edges).</p><p>Definition 2 Graphs with strong homophily have high edge homophily ratio h → 1, while graphs with strong heterophily (i.e., low/weak homophily) have small edge homophily ratio h → 0.</p><p>The edge homophily ratio in Dfn. 1 gives an overall trend for all the edges in the graph. The actual level of homophily may vary within different pairs of node classes, i.e., there is different tendency of connection between each pair of classes. In App. B, we give more details about capturing these more complex network characteristics via an empirical class compatibility matrix H, whose i, j-th entry is the fraction of outgoing edges to nodes in class j among all outgoing edges from nodes in class i. Heterophily = Heterogeneity. We remark that heterophily, which we study in this work, is a distinct network concept from heterogeneity. Formally, a network is heterogeneous <ref type="bibr" target="#b33">[34]</ref> if it has at least two types of nodes and different relationships between them (e.g., knowledge graphs), and homogeneous if it has a single type of nodes (e.g., users) and a single type of edges (e.g., friendship). The type of nodes in heterogeneous graphs does not necessarily match the class labels y v , therefore both homogeneous and heterogeneous networks may have different levels of homophily. While many GNN models have been proposed, most of them are designed under the assumption of homophily, and are not capable of handling heterophily. As a motivating example, Table <ref type="table" target="#tab_0">1</ref> shows the mean classification accuracy for several leading GNN models on our synthetic benchmark syn-cora, where we can control the homophily/heterophily level (see App. G for details on the data and setup). Here we consider two homophily ratios, h = 0.1 and h = 0.7, one for high heterophily and one for high homophily. We observe that for heterophily (h = 0.1) all existing methods fail to perform better than a Multilayer Perceptron (MLP) with 1 hidden layer, a graph-agnostic baseline that relies solely on the node features for classification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type="bibr" target="#b16">[17]</ref> and GAT <ref type="bibr" target="#b35">[36]</ref> show up to 42% worse performance than MLP, highlighting that methods that work well under high homophily (h = 0.7) may not be appropriate for networks with low/medium homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Over Networks with Heterophily</head><p>Motivated by this limitation, in the following subsections, we discuss and theoretically justify a set of key design choices that, when appropriately incorporated in a GNN framework, can improve the performance in the challenging heterophily settings. Then, we present H 2 GCN, a model that, thanks to these designs, adapts well to both homophily and heterophily (Table <ref type="table" target="#tab_0">1</ref>, last row). In Section 5, we provide a comprehensive empirical analysis on both synthetic and real data with varying homophily levels, and show that the identified designs significantly improve the performance of GNNs (not limited to H 2 GCN) by effectively leveraging the graph structure in challenging heterophily settings, while maintaining competitive performance in homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effective Designs for Networks with Heterophily</head><p>We have identified three key designs that-when appropriately integrated-can help improve the performance of GNN models in heterophily settings: (D1) ego-and neighbor-embedding separation; (D2) higher-order neighborhoods; and (D3) combination of intermediate representations. While these designs have been utilized separately in some prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref>, we are the first to discuss their importance under heterophily by providing novel theoretical justifications and an extensive empirical analysis on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">(D1) Ego-and Neighbor-embedding Separation</head><p>The first design entails encoding each ego-embedding (i.e., a node's embedding) separately from the aggregated embeddings of its neighbors, since they are likely to be dissimilar in heterophily settings. Formally, the representation (or hidden state vector) learned for each node v at round k is given as:</p><formula xml:id="formula_1">r (k) v = COMBINE r (k−1) v , AGGR({r (k−1) u : u ∈ N (v) }) ,<label>(2)</label></formula><p>the neighborhood N (v) does not include v (no self-loops), the AGGR function aggregates representations only from the neighbors (in some way-e.g., average), and AGGR and COMBINE may be followed by a non-linear transformation. For heterophily, after aggregating the neighbors' representations, the definition of COMBINE (akin to 'skip connection' between layers) is critical: a simple way to combine the ego-and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type="bibr" target="#b10">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type="bibr" target="#b16">[17]</ref>.</p><p>Intuition. In heterophily settings, by definition (Dfn. 2), the class label y v and original features x v of a node and those of its neighboring nodes {(y u , x u ) : u ∈ N (v)} (esp. the direct neighbors N1 (v)) may be different. However, the typical GCN design that mixes the embeddings through an average <ref type="bibr" target="#b16">[17]</ref> or weighted average <ref type="bibr" target="#b35">[36]</ref> as the COMBINE function results in final embeddings that are similar across neighboring nodes (especially within a community or cluster) for any set of original features <ref type="bibr" target="#b27">[28]</ref>. While this may work well in the case of homophily, where neighbors likely belong to the same cluster and class, it poses severe challenges in the case of heterophily: it is not possible to distinguish neighbors from different classes based on the (similar) learned representations. Choosing a COMBINE function that separates the representations of each node v and its neighbors N (v) allows for more expressiveness, where the skipped or non-aggregated representations can evolve separately over multiple rounds of propagation without becoming prohibitively similar.</p><p>Theoretical Justification. We prove theoretically that, under some conditions, a GCN layer that co-embeds ego-and neighbor-features is less capable of generalizing to heterophily than a layer that embeds them separately. We measure its generalization ability by its robustness to test/train data deviations. We give the proof of the theorem in App. C.1. Though the theorem applies to specific conditions, our empirical analysis shows that it holds in more general cases ( § 5).</p><p>Theorem 1 Consider a graph G without self-loops ( § 2) with node features x v = onehot(y v ) for each node v, and an equal number of nodes per class y ∈ Y in the training set T V . Also assume that all nodes in T V have degree d, and proportion h of their neighbors belong to the same class, while proportion<ref type="foot" target="#foot_0">1</ref>−h |Y|−1 of them belong to any other class (uniformly). Then for h &lt; 1−|Y|+2d 2|Y|d , a simple GCN layer formulated as (A + I)XW is less robust, i.e., misclassifies a node for smaller train/test data deviations, than a AXW layer that separates the ego-and neighbor-embeddings.</p><p>Observations. In Table <ref type="table" target="#tab_0">1</ref>, we observe that GCN, GAT, and MixHop, which 'mix' the ego-and neighbor-embeddings explicitly 1 , perform poorly in the heterophily setting. On the other hand, GraphSAGE that separates the embeddings (e.g., it concatenates the two embeddings and then applies a non-linear transformation) achieves 33-40% better performance in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">(D2) Higher-order Neighborhoods</head><p>The second design involves explicitly aggregating information from higher-order neighborhoods in each round k, beyond the immediate neighbors of each node:</p><formula xml:id="formula_2">r (k) v = COMBINE r (k−1) v , AGGR({r (k−1) u : u ∈ N1(v) }), AGGR({r (k−1) u : u ∈ N2(v) }), . . .<label>(3)</label></formula><p>where N i (v) denotes the neighbors of v at exactly i hops away, and the AGGR functions applied to different neighborhoods can be the same or different. This design-employed in GCN-Cheby <ref type="bibr" target="#b6">[7]</ref> and MixHop <ref type="bibr" target="#b0">[1]</ref>-augments the implicit aggregation over higher-order neighborhoods that most GNN models achieve through multiple rounds of first-order propagation based on variants of Eq. ( <ref type="formula" target="#formula_1">2</ref>).</p><p>Intuition. To show why higher-order neighborhoods help in the heterophily settings, we first define homophily-dominant and heterophily-dominant neighborhoods:</p><formula xml:id="formula_3">Definition 3 N (v) is expectedly homophily-dominant if P (y u = y v |y v ) ≥ P (y u = y|y v ), ∀u ∈ N (v) and y ∈ Y = y v . If the opposite inequality holds, N (v) is expectedly heterophily-dominant.</formula><p>From this definition, we can see that expectedly homophily-dominant neighborhoods are more beneficial for GNN layers, as in such neighborhoods the class label y v of each node v can in expectation be determined by the majority of the class labels in N (v). In the case of heterophily, we have seen empirically that although the immediate neighborhoods may be heterophily-dominant, the higher-order neighborhoods may be homophily-dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> in the context of binary attribute prediction.</p><p>Theoretical Justification. Below we formalize the above observation for 2-hop neighborhoods under non-binary attributes (labels), and prove one case when they are homophily-dominant in App. C.2:</p><p>Theorem 2 Consider a graph G without self-loops ( § 2) with label set Y, where for each node v, its neighbors' class labels {y u : u ∈ N (v)} are conditionally independent given y v , and</p><formula xml:id="formula_4">P (y u = y v |y v ) = h, P (y u = y|y v ) = 1−h |Y|−1 , ∀y = y v .</formula><p>Then, the 2-hop neighborhood N 2 (v) for a node v will always be homophily-dominant in expectation.</p><p>Observations. Under heterophily (h = 0.1), GCN-Cheby, which models different neighborhoods by combining Chebyshev polynomials to approximate a higher-order graph convolution operation <ref type="bibr" target="#b6">[7]</ref>, outperforms GCN and GAT, which aggregate over only the immediate neighbors N 1 , by up to +31% (Table <ref type="table" target="#tab_0">1</ref>). MixHop, which explicitly models 1-hop and 2-hop neighborhoods (though 'mixes' the ego-and neighbor-embeddings 1 , violating design D1), also outperforms these two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">(D3) Combination of Intermediate Representations</head><p>The third design combines the intermediate representations of each node at the final layer:</p><formula xml:id="formula_5">r (final) v = COMBINE r (1) v , r (2) v , . . . , r (K) v<label>(4)</label></formula><p>to explicitly capture local and global information via COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type="bibr" target="#b37">[38]</ref>. This design is introduced in jumping knowledge networks <ref type="bibr" target="#b37">[38]</ref> and shown to increase the representation power of GCNs under homophily.</p><p>Intuition. Intuitively, each round collects information with different locality-earlier rounds are more local, while later rounds capture increasingly more global information (implicitly, via propagation). Similar to D2 (which models explicit neighborhoods), this design models the distribution of neighbor representations in low-homophily networks more accurately. It also allows the class prediction to leverage different neighborhood ranges in different networks, adapting to their structural properties.</p><p>Theoretical Justification. The benefit of combining intermediate representations can be theoretically explained from the spectral perspective. Assuming a GCN-style layer-where propagation can be viewed as spectral filtering-, the higher order polynomials of the normalized adjacency matrix A is a low-pass filter <ref type="bibr" target="#b36">[37]</ref>, so intermediate outputs from earlier rounds contain higher-frequency components than outputs from later rounds. At the same time, the following theorem holds for graphs with heterophily, where we view class labels as graph signals (as in graph signal processing):</p><p>Theorem 3 Consider graph signals (label vectors) s, t ∈ {0, 1} |V| defined on an undirected graph G with edge homophily ratios h s and h t , respectively. If h s &lt; h t , then signal s has higher energy (Dfn. 5) in high-frequency components than t in the spectrum of unnormalized graph Laplacian L.</p><p>In other words, in heterophily settings, the label distribution contains more information at higher than lower frequencies (see proof in App. C.3). Thus, by combining the intermediate outputs from different layers, this design captures both low-and high-frequency components in the final representation, which is critical in heterophily settings, and allows for more expressiveness in the general setting.</p><p>Observations. By concatenating the intermediate representations from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type="bibr" target="#b37">[38]</ref>), GCN's accuracy increases to 58.93%±3.17 for h = 0.1, a 20% improvement over its counterpart without design D3 (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of designs</head><p>To sum up, D1 models (at each layer) the ego-and neighbor-representations distinctly, D2 leverages (at each layer) representations of neighbors at different distances distinctly, and D3 leverages (at the final layer) the learned ego-representations at previous layers distinctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">H 2 GCN: A Framework for Networks with Homophily or Heterophily</head><p>We now describe H 2 GCN, which exemplifies how effectively combining designs D1-D3 can help better adapt to the whole spectrum of low-to-high homophily, while avoiding interference with other designs. It has three stages (Alg. 1, App. D): (S1) feature embedding, (S2) neighborhood aggregation, and (S3) classification.</p><p>The feature embedding stage (S1) uses a graph-agnostic dense layer to generate for each node v the feature embedding r (0) v ∈ R p based on its ego-feature xv: r (0) v = σ(xvWe), where σ is an optional non-linear function, and We ∈ R F ×p is a learnable weight matrix.</p><p>In the neighborhood aggregation stage (S2), the generated embeddings are aggregated and repeatedly updated within the node's neighborhood for K rounds. Following designs D1 and D2, the neighborhood N (v) of our framework involves two sub-neighborhoods without the egos: the 1-hop graph neighbors N1 (v) and the 2-hop neighbors N2 (v), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>:</p><formula xml:id="formula_6">r (k) v = COMBINE AGGR{r (k−1) u : u ∈ N1(v)}, AGGR{r (k−1) u : u ∈ N2(v)} .<label>(5)</label></formula><p>We set COMBINE as concatenation (as to not mix different neighborhood ranges), and AGGR as a degree-normalized average of the neighbor-embeddings in sub-neighborhood Ni (v):</p><formula xml:id="formula_7">r (k) v = r (k) v,1 r (k) v,2</formula><p>and r</p><formula xml:id="formula_8">(k) v,i = AGGR{r (k−1) u : u ∈ Ni(v)} = u∈ Ni (v) r (k−1) u d −1/2 v,i d −1/2 u,i ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">d v,i = | Ni (v)| is the i-hop degree of node v (i.e.</formula><p>, number of nodes in its i-hop neighborhood). Unlike Eq. ( <ref type="formula" target="#formula_1">2</ref>), here we do not combine the ego-embedding of node v with the neighbor-embeddings. We found that removing the usual nonlinear transformations per round, as in SGC <ref type="bibr" target="#b36">[37]</ref>, works better (App. D.2), in which case we only need to include the ego-embedding in the final representation. By design D3, each node's final representation combines all its intermediate representations:</p><formula xml:id="formula_10">r (final) v = COMBINE r (0) v , r (1) v , . . . , r (K) v ,<label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type="bibr" target="#b37">[38]</ref> as the COMBINE function.</p><p>In the classification stage (S3), the node is classified based on its final embedding r</p><formula xml:id="formula_11">(final) v : yv = arg max{softmax(r (final) v Wc)},<label>(8)</label></formula><p>where</p><formula xml:id="formula_12">W c ∈ R (2 K+1 −1)</formula><p>p×|Y| is a learnable weight matrix. We visualize our framework in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time complexity</head><p>The feature embedding stage (S1) takes O(nnz(X) p), where nnz(X) is the number of non-0s in feature matrix X ∈ R n×F , and p is the dimension of the feature embeddings. The neighborhood aggregation stage (S2) takes O (|E|d max ) to derive the 2-hop neighborhoods via sparsematrix multiplications, where d max is the maximum degree of all nodes, and</p><formula xml:id="formula_13">O 2 K (|E| + |E 2 |)p for K rounds of aggregation, where |E 2 | = 1 2 v∈V | N2 (v)|.</formula><p>We give a detailed analysis in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Other Related Work</head><p>We discuss relevant work on GNNs here, and give other related work (e.g., classification under heterophily) in Appendix E. Besides the models mentioned above, there are various comprehensive reviews describing previously proposed architectures <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref>. Recent work has investigated GNN's ability to capture graph information, proposing diagnostic measurements based on feature smoothness and label smoothness <ref type="bibr" target="#b11">[12]</ref> that may guide the learning process. To capture more graph information, other works generalize graph convolution outside of immediate neighborhoods. For example, apart from MixHop <ref type="bibr" target="#b0">[1]</ref> (cf. § 3.1), Graph Diffusion Convolution <ref type="bibr" target="#b17">[18]</ref> replaces the adjacency matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type="bibr" target="#b25">[26]</ref> precomputes unsupervised node embeddings and uses neighborhoods defined by geometric relationships in the resulting latent space to define graph convolution. Some of these works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref> acknowledge the challenges of learning from graphs with heterophily. Others have noted that node labels may have complex relationships that should be modeled directly. For instance, Graph Agreement Models <ref type="bibr" target="#b32">[33]</ref> augment the classification task with an agreement task, co-training a model to predict whether pairs of nodes share the same label; Graph Markov Neural Networks <ref type="bibr" target="#b26">[27]</ref> model the joint label distribution with a conditional random field, trained with expectation maximization using GNNs; Correlated Graph Neural Networks <ref type="bibr" target="#b14">[15]</ref> model the correlation structure in the residuals of a regression task with a multivariate Gaussian, and can learn negative label correlations for neighbors in heterophily (for binary class labels); and the recent CPGNN <ref type="bibr" target="#b42">[43]</ref> method models more complex label correlations by integrating the compatibility matrix notion from belief propagation <ref type="bibr" target="#b9">[10]</ref> into GNNs. GCN <ref type="bibr" target="#b16">[17]</ref> GAT <ref type="bibr" target="#b35">[36]</ref> GCN-Cheby <ref type="bibr" target="#b6">[7]</ref> GraphSAGE <ref type="bibr" target="#b10">[11]</ref> MixHop <ref type="bibr" target="#b0">[1]</ref> H2GCN (proposed)</p><p>Comparison of H 2 GCN to existing GNN models As shown in Table <ref type="table" target="#tab_1">2</ref>, H 2 GCN differs from existing GNN models with respect to designs D1-D3, and their implementations (we give more details in App. D). Notably, H 2 GCN learns a graphagnostic feature embedding in stage (S1), and skips the nonlinear embeddings of aggregated representations per round that other models use (e.g., GraphSAGE, MixHop, GCN), resulting in a simpler yet powerful architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>We show the significance of designs D1-D3 on synthetic and real graphs with low-to-high homophily (Tab. 3, 5) via an ablation study of H 2 GCN and comparison of models with and without the designs.</p><p>Baseline models We consider MLP with 1 hidden layer, and all the methods listed in Table <ref type="table" target="#tab_1">2</ref>.</p><p>For H 2 GCN, we model the first-and second-order neighborhoods ( N1 and N2 ), and consider two variants: H 2 GCN-1 uses one embedding round (K = 1) and H 2 GCN-2 uses two rounds (K = 2).</p><p>We tune all the models on the same train/validation splits (see App. F for details).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic datasets &amp; setup</head><p>We generate synthetic graphs with various homophily ratios h (Tab. 3) by adopting an approach similar to <ref type="bibr" target="#b15">[16]</ref>. In App. G, we describe the data generation process, the experimental setup, and the data statistics in detail. All methods share the same training, validation and test splits (25%, 25%, 50% per class), and we report the average accuracy and standard deviation (stdev) over three generated graphs per heterophily level and benchmark dataset.</p><p>Model comparison Figure <ref type="figure" target="#fig_1">2</ref> shows the mean test accuracy (and stdev) over all random splits of our synthetic benchmarks. We observe similar trends on both benchmarks: H 2 GCN has the best trend overall, outperforming the baseline models in most heterophily settings, while tying with other models in homophily. The performance of GCN, GAT and MixHop, which mix the ego-and neighbor-embeddings, increases with respect to the homophily level. But, while they achieve nearperfect accuracy under strong homophily (h → 1), they are significantly less accurate than MLP (near-flat performance curve as it is graph-agnostic) for many heterophily settings. GraphSAGE and GCN-Cheby, which leverage some of the identified designs D1-D3 (Table <ref type="table" target="#tab_1">2</ref>, § 3), are more competitive in such settings. We note that all the methods-except GCN and GAT-learn more effectively under perfect heterophily (h=0) than weaker settings (e.g., h ∈ [0.1, 0.3]), as evidenced by the J-shaped performance curves in low-homophily ranges.</p><p>Significance of design choices Using syn-products, we show the significance of designs D1-D3 ( § 3.1) through ablation studies with variants of H 2 GCN (Fig. <ref type="figure" target="#fig_3">3</ref>, Table <ref type="table" target="#tab_9">G</ref>.4).</p><p>(D1) Ego-and Neighbor-embedding Separation. We consider H 2 GCN-1 variants that separate the ego-and neighbor-embeddings and model: (S0) neighborhoods N1 and N2 (i.e., H 2 GCN-1); (S1) only the 1-hop neighborhood N1 in Eq. ( <ref type="formula" target="#formula_6">5</ref>); and their counterparts that do not separate the two embeddings and use: (NS0) neighborhoods N 1 and N 2 (including v); and (NS1) only the 1hop neighborhood N 1 . Figure <ref type="figure" target="#fig_3">3a</ref> shows that the variants that learn separate embedding functions significantly outperform the others (NS0/1) in heterophily settings (h &lt; 0.7) by up to 40%, which shows that design D1 is critical for success in heterophily. H 2 GCN-1 (S0) performs best in homophily.</p><p>(D2) Higher-order Neighborhoods. For this design, we consider three variants of H 2 GCN-1 without specific neighborhoods: (N0) without the 0-hop neighborhood N 0 (v) = v (i.e, the ego-embedding) (N1) without N1 (v); and (N2) without N2 (v). Figure <ref type="figure" target="#fig_3">3b</ref> shows that H 2 GCN-1 consistently performs better than all the variants, indicating that combining all sub-neighborhoods works best. Among the variants, in heterophily settings, N 0 (v) contributes most to the performance (N0 causes significant decrease in accuracy), followed by N1 (v), and N2 (v). However, when h ≥ 0.7, the importance of sub-neighborhoods is reversed. Thus, the ego-features are the most important in heterophily, and  In heterophily, the performance gap between low-and high-degree nodes is significantly larger than in homophily, i.e., low-degree nodes pose challenges.</p><p>higher-order neighborhoods contribute the most in homophily. The design of H 2 GCN allows it to effectively combine information from different neighborhoods, adapting to all levels of homophily.</p><p>(D3) Combination of Intermediate Representations. We consider three variants (K-0,1,2) of H 2 GCN-2 that drop from the final representation of Eq. ( <ref type="formula" target="#formula_10">7</ref>) the 0 th , 1 st or 2 nd -round intermediate representation, respectively. We also consider only the 2 nd intermediate representation as final, which is akin to what the other GNN models do. Figure <ref type="figure" target="#fig_3">3c</ref> shows that H 2 GCN-2, which combines all the intermediate representations, performs the best, followed by the variant K2 that skips the round-2 representation.</p><p>The ego-embedding is the most important for heterophily h ≤ 0.5 (see trend of K0).</p><p>The challenging case of low-degree nodes Figure <ref type="figure" target="#fig_3">3d</ref> plots the mean accuracy of H 2 GCN variants on syn-products for different node degree ranges both in a heterophily and a homophily setting (h ∈ {0.2, 0.8}). We observe that under heterophily there is a significantly bigger performance gap between low-and high-degree nodes: 13% for H 2 GCN-1 (10% for H 2 GCN-2) vs. less than 3% under homophily. This is likely due to the importance of the distribution of class labels in each neighborhood under heterophily, which is harder to estimate accurately for low-degree nodes with few neighbors. On the other hand, in homophily, neighbors are likely to have similar classes y ∈ Y, so the neighborhood size does not have as significant impact on the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Real Benchmarks</head><p>Table <ref type="table">4</ref>: Real benchmarks: Average rank per method (and their employed designs among D1-D3) under heterophily (benchmarks with h ≤ 0.3), homophily (h ≥ 0.7), and across the full spectrum ("Overall"). The "*" denotes ranks based on results reported in <ref type="bibr" target="#b25">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of our model and existing GNNs on a variety of real-world datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily ratio h ranging from strong heterophily to strong homophily, going beyond the traditional Cora, Pubmed and Citeseer graphs that have strong homophily (hence the good performance of existing GNNs on them). We summarize the data in Table <ref type="table" target="#tab_5">5</ref>, and describe them in App. H, where we also point out potential data limitations. For all benchmarks (except Cora-Full), we use the feature vectors, class labels, and 10 random splits (48%/32%/20% of nodes per class for train/validation/test<ref type="foot" target="#foot_2">2</ref> ) provided by <ref type="bibr" target="#b25">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25%/25%/50% of nodes per class for train/validation/test.</p><p>Effectiveness of design choices Table <ref type="table">4</ref> gives the average ranks of our H 2 GCN variants and other models on real benchmarks with heterophily, homophily, and across the full spectrum. Table <ref type="table" target="#tab_5">5</ref> gives detailed results (mean accuracy and stdev) per benchmark. We observe that models which utilize all or subsets of our identified designs D1-D3 ( § 3.1) perform significantly better than GCN and GAT which lack these designs, especially in heterophily. Next, we discuss the effectiveness of each design.</p><p>(D1) Ego-and Neighbor-embedding Separation. We compare GraphSAGE, which separates the ego-and neighbor-embeddings, and GCN that does not. In heterophily settings, GraphSAGE has an average rank of 5.0 compared to 9.8 for GCN, and outperforms GCN in almost all heterophily benchmarks by up to 23%. In homophily settings (h ≥ 0.7), GraphSAGE ranks close to GCN (6.0 vs. 5.3), and GCN never outperforms GraphSAGE by more than 1% in mean accuracy. These results support the importance of D1 for success in heterophily and comparable performance in homophily.</p><p>(D2) Higher-order Neighborhoods. To show the benefits of design D2 under heterophily, we compare the performance of GCN-Cheby and MixHop-which define higher-order graph convolutions-to that of (first-order) GCN. Under heterophily, GCN-Cheby (rank 7.0) and MixHop (rank 6.5) have better performance than GCN (rank 9.8), and outperform the latter in all but one heterophily benchmarks by up to 20%. In most homophily benchmarks, the performance difference between these methods is less than 1%. Our observations highlight the importance of D2, especially in heterophily.</p><p>(D3) Combination of Intermediate Representations. We compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type="bibr" target="#b37">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in heterophily: their average ranks improve (9.8 vs. 7.2 and 7 vs 3.7, respectively) and their mean accuracies increase by up to 14% and 8%, respectively, in heterophily benchmarks. Though GraphSAGE+JK performs better than GraphSAGE on half of the heterophily benchmarks, its average rank remains unchanged. This may be due to the marginal benefit of D3 when combined with D1, which GraphSAGE employs. Under homophily, the performance with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type="bibr" target="#b37">[38]</ref>.</p><p>While other design choices and implementation details may confound a comparative evaluation of D1-D3 in different models (motivating our introduction of H 2 GCN and our ablation study in § 3.1), these observations support the effectiveness of our identified designs on diverse GNN architectures and real-world datasets, and affirm our findings in the ablation study. We also observe that our H 2 GCN variants, which combine the three identified designs, have consistently strong performance across the full spectrum of low-to-high homophily: H 2 GCN-2 achieves the best average rank (3.3) across all datasets (or homophily ratios h), followed by H 2 GCN-1 (3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional model comparison</head><p>In Table <ref type="table">4</ref>, we also report the best results among the three recentlyproposed GEOM-GCN variants ( § 4), directly from the paper <ref type="bibr" target="#b25">[26]</ref>: other models (including ours) outperform this method significantly under heterophily. We note that MLP is a competitive baseline under heterophily (ranked 6.2), indicating that many existing models do not use the graph information effectively, or the latter is misleading in such cases. All models perform poorly on Squirrel and Actor likely due to their low-quality node features (small correlation with class labels). Also, Squirrel and Chameleon are dense, with many nodes sharing the same neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have focused on characterizing the representation power of GNNs in challenging settings with heterophily or low homophily, which is understudied in the literature. We have highlighted the current limitations of GNNs, presented designs that increase representation power under heterophily and are theoretically justified with perturbation analysis and graph signal processing, and introduced the H 2 GCN model that adapts to both heterophily and homophily by effectively synthetizing these designs. We analyzed various challenging datasets, going beyond the often-used benchmark datasets (Cora, Pubmed, Citeseer), and leave as future work extending to a larger-scale experimental testbed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Homophily and heterophily are not intrinsically ethical or unethical-they are both phenomena existing in the nature, resulting in the popular proverbs "birds of a feather flock together" and "opposites attract". However, many popular GNN models implicitly assume homophily; as a result, if they are applied to networks that do not satisfy the assumption, the results may be biased, unfair, or erroneous. In some applications, the homophily assumption may have ethical implications.</p><p>For example, a GNN model that intrinsically assumes homophily may contribute to the so-called "filter bubble" phenomenon in a recommendation system (reinforcing existing beliefs/views, and downplaying the opposite ones), or make minority groups less visible in social networks. In other cases, a reliance on homophily may hinder scientific progress. Among other domains, this is critical for applying GNN models to molecular and protein structures, where the connected nodes often belong to different classes, and thus successful methods will need to model heterophily successfully.</p><p>Our work has the potential to rectify some of these potential negative consequences of existing GNN work. While our methodology does not change the amount of homophily in a network, moving beyond a reliance on homophily can be a key to improve the fairness, diversity and performance in applications using GNNs. We hope that this paper will raise more awareness and discussions regarding the homophily limitations of existing GNN models, and help researchers design models which have the power of learning in both homophily and heterophily settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Nomenclature</head><p>We summarize the main symbols used in this work and their definitions below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Homophily and Heterophily: Compatibility Matrix</head><p>As we mentioned in § 2, the edge homophily ratio in Definition 1 gives an overall trend for all the edges in the graph. The actual level of homophily may vary within different pairs of node classes, i.e., there is different tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type="bibr" target="#b23">[24]</ref> with three classes-fraudsters, accomplices, and honest users-, fraudsters connect with higher probability to accomplices and honest users. Moreover, within the same network, it is possible that some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type="bibr" target="#b39">[40]</ref>, a message-passing algorithm used for inference on graphical models, the different levels of homophily or affinity between classes are captured via the class compatibility, propagation or coupling matrix, which is typically pre-defined based on domain knowledge. In this work, we define the empirical class compatibility matrix H as follows:</p><p>Definition 4 The class compatibility matrix H has entries [H] i,j that capture the fraction of outgoing edges from a node in class i to a node in class j:</p><formula xml:id="formula_14">[H] i,j = |{(u, v) : (u, v) ∈ E ∧ y u = i ∧ y v = j}| |{(u, v) : (u, v) ∈ E ∧ y u = i}|</formula><p>By definition, the class compatibility matrix is a stochastic matrix, with each row summing up to 1. Without loss of generality, we reorder T V accordingly such that the one-hot encoding of labels for nodes in training set [Y] T V ,: is in increasing order of the class label y v :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs and Discussions of Theorems</head><formula xml:id="formula_15">[Y] T V ,: =                         1 0 0 • • • 0 . . . . . . . . . . . . . . . 1 0 0 • • • 0 0 1 0 • • • 0 . . . . . . . . . . . . . . . 0 1 0 • • • 0 . . . . . . . . . . . . . . . 0 0 0 • • • 1 . . . . . . . . . . . . . . . 0 0 0 • • • 1                         |V|×|Y|<label>(9)</label></formula><p>Now we look into the term [(A + I)X] T V ,: , which is the aggregated feature vectors within neighborhood N 1 for nodes in the training set. Since we assumed that all nodes in T V have degree d, proportion h of their neighbors belong to the same class, while proportion 1−h |Y|−1 of them belong to any other class uniformly, and one-hot representations of node features x v = onehot(y v ) for each node v, we obtain:</p><formula xml:id="formula_16">[(A + I)X] T V ,: =                            hd + 1 1−h |Y|−1 d 1−h |Y|−1 d • • • 1−h |Y|−1 d . . . . . . . . . . . . . . . hd + 1 1−h |Y|−1 d 1−h |Y|−1 d • • • 1−h |Y|−1 d 1−h |Y|−1 d hd + 1 1−h |Y|−1 d • • • 1−h |Y|−1 d . . . . . . . . . . . . . . . 1−h |Y|−1 d hd + 1 1−h |Y|−1 d • • • 1−h |Y|−1 d . . . . . . . . . . . . . . . 1−h |Y|−1 d 1−h |Y|−1 d 1−h |Y|−1 d • • • hd + 1 . . . . . . . . . . . . . . . 1−h |Y|−1 d 1−h |Y|−1 d 1−h |Y|−1 d • • • hd + 1                            |V|×|Y|<label>(10)</label></formula><p>For [Y] T V ,: and [(A+I)X] T V ,: that we derived in Eq. ( <ref type="formula" target="#formula_15">9</ref>) and (10), we can find an optimal weight matrix</p><formula xml:id="formula_17">W * such that [(A+I)X] T V ,: W * = [Y] T V ,: , making the loss L([(A+I)X] T V ,: W * , [Y] T V ,: ) = 0.</formula><p>We can use the following way to find W * : First, sample one node from each class to form a smaller set T S ⊂ T V , therefore we have:</p><formula xml:id="formula_18">[Y] T S ,: =     1 0 0 • • • 0 0 1 0 • • • 0 . . . . . . . . . . . . . . . 0 0 0 • • • 1     |Y|×|Y| = I |Y|×|Y| and [(A + I)X] T S ,: =       hd + 1 1−h |Y|−1 d 1−h |Y|−1 d • • • 1−h |Y|−1 d 1−h |Y|−1 d hd + 1 1−h |Y|−1 d • • • 1−h |Y|−1 d . . . . . . . . . . . . . . . 1−h |Y|−1 d 1−h |Y|−1 d 1−h |Y|−1 d • • • hd + 1       |Y|×|Y| Note that [(A + I)X]</formula><p>T S ,: is a circulant matrix, therefore its inverse exists. Using the Sherman-Morrison formula, we can find its inverse as:</p><formula xml:id="formula_19">([(A + I)X]T S ,:) −1 = 1 (d + 1)(|Y| − 1 + (|Y|h − 1)d) •     (|Y| − 1) + (|Y| − 2 + h)d (h − 1)d • • • (h − 1)d (h − 1)d (|Y| − 1) + (|Y| − 2 + h)d • • • (h − 1)d . . . . . . . . . . . . (h − 1)d (h − 1)d • • • (|Y| − 1) + (|Y| − 2 + h)d     Let W * = ([(A + I)X] T S ,: ) −1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and we have</head><formula xml:id="formula_20">[(A + I)X] T S ,: W * = [Y] T S ,: = I |Y|×|Y| . It is also easy to verify that [(A + I)X] T V ,: W * = [Y] T V ,: . W * = ([(A + I)X] T S ,: ) −1 is the optimal weight matrix we can learn under T V , since it satisfies L([(A + I)X] T V ,: W * , [Y] T V ,: ) = 0.</formula><p>Now consider an arbitrary training datapoint (v, y v ) ∈ T V , and a perturbation added to the neighborhood N (v) of node v, such that the number of nodes with a randomly selected class label y p ∈ Y = y v is δ 1 less than expected in N (v). We denote the perturbed graph adjacency matrix as A ∆ . Without loss of generality, we assume node v has y v = 1, and the perturbed class is y p = 2. In this case we have</p><formula xml:id="formula_21">[(A ∆ + I)X] v,: = hd + 1 1−h |Y|−1 d − δ 1 1−h |Y|−1 d • • • 1−h |Y|−1 d</formula><p>Applying the optimal weight matrix we learned on T V to the aggregated feature on the perturbed neighborhood [(A ∆ + I)X] v,: , we obtain [(A ∆ + I)X] v,: W * which equals to:</p><formula xml:id="formula_22">1 − (h−1)dδ1 (d+1)(|Y|−1+(|Y|h−1)d) − ((|Y|−1)+(|Y|−2+h)d)δ1 (d+1)(|Y|−1+(|Y|h−1)d) − (h−1)dδ1 (d+1)(|Y|−1+(|Y|h−1)d) • • • − (h−1)dδ1 (d+1)(|Y|−1+(|Y|h−1)d)</formula><p>Notice that we always have 1 −</p><formula xml:id="formula_23">(h−1)dδ1 (d+1)(|Y|−1+(|Y|h−1)d) &gt; − (h−1)dδ1 (d+1)(|Y|−1+(|Y|h−1)d)</formula><p>, thus the GCN layer formulated as (A + I)XW would misclassify only if the following inequality holds:</p><formula xml:id="formula_24">1 − (h − 1)dδ 1 (d + 1)(|Y| − 1 + (|Y|h − 1)d) &lt; − ((|Y| − 1) + (|Y| − 2 + h)d)δ 1 (d + 1)(|Y| − 1 + (|Y|h − 1)d)</formula><p>Solving the above inequality for δ 1 , we get the amount of perturbation needed as <ref type="bibr" target="#b10">(11)</ref> and the least absolute amount of perturbation needed is |δ</p><formula xml:id="formula_25">δ 1 &gt; −h|Y|d−|Y|+d+1 |Y|−1 , when 0 ≤ h &lt; −|Y|+d+1 |Y|d δ 1 &lt; −h|Y|d−|Y|+d+1 |Y|−1 , when h &gt; −|Y|+d+1 |Y|d</formula><formula xml:id="formula_26">1 | = | −h|Y|d−|Y|+d+1 |Y|−1 |.</formula><p>Now we move on to discuss the GCN layer formulated as f (X; A, W) = AXW without self loops. Following similar derivations, we obtain the optimal weight matrix W * which makes L([AX] T V ,: W * , [Y] T V ,: ) = 0 as:</p><formula xml:id="formula_27">W * = ([AX] T S ,: ) −1 = 1 (1 − h|Y|)d     −(|Y| − 2 + h) 1 − h • • • 1 − h 1 − h −(|Y| − 2 + h) • • • 1 − h . . . . . . . . . . . . 1 − h 1 − h • • • −(|Y| − 2 + h)    <label>(12)</label></formula><p>Again if for an arbitrary (v, y v ) ∈ T V , a perturbation is added to the neighborhood N (v) of the node v, such that the number of nodes with a randomly selected class label y p ∈ Y = y v is δ 2 less than expected in N (v), we have:</p><formula xml:id="formula_28">[A ∆ X] v,: = hd 1−h |Y|−1 d − δ 2 1−h |Y|−1 d • • • 1−h |Y|−1 d</formula><p>Then applying the optimal weight matrix that we learned on T V to the aggregated feature on perturbed neighborhood [A ∆ X] v,: , we obtain [A ∆ X] v,: W * which equals to:</p><formula xml:id="formula_29">1 − (1−h)δ2 (1−h|Y|)d (|Y|−2+h)δ2 (1−h|Y|)d − (1−h)δ2 (1−h|Y|)d • • • − (1−h)δ2 (1−h|Y|)d</formula><p>Thus, the GCN layer formulated as AXW would misclassify when the following inequality holds:</p><formula xml:id="formula_30">1 − (1 − h)δ 2 (1 − h|Y|)d &lt; (|Y| − 2 + h)δ 2 (1 − h|Y|)d</formula><p>Or the amount of perturbation is:</p><formula xml:id="formula_31">δ 2 &gt; (1−h|Y|)d |Y|−1 , when 0 ≤ h &lt; 1 |Y| δ 2 &lt; (1−h|Y|)d |Y|−1 , when h &gt; 1 |Y| (<label>13</label></formula><formula xml:id="formula_32">)</formula><p>As a result, the least absolute amount of perturbation needed is</p><formula xml:id="formula_33">|δ 2 | = | (1−h|Y|)d |Y|−1 |.</formula><p>By comparing the least absolute amount of perturbation needed for both formulations to misclassify  <ref type="figure" target="#fig_7">4a</ref>, we can see that the least absolute amount of perturbations |δ| needed for both formulation first decreases as the assumed homophily level h increases, until δ reaches 0, where the GCN layer predicts the same probability for all class labels; after that, δ decreases further below 0, and |δ| increases as h increases; the (A + I)XW formulation is less robust to perturbation than the AXW formulation at low homophily level until h = 1−|Y|+2d 2|Y|d as our proof shows, where |δ 1 | = |δ 2 |. Figure <ref type="figure" target="#fig_7">4b</ref> shows the changes of |δ| as a function of |Y| when fixed h = 0.1 and d = 20. For both formulations, |δ| first decrease rapidly as |Y| increases until δ reaches 0, after that δ increases slowly as |Y| increases; this reveals that both GCN formulations are more robust when |Y| &lt;&lt; d under high homophily level, and in that case AXW formulation is more robust than the (A + I)XW formulation. Figure <ref type="figure" target="#fig_7">4c</ref> shows the changes of |δ| as a function of d for fixed h = 0.1 and |Y| = 5: in this case the AXW formulation is always more robust than the (A + I)XW formulation, and for the (A + I)XW formulation, |δ| follows again a "V"-shape curve as d changes.</p><formula xml:id="formula_34">(|δ 1 | = | −h|Y|d−|Y|+d+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Detailed Analysis of Theorem 2</head><p>Proof 2 (for Theorem 2) For all v ∈ V, since its neighbors' class labels {y u : u ∈ N (v)} are conditionally independent given y v , we can define a matrix P v for each node v as [P v ] i,j = P (y u = j|y v = i), ∀i, j ∈ Y, u ∈ N (v). Following the assumption that for all v ∈ V, P (y u = y v |y v ) = h, P (y u = y|y v ) = 1−h |Y|−1 , ∀y = y v , we have  Now consider node w ∈ N 2 (v), we have:</p><formula xml:id="formula_35">P v = P =       h 1−h |Y|−1 • • • 1−h |Y|−1 1−h |Y|−1 h • • • 1−h |Y|−1 . . . . . . . . . . . . 1−h |Y|−1 1−h |Y|−1 • • • h       , ∀v ∈ V<label>(14)</label></formula><formula xml:id="formula_36">P (y w = k|y v = i) = j∈|Y| P (y w = k|y u = j)P (y u = j|y v = i) = j∈|Y| [P] j,k [P] i,j = P 2 (15)</formula><p>Therefore, to prove that the 2-hop neighborhood N 2 (v) for any node v ∈ V is homophily-dominant in expectation (i.e. P (y</p><formula xml:id="formula_37">w = i|y v = i) ≥ P (y w = j|y v = i), ∀j ∈ Y = i, w ∈ N 2 (v))</formula><p>, we need to show that the diagonal entries [P 2 ] i,i of P 2 are larger than the off-diagonal entries [P 2 ] i,j .</p><p>Denote ρ = 1−h |Y|−1 . From Eq. ( <ref type="formula" target="#formula_35">14</ref>), we have</p><formula xml:id="formula_38">[P 2 ] i,i = h 2 + (|Y| − 1)ρ 2<label>(16)</label></formula><p>and for i = j</p><formula xml:id="formula_39">[P 2 ] i,j = 2hρ + (|Y| − 2)ρ 2<label>(17)</label></formula><p>Thus,</p><formula xml:id="formula_40">[P 2 ] i,i − [P 2 ] i,j = h 2 − 2hρ + ρ 2 = (h − ρ) 2 ≥ 0</formula><p>with equality if and only if h = ρ, namely h = 1 |Y| . Therefore, we proved that the 2-hop neighborhood N 2 (v) for any node v ∈ V will always be homophily-dominant in expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Detailed Analysis of Theorem 3</head><p>Preliminaries We define unnormalized Laplacian matrix of graph G as L = D − A, where A ∈ {0, 1} |V|×|V| is the adjacency matrix and D is the diagonal matrix with [D] i,i = j [A] i,j . Without loss of generality, since the eigenvalues {λ i } of L are real and nonnegative <ref type="bibr" target="#b31">[32]</ref>, we assume the following order for the eigenvalues of L:</p><formula xml:id="formula_41">0 = λ 0 &lt; λ 1 ≤ λ 2 ≤ • • • ≤ λ |V|−1 = λ max .</formula><p>Furthermore, since L is real and symmetric, there exists a set of orthonormal eigenvectors {v i } that form a complete basis of R |V| . This means that for any graph signal s ∈ R |V| , where s u is the value of the signal on node u ∈ V, it can be decomposed to a weighted sum of {v i }. Mathematically, s is represented as s = |V|−1 i=0 c s,i v i , where c s,i = s T v i . We regard c s,i as the coefficient of s at frequency component i and regard the coefficients at all frequencies components {c s,i } as the spectrum of signal s with respect to graph G. In the above order of the eigenvalues, λ i which are closer to 0 would correspond to lower-frequency components, and λ i which are closer to λ max would correspond to higher-frequency components. Interested readers are referred to <ref type="bibr" target="#b31">[32]</ref> for further details regarding signal processing on graphs.</p><p>The smoothness score of a signal s on graph G, which measures the amount of changes of signal s along the edges of graph G, can be defined using L as</p><formula xml:id="formula_42">s T Ls = i,j A ij (s i − s j ) 2 = u∈V v∈N (u) (s u − s v ) 2 .<label>(18)</label></formula><p>Then, for two eigenvectors v i and v j corresponding to eigenvalues λ i ≤ λ j of L, we have:</p><formula xml:id="formula_43">v T i Lv i = λ i ≤ λ j = v T j</formula><p>Lv j which means that v i is more smooth than v j . This matches our expectations that a lower-frequency signal on G should have smaller smoothness score. The smoothness score for arbitrary graph signal s ∈ R |V| can be represented by its coefficients of each frequency component as:</p><formula xml:id="formula_44">s T Ls = i c s,i v i L i c s,i v i = |V|−1 i=0 c 2 s,i λ i<label>(19)</label></formula><p>with the above preliminaries, we can define the following concept:</p><formula xml:id="formula_45">Definition 5 Suppose s = |V|−1 i=0 c s,i v i and t = |V|−1 i=0 c t,i v i are two graph signals defined on G.</formula><p>In the spectrum of the unnormalized graph laplacian L, graph signal s has higher energy on high-frequency components than t if there exists integer 0</p><formula xml:id="formula_46">&lt; M ≤ |V| − 1 such that |V|−1 i=M c 2 s,i &gt; |V|−1 i=M c 2 t,i .</formula><p>Based on these preliminary definitions, we can now proceed with the proof of the theorem:</p><p>Proof 3 (for Theorem 3) We first prove that for graph signals s, t ∈ {0, 1} |V| , edge homophily ratio h s &lt; h t if and only if s T Ls &gt; t T Lt. Following Dfn. 1, the edge homophily ratio for signal s (similarly for t) can be calculated as:</p><formula xml:id="formula_47">h s = 1 2|E| u∈V   d u − v∈N (v) (s u − s v ) 2   = 1 2|E| u∈V d u − 1 2|E| u∈V v∈N (v) (s u − s v ) 2 (20)</formula><p>Plugging this in Eq. (18), we obtain:</p><formula xml:id="formula_48">h s = 1 2|E| u∈V d u − 1 2|E| s T Ls = 1 − 1 2|E| s T Ls</formula><p>where |E| is the number of edges in G. From the above equation, we have</p><formula xml:id="formula_49">h s &lt; h t ⇔ 1 − 1 2|E| s T Ls &lt; 1 − 1 2|E| t T Lt ⇔ s T Ls &gt; t T Lt</formula><p>i.e. edge homophily ratio h s &lt; h t if and only if s T Ls &gt; t T Lt.</p><p>Next we prove that if s T Ls &gt; t T Lt, then following Dfn.5, signal s has higher energy on highfrequency components than t. We prove this by contradiction: suppose integer 0 &lt; M ≤ |V| − 1 does not exist such that</p><formula xml:id="formula_50">|V|−1 i=M c 2 si &gt; |V|−1 i=M c 2 ti when s T Ls &gt; t T</formula><p>Lt, then all of the following inequalities must hold, as the eigenvalues of</p><formula xml:id="formula_51">L satisfy 0 = λ 0 &lt; λ 1 ≤ λ 2 ≤ • • • ≤ λ |V|−1 = λ max : 0 = λ 0 (c 2 s,0 + c 2 s,1 + c 2 s,2 + • • • + c 2 s,|V|−1 ) = λ 0 (c 2 t,0 + c 2 t,1 + c 2 t,2 + • • • + c 2 t,|V|−1 ) = 0 (λ 1 − λ 0 )(c 2 s,1 + c 2 s,2 + • • • + c 2 s,|V|−1 ) ≤ (λ 1 − λ 0 )(c 2 t,1 + c 2 t,2 + • • • + c 2 t,|V|−1 ) (λ 2 − λ 1 )(c 2 s,2 + • • • + c 2 s,|V|−1 ) ≤ (λ 2 − λ 1 )(c 2 t,2 + • • • + c 2 t,|V|−1 ) . . . (λ |V|−1 − λ |V|−2 )c 2 s,|V|−1 ≤ (λ |V|−1 − λ |V|−2 )c 2 t,|V|−1</formula><p>Summing over both sides of all the above inequalities, we have</p><formula xml:id="formula_52">λ 0 •c 2 s,0 +λ 1 •c 2 s,1 +λ 2 •c 2 s,2 +• • •+λ |V|−1 •c 2 s,|V|−1 ≤ λ 0 •c 2 t,0 +λ 1 •c 2 t,1 +λ 2 •c 2 t,2 +• • •+λ |V|−1 •c 2 t,|V|−1 i.e., |V|−1 i=0 c 2 si λ i ≤ |V|−1 i=0 c 2 ti λ i . However, from Eq. (<label>19</label></formula><p>), we should have</p><formula xml:id="formula_53">s T Ls &gt; t T Lt ⇔ |V|−1 i=0 c 2 si λ i &gt; |V|−1 i=0 c 2 ti λ i</formula><p>which contradicts with the previous resulting inequality. Therefore, the assumption should not hold, and there must exist an integer 0 &lt; M ≤ |V| − 1 such that</p><formula xml:id="formula_54">|V|−1 i=M c 2 si &gt; |V|−1 i=M c 2 ti when s T Ls &gt; t T Lt.</formula><p>Extension of Theorem 3 to one-hot encoding of class label vectors Theorem 3 discusses only the graph signal s, t ∈ {0, 1} |V| with only 1 channel (i.e., with only 1 value assigned to each node). It is possible to generalize the theorem to one-hot encoding Y s , Y t ∈ {0, 1} |V|×|Y| as graph signal with |Y|-channels by modifying Dfn. 5 as follows:</p><formula xml:id="formula_55">Definition 6 Suppose [Y s ] :,j = |V|−1 i=0 c s,j,i v i and [Y t ] :,j =</formula><p>|V|−1 i=0 c t,j,i v i are one-hot encoding of class label vector y s , y t defined as graph signals on G, where c s,j,i = [Y s ] T :,j v i is the coefficient of the jth-channel of Y s at frequency component i. In the spectrum of the unnormalized graph laplacian L, graph signal Y s has higher energy on high-frequency components than Y t if there exists integer 0 &lt; M ≤ |V| − 1 such that</p><formula xml:id="formula_56">|V|−1 i=M φ j=1 c 2 s,j,i &gt; |V|−1 i=M φ j=1 c 2 t,j,i .</formula><p>Under this definition, we can prove Theorem 3 for one-hot encoding of class label vectors Y s , Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type="bibr" target="#b19">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula xml:id="formula_57">h s = 1 4|E| u∈V   2d u − v∈N (v) φ j=1 ([Y s ] u,j − [Y t ] v,j ) 2   instead of Eq.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Our H 2 GCN model: Details</head><p>In this section, we give the pipeline and pseudocode of H 2 GCN, elaborate on its differences from existing GNN models, and present a detailed analysis of its computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Pseudocode &amp; Pipeline</head><p>In Fig. <ref type="figure" target="#fig_8">5</ref> we visualize H 2 GCN, which we describe in § 3.2. We also give its pseudocode in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Detailed Comparison of H 2 GCN to existing GNN models</head><p>In § 4, we discussed several high-level differences between H 2 GCN and the various GNN models that we consider in this work, including the inclusion or not of designs D1-D3. Here we give some additional conceptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type="bibr" target="#b16">[17]</ref> in a number of ways: <ref type="bibr" target="#b0">(1)</ref> In each round of propagation/aggregation, GCN "mixes" the ego-and neighbor-representations by repeatedly averaging them to obtain the new node representations, while H 2 GCN keeps them distinct via concatenation; (2) GCN considers only the 1-hop neighbors (including the ego / self-loops), while H 2 GCN considers higher-order neighborhoods ( N1 and N2 ); (3) GCN applies non-linear embedding transformations per round (e.g., RELU), while H 2 GCN perform feature embedding for the ego in the first layer and drops all other non-linearities in the aggregation stage; and (4) GCN does not use the jumping knowledge framework (unlike H 2 GCN), and makes the node classification predictions based on the last-round representations.</p><p>Unlike GAT, H 2 GCN does not use any attention mechanism. Creating attention mechanisms that can generalize well to heterophily is an interesting future direction. Moreover, GCN-Cheby uses entirely different mechanisms than the other GNN models that we consider (i.e., Chebysev polynomials), though it has some conceptual similarities to H 2 GCN in terms of the higher-order neighborhoods that it models. GraphSAGE differs from H 2 GCN in the same ways that are described in (2)-( <ref type="formula" target="#formula_5">4</ref>) above. In addition to leveraging only the 1-hop neighborhood, GraphSAGE also samples a fixed number of neighbors per round, while H 2 GCN uses the full neighborhood. With respect to ego-and neighbor-representations, GraphSAGE concatenates them (as we do) but subsequently applies non-linear embedding transformations to them jointly (while we simplify all non-linear transformations). Our empirical analysis has revealed that such transformations lead to a decrease in performance in heterophily settings (see paragraph below on "Non-linear embedding transformations..."). Finally, MixHop differs from H 2 GCN in the same ways that are described in (1) and ( <ref type="formula" target="#formula_2">3</ref>)-(4) above. It explicitly considers higher-order neighborhoods up to N 2 , though <ref type="bibr" target="#b0">[1]</ref> defines the 2-hop neighborhoods as that including neighbors up to 2-hop away neighbors. In our framework, we define the i-hop neighborhood as the set of neighbors with minimum distance exactly i from the ego ( § 2). Finally, the output layer of MixHop uses a tailored, column-wise attention layer, which prioritizes specific features, before the softmax layer. In contrast, before the classification layer, H 2 GCN uses concatenationbased jumping knowledge in order to represent the high-frequency components that are critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type="bibr" target="#b16">[17]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref> and other GNN models embed the intermediate representations per round of feature propagation and aggregation. However, as we show in the ablation study in App. G.2 (Table G.4, last row "Non-linear"), introducing non-linear transformations per round of the neighborhood aggregation stage (S2) of H 2 GCN-2 (i.e., with K = 2) as follows leads to worse performance than the framework design that we introduce in Eq. ( <ref type="formula" target="#formula_6">5</ref>) of § 3.2:</p><formula xml:id="formula_58">r (k) v = COMBINE σ W r (k−1) v , AGGR{r (k−1) u : u ∈ N1(v)}, AGGR{r (k−1) u : u ∈ N2(v)} , (<label>21</label></formula><formula xml:id="formula_59">)</formula><p>where σ is RELU and W is a learnable matrix. Our design in Eq. 5 aggregates different neighborhoods in a similar way to SGC <ref type="bibr" target="#b36">[37]</ref>, which has shown that removing non-linearities does not negatively impact performance in homophily settings. We actually find that removing non-linearities even improves the performance under heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 H 2 GCN: Time Complexity in Detail</head><p>Preliminaries The worst case time complexity for calculating A • B when both A and B are sparse matrices is O(nnz(A) • c B ), where nnz(A) is the number of non-zero elements in matrix bedding R (k−1) ∈ R n×2 (k−1) p as input. We aggregate embedding vectors within neighborhood by </p><formula xml:id="formula_60">R (k) = Ā1 R (k−1) Ā2 R (k−</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Related Work</head><p>In § 4, we discuss relevant work on GNNs. Here we briefly mention other approaches for node classification.</p><p>Collective classification in statistical relational learning focuses on the problem of node classification by leveraging the correlations between the node labels and their attributes <ref type="bibr" target="#b29">[30]</ref>. Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type="bibr" target="#b39">[40]</ref> is a classic messagepassing algorithm for graph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily <ref type="bibr" target="#b18">[19]</ref> and has fast linearized versions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Different from the setup where GNNs are employed, BP does not by itself leverage node features, and usually assumes a pre-defined class compatibility or edge potential matrix ( § 2). We note, however, that Gatterbauer <ref type="bibr" target="#b8">[9]</ref> proposed estimating the class compatibility matrix instead of using a pre-defined one in the BP formulation. Moreover, the recent CPGNN model <ref type="bibr" target="#b42">[43]</ref> integrates the compatibility matrix as a set of learnable parameters into GNN, which it initializes with an estimated class compatibility matrix. Another classic approach for collective classification or graph-based semi-supervised learning is label propagation, which iteratively propagates the (up-to-date) label information of each node to its neighbors in order to minimize the overall smoothness penalty of label assignments in the graph. Standard label propagation approaches inherently assume homophily by penalizing different label assignments among immediate neighborhoods, but more recent works have also looked into formulations which can better address heterophily: Before applying label propagation, Peel <ref type="bibr" target="#b24">[25]</ref> transforms the original graph into either a similarity graph by measuring similarity between node neighborhoods or a new graph connecting nodes that are two hops away; Chin et al. <ref type="bibr" target="#b5">[6]</ref> decouple graph smoothing where the notion of "identity" and "preference" for each node are considered separately. However, like BP, these approaches do not by themselves utilize node features. Baseline Implementations For all baselines besides MLP, we used the official implementation released by the authors on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Setup &amp; Hyperparameter Tuning</head><p>• GCN &amp; GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>: https://github.com/tkipf/gcn • GraphSAGE <ref type="bibr" target="#b10">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch implementation) • MixHop <ref type="bibr" target="#b0">[1]</ref>: https://github.com/samihaija/mixhop • GAT <ref type="bibr" target="#b35">[36]</ref>: https://github.com/PetarV-/GAT. (For large datasets, we make use of the sparse version provided by the author.)</p><p>For MLP, we used our own implementation of MLP with 1-hidden layer, which is equivalent to the case of K = 0 in Algorithm 1. We use the same loss function as H 2 GCN for training MLP.</p><p>Hardware Specifications We run experiments on synthetic benchmarks with an Amazon EC2 instance with instance size as p3.2xlarge, which features an 8-core CPU, 61 GB Memory, and a Tesla V100 GPU with 16 GB GPU Memory. For experiments on real benchmarks, we use a workstation with a 12-core AMD Ryzen 9 3900X CPU, 64GB RAM, and a Quadro P6000 GPU with 24 GB GPU Memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Tuning the GNN Models</head><p>To avoid bias, we tuned the hyperparameters of each method (H 2 GCN and baseline models) on each benchmark. Below we list the hyperparameters tested on each benchmark per model. As the hyperparameters defined by each baseline model differ significantly, we list the combinations of non-default command line arguments we tested, without explaining them in detail. We refer the interested reader to the corresponding original implementations for further details on the arguments, including their definitions.</p><p>Synthetic Benchmark Tuning For each synthetic benchmark, we report the results for different heterophily levels under the same set of hyperparameters for each method, so that we can compare how the same hyperparameters perform across the full spectrum of low-to-high homophily. We report the best performance, for the set of hyperparameters which performs the best on the validation set on the majority of the heterophily levels for each method.</p><p>For syn-cora, we test the following command-line arguments for each baseline method:</p><p>•</p><formula xml:id="formula_61">H 2 GCN-1 &amp; H 2 GCN-2:</formula><p>-Dimension of Feature Embedding p: 64 -Non-linearity Function σ: ReLU -Dropout Rate: a ∈ {0, 0.5}</p><p>We report the best performance, for a = 0. • GCN <ref type="bibr" target="#b16">[17]</ref>:</p><p>-hidden1: a ∈ {16, 32, 64} -early_stopping: b ∈ {40, 100, 200} epochs: 2000</p><p>We report the best performance, for a = 32, b = 40. • GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>:</p><p>-Set 1:</p><p>-hidden1: 64 -early_stopping: {40, 100, 200} -weight_decay: {5e-5, 1e- For GCN+JK, GCN-Cheby+JK and GraphSAGE+JK, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type="bibr" target="#b37">[38]</ref> without changing the number of layers or other hyperparameters for the base method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Synthetic Datasets: Details G.1 Data Generation Process &amp; Setup</head><p>Synthetic graph generation We generate synthetic graphs with various heterophily levels by adopting an approach similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type="bibr" target="#b2">[3]</ref>: The number of class labels |Y| in the synthetic graph is prescribed. Then, starting from a small initial graph, new nodes are added into the graph one by one, until the number of nodes |V| has reached the preset level. The probability p uv for a newly added node u in class i to connect with an existing node v in class j is proportional to both the class compatibility H ij between class i and j, and the degree d v of the existing node v. As a result, the degree distribution for the generated graphs follow a power law, and the heterophily can be controlled by class compatibility matrix H. Table <ref type="table" target="#tab_2">3</ref> shows an overview of these synthetic benchmarks, and more detailed statistics can be found in Table <ref type="table" target="#tab_9">G</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node features &amp; classes</head><p>Nodes are assigned randomly to each class during the graph generation. Then, in each synthetic graph, the feature vectors of nodes in each class are generated by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> or ogbn-products <ref type="bibr" target="#b12">[13]</ref>): We first establish a class mapping ψ : Y s → Y b between classes in the synthetic graph Y s to classes in an existing benchmark Y b . The only requirement is that the class size in the existing benchmark is larger than that of the synthetic graph so that an injection between nodes from both classes can be established, and the feature vectors for the synthetic graph can be sampled accordingly. For syn-products, we further restrict the feature sampling to ensure that nodes in the training, validation and test splits are only mapped to nodes in the corresponding splits in the benchmark. This process respects the data splits used in ogbn-products, which are more realistic and challenging than random splits <ref type="bibr" target="#b12">[13]</ref>. For simplicity, in our synthetic benchmarks, all the classes (5 for syn-cora and 10 for syn-products -Table <ref type="table" target="#tab_9">G</ref>.1) are of the same size. Experimental setup For each heterophily ratio h of each benchmark, we independently generate 3 different graphs. For syn-cora and syn-products, we randomly partition 25% of nodes into training set, 25% into validation and 50% into test set. All methods share the same training, partition and test splits, and the average and standard derivation of the performance values under the 3 generated graphs are reported as the performance under each heterophily level of each benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Detailed Results on Synthetic Benchmarks</head><p>Tables G.2 and G.3 give the results on syn-cora and syn-products shown in Figure <ref type="figure" target="#fig_1">2</ref> of the main paper ( § 5.1). Table <ref type="table" target="#tab_9">G</ref>.4 provides the detailed results of the ablation studies that we designed in order to investigate the significance of our design choices, and complements Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Real Datasets: Details</head><p>Datasets In our experiments, we use the following real-world datasets with varying levels of homophily ratios h. Some network statistics are given in Table <ref type="table" target="#tab_5">5</ref>.</p><p>• Texas, Wisconsin and Cornell are graphs representing links between web pages of the corresponding universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type="bibr" target="#b25">[26]</ref>. In these networks, nodes are web pages, which are classified into 5 categories: course, faculty, student, project, staff. • Squirrel and Chameleon are subgraphs of web pages in Wikipedia discussing the corresponding topics, collected by <ref type="bibr" target="#b28">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type="bibr" target="#b25">[26]</ref>, where the nodes are categorized into 5 classes based on the amount of their average traffic. • Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type="bibr" target="#b25">[26]</ref> based on the film-director-actor-writer network in <ref type="bibr" target="#b34">[35]</ref>. We also use the class labels generated by <ref type="bibr" target="#b25">[26]</ref>. • Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>, which are among the most widely used benchmarks for semi-supervised node classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>. Each node is assigned a class label based on the research field. These datasets use a bag of words representation as the feature vector for each node. • Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more papers and research fields than Cora. This dataset also uses a bag of words representation as the feature vector for each node.</p><p>Data Limitations As discussed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, Cora, Pubmed and Citeseer are widely adopted as benchmarks for semi-supervised node classification tasks; however, all these benchmark graphs display strong homophily, with edge homophily ratio h ≥ 0.7. As a result, the wide adaptation of these benchmarks have masked the limitations of the homophily assumption in many existing GNN models. Open Graph Benchmark is a recent effort of proposing more challenging, realistic benchmarks with improved data quality comparing to the existing benchmarks <ref type="bibr" target="#b12">[13]</ref>. However, with respect to homophily, we found that the proposed OGB datasets display homophily h &gt; 0.5.</p><p>In our synthetic experiments ( § G), we used ogbn-products from this effort to generate higher quality synthetic benchmarks while varying the homophily ratio h. In our experiments on real datasets, we go beyond the typically-used benchmarks (Cora, Pubmed, Citeseer) and consider benchmarks with strong heterophily (Table <ref type="table" target="#tab_5">5</ref>). That said, these datasets also have limitations, including relatively small sizes (e.g., WebKB benchmarks), artificial classes (e.g., Squirrel and Chameleon have class labels based on ranking of page traffic), or unusual network structure (e.g., Squirrel and Chameleon are dense, with many nodes sharing the same neighbors -cf. § 5.2). We hope that this paper will encourage future work on more diverse datasets with different levels of homophily, and lead to higher quality datasets for benchmarking GNN models in the heterophily settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neighborhoods.</figDesc><graphic url="image-1.png" coords="2,401.04,281.42,99.00,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of GNN models on synthetic datasets. H 2 GCN-2 outperforms baseline models in most heterophily settings, while tying with other models in homophily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>N₀(v) [N0] w/o N₁(v) [N1] w/o N₂(v) [N2]Accuracy per degree in hetero/homo-phily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a)-(c): Significance of design choices D1-D3 via ablation studies. (d):Performance of H 2 GCN for different node degree ranges. In heterophily, the performance gap between low-and high-degree nodes is significantly larger than in homophily, i.e., low-degree nodes pose challenges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>C. 1 Detailed Analysis of Theorem 1 Proof 1 (for Theorem 1 )</head><label>1111</label><figDesc>We first discuss the GCN layer formulated as f (X; A, W) = (A+I)XW.Given training set T V , the goal of the training process is to optimize the weight matrix W to minimize the loss function L([(A + I)X] T V ,: W, [Y] T V ,: ), where [Y] T V ,: is the one-hot encoding of class labels provided in the training set, and [(A + I)X] T V ,: W is the predicted probability distribution of class labels for each node v in the training set T V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>|Y|− 1 |</head><label>1</label><figDesc>derived in Eq. (11) for the (A + I)XW formulation; |δ 2 | = | (1−h|Y|)d |Y|−1 | derived in Eq. (13) for the AXW formulation), we can see that |δ 1 | = |δ 2 | if and only if δ 1 = −δ 2 , which happens when h = 1−|Y|+2d 2|Y|d . When h &lt; 1−|Y|+2d 2|Y|d (heterophily), we have |δ 1 | &lt; |δ 2 |, which means the (A + I)XW formulation is less robust to perturbation than the AXW formulation. Discussions From the above proof, we can see that the least absolute amount of perturbation |δ| needed for both GCN formulations is a function of the assumed homophily ratio h, the node degree d for each node in the training set T V , and the size of the class label set |Y|. Fig. 4 shows the plots of |δ 1 | and |δ 2 | as functions of h, |Y| and d: from Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>|δ 1 | 2 |</head><label>12</label><figDesc>for the (A+I)XW Formulation |δ 2 | for the AXW Formulation 0|δ| as a function of h under d = 20, |Y| = 5. |δ 1 | for the (A+I)XW Formulation |δ for |δ| as a function of |Y| under h = 0.1, d = 20. |δ 1 | for the (A+I)XW Formulation |δ 2 | for the AXW |δ| as a function of d under h = 0.1, |Y| = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perturbation |δ| needed in order for GCN layers (A + I)XW and AXW to misclassify a node: Examples of perturbation |δ| as functions of h, |Y| and d, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: H 2 GCN-2 pipeline. It consists of 3 stages: (S1) feature embedding, (S2) neighborhood aggregation, and (S3) classification. The feature embedding stage (S1) uses a graph-agnostic dense layer to generate the feature embedding r (0) v of each node v based on its ego-feature x v . In the neighborhood aggregation stage (S2), the generated embeddings are aggregated and repeatedly updated within the node's neighborhood; the 1-hop neighbors N 1 (v) and 2-hop neighbors N 2 (v) are aggregated separately and then concatenated, following our design D2. In the classification stage (S3), each node is classified based on its final embedding r (final) v , which consists of its intermediate representations concatenated as per design D3.</figDesc><graphic url="image-3.png" coords="20,270.32,205.89,120.51,120.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F. 1 SetupH 2</head><label>12</label><figDesc>GCN Implementation We use K = 1 for H 2 GCN-1 and K = 2 for H 2 GCN-2. For loss function, we calculate the cross entropy between the predicted and the ground-truth labels for nodes within the training set, and add L 2 regularization of network parameters W e and W c . (cf. Alg. 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>3 in § 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of a heterophily setting (h = 0.1) where existing GNNs fail to generalize, and a typical homophily setting (h = 0.7): mean accuracy and standard deviation over three runs (cf. App. G).</figDesc><table><row><cell></cell><cell>h = 0.1</cell><cell>h = 0.7</cell></row><row><cell>GCN [17]</cell><cell>37.14±4.60</cell><cell>84.52±0.54</cell></row><row><cell>GAT [36]</cell><cell>33.11±1.20</cell><cell>84.03±0.97</cell></row><row><cell>GCN-Cheby [7]</cell><cell>68.10±1.75</cell><cell>84.92±1.03</cell></row><row><cell cols="2">GraphSAGE [11] 72.89±2.42</cell><cell>85.06±0.51</cell></row><row><cell>MixHop [1]</cell><cell>58.93±2.84</cell><cell>84.43±0.94</cell></row><row><cell>MLP</cell><cell>74.85±0.76</cell><cell>71.72±0.62</cell></row><row><cell>H 2 GCN (ours)</cell><cell cols="2">76.87±0.43 88.28±0.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Design Comparison.</figDesc><table><row><cell>Method</cell><cell>D1</cell><cell>D2</cell><cell>D3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics for Synthetic Datasets</figDesc><table><row><cell>Benchmark Name</cell><cell>#Nodes |V|</cell><cell>#Edges |E|</cell><cell>#Classes |Y|</cell><cell>#Features F</cell><cell>Homophily h</cell><cell>#Graphs</cell></row><row><cell>syn-cora</cell><cell>1, 490</cell><cell>2, 965 to 2, 968</cell><cell>5</cell><cell>cora [30, 39]</cell><cell>[0, 0.1, . . . , 1]</cell><cell>33 (3 per h)</cell></row><row><cell>syn-products</cell><cell>10, 000</cell><cell>59, 640 to 59, 648</cell><cell>10</cell><cell>ogbn-products [13]</cell><cell>[0, 0.1, . . . , 1]</cell><cell>33 (3 per h)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Real data: mean accuracy ± stdev over different data splits. Best model per benchmark highlighted in gray. The "*" results are obtained from<ref type="bibr" target="#b25">[26]</ref> and "N/A" denotes non-reported results.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell cols="6">Squirrel Chameleon Cornell Cora Full Citeseer Pubmed</cell><cell>Cora</cell></row><row><cell>Hom. ratio h</cell><cell>0.11</cell><cell>0.21</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>0.3</cell><cell>0.57</cell><cell>0.74</cell><cell>0.8</cell><cell>0.81</cell></row><row><cell>#Nodes |V|</cell><cell>183</cell><cell>251</cell><cell>7,600</cell><cell>5,201</cell><cell>2,277</cell><cell>183</cell><cell>19,793</cell><cell>3,327</cell><cell>19,717</cell><cell>2,708</cell></row><row><cell>#Edges |E|</cell><cell>295</cell><cell>466</cell><cell>26,752</cell><cell>198,493</cell><cell>31,421</cell><cell>280</cell><cell>63,421</cell><cell>4,676</cell><cell>44,327</cell><cell>5,278</cell></row><row><cell>#Classes |Y|</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>70</cell><cell>7</cell><cell>3</cell><cell>6</cell></row><row><cell>H 2 GCN-1</cell><cell cols="10">84.86±6.77 86.67±4.69 35.86±1.03 36.42±1.89 57.11±1.58 82.16±4.80 68.13±0.49 77.07±1.64 89.40±0.34 86.92±1.37</cell></row><row><cell>H 2 GCN-2</cell><cell cols="10">82.16±5.28 85.88±4.22 35.62±1.30 37.90±2.02 59.39±1.98 82.16±6.00 69.05±0.37 76.88±1.77 89.59±0.33 87.81±1.35</cell></row><row><cell>GraphSAGE</cell><cell cols="10">82.43±6.14 81.18±5.56 34.23±0.99 41.61±0.74 58.73±1.68 75.95±5.01 65.14±0.75 76.04±1.30 88.45±0.50 86.90±1.04</cell></row><row><cell>GCN-Cheby</cell><cell cols="10">77.30±4.07 79.41±4.46 34.11±1.09 43.86±1.64 55.24±2.76 74.32±7.46 67.41±0.69 75.82±1.53 88.72±0.55 86.76±0.95</cell></row><row><cell>MixHop</cell><cell cols="10">77.84±7.73 75.88±4.90 32.22±2.34 43.80±1.48 60.50±2.53 73.51±6.34 65.59±0.34 76.26±1.33 85.31±0.61 87.61±0.85</cell></row><row><cell cols="11">GraphSAGE+JK 83.78±2.21 81.96±4.96 34.28±1.01 40.85±1.29 58.11±1.97 75.68±4.03 65.31±0.58 76.05±1.37 88.34±0.62 85.96±0.83</cell></row><row><cell>Cheby+JK</cell><cell cols="10">78.38±6.37 82.55±4.57 35.14±1.37 45.03±1.73 63.79±2.27 74.59±7.87 66.87±0.29 74.98±1.18 89.07±0.30 85.49±1.27</cell></row><row><cell>GCN+JK</cell><cell cols="10">66.49±6.64 74.31±6.43 34.18±0.85 40.45±1.61 63.42±2.00 64.59±8.68 66.72±0.61 74.51±1.75 88.41±0.45 85.79±0.92</cell></row><row><cell>GCN</cell><cell cols="10">59.46±5.25 59.80±6.99 30.26±0.79 36.89±1.34 59.82±2.58 57.03±4.67 68.39±0.32 76.68±1.64 87.38±0.66 87.28±1.26</cell></row><row><cell>GAT</cell><cell cols="10">58.38±4.45 55.29±8.71 26.28±1.73 30.62±2.11 54.69±1.95 58.92±3.32 59.81±0.92 75.46±1.72 84.68±0.44 82.68±1.80</cell></row><row><cell>GEOM-GCN*</cell><cell>67.57</cell><cell>64.12</cell><cell>31.63</cell><cell>38.14</cell><cell>60.90</cell><cell>60.81</cell><cell>N/A</cell><cell>77.99</cell><cell>90.05</cell><cell>85.27</cell></row><row><cell>MLP</cell><cell cols="10">81.89±4.78 85.29±3.61 35.76±0.98 29.68±1.81 46.36±2.52 81.08±6.37 58.76±0.50 72.41±2.18 86.65±0.35 74.75±2.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>1: Major symbols and definitions.</figDesc><table><row><cell cols="2">Symbols</cell><cell>Definitions</cell></row><row><cell cols="2">G = (V, E)</cell><cell>graph G with nodeset V, edgeset E</cell></row><row><cell cols="2">A</cell><cell>n × n adjacency matrix of G</cell></row><row><cell cols="2">X</cell><cell>n × F node feature matrix of G</cell></row><row><cell cols="2">x v</cell><cell>F -dimensional feature vector for node v</cell></row><row><cell cols="2">L</cell><cell>unnormalized graph Laplacian matrix</cell></row><row><cell cols="2">Y</cell><cell>set of class labels</cell></row><row><cell cols="2">y v</cell><cell>class label for node v ∈ V</cell></row><row><cell cols="2">y</cell><cell>n-dimensional vector of class labels (for all the nodes)</cell></row><row><cell cols="3">T V = {(v 1 , y 1 ), (v 2 , y 2 ), ...} training data for semi-supervised node classification</cell></row><row><cell cols="2">N (v) N (v) N i (v), Ni (v)</cell><cell>general type of neighbors of node v in graph G general type of neighbors of node v in G without self-loops (i.e., excluding v) i-hop/step neighbors of node v in G (at exactly distance i) maybe-with/without</cell></row><row><cell></cell><cell></cell><cell>self-loops, resp.</cell></row><row><cell cols="2">E 2</cell><cell>set of pairs of nodes (u, v) with shortest distance between them being 2</cell></row><row><cell cols="2">d, d max</cell><cell>node degree, and maximum node degree across all nodes v ∈ V, resp.</cell></row><row><cell cols="2">h</cell><cell>edge homophily ratio</cell></row><row><cell cols="2">H</cell><cell>class compatibility matrix</cell></row><row><cell>r</cell><cell>(k) v</cell><cell>node representations learned in GNN model at round / layer k</cell></row><row><cell cols="2">K</cell><cell>the number of rounds in the neighborhood aggregation stage</cell></row><row><cell cols="2">W</cell><cell>learnable weight matrix for GNN model</cell></row><row><cell cols="2">σ</cell><cell>non-linear activation function</cell></row><row><cell></cell><cell></cell><cell>vector concatenation operator</cell></row><row><cell cols="2">AGGR</cell><cell>function that aggregates node feature representations within a neighborhood</cell></row><row><cell cols="2">COMBINE</cell><cell>function that combines feature representations from different neighborhoods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1) , in which Āi corresponds to the adjacency matrix of neighborhood Ni . The two sparse matrix-matrix multiplications in the concatenation takeO |E|2 (k−1) p + |E 2 |2 (k−1) p , where |E 2 | = 1 2 v∈V | N2 (v)|. Over K rounds of embedding, the complexity becomes O 2 K (|E| + |E 2 |)p .Adding all the big-O terms above, we have the overall time complexity for stages (S1) and (S2) of H 2 GCN as:O nnz(X) p + |E|d max + 2 K (|E| + |E 2 |)p ,where K is usually a small number (e.g., 2). For small values of K, the complexity becomes O (|E|d max + (nnz(X) + |E| + |E 2 |)p).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table G .</head><label>G</label><figDesc>1: Statistics for Synthetic Datasets</figDesc><table><row><cell cols="2">Benchmark Name syn-cora</cell><cell>syn-products</cell></row><row><cell># Nodes</cell><cell>1490</cell><cell>10000</cell></row><row><cell># Edges</cell><cell>2965 to 2968</cell><cell>59640 to 59648</cell></row><row><cell># Classes</cell><cell>5</cell><cell>10</cell></row><row><cell>Features</cell><cell cols="2">cora [30, 39] ogbn-products [13]</cell></row><row><cell>Homophily h</cell><cell>[0, 0.1, . . . , 1]</cell><cell>[0, 0.1, . . . , 1]</cell></row><row><cell>Degree Range</cell><cell>1 to 94</cell><cell>1 to 336</cell></row><row><cell>Average Degree</cell><cell>3.98</cell><cell>11.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table G .</head><label>G</label><figDesc>2: syn-cora (Fig.2a): Mean accuracy and standard deviation per method and synthetic dataset (with different homophily ratio h). Best method highlighted in gray.GCN-1 77.40±0.89 76.82±1.30 73.38±0.95 75.26±0.56 75.66±2.19 80.22±1.35 H 2 GCN-2 77.85±1.63 76.87±0.43 74.27±1.30 74.41±0.43 76.33±1.35 79.60±0.48 GraphSAGE 75.97±1.94 72.89±2.42 70.56±1.42 71.81±0.67 72.04±1.68 76.55±0.81 GCN-Cheby 74.23±0.54 68.10±1.75 64.70±1.17 66.71±1.63 68.14±1.56 73.33±2.05 MixHop 62.64±1.16 58.93±2.84 60.89±1.20 65.73±0.41 67.87±4.01 70.11±0.34 GCN 33.65±1.68 37.14±4.60 42.82±1.89 51.10±0.77 56.91±2.56 66.22±1.04 GAT 30.16±1.32 33.11±1.20 39.11±0.28 48.81±1.57 55.35±2.35 64.52±0.47 MLP 72.75±1.51 74.85±0.76 74.05±0.69 73.78±1.14 73.33±0.34 74.81±1.90 GCN-1 83.62±0.82 88.14±0.31 91.63±0.77 95.53±0.61 99.06±0.27 H 2 GCN-2 84.43±1.89 88.28±0.66 92.39±1.34 95.97±0.59 100.00±0.00 GraphSAGE 81.25±1.04 85.06±0.51 90.78±1.02 95.08±1.16 99.87±0.00 GCN-Cheby 78.88±0.21 84.92±1.03 90.92±1.62 95.97±1.07 100.00±0.00 MixHop 79.78±1.92 84.43±0.94 91.90±2.02 96.82±0.08 100.00±0.00 GCN 77.32±1.17 84.52±0.54 91.23±1.29 96.11±0.82 100.00±0.00 GAT 76.29±1.83 84.03±0.97 90.92±1.51 95.88±0.21 100.00±0.00 MLP 73.42±1.07 71.72±0.62 72.26±1.53 72.53±2.77 73.65±0.41 Table G.3: syn-products (Fig. 2b): Mean accuracy and standard deviation per method and synthetic dataset (with different homophily ratio h). Best method highlighted in gray. GCN-1 82.06±0.24 78.39±1.56 79.37±0.21 81.10±0.22 84.25±1.08 88.15±0.28 H 2 GCN-2 83.37±0.38 80.03±0.84 81.09±0.41 82.79±0.49 86.73±0.66 90.75±0.43 GraphSAGE 77.66±0.72 74.04±1.07 75.29±0.82 76.39±0.24 80.49±0.96 84.51±0.51 GCN-Cheby 84.35±0.62 76.95±0.30 77.07±0.49 78.43±0.73 85.09±0.29 89.66±0.53 MixHop 15.39±1.38 11.91±1.17 14.03±1.70 14.92±0.56 17.04±0.40 18.90±1.49 GCN 56.44±0.59 51.51±0.56 54.97±0.66 64.90±0.90 76.25±0.04 86.43±0.58 GAT 27.39±2.47 21.49±2.25 37.27±3.99 44.46±0.68 51.86±8.52 69.42±5.30 MLP 68.63±0.58 68.20±1.20 68.85±0.73 68.65±0.18 68.37±0.85 68.70±0.61 GCN-1 92.39±0.06 95.69±0.19 98.09±0.23 99.63±0.13 99.93±0.01 H 2 GCN-2 94.81±0.27 97.67±0.18 99.13±0.05 99.89±0.08 99.99±0.01 GraphSAGE 89.51±0.29 93.61±0.52 96.66±0.19 98.78±0.11 99.63±0.08 GCN-Cheby 94.99±0.34 98.26±0.11 99.58±0.11 99.93±0.06 100.00±0.00 MixHop 19.47±5.21 21.15±2.28 24.16±3.19 23.21±5.30 25.09±5.08</figDesc><table><row><cell>h</cell><cell>0.00</cell><cell>0.10</cell><cell>0.20</cell><cell>0.30</cell><cell>0.40</cell><cell>0.50</cell></row><row><cell>H 2 h</cell><cell>0.60</cell><cell>0.70</cell><cell>0.80</cell><cell>0.90</cell><cell>1.00</cell><cell></cell></row><row><cell>H 2 h</cell><cell>0.00</cell><cell>0.10</cell><cell>0.20</cell><cell>0.30</cell><cell>0.40</cell><cell>0.50</cell></row><row><cell>H 2 h</cell><cell>0.60</cell><cell>0.70</cell><cell>0.80</cell><cell>0.90</cell><cell>1.00</cell><cell></cell></row><row><cell>H 2 GCN</cell><cell cols="5">93.35±0.28 97.61±0.24 99.33±0.08 99.93±0.01 99.99±0.01</cell><cell></cell></row><row><cell>GAT</cell><cell cols="5">85.36±3.67 93.52±1.93 98.84±0.12 99.87±0.06 99.98±0.02</cell><cell></cell></row><row><cell>MLP</cell><cell cols="5">68.21±0.93 68.72±1.11 68.10±0.54 68.36±1.42 69.08±1.03</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">These models consider self-loops, which turn each ego also into a neighbor, and thus mix the ego-and neighbor-representations. E.g., GCN and MixHop operate on the symmetric normalized adjacency matrix augmented with self-loops: Â = D− 1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">(A + I) D−1  2  , where I is the identity and D the degree matrix of A + I.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><ref type="bibr" target="#b25">[26]</ref> claims that the ratios are 60%/20%/20%, which is different from the actual data splits shared on GitHub.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">* hidden1: a ∈ {16, 32, 64} * dropout: 0.6 * weight_decay: b ∈ {1e-5, 5e-4} * max_degree: 2 * early_stopping: 40 -Set 2: * hidden1: a ∈ {16, 32, 64} * dropout: 0.5 * weight_decay: 5e-4 * max_degree: 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.• GraphSAGE<ref type="bibr" target="#b10">[11]</ref>:-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs: 500</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank the reviewers for their constructive feedback. This material is based upon work supported by the National Science Foundation under CAREER Grant No. IIS 1845491 and 1452425, Army Young Investigator Award No. W911NF1810397, an Adobe Digital Experience research faculty award, an Amazon faculty award, a Google faculty award, and AWS Cloud Credits for Research. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or other funding parties.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The feature embedding stage (S1) takes O(nnz(X)p) to calculate σ(XW e ) where W e ∈ R F ×p is a learnable dense weight matrix, and X ∈ R n×F is the node feature matrix.</p><p>In the neighborhood aggregation stage (S2), we perform the following computations:</p><p>• Calculation of higher-order neighborhoods. Given that A is sparse, we can obtain the 2-hop neighborhood by calculating A 2 in O (|E|d max ), where |E| is the number of edges in G (equal to the number of non-zeroes in A), and d max is the maximum degree across all nodes v ∈ V (which is equal to the maximum number of non-zeroes in any row of A).</p><p>• Feature Aggregation. We begin with a p-dimensional embedding for each node after feature embedding. In round k, since we are using the neighborhoods N1 and N2 , we have an em-</p><p>We report the performance with a = 64, b = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• MixHop [1]:</head><p>-hidden_dims_csv: a ∈ {64, 192} -adj_pows: 0, 1, 2</p><p>We report the performance with a = 192. • GAT <ref type="bibr" target="#b35">[36]</ref>:</p><p>-hid_units: a ∈ {8, 16, 32, 64} For syn-products, we test the following command-line arguments for each baseline method:</p><p>•</p><p>-Dimension of Feature Embedding p: 64 -Non-linearity Function σ: ReLU -Dropout Rate: a ∈ {0, 0.5} We report the best performance, for a = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• GCN [17]:</head><p>-hidden1: 64 -early_stopping: a ∈ {40, 100, 200} epochs: 2000</p><p>In addition, we disabled the default feature normalization in the official implementation, as the feature vectors in this benchmark have already been normalized, and we found the default normalization method hurts the performance significantly. We report the best performance, for a = 40. • GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>:</p><p>-hidden1: 64 -max_degree: 2 -early_stopping: 40 epochs: 2000 We also disabled the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type="bibr" target="#b10">[11]</ref>:</p><p>-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs: 500 We report the performance with a = 128, b = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• MixHop [1]:</head><p>-hidden_dims_csv: a ∈ {64, 192} -adj_pows: 0, 1, 2</p><p>We report the performance with a = 192. • GAT <ref type="bibr" target="#b35">[36]</ref>:</p><p>-hid_units: 8</p><p>We also disabled the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type="bibr" target="#b10">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>• MixHop <ref type="bibr" target="#b0">[1]</ref>:</p><p>-hidden_dims_csv: {64, 192} -adj_pows: 0, 1, 2</p><p>• GAT <ref type="bibr" target="#b35">[36]</ref>:</p><p>-hid_units: 8</p><p>• MLP -Dimension of Feature Embedding p: 64 -Non-linearity Function σ: {ReLU, None} -Dropout Rate: {0, 0.5}</p><p>For GCN+JK, GCN-Cheby+JK and GraphSAGE+JK, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type="bibr" target="#b37">[38]</ref> without changing the number of layers or other hyperparameters for the base method.</p><p>Cora Full Benchmark Tuning The number of class labels in Cora-Full are many more compared to the other benchmarks (Table <ref type="table">5</ref>), which leads to a significant increase in the size of training parameters for each model. Therefore, we need to re-tune the hyperparameters, especially the regularization weights and learning rates, in order to get reasonable performance. We test the following command-line arguments for each baseline method:</p><p>•</p><p>-Dimension of Feature Embedding p: 64 -Non-linearity Function σ: {ReLU, None} -Dropout Rate: {0, 0.5} -L2 Regularization Weight: {1e-5, 1e-6}</p><p>• GCN <ref type="bibr" target="#b16">[17]</ref>: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-Order Graph Convolution Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monophily in social networks introduces similarity among friends-of-friends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="284" to="290" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<ptr target="http://view.ncbi.nlm.nih.gov/pubmed/10521342" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999-10">1999. October 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1ZdKJ-0W" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupled smoothing on graphs</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><forename type="middle">M</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference</title>
				<meeting>the 2019 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zoobp: Belief propagation for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Dhivya</forename><surname>Eswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disha</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="625" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with heterophily</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Gatterbauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3100</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linearized and Single-Pass Belief Propagation</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Gatterbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring and Improving the Use of Graph Information in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative classification in relational data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI. Workshop on Learning Statistical Models from Relational</title>
				<meeting>AAAI. Workshop on Learning Statistical Models from Relational</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Residual Correlation in Graph Neural Network Regression</title>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Austion R Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Fariba</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Génois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Strohmaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00150</idno>
		<title level="m">Visibility of minorities in social networks</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms</title>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai-You</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsing-Kuo Kenneth</forename><surname>Horng Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</title>
				<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="245" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Link-Based Classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML)</title>
				<meeting>the Twentieth International Conference on International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Birds of a Feather: Homophily in Social Networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Networks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NetProbe: A Fast and Scalable System for Fraud Detection in Online Auction Networks</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning for relational networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining. SIAM</title>
				<meeting>the 2017 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1e2agrFvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GMNN: Graph Markov Neural Networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On Proximity and Structural Role-based Embeddings in Networks: Misconceptions, Techniques, and Applications</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
		<title level="m">Multi-scale attributed node embedding</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Agreement Models for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Otilia</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8713" to="8723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mining Heterogeneous Information Networks: Principles and Methodologies</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML</title>
				<meeting>the 35th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding Belief Propagation and its Generalizations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exploring Artificial Intelligence in the New Millennium</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: a comprehensive review</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Social Networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Learning on Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph Neural Networks with Heterophily</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with graphs</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1104523" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
