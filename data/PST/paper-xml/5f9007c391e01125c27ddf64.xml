<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
							<email>lei.cai@wsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong@virginia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<email>jiewangx@ustc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Washington State University</orgName>
								<address>
									<postCode>99164</postCode>
									<settlement>Pullman</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Data Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<postCode>22904</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic Engineering and Information Science</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line Graph Neural Networks for Link Prediction</head><p>Lei Cai, Jundong Li, Jie Wang, Shuiwang Ji Senior Member, IEEE Abstract-We consider the graph link prediction task, which is a classic graph analytical problem with many real-world applications. With the advances of deep learning, current link prediction methods commonly compute features from subgraphs centered at two neighboring nodes and use the features to predict the label of the link between these two nodes. In this formalism, a link prediction problem is converted to a graph classification task. In order to extract fixed-size features for classification, graph pooling layers are necessary in the deep learning model, thereby incurring information loss. To overcome this key limitation, we propose to seek a radically different and novel path by making use of the line graphs in graph theory. In particular, each node in a line graph corresponds to a unique edge in the original graph. Therefore, link prediction problems in the original graph can be equivalently solved as a node classification problem in its corresponding line graph, instead of a graph classification task. Experimental results on fourteen datasets from different applications demonstrate that our proposed method consistently outperforms the state-of-the-art methods, while it has fewer parameters and high training efficiency.</p><p>Index Terms-Deep learning, link prediction, graph neural networks, line graphs.</p><p>L INK prediction models are used to learn the distribution of links in graphs and predict the existence of potential links <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In many real-world applications, the input data is represented as graphs, and link prediction models can be applied to tasks like friend recommendation in social networks <ref type="bibr" target="#b4">[5]</ref>, product recommendation in e-commerce <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, knowledge graph completion <ref type="bibr" target="#b8">[9]</ref>, protein interaction analysis <ref type="bibr" target="#b9">[10]</ref>, and metabolic network reconstruction <ref type="bibr" target="#b10">[11]</ref>.</p><p>To solve the link prediction problem, various heuristic methods were proposed to measure the similarity between two target nodes and predict the existence of link <ref type="bibr" target="#b11">[12]</ref>. However, the heuristic functions in these methods are often manually designed for a specific network, limiting their applicability to diverse areas. For example, the number of common neighbors <ref type="bibr" target="#b4">[5]</ref> is employed as a first-order heuristic function to predict the potential friendship relations in social networks and achieves satisfactory performance. However, this heuristic may not work well on protein-protein interaction networks, since two proteins sharing many common neighbors may have a low probability of interacting <ref type="bibr" target="#b12">[13]</ref>.</p><p>Many heuristic methods have been proposed to solve graph link prediction problems from different areas. However, there still exist challenges to select heuristic functions given a new network. To tackle these challenges, the link prediction model based on graph neural networks (SEAL) was proposed to learn heuristic functions from hâˆ’hop neighborhood automatically <ref type="bibr" target="#b13">[14]</ref>. This method achieved the state-of-the-art performance on a variety of graphs. The SEAL model extracts an hhop enclosing subgraph centered on the two target nodes and predicts the existence of link based on the topology of enclosing subgraph. Therefore, the link prediction task is converted to the graph classification problem, where the model takes the enclosing subgraph as inputs and predicts the existence of link between them. Normally, graph neural networks <ref type="bibr" target="#b14">[15]</ref> are employed to learn features to represent the topology of the subgraph. Therefore, graph pooling layers <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> are required to compute a fixed-size feature vector from the whole graph while some information may be lost in this operation. For example, in sort pooling operations <ref type="bibr" target="#b14">[15]</ref>, only partial nodes can be selected to represent the graph. In addition, a graph neural network with pooling layers often requires more training time to converge.</p><p>Although the SEAL works well in many types of graphs, it still has some limitations, due to the usage of pooling operations in the graph neural network. To solve the information loss in pooling layers, we propose to learn the features of the target link directly instead of extracting features from the whole enclosing subgraph. Compared with extracting features from the whole graph, learning node embedding is more effective. Graph convolution layers <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> have shown promising performance for learning node embeddings. However, graph convolution layers are not effective enough to learn edge embeddings from graphs. To address this issue, we propose to convert the original enclosing subgraph into a corresponding line graph. Each node in the line graph has a unique corresponding edge in the original graph. In addition, the topology information can be well preserved during the transformation. Therefore, graph convolution layers can be directly applied to learn the node embeddings in the line graph. The node embeddings in the line graph are used as features for the edges in the original graph to predict the existence of links. Therefore, the link prediction task can be regarded as the node classification problem in our proposed framework. Our contributions can be summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line Graph Transformation</head><p>Fig. <ref type="figure">1</ref>. Illustration of our proposed model based on line graph neural networks. The two target nodes in the graph are marked with double circles. To predict the existence of the link, an h-hop enclosing subgraph centered on two target nodes is extracted. A node labeling function is employed to assign the label for each node to represent the structural importance to the target link. To learn the feature of the target link, we transform the enclosing subgraph into a corresponding line graph. The graph convolution networks are used to learn the feature that is employed to predict the existence of link. areas. Our proposed method can achieve promising performance on different datasets and outperform all baseline methods, including the previous state-of-the-art model. 4) Our proposed method can achieve promising performance only with graph convolution layers, and thus requires fewer parameters. In addition, the neural network consisting of graph convolution layers converges significantly faster than the state-of-the-art model.</p><p>I. RELATED WORK Link prediction models can be grouped into three categories -heuristic methods, embedding methods, and deep learning methods.</p><p>Heuristic Methods: The key idea of heuristic methods is to compute the similarity score from the neighborhood of two target nodes. Based on the maximum hop of neighbors used in the computation procedure, heuristic methods can be categorized into three groups, including first-order, secondorder, and high-order heuristics. Common neighbors and preferential attachment <ref type="bibr" target="#b11">[12]</ref> are typical first-order heuristics since only one-hop neighbors are employed to compute the similarity. Second-order heuristic methods that involve two-hop neighbors include Adamic-Adar <ref type="bibr" target="#b4">[5]</ref> and resource allocation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In addition, high-order heuristics, including Katz <ref type="bibr" target="#b23">[24]</ref>, rooted PageRank <ref type="bibr" target="#b24">[25]</ref>, and SimRank <ref type="bibr" target="#b25">[26]</ref> were proposed to compute the similarity score between a pair of nodes using the whole graph. High-order heuristic methods can often achieve better performance than low-order heuristics but require more computation cost. Since many heuristic methods were proposed to handle different graphs, selecting a favorable heuristic method becomes a challenging problem.</p><p>Embedding Methods: The similarity between two target nodes can also be calculated based on node embeddings <ref type="bibr" target="#b26">[27]</ref>. Therefore, embedding methods that can learn the features of nodes from graph topology were also employed to solve the link prediction task, and typical methods along this line include matrix factorization <ref type="bibr" target="#b5">[6]</ref> and stochastic block <ref type="bibr" target="#b9">[10]</ref> etc. Recently, inspired by world embedding methods in natural language processing tasks, recent advances such as deepwalk <ref type="bibr" target="#b27">[28]</ref>, LINE <ref type="bibr" target="#b28">[29]</ref>, and node2vec <ref type="bibr" target="#b29">[30]</ref> were proposed to learn node embedding via the skip-gram method. Deepwalk generates random walks for each vertex with a given length and picks the next visited node uniformly from the neighbors of the current node. Later on, the skip-gram method is employed to learn node embeddings from the generated node sequence. The node embedding methods can learn informative features from the graph and thus achieve satisfactory performance for the link prediction task. However, the performance of link node embedding methods can be affected if the graph becomes very sparse.</p><p>Deep Learning: To overcome the limitations of heuristic methods, deep learning based methods were proposed to learn the distribution of links from the graph automatically <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Weisfeiler-Lehman Neural Machine was proposed to predict the existence of a link using a fully-connected neural network based on a fixed-size enclosing subgraph centered on the two target nodes <ref type="bibr" target="#b30">[31]</ref>. To predict the existence of a link from a general enclosing subgraph, SEAL <ref type="bibr" target="#b13">[14]</ref> converts the link prediction task to a graph classification problem and solve it using graph neural networks. Due to the promising learning ability of graph neural networks, the SEAL model achieves the state-of-the-art performance for the link prediction problem. Later on, a multi-scale link model was also proposed to extend SEAL to achieve better performance on plain graphs <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>In the link prediction task, we are often given a network represented as an undirected graph G = (V, E) which consists of a set of vertices V = {v 1 , v 2 , ..., v n } and a set of links E âŠ† V Ã— V . The graph can also be represented by the adjacency matrix A. If there exists a link between vertex i and j, then A i,j = 1 and A i,j = 0 otherwise. The goal of link prediction is to predict potential or missing links that may appear in a foreseeable future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Framework</head><p>Deep learning based link prediction models were proposed to learn the link distribution from the existing links and determine whether a link exists between two target nodes in the graph. For example, when we predict if there exists a link between two users in a social network, the number of mutual friends is commonly considered as a main criterion. If two users share many mutual friends, they are more likely to be connected. In this sense, if the 1-hop subgraph induced from two target nodes are densely connected, we will have higher chances to observe a link between them. Considering the variation of networks from different areas, deep learning based methods were proposed to learn the topology feature of subgraphs automatically and predict the existence of links <ref type="bibr" target="#b13">[14]</ref>. In particular, the deep learning based link prediction models generally consist of the following three components:</p><p>1) Enclosing subgraph extraction: The existence of a potential link can be determined by the topology of a local enclosing subgraph centered on two target nodes. To seek a balance between computation cost and prediction performance, an hâˆ’hop enclosing subgraph is extracted for learning features and predicting the existence of potential links. 2) Node labeling: Given an enclosing subgraph, we are required to identify the role of each node in the graph before learning features and predict the existence of the link. That is, we need to identify the target nodes and mark the structural importance of other nodes. A favorable labeling function is of great importance for the further feature learning procedure. 3) Feature learning and link prediction: The output of node labeling function can be used as the attribute of each node in the graph. The attribute can indicate the structural importance of the link to be predicted. Graph neural networks are commonly employed to learn features from the given enclosing subgraph, which can be further used to predict the existence of a link. In this work, we propose line graph neural networks for the link prediction task. Our proposed model can be illustrated in Figure <ref type="figure">1</ref>. Following the general framework of deep learning based link prediction models, we extract an h-hop enclosing subgraph centered on two target nodes and assign each node with a label that can represent the structural importance to the target link. The key contribution of our proposed method is the feature learning component. In the previous state-ofthe-art model, graph convolution and graph pooling layers are employed to obtain a fixed-size feature vector to predict the existence of the link considering the scale variation of different graphs. Since this graph pooling layer is employed in the state-of-the-art model, only part of graph information can be preserved for further prediction. To overcome the limitations of the SEAL method, we propose to convert the enclosing subgraph to a line graph where each node corresponds to a unique link in the original graph. The feature of the link can be learned directly using the entire input from line graph representation. Thus the proposed method can greatly improve the performance of the link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Line Graph Neural Networks</head><p>Line Graph Space Transformation In order to predict the existence of a link, graph neural networks are employed to learn features from a given enclosing subgraph G h v1,v2 , where G h v1,v2 is an h-hop enclosing subgraph centered on two target nodes v 1 and v 2 , and each node in the enclosing subgraph is associated with a label that can indicate the structural importance to the target link. Different enclosing subgraphs commonly contain a different number of nodes. To extract a fixed-size feature vector for the further prediction, we will lose some information during the procedure. To overcome this challenge, we propose to convert the enclosing subgraph to the line graph, which represents the adjacencies between edges of the original graph. Thus, the feature of the link to be predicted can be learned directly in the line graph representation using graph convolution neural networks. The line graph L(G) of a given undirected graph G is proposed to represent the adjacencies between edges of G <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The definition of line graph L(G) can be defined as follows.</p><p>Definition 1: The edges in the original graph G are considered as nodes in the line graph L(G). Two nodes in L(G) are connected if and only if the two corresponding links share the same node. An example of the line graph transformation procedure is illustrated in Figure <ref type="figure" target="#fig_0">2</ref> This property guarantees that learning features in the line graph space will not increase the computation complexity significantly. In additional, converting a graph G to line graph L(G) only costs linear time complexity <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>Node Label Transformation An enclosing subgraph can be converted into the corresponding line graph through transformation. However, this procedure can only transform the topology of a given graph. Each node in the enclosing subgraph also contains a label l âˆˆ R generated by labeling function as node attributes that can represent the structural importance. During the transformation procedure, edges in the original graph are represented as nodes in the line graph. The label l is only assigned for nodes in the original graph. To transfer the node label from the original graph directly, a transformation function is required to convert the node label to edge attribute. Thus the edge attribute can be assigned directly as the node attribute in the line graph. In this work, we propose to generate the edge attribute from the node label through the following function:</p><formula xml:id="formula_0">l (v1,v2) = concate(min(f l (v 1 ), f l (v 2 )), max(f l (v 1 ), f l (v 2 ))),</formula><p>(1) where f l (â€¢) is the node labeling function, v 1 and v 2 are the two end nodes of the edge, and concate(â€¢) represents the concatenation operation for the two inputs. Since we only consider undirected graph link prediction in this work, the attribute of edge (v1, v2) and (v2, v1) should be the same. It is easy to prove that the edge attribute generated by equation ( <ref type="formula">1</ref>) is consistent when switching the end nodes. In addition, the structural importance information of the node can be well preserved in the function.</p><p>The proposed method in equation ( <ref type="formula">1</ref>) can well address the edge attribute transformation in plain graphs. In some cases, graphs are commonly provided with node attributes. For example, in citation networks, the node attribute describing a summary of the paper can be provided in the graph. For attributed graphs, node attributes also play an important role in the link prediction task. Therefore, the edge attribute transformation function should be generalized to deal with attributed graph. Following the edge attribute transformation function in equation ( <ref type="formula">1</ref>), we can concatenate the original node attribute with the node label as the edge attribute. But the edge attribute will not be consistent when we switch the order of two end nodes in the undirected graph. To overcome this limitation, we propose to deal with the original node attribute and node label in different ways by:</p><formula xml:id="formula_1">l (v1,v2) = concate(min(f l (v 1 ), f l (v 2 )), max(f l (v 1 ), f l (v 2 )), X v1 +X v2 ),<label>(2)</label></formula><p>where X v1 and X v2 are the original attribute of node v 1 and v 2 . We propose to combine the node attribute using summation operation, which can guarantee the invariance of edge attributes when switching the end nodes. The generated edge attribute l (v1,v2) can be used as the node attribute directly in the line graph. Therefore, the link prediction task is converted to a node classification problem which can be solved by graph convolution neural networks.</p><p>Feature Learning by Graph Neural Networks With recent progress in graph neural networks, learning the graph feature becomes a favorable solution that has been explored in various graph analytical tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In this work, we employ graph convolution neural networks to learn the node embedding in the line graph, which can represent an edge in the original graph. Thus, the node embedding in the line graph can be used to predict whether a potential link is likely to exist in the network.</p><p>Given a line graph representation of the enclosing subgraph L(G h v1,v2 ), the node embedding of (v i , v j ) in the k-th layer of the graph convolution neural network is indicated as</p><formula xml:id="formula_2">Z (k) (vi,vj ) .</formula><p>Then the embedding of (v i , v j ) in the (k + 1)-th layer is given by:</p><formula xml:id="formula_3">Z (k+1) (vi,vj ) = (Z (k) (vi,vj ) + Î² dâˆˆN (v i ,v j ) Z (k) d )W (k) ,<label>(3)</label></formula><p>where N (vi,vj ) is the set of neighbors of node (v i , v j ) in the line graph, W (k) is the weight matrix for the k-th layer, Î² is a normalization coefficient. The input for the first layer of graph convolution neural network is set to node attribute in the line graph as Z 0 (vi,vj ) = l (v1,v2) . We then consider the link prediction task as a binary classification problem and train the neural network by minimizing the cross-entropy loss for all potential links as:</p><formula xml:id="formula_4">L CE = âˆ’ lâˆˆLt (y l log(p l ) + (1 âˆ’ y l ) log(1 âˆ’ p l )), (4)</formula><p>where L t is the set of target links to be predicted, p l is the probability that the link l exists in the graph, and y l âˆˆ {0, 1} is the label of a target link that indicating whether the link exists or not.</p><p>Connection with Learning on Original Graphs The key idea of our proposed method is to learn edge features from the enclosing subgraph and predict the existence of edge using the features. In this work, the feature of edge e = (v 1 , v 2 ) is learned based on attributes of two end nodes as:</p><formula xml:id="formula_5">f e = g(f l (v1), f l (v2)),<label>(5)</label></formula><p>where f e is the edge feature, g(â€¢) is the graph neural network function. Although graph convolutional layers are performed on the line graph, it still has connections with the same operation on the original graph. We use the first layer graph convolution layer as an example to illustrate this relationship. We reformulate the equation (3) as:</p><formula xml:id="formula_6">Z 1 (vi,vj ) = (l (v1,v2) + Î² d1âˆˆNv1 d2âˆˆN d1 l (d1,d2) + Î² d3âˆˆNv2 d4âˆˆN d3 l (d3,d4) )W (0) .(<label>6</label></formula><formula xml:id="formula_7">)</formula><p>The graph convolution operation learns embedding for each node by aggregating node embedding from its 1âˆ’hop neighbors. It can be seen from equation ( <ref type="formula" target="#formula_6">6</ref>) that the graph convolution on the line graph can aggregate the node embedding from 2âˆ’hop neighbors. In the line graph transformation procedure, each node attribute is derived from two corresponding node attributes. That is, the attribute of each node in the line graph contains attributes from two nodes in the original graph. Therefore, aggregating information from 1 âˆ’ hop neighbors is equivalent to performing the same operation on 2âˆ’hop neighbors. It also shows that learning node embedding through graph convolution in the line graph is more efficient than that in the original graph in terms of neighbor embedding aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Proposed Algorithm</head><p>In this section, we provide a detailed description of the three components for our proposed framework. There is no strict restriction for the three steps. The given enclosing subgraph extraction and graph topology labeling function in this section can work well for most networks.</p><p>Enclosing Subgraph Extraction The existence of the link between two nodes can be determined by the graph topology centered on them. In general, we can achieve better performance when more topology information is involved. However, it will incur more computation cost. To seek a balance between performance and computation cost, we predict the existence of the link between node v i and v j using 2âˆ’hop enclosing subgraph as:</p><formula xml:id="formula_8">G 2 (vi,vj ) = {v| min(d(v, v i ), d(v, v j ) â‰¤ 2)},<label>(7)</label></formula><p>where d(v, v i ) is the shortest path/geodesic distance between v and v i . Node Labeling Given an enclosing subgraph, we only know the topology of the graph. Before we learn features of the target link, we need to identify the role of each node in the graph through a labeling function. The node labeling function must satisfy the following criteria: (1) Identifying the two target nodes. ( <ref type="formula" target="#formula_1">2</ref>) Provide the structural importance of each node to the target nodes. In this work, we employ an effective node labeling function proposed by <ref type="bibr" target="#b13">[14]</ref> as: <ref type="figure" target="#fig_0">and (d s %2</ref>) are the integer quotient and remainder of d divided by 2, respectively. In addition, the two target nodes v 1 and v 2 are assigned with label 1 as f l (v 1 ) = 1 and f l (v 2 ) = 1. For any node v satisfying d(v, v 1 ) = âˆž or d(v, v 2 ) = âˆž, it will be assigned with label 0 as f l (v) = 0. The node labeling function provides a label f l (â€¢) âˆˆ R. In practice, the node label is represented as a onehot vector. As discussed above, the edge is represented as an order invariant pair. The edge feature pair is represented as a concatenation of two one-hot vectors. Algorithm 1 shows the link prediction procedure using our proposed framework. </p><formula xml:id="formula_9">f l (v) = 1+min(d(v, v 1 ), d(v, v 2 ))+(d s /2)[(d s /2)+(d s )%2âˆ’1], (<label>8</label></formula><formula xml:id="formula_10">) where d s = d(v, v 1 ) + d(v, v 2 ), (d s /2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, we evaluate our proposed method on 14 different datasets for the link prediction task. Two evaluation metrics, including area under the curve (AUC) and average precision (AP) are employed in this work to measure the performance of different models. The code and dataset used in this work will be available online after the paper is published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datatsets and Baseline Models</head><p>In this work, we perform our proposed line graph link prediction (LGLP) model on 14 different datasets, including BUP, C.ele, HPD, YST, SMG, NSC, KHN, GRQ, LDG, ZWL, USAir, EML, Power, and ADV <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. To demonstrate that our proposed method can work well in different areas, 14 datasets are collected from 6 areas. In addition, graphs in  <ref type="table" target="#tab_2">I</ref>.</p><p>In this work, we compare our proposed method with three high-order heuristic methods including Katz <ref type="bibr" target="#b23">[24]</ref>, PageRank (PR) <ref type="bibr" target="#b24">[25]</ref>, SimRank (SR) <ref type="bibr" target="#b25">[26]</ref>. In addition, graph embedding method node2vec (N2V) <ref type="bibr" target="#b38">[39]</ref> and the state-of-the-art method SEAL <ref type="bibr" target="#b13">[14]</ref> are selected as baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>To assess the performance of our proposed method, we randomly select 50% of existing links as positive training samples, and the rest are used as positive test samples. In addition, the same number of non-existed links are randomly selected from the graph as negative samples for training and testing. To demonstrate the effectiveness of our proposed method with a different number of training samples, we also select 80% training links for the experiments.</p><p>The parameters of baseline methods are tuned to achieve the best performance on datasets. The damping factor in Katz method is set to 0.001. The damping factor in PageRank is set to 0.85. The constant factor in the SimRank is set to 0.8. The dimension of node embedding for node2vec is set to 128.</p><p>For the SEAL framework, we employ the same setting as the original paper <ref type="bibr" target="#b13">[14]</ref>. The 2 âˆ’ hop enclosing subgraph is extracted for the SEAL framework, and the labeling function is the same as equation ( <ref type="formula" target="#formula_9">8</ref>) in this work. Three graph convolution layers are employed to compute node embeddings, and the sort pooling <ref type="bibr" target="#b14">[15]</ref> is used to generate a fixed-size feature vector for the enclosing subgraph. The output feature map for three graph convolution layers is set to 32. The ratio of the sort pooling layer is set to 0.6. Two 1-D convolution layers with the number of output channels as 16 and 32, and two fully connected layers are employed as a classifier to predict the existence of a link. The SEAL model is trained for 50 epochs on each dataset.</p><p>To guarantee the comparison between our proposed method and SEAL model is fair, we employ the same graph neural  network architecture to compute node embeddings in the line graph. It is worth noting that our proposed method does not employ graph pooling and 1-D convolution layers. Therefore, the number of parameters in our proposed method is much fewer than that in the SEAL model. Our proposed method is trained for 15 epochs on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>Plain Graph Link Prediction We perform our proposed method and baseline methods on 14 datasets to compare the performance of each model. We randomly split each dataset into training and testing dataset for ten times. The averaged AUC and standard deviations using 80% training links are shown in Table <ref type="table" target="#tab_3">II</ref>. The results in terms of AP are shown in Table <ref type="table" target="#tab_4">III</ref>. It can be seen from results that heuristic methods cannot achieve satisfactory performance on all datasets since the heuristic function is manually designed thus cannot handle different cases. We find that the state-ofthe-art model SEAL always outperforms all heuristic methods and embedding methods since it can learn the distribution of links automatically from datasets. Our proposed LGLP model can consistently achieve better performance than all baseline methods, including SEAL in terms of two evaluation metrics. It shows that our proposed method can learn better features to represent the target link for prediction in line graph space. In addition, our proposed method is more stable than other baseline methods.</p><p>To demonstrate our proposed method can still achieve satisfactory performance with limited training samples, we conduct experiments on all datasets using 50% training links. The averaged AUC and AP are shown in Table <ref type="table" target="#tab_5">IV</ref> and Table <ref type="table" target="#tab_6">V</ref>, respectively. It can be seen from the results that our proposed method outperforms all baseline methods significantly on most datasets. We find that our proposed method can still perform well, even using 50% training links. The AUC and AP are close to that of using 80% training links.</p><p>In the experiments, we dynamically take 30%, 40%, 50%, 60%, 70%, and 80% of all the links in G as the training set and the rest as the test set, respectively. We conduct experiments with different training percentages and describe the AUC results in Figure <ref type="figure">4</ref>. The AUC value of our proposed method is marked with a sold line, and other baseline methods are marked with dashed lines in different colors. It can be seen from the results that our proposed method can outperform all baseline methods with different percentages of the training data. In addition, the performance of our proposed method is not sensitive to the number of training samples.</p><p>Attributed Graph Link Prediction We also conduct experiments on attributed graphs. Since the heuristic method can only be applied to plain graphs, we mainly focus on the comparison between our proposed method and SEAL. In the SEAL framework, the attribute is concatenated with the node label as the input for graph neural networks. In this work, we propose a new function to combine the node label and node attributes. We perform the experiment on Cora dataset   Convergence Speed Analysis Our proposed method can learn features for the target link directly in the line graph. Therefore, only graph convolution layers are required to extract features. In the SEAL model, the procedure is completed in the original graph and thus requires graph convolution and pooling layers to achieve this goal. Compared with the SEAL model, our proposed method contains fewer parameters and converges faster. To analyze the converging speed of two models, we run the models on different datasets and collect the loss and test AUC value for each epoch. The result is shown in Figure <ref type="figure">3</ref>. The loss and AUC of our proposed method are marked with solid lines. Those of the SEAL model are marked with dashed lines. It can be seen from the results that our Fig. <ref type="figure">4</ref>. AUC comparison on all datasets for Katz, PR, SR, SEAL, and LGLP using different percent of training links. On each dataset, we take 30%, 40%, 50%, 60%, 70%, and 80% of all the links in G as the training set. Our proposed method LGLP is marked with solid line and all baseline methods are marked with dashed lines in different colors.</p><p>proposed model can converge faster than the SEAL. Only 10 to 15 epochs are required to achieve the best performance for our proposed method. It takes 50 epochs for SEAL to converge. Therefore, our proposed method saves training time and requires fewer model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work, we propose a novel link prediction model based on line graph neural networks. Graph neural networks have achieved promising performance for the link prediction task. To deal with graphs in different scales, graph pooling layers are employed to extract a fixed-size feature vector in predicting the existence of a link. However, valuable information can be ignored in the pooling operation. In addition, graph neural networks with pooling layers commonly require more training time to converge. To overcome these limitations, we propose to transform the original input graph into line graph and thus the feature of the target link can be learned directly in the line graph without pooling operation. Experimental results on 14 datasets from different areas demonstrate that our proposed method can outperform all baseline methods, including the state-of-the-art models. In addition, our proposed method can converge faster than the state-of-the-art model significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the line graph transformation procedure. Each node in the line graph corresponds to a unique edge in the original graph and is marked with the name of two end nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. The original undirected graph G contains four nodes and five edges. Therefore, the line graph L(G) contains five nodes. The node (a âˆ’ b) and (a âˆ’ c) in the line graph are connected since the corresponding edges in the original graph G share a common node a based on the definition of the line graph. Based on the definition of the line graph, we can obtain the following property of L(G): Given a graph G with m nodes and n edges, the number of nodes of the line graph L(G) equals to n. The number of edges in L(G) is 1 2 m i=1 d 2 i âˆ’ n, where d i is the degree of node i in graph G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF DATASETS USED IN OUR EXPERIMENTS. THE NUMBER OF NODE, LINK, AVERAGE NODE DEGREE, AND GRAPH TYPE ARE PROVIDED FOR EACH DATASET.</figDesc><table><row><cell>Name</cell><cell cols="3">#Nodes #Links Degree</cell><cell>Type</cell></row><row><cell>BUP</cell><cell>105</cell><cell>441</cell><cell>8.4</cell><cell>Political Blogs</cell></row><row><cell>C.ele</cell><cell>297</cell><cell>2148</cell><cell>14.46</cell><cell>Biology</cell></row><row><cell>USAir</cell><cell>332</cell><cell>2126</cell><cell>12.81</cell><cell>Transportation</cell></row><row><cell>SMG</cell><cell>1024</cell><cell>4916</cell><cell>9.6</cell><cell>Co-authorship</cell></row><row><cell>EML</cell><cell>1133</cell><cell>5451</cell><cell>9.62</cell><cell>Shared Emails</cell></row><row><cell>NSC</cell><cell>1461</cell><cell>2742</cell><cell>3.75</cell><cell>Co-authorship</cell></row><row><cell>YST</cell><cell>2284</cell><cell>6646</cell><cell>5.82</cell><cell>Biology</cell></row><row><cell>Power</cell><cell>4941</cell><cell>6594</cell><cell>2.669</cell><cell>Power Network</cell></row><row><cell>KHN</cell><cell>3772</cell><cell>12718</cell><cell>6.74</cell><cell>Co-authorship</cell></row><row><cell>ADV</cell><cell>5155</cell><cell>39285</cell><cell>15.24</cell><cell>Social Network</cell></row><row><cell>GRQ</cell><cell>5241</cell><cell>14484</cell><cell>5.53</cell><cell>Co-authorship</cell></row><row><cell>LDG</cell><cell>8324</cell><cell>41532</cell><cell>9.98</cell><cell>Co-authorship</cell></row><row><cell>HPD</cell><cell>8756</cell><cell>32331</cell><cell>7.38</cell><cell>Biology</cell></row><row><cell>ZWL</cell><cell>6651</cell><cell>54182</cell><cell>16.29</cell><cell>Co-authorship</cell></row><row><cell cols="5">different scales, including the number of nodes and links, are</cell></row><row><cell cols="5">used in the experiments. The details of the datasets are shown</cell></row><row><cell>in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II AUC</head><label>II</label><figDesc>COMPARISON WITH BASELINE METHODS (80% TRAINING LINKS).</figDesc><table><row><cell>Model</cell><cell>BUP</cell><cell>C.ele</cell><cell>USAir</cell><cell>SMG</cell><cell>EML</cell><cell>NSC</cell><cell>YST</cell></row><row><cell>Katz</cell><cell cols="7">87.10(Â±2.73) 84.84(Â±2.05) 92.01(Â±0.88) 86.09(Â±1.06) 88.45(Â±0.68) 98.00(Â±0.31) 80.56(Â±0.78)</cell></row><row><cell>PR</cell><cell>90.13(Â±2.45)</cell><cell>89.14(Â±1.35)</cell><cell cols="5">93.74(Â±1.01) 89.13(Â±0.90) 89.46(Â±0.63) 98.05(Â±0.29) 81.40(Â±0.75)</cell></row><row><cell>SR</cell><cell cols="7">85.47(Â±2.75) 75.65(Â±2.24) 79.21(Â±1.50) 78.39(Â±1.14) 86.90(Â±0.71) 97.19(Â±0.48) 73.93(Â±0.95)</cell></row><row><cell>N2V</cell><cell cols="7">80.25(Â±5.55) 80.08(Â±1.52) 85.40(Â±0.96) 78.30(Â±1.22) 83.06(Â±1.42) 96.23(Â±0.95) 77.07(Â±0.36)</cell></row><row><cell>SEAL</cell><cell cols="7">93.32(Â±0.84) 87.44(Â±1.21) 95.21(Â±0.77) 91.53(Â±0.46) 92.01(Â±0.38) 99.55(Â±0.01) 90.72(Â±0.25)</cell></row><row><cell>LGLP</cell><cell>95.24(Â±0.53)</cell><cell>90.16(Â±0.76)</cell><cell>97.44(Â±0.32)</cell><cell>92.53(Â±0.29)</cell><cell>92.03(Â±0.28)</cell><cell>99.82(Â±0.01)</cell><cell>91.97(Â±0.12)</cell></row><row><cell>Model</cell><cell>Power</cell><cell>KHN</cell><cell>ADV</cell><cell>LDG</cell><cell>HPD</cell><cell>GRQ</cell><cell>ZWL</cell></row><row><cell>Katz</cell><cell cols="7">59.59(Â±1.51) 84.60(Â±0.79) 92.13(Â±0.21) 92.96(Â±0.19) 85.47(Â±0.35) 89.81(Â±0.59) 96.42(Â±0.12)</cell></row><row><cell>PR</cell><cell cols="7">59.88(Â±1.51) 88.43(Â±0.80) 92.78(Â±0.18) 94.46(Â±0.19) 87.19(Â±0.34) 89.98(Â±0.57) 97.20(Â±0.12)</cell></row><row><cell>SR</cell><cell cols="7">70.18(Â±0.75) 79.55(Â±0.90) 86.18(Â±0.22) 90.95(Â±0.14) 81.73(Â±0.37) 89.81(Â±0.58) 95.97(Â±0.16)</cell></row><row><cell>N2V</cell><cell cols="7">70.37(Â±1.15) 82.21(Â±1.19) 77.70(Â±0.83) 91.88(Â±0.56) 79.61(Â±1.14) 91.33(Â±0.53) 94.38(Â±0.51)</cell></row><row><cell>SEAL</cell><cell cols="7">81.37(Â±0.93) 92.69(Â±0.14) 95.07(Â±0.13) 96.44(Â±0.13) 92.26(Â±0.09) 97.10(Â±0.12) 97.46(Â±0.02)</cell></row><row><cell>LGLP</cell><cell>82.17(Â±0.57)</cell><cell>93.30(Â±0.09)</cell><cell>95.40(Â±0.10)</cell><cell>96.70(Â±0.07)</cell><cell>92.58(Â±0.08)</cell><cell>97.68(Â±0.10)</cell><cell>97.76(Â±0.01)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III AP</head><label>III</label><figDesc>COMPARISON WITH BASELINE METHODS (80% TRAINING LINKS).</figDesc><table><row><cell>Model</cell><cell>BUP</cell><cell>C.ele</cell><cell>USAir</cell><cell>SMG</cell><cell>EML</cell><cell>NSC</cell><cell>YST</cell></row><row><cell>Katz</cell><cell cols="7">85.94(Â±3.46) 85.94(Â±3.46) 93.51(Â±0.79) 87.68(Â±0.90) 90.54(Â±0.53) 98.02(Â±0.43) 85.76(Â±0.64)</cell></row><row><cell>PR</cell><cell>89.53(Â±3.11)</cell><cell>87.96(Â±1.69)</cell><cell cols="5">94.30(Â±1.27) 91.07(Â±0.59) 91.01(Â±0.67) 98.08(Â±0.34) 86.34(Â±0.72)</cell></row><row><cell>SR</cell><cell cols="7">81.10(Â±3.31) 66.43(Â±2.39) 69.80(Â±1.99) 70.39(Â±1.67) 87.24(Â±0.84) 96.55(Â±1.14) 77.56(Â±1.09)</cell></row><row><cell>N2V</cell><cell cols="7">81.47(Â±4.48) 77.98(Â±1.54) 82.53(Â±1.12) 77.01(Â±1.79) 83.08(Â±1.36) 96.81(Â±0.86) 78.48(Â±1.03)</cell></row><row><cell>SEAL</cell><cell cols="7">93.58(Â±0.68) 86.49(Â±1.08) 95.46(Â±0.59) 91.90(Â±0.31) 91.93(Â±0.31) 99.51(Â±0.01) 91.85(Â±0.20)</cell></row><row><cell>LGLP</cell><cell>95.46(Â±0.43)</cell><cell>89.70(Â±0.53)</cell><cell>97.37(Â±0.25)</cell><cell>92.92(Â±0.21)</cell><cell>92.61(Â±0.23)</cell><cell>99.82(Â±0.01)</cell><cell>92.98(Â±0.10)</cell></row><row><cell>Model</cell><cell>Power</cell><cell>KHN</cell><cell>ADV</cell><cell>LDG</cell><cell>HPD</cell><cell>GRQ</cell><cell>ZWL</cell></row><row><cell>Katz</cell><cell cols="7">74.29(Â±0.83) 88.27(Â±0.32) 93.72(Â±0.16) 94.91(Â±0.27) 89.52(Â±0.32) 93.08(Â±0.29) 97.08(Â±0.09)</cell></row><row><cell>PR</cell><cell cols="7">74.74(Â±0.81) 92.17(Â±0.24) 94.03(Â±0.24) 96.26(Â±0.22) 91.01(Â±0.23) 93.18(Â±0.34) 97.69(Â±0.08)</cell></row><row><cell>SR</cell><cell cols="7">70.69(Â±0.67) 77.16(Â±0.81) 83.31(Â±0.35) 88.71(Â±0.79) 84.16(Â±0.42) 92.97(Â±0.31) 95.44(Â±0.15)</cell></row><row><cell>N2V</cell><cell cols="7">76.55(Â±0.75) 83.26(Â±0.79) 79.02(Â±0.65) 92.12(Â±0.50) 80.57(Â±0.81) 93.92(Â±0.31) 93.82(Â±0.39)</cell></row><row><cell>SEAL</cell><cell cols="7">83.91(Â±0.83) 93.40(Â±0.13) 95.18(Â±0.12) 96.55(Â±0.11) 93.41(Â±0.09) 97.86(Â±0.11) 97.54(Â±0.02)</cell></row><row><cell>LGLP</cell><cell>84.78(Â±0.53)</cell><cell>94.14(Â±0.09)</cell><cell>95.72(Â±0.08)</cell><cell>96.86(Â±0.06)</cell><cell>93.65(Â±0.08)</cell><cell>98.14(Â±0.10)</cell><cell>97.91(Â±0.01)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV AUC</head><label>IV</label><figDesc>COMPARISON WITH BASELINE METHODS (50% TRAINING LINKS).</figDesc><table><row><cell>Model</cell><cell>BUP</cell><cell>C.ele</cell><cell>USAir</cell><cell>SMG</cell><cell>EML</cell><cell>NSC</cell><cell>YST</cell></row><row><cell>Katz</cell><cell cols="7">81.61(Â±3.40) 79.99(Â±0.59) 88.91(Â±0.39) 80.65(Â±0.58) 84.16(Â±0.64) 95.99(Â±0.62) 77.28(Â±0.37)</cell></row><row><cell>PR</cell><cell>84.07(Â±3.39)</cell><cell>84.95(Â±0.58)</cell><cell cols="5">90.57(Â±0.39) 84.59(Â±0.45) 85.43(Â±0.63) 96.06(Â±0.60) 77.90(Â±3.69)</cell></row><row><cell>SR</cell><cell cols="7">80.98(Â±3.03) 76.05(Â±0.80) 81.09(Â±0.59) 75.28(Â±0.74) 83.05(Â±0.64) 95.59(Â±0.68) 73.71(Â±0.41)</cell></row><row><cell>N2V</cell><cell cols="7">80.94(Â±2.65) 75.53(Â±1.23) 84.63(Â±1.58) 73.50(Â±1.22) 80.15(Â±1.26) 94.20(Â±1.25) 73.62(Â±0.74)</cell></row><row><cell>SEAL</cell><cell cols="7">85.10(Â±0.82) 81.23(Â±1.52) 93.23(Â±1.46) 86.56(Â±0.53) 85.83(Â±0.46) 99.07(Â±0.02) 85.56(Â±0.28)</cell></row><row><cell>LGLP</cell><cell>88.57(Â±0.52)</cell><cell>84.60(Â±0.82)</cell><cell>95.18(Â±0.33)</cell><cell>89.54(Â±0.36)</cell><cell>86.77(Â±0.26)</cell><cell>99.33(Â±0.01)</cell><cell>87.63(Â±0.15)</cell></row><row><cell>Model</cell><cell>Power</cell><cell>KHN</cell><cell>ADV</cell><cell>LDG</cell><cell>HPD</cell><cell>GRQ</cell><cell>ZWL</cell></row><row><cell>Katz</cell><cell cols="7">57.34(Â±0.51) 78.99(Â±0.20) 90.04(Â±0.17) 88.61(Â±0.19) 81.60(Â±0.12) 82.50(Â±0.21) 93.72(Â±0.06)</cell></row><row><cell>PR</cell><cell cols="6">57.34(Â±0.52) 82.34(Â±0.21) 90.97(Â±0.15) 90.50(Â±0.19) 83.15(Â±0.17) 82.64(Â±0.22)</cell><cell>95.11(Â±0.09)</cell></row><row><cell>SR</cell><cell cols="7">56.16(Â±0.45) 75.87(Â±0.19) 84.87(Â±0.14) 87.95(Â±0.14) 78.88(Â±0.22) 82.68(Â±0.24) 94.00(Â±0.10)</cell></row><row><cell>N2V</cell><cell cols="7">55.40(Â±0.84) 78.53(Â±0.72) 74.67(Â±0.98) 88.82(Â±0.44) 75.84(Â±1.03) 84.24(Â±0.35) 92.06(Â±0.61)</cell></row><row><cell>SEAL</cell><cell cols="7">65.80(Â±1.10) 87.43(Â±0.17) 92.75(Â±0.14) 92.98(Â±0.16) 88.05(Â±0.10) 90.07(Â±0.15) 94.94(Â±0.02)</cell></row><row><cell>LGLP</cell><cell>66.94(Â±0.60)</cell><cell>88.88(Â±0.13)</cell><cell>93.28(Â±0.10)</cell><cell>93.43(Â±0.11)</cell><cell>88.65(Â±0.09)</cell><cell>91.31(Â±0.11)</cell><cell>95.51(Â±0.01)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V AP</head><label>V</label><figDesc>COMPARISON WITH BASELINE METHODS (50% TRAINING LINKS).Fig. 3. Training loss and testing AUC comparison between our proposed LGLP and SEAL method. The training loss and testing AUC on BUP, C.ele, EML, and SMG dataset. The training loss and testing AUC of LGLP are marked with blue and orange solid lines. Those of SEAL are marked with blue, orange dashed lines.</figDesc><table><row><cell>Model</cell><cell>BUP</cell><cell>C.ele</cell><cell>USAir</cell><cell>SMG</cell><cell>EML</cell><cell>NSC</cell><cell>YST</cell></row><row><cell>Katz</cell><cell cols="7">85.94(Â±2.03) 83.99(Â±0.79) 93.51(Â±0.35) 87.68(Â±0.79) 80.54(Â±0.31) 98.02(Â±0.53) 81.63(Â±0.41)</cell></row><row><cell>PR</cell><cell>89.53(Â±2.58)</cell><cell>87.96(Â±0.86)</cell><cell cols="5">94.30(Â±0.49) 91.07(Â±0.69) 91.01(Â±0.52) 98.08(Â±0.59) 82.08(Â±0.46)</cell></row><row><cell>SR</cell><cell cols="7">81.09(Â±2.57) 66.43(Â±1.17) 69.78(Â±0.84) 70.39(Â±0.96) 87.24(Â±0.52) 96.55(Â±0.75) 76.02(Â±0.49)</cell></row><row><cell>N2V</cell><cell cols="7">76.05(Â±3.20) 73.37(Â±1.23) 81.03(Â±1.18) 73.32(Â±1.34) 81.12(Â±0.92) 95.32(Â±1.08) 76.61(Â±0.94)</cell></row><row><cell>SEAL</cell><cell cols="7">84.17(Â±0.62) 83.94(Â±1.31) 94.31(Â±1.13) 86.76(Â±0.41) 87.45(Â±0.41) 99.09(Â±0.02) 86.45(Â±0.25)</cell></row><row><cell>LGLP</cell><cell>89.03(Â±0.41)</cell><cell>84.80(Â±0.63)</cell><cell>94.89(Â±0.33)</cell><cell>90.23(Â±0.26)</cell><cell>88.49(Â±0.23)</cell><cell>99.38(Â±0.01)</cell><cell>89.22(Â±0.13)</cell></row><row><cell>Model</cell><cell>Power</cell><cell>KHN</cell><cell>ADV</cell><cell>LDG</cell><cell>HPD</cell><cell>GRQ</cell><cell>ZWL</cell></row><row><cell>Katz</cell><cell cols="7">57.63(Â±0.51) 83.04(Â±0.38) 91.76(Â±0.15) 91.57(Â±0.17) 85.73(Â±0.89) 86.59(Â±0.20) 95.12(Â±0.05)</cell></row><row><cell>PR</cell><cell cols="6">57.61(Â±0.56) 87.18(Â±0.26) 92.43(Â±0.17) 93.53(Â±0.14) 87.20(Â±0.15) 86.73(Â±0.20)</cell><cell>96.24(Â±0.05)</cell></row><row><cell>SR</cell><cell cols="7">56.19(Â±0.49) 75.87(Â±0.66) 83.22(Â±0.20) 88.11(Â±0.25) 81.07(Â±0.18) 86.27(Â±0.20) 94.26(Â±0.11)</cell></row><row><cell>N2V</cell><cell cols="7">60.46(Â±0.86) 80.60(Â±0.74) 76.70(Â±0.82) 89.57(Â±0.64) 77.66(Â±0.54) 88.70(Â±0.26) 91.61(Â±0.49)</cell></row><row><cell>SEAL</cell><cell cols="7">68.67(Â±0.98) 90.37(Â±0.16) 93.52(Â±0.13) 94.33(Â±0.15) 90.25(Â±0.10) 92.80(Â±0.12) 95.88(Â±0.02)</cell></row><row><cell>LGLP</cell><cell>69.41(Â±0.50)</cell><cell>90.83(Â±0.11)</cell><cell>93.82(Â±0.10)</cell><cell>94.63(Â±0.10)</cell><cell>90.34(Â±0.09)</cell><cell>93.01(Â±0.10)</cell><cell>96.19(Â±0.01)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>ON CORA DATASET USING PLAIN GRAPH AND ATTRIBUTED GRAPH (50% TRAINING LINKS).[40] that contains 2,708 nodes and 5,429 links. Each node in the Cora dataset is associated with an attribute vector in 1433 dimensions. We conduct the experiments without node attribute first, and then involve the node attributes to analyze the performance. The results are shown in TableVI. We can find both AUC and AP decrease after using node attributes as input in the SEAL model. In our proposed method, the performance does not change significantly. It shows that our proposed method can work well for both plain and attributed graphs.</figDesc><table><row><cell></cell><cell cols="2">Attribute</cell><cell cols="2">Plain</cell></row><row><cell></cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell cols="2">SEAL 75.33</cell><cell>77.69</cell><cell>79.95</cell><cell>82.91</cell></row><row><cell>LGLP</cell><cell cols="2">81.45 81.99</cell><cell cols="2">79.96 83.30</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported in part by National Science Foundation grant DBI-2028361.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Link prediction using supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM06: workshop on link analysis, counterterrorism and security</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>LÃ¼</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical temporal convolutional networks for dynamic recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boostgapfill: Improving the fidelity of metabolic network reconstructions through integrated constraint and pattern-based methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oyetunde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="611" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>BarabÃ¡si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network-based prediction of protein interactions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>KovÃ¡cs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spirohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1240</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structpool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>LÃ¼</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reprint of: The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3825" to="3833" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simrank: a measure of structural-context similarity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multi-scale approach for graph link prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shuiwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some properties of line digraphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Z</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendiconti del Circolo Matematico di Palermo</title>
				<imprint>
			<date type="published" when="1960">1960</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1g0Z3A9Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A max {m, n} algorithm for determining the graph h from its line graph g</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Roussopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="108" to="112" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An optimal algorithm to detect a line graph and output its root graph</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lehot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="569" to="575" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;smallworld&apos;networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The structure of scientific collaboration networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="404" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model of complex networks based on citation dynamics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Å ubelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bajec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
				<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="527" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
