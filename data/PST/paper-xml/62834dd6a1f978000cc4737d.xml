<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MReD: A Meta-Review Dataset for Structure-Controllable Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-11">11 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
							<email>chenhui.shen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
							<email>liying.cheng@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Zhou</surname></persName>
							<email>ran.zhou@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>You</surname></persName>
							<email>youy@comp.nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MReD: A Meta-Review Dataset for Structure-Controllable Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-11">11 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.07474v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain. 1 * * Equally Contributed. † Chenhui, Liying, and Ran are under the Joint PhD Program between Alibaba and their corresponding universities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text generation entered a new era because of the development of neural network based generation techniques. Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks meta-review: [This paper studies n-step returns in off-policy RL and introduces a novel algorithm which adapts the return's horizon n in function of a notion of policy's age.]←ABSTRACT <ref type="bibr">[Over-</ref>all, the reviewers found that the paper presents interesting observations and promising experimental results.]←STRENGTH [However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n-step returns) and parts of the experimental results. ]←WEAKNESS [In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered.]←REBUTTAL PROCESS [Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction.]←DECISION Table <ref type="table">1</ref>: An example of annotated meta-review. CATE-GORY indicates the category of each sentence.</p><p>into three clusters: more-to-less, less-to-more, and neck-to-neck. The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization <ref type="bibr" target="#b46">(Tan et al., 2017;</ref><ref type="bibr" target="#b18">Kryściński et al., 2018)</ref>. The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation <ref type="bibr" target="#b9">(Fan et al., 2018b)</ref>. The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref>.</p><p>To some extent, the existing task settings are not so adequate because they do not have a deep understanding of the domains they are working on, i.e., domain knowledge. Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail <ref type="bibr" target="#b30">(Nallapati et al., 2016)</ref> is composed of the training pairs of news content and news titles. However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income. Obviously, there is not a standard answer regarding right or wrong. For such cases, if we can specify a control signal, e.g., "emphasizing new business", the generated text would make more sense to users using the text generator.</p><p>To allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR. MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent. Thus from the same input text, the trained generator can generate varied outputs according to the given control signals. For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of "abstract | strength | decision" to generate a meta-review, or may use a structure of "abstract | weakness | suggestion" otherwise. Note that for ease of preparation and explanation, we ground our dataset in the peer review domain. However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort. Specifically, we collect 7,089 meta-reviews of ICLR in recent years <ref type="bibr">(2018 -2021)</ref> and fully annotate the dataset. Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc). Table <ref type="table">1</ref> shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence. Our MReD is obviously different from the previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage. Our proposed task is also noticeably different from the existing controllable text generation tasks (e.g., text style transfer on sentiment polarity <ref type="bibr" target="#b44">(Shen et al., 2017;</ref><ref type="bibr" target="#b24">Liao et al., 2018)</ref> and formality <ref type="bibr" target="#b41">(Shang et al., 2019</ref>)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.</p><p>To summarize, our contributions are as follows.</p><p>(1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation. With thorough data analysis, we derive useful insights into the domain characteristics.</p><p>(2) We propose a new task of controllable generation focusing on controlling the passage macro structures. It offers stronger generation flexibility and applicability for practical use cases.</p><p>(3) We design simple yet effective control methods that are independent of the model architecture. We show the effectiveness of enforcing different generation structures with a detailed model analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MReD: Meta-Review Dataset</head><p>In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peerreviewing system. Unlike the previous datasets that mainly focus on domains like news, the domain for meta-reviews is worth-studying because it contains essential and high-density opinions. Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers. We first introduce the data collection process and then describe the annotation details, followed by dataset analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>We collect the meta-review related data of ICLR from an online peer-reviewing platform, i.e., Open-Review<ref type="foot" target="#foot_0">2</ref> from 2018 to 2021. Note that the submissions from earlier years are not collected because their meta-reviews are not released. To prepare our dataset for controllable text generation, for each submission, we collect all of its corresponding official reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage. Table <ref type="table" target="#tab_1">2</ref> shows the statistics of data collected from each year. Initially, 7,894 submissions are collected. After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews. Note that even without any further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS). Compared with those conven-   tional datasets for MDS, such as TAC <ref type="bibr" target="#b34">(Owczarzak and Dang, 2011)</ref> and DUC <ref type="bibr" target="#b33">(Over and Yen, 2004)</ref>, which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Annotation</head><p>As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage. Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures. Specifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc). Table <ref type="table" target="#tab_2">3</ref> shows the definition for each category (see example sentences in Appendix A.1). The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous. Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2</p><p>For conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round. These 12 annotators are fully paid for their work. Each meta-review sentence is independently labeled by 2 different annotators, and a third expert annotator resolves any disagreement between the first two annotators. We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the first two annotators, showing that the annotation is of quite high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Analysis</head><p>To better understand the MReD dataset, we conduct the following analysis along different dimensions.</p><p>Sentence distribution across categories. The number of sentences in different categories are shown in Figure <ref type="figure" target="#fig_0">1</ref>, breakdown by the decision (i.e., accept or reject). Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected. Among all submissions and the rejected submissions, "weakness" accounts for the largest proportion, while across the accepted ones, "abstract" and "strength" take up a great proportion. To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers' comments. However, some minor or subjective categories (e.g., "ac disagreement") are hard to generate.</p><p>Breakdown analysis by meta-review lengths and average rating scores. We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. For example, among the meta-reviews that receive the reviewers' average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words. We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores. This coincides with our commonsense that for high-score and low-score sub-  missions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis). As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing "weakness" and "suggestion" for authors to improve the submissions. Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.</p><p>Meta-review patterns. To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure <ref type="figure" target="#fig_3">4</ref>, where the sum of each row is 1. Note that each segment represents the longest consecutive sentences with the same category. We add "&lt;start&gt;" and "&lt;end&gt;" tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews. It is clear to see that "abstract" usually positions at the beginning of the meta-review, while "suggestion" and "decision" usually appear at the end. There are also some clear patterns appearing in the metareviews, such as "abstract | strength | weakness", "rating summary | weakness | rebuttal process", and "abstract | weakness | decision".</p><p>3 Structure-Controllable Text Generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects. However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the "strength" of the paper following a structure of "abstract | strength | decision", whereas another AC may prefer a differ-  .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 ent structure with more focus on reviewers' opinions and suggestions (i.e., "rating summary" and "suggestion"). To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review that is derivable from the reviews and presents the required structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explored Methods</head><p>As the recent generation works <ref type="bibr" target="#b49">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Liu and Lapata, 2019;</ref><ref type="bibr" target="#b52">Xing et al., 2020)</ref> basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task. Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder. We also explore other baselines in the experiments later.</p><p>In order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation. Specifically, we intuitively add the control se-  quence in front of the input text. By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures. Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence. Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.</p><p>Given the multiple review inputs, we need to linearize them into a single input. One simple method, concat, is to concatenate all inputs one after another <ref type="bibr" target="#b7">(Fabbri et al., 2019)</ref>. Besides the text inputs, the review rating, which cannot be found in the review passages but exists in the field of rating score, is also crucial information for writing meta-reviews. Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input. We name this method rate-concat (see Table <ref type="table" target="#tab_6">4</ref>, upper). We also explore an alternative method, merge, as follows: From all review inputs, we use the longest one as a backbone. We segment all reviews' content on a paragraph level, and encode them using Sen-tenceTransformers <ref type="bibr" target="#b38">(Reimers and Gurevych, 2019)</ref>. Then, for each paragraph embedding in the nonbackbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding. We then insert each non-backbone paragraph after the backbone paragraph with which it has the highest similarity score. We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage. We further add rating sentences in front of the results of merge to obtain rate-merge.</p><p>Additionally, we provide a longest-review baseline, which does not combine reviews but only uses the longest review as the input.</p><p>As aforementioned, we place the control sequence in front of the re-organized review information. Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl. Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level. Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation. Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment. Example inputs of different control settings are shown in Table <ref type="table" target="#tab_6">4</ref> (lower). For instance, sent-ctrl repeats "abstract" in its control sequence whereas seg-ctrl does not. This is because seg-ctrl treats the 1 st and 2 nd target sentences of "abstract" as the same segment and only uses a single label to indicate it in the sequence. Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.</p><p>Using the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation. Many transformer-based models have achieved state-of-the-art performance. Common abstractive summarization models include BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>, T5 <ref type="bibr" target="#b35">(Raffel et al., 2020)</ref> and PEGASUS <ref type="bibr" target="#b53">(Zhang et al., 2020)</ref>. In this paper we focus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix C.1, which show similar trend). More specifically, we use the Py-Torch implementation in the open-source library Hugging Face Transformers <ref type="bibr" target="#b51">(Wolf et al., 2020)</ref> and its hosted pretrained models<ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>Extractive Baselines. We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences. LexRank <ref type="bibr" target="#b6">(Erkan and Radev, 2004)</ref> represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores. TextRank <ref type="bibr" target="#b29">(Mihalcea and Tarau, 2004</ref>) is another graph-based sentence ranking method that obtains vertex scores by running a "random-surfer model" until convergence. MMR <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998)</ref> calculates sentence scores by balancing the redundancy score with the information relevance score. After ranking with each of the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings. For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output. For the controlled setting, we select only the top sentences with the right category labels according to the control sequence. To do so, we employ an LSTM-CRF <ref type="bibr" target="#b20">(Lample et al., 2016)</ref> tagger trained on the labeled meta-reviews to predict the labels of each input review sentence. Refer to Appendix C.2 for more details of the tagger.</p><p>Generic Sentence Baselines. Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as "This paper proposes ..." for abstract, and "I recommend acceptance." for decision, etc. To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences. For instance, if the control sequence is "abstract | strength | decision", we take the most generic sentences from the categories of "abstract", "strength" and "decision" respectively to form the output. Specifically, we create two generic sentence baselines by obtaining generic sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely "Target Generic" and "Source Generic". Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure <ref type="figure" target="#fig_1">2</ref>. See Appendix C.3 for more details and results on generic sentence baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setting</head><p>To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long. After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1. We evaluate our generated outputs against the reference meta-reviews using the F 1 scores of ROUGE ROUGE 2 , and ROUGE L <ref type="bibr" target="#b25">(Lin, 2004)</ref> <ref type="foot" target="#foot_2">4</ref> .</p><p>For the extractive and generic baselines, a key hyperparameter is the sentence number k. Recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target meta-review. Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of labels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines. We also adopt the same k for the generic baselines.</p><p>For bart-large-cnn, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU. During fine-tuning, we set the hyperparameters of "minimum_target_length" to 20, and "maximum_target_length" to 400, according to our filter range on the meta-review lengths. Due to long inputs (see Table <ref type="table" target="#tab_15">17</ref>), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens. We cannot explore truncation length of more than 3072 tokens due to the limitation of GPU space. Our learning rate is 5e-5, and we use Adam optimizer with momentum β 1 = 0.9, β 2 = 0.999 without any warm-up steps or weight decay. We set the seed to be 0, and train the model for 3 epochs with gradient accumulation step of 1. For decoding, we use a beam size of 4 and length penalty of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>We show results in Table <ref type="table" target="#tab_7">5</ref>. Only the best settings of rate-concat ( Section 4.4) and input truncation of 2048 tokens (Appendix C.4) for bart-large-cnn are included. Amongst the extractive baselines, TextRank performs the best in both unctrl and sentctrl settings. Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers). This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences. On the other hand, for bart-large-cnn, sent-ctrl is the best, followed by seg-ctrl. This is most likely due to the former's more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control.</p><p>Moreover, bart-large-cnn far outperforms the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn't work well. This is also validated by the "Target Generic" baseline's consistent improvement over the "Source Generic" baseline, which shows that generic sentences from meta-reviews can suit generation better than those in reviews. Nevertheless, all Transformers results are still much better than the "Target Generic" sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Review Combination Results</head><p>We also show uncontrolled generation results for different review combination methods in Table <ref type="table" target="#tab_8">6</ref>, with source truncation of 2048. The longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information. Rateconcat has the best overall performance, which is the setting we used for the main results. Never- theless, it is not significantly better than merge. It is also interesting to see that for merge, providing additional rating information (rate-merge) slightly worsens the performance. We will leave the investigation of better review combination methods for future work.</p><formula xml:id="formula_0">R 1 R 2 R L longest-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>We study some cases for a better understanding of the structure-controllable generation.</p><p>Identify the control label for each sentence.</p><p>We first evaluate whether the model is able to attend to the correct control label during generation. For each generation step, we obtain the cross attention weights from the decoder's output token towards the control labels, and plot them in Figure <ref type="figure" target="#fig_4">5</ref>. The given control sequence is "abstract | weakness | decision". When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract information from the input sentences.</head><p>To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix C.5). Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Content Attention Attribution</head><p>Sent 1 (abstract):</p><p>This paper proposes a selfsupervised contrastive learning method for few-shot learning.</p><p>Sent 2 (weakness):</p><p>The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the lack of comparison to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sent 3 (decision):</head><p>The paper is not suitable for publication at ICLR in its current form.</p><p>Table <ref type="table">8</ref>: Attention analysis for each output sentence.</p><p>the selected sentences and the control sequence in Table <ref type="table">8</ref>. As shown, the model can correctly extract relevant information from the source sentences. For example, it identifies important phrases such as "interesting", "clarity" and "lack of comparison to baselines" when generating "Sent 2".</p><p>Generate varied outputs given different control sequences. To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews. In Table 7, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3. From the outputs, we can see that indeed each generated sentence corresponds to its control label well. In Row 2, we add an additional control label in the sequence and by repeating the "abstract" label, the generator can further elaborate more details of the studied method. This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details. In Row 3, a very comprehensive control sequence is specified. We can see that the output meta-review is quite fluent and polite to reject the borderline paper. See Appendix C.6 for more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation</head><p>In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the bart-large-cnn model trained under different control methods from Table <ref type="table" target="#tab_7">5</ref> on 100 random test instances. For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models, and ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? ( <ref type="formula">4</ref>) Decision Correctness: does the gen- eration correctly predicts the gold human decision?</p><p>We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix C.7). For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.</p><p>As shown in Table <ref type="table">9</ref>, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation. Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit readability and decision generation. For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p &gt; 0.08). Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves. It is because a tighter control narrows the content that a model can use from the reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT <ref type="bibr" target="#b40">(Sandhaus, 2008)</ref>, Gigaword <ref type="bibr" target="#b31">(Napoles et al., 2012)</ref>, <ref type="bibr">CNN/Daily Mail (Hermann et al., 2015)</ref>, NEWSROOM <ref type="bibr" target="#b12">(Grusky et al., 2018)</ref> and XSUM <ref type="bibr" target="#b32">(Narayan et al., 2018)</ref>. Datasets for long documents include <ref type="bibr" target="#b43">Sharma et al. (2019)</ref>, <ref type="bibr" target="#b3">Cohan et al. (2018), and</ref><ref type="bibr" target="#b10">Fisas et al. (2016)</ref>. In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely, structurecontrollable text generation.</p><p>Researchers recently explore the peer review domain data for a few tasks, such as PeerRead <ref type="bibr" target="#b16">(Kang et al., 2018)</ref> for paper decision predictions, AM-PERE <ref type="bibr">(Hua et al., 2019)</ref> for proposition classification in reviews, and RR <ref type="bibr" target="#b2">(Cheng et al., 2020)</ref> for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by <ref type="bibr" target="#b0">Bhatia et al. (2020)</ref> without any annotation. Our work is the first fully-annotated dataset in this domain for the structure-controllable generation task. There are also some datasets and annotation schemes on research articles <ref type="bibr" target="#b48">(Teufel et al., 1999;</ref><ref type="bibr" target="#b23">Liakata et al., 2010;</ref><ref type="bibr" target="#b21">Lauscher et al., 2018)</ref>, which differ in nature from the peer review domain and cannot be easily transferred to our task.</p><p>A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments <ref type="bibr" target="#b5">(Duan et al., 2020)</ref>, politeness <ref type="bibr" target="#b28">(Madaan et al., 2020)</ref>, formality <ref type="bibr" target="#b50">(Wang et al., 2019)</ref>, domains <ref type="bibr" target="#b45">(Takeno et al., 2017)</ref> and persona <ref type="bibr" target="#b54">(Zhang et al., 2018)</ref>) and content control (e.g., length <ref type="bibr" target="#b5">(Duan et al., 2020)</ref>, entities <ref type="bibr" target="#b8">(Fan et al., 2018a)</ref>, and keywords <ref type="bibr" target="#b47">(Tang et al., 2019)</ref>). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning <ref type="bibr" target="#b39">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b42">Shao et al., 2019;</ref><ref type="bibr">Hua and Wang, 2019)</ref>, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics. With such rich annotations, we propose simple yet effective methods for structure-controllable text generation. Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Concerns</head><p>We have obtained approval from ICLR organizers to use the data collected from ICLR 2018-2021 on OpenReview. trend. In addition, the proportion of meta-reviews having "rebuttal process" is larger for submissions with lower scores. This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted. On the other hand, for rejected papers, the percentage of meta-reviews having "strength" increases as the average score increases. This coincides with our common sense that the submissions receiving higher scores tend to have more strengths. One interesting finding here is that the percentage of "weakness" and "suggestion" also increases as the average rating score increases. This may be due to two main reasons. First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions. Second, compared to the percentage of "strength", "weakness" definitely has a larger percentage within any range of rating scores. The difference in the percentage of "strength" and "weakness" is intuitively different between the accepted papers and the rejected papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments C.1 Additional transformers models</head><p>We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table <ref type="table" target="#tab_12">13</ref>. Note that due to limited GPU space, we cannot fit 2048 input tokens for T5. Thus, for fair comparison, all results shown are from source truncation of 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Tagger for source sentences</head><p>To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews, then use it to predict labels on the input sentences. Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) <ref type="bibr" target="#b19">(Lafferty et al., 2001)</ref> (i.e., LSTM-CRF <ref type="bibr" target="#b20">(Lample et al., 2016)</ref>) model on the annotated MReD dataset.</p><p>The same data split as the meta-review generation task is used. We adopt the standard IOBES tagging scheme <ref type="bibr" target="#b36">(Ramshaw, 1995;</ref><ref type="bibr" target="#b37">Ratinov and Roth, 2009)</ref>, and fine-tune BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr">(Liu et al., 2019)</ref> models in Hugging Face. All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes. We select the best model parameters based on the best micro F 1 score on the development set and apply it to the test set for evaluation. All models are run with single V100 GPUs. We use Adam <ref type="bibr" target="#b17">(Kingma and Ba, 2014)</ref> with an initial learning rate of 2e-5.</p><p>We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14. Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation. Since some of the category labels (eg. "ac disagreement") are very rare, their classification accuracy is low. Overall, micro F1 is a more important metric since it suggests general performance. The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories. RoBERTabase is the best performing model, therefore we use this model to predict review sentence labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Generic sentence baselines</head><p>Besides the baselines of "Source Generic" and "Target Generic", we explore subsets of papers with high scores (average reviewers' rating 7) or low scores (average reviewers' rating 3) to obtain 4 additional generic baselines: "Source High Score", "Source Low Score", "Target High Score", "Target Low Score". We use "Target High Score" as an example to explain how we obtain the generic sentences: From the training subset of high score papers, We first separate all meta-review sentences into the corresponding label categories, obtaining a total of 9 groups of sentences. Then, we re-arrange the sentences in each group using TextRank (our best extractive model). Since TextRank ranks the input sentences based on each sentence's content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others.</p><p>After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence on the corresponding high score paper test data. We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.</p><p>All generic sentence baselines can be obtained in a similarly procedure as outlined above, and we show results in Table <ref type="table" target="#tab_13">15</ref>. Both "Target High Score" and "Target Low Score" perform much better than the "Target Genric" baseline, suggesting that pa-pers with very high or low scores tend to have more typical patterns in their meta-reviews. Nevertheless, the pattern is less evident in the source (reviews) baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Ablation on truncation length</head><p>By default, the Transformers truncate the source to 1024 tokens. We further investigate the performance of different source truncation lengths under the setting of rate-concat. As shown in Table <ref type="table" target="#tab_16">18</ref>, truncating the source to 2048 tokens consistently achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Attention aggregation method</head><p>During generation, we can obtain the attention weights of each output token towards all input tokens. Specifically, we average all decoder layers' cross attention weights for the same output token generated at each decoding step. We then calculate an attention value for that output token on each input sentence, by aggregating the token's attention weights on the list of input tokens that belong to the same sentence by max pooling. Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same sentence.</p><p>Common attention aggregation methods include summation, average-pooling, and max-pooling. We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out. With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths. For attention aggregation on the samesentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant. We conduct trial runs of all aggregation methods on input tokens with summation for  output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence.</p><p>Once we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences. Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Structure-controlled generation examples</head><p>We show examples of the generation results using alternative control sequences on another submission in Table <ref type="table" target="#tab_14">16</ref>. We can see the effectiveness of controlling the output structure using our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Human evaluation</head><p>For structure similarity, we instruct the judges to label each generated sentence with the closest category. We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, where each label is considered as a single token, and finally deduct this value from 1.</p><p>For decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise. More specifically, we give 0 if the generation produces either contradictory decisions or a wrong decision, or if the generation does not show enough hints for rejection or acceptance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence numbers in different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Meta-review length distribution across ratings. Bracketed numbers show the submission count.</figDesc><graphic url="image-1.png" coords="4,102.46,81.89,188.13,94.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sentence-level category distribution percentage breakdown by different lengths of meta-reviews.</figDesc><graphic url="image-2.png" coords="4,361.98,257.54,122.78,122.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Transition matrix of different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cross attention weights of each generated token towards the control tokens in logarithmic scale.</figDesc><graphic url="image-4.png" coords="7,73.58,71.86,450.86,84.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>presents a self-supervised model based on a contrastive autoencoder that can make use of a small training set for upstream multi-label/class tasks.]←ABSTRACT [Reviewers have several concerns, including the lack of comparisons and justification for the setting, as well as the potentially narrow setting.]←WEAKNESS [Overall, I found the paper to be borderline, the cons slightly greater than the pros, so I recommend to reject it.proposes a self-supervised contrastive learning method for few-shot learning.]←ABSTRACT [The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the of comparison to the baselines.]←WEAKNESS [The paper is not suitable for publication at ICLR in its current form.]←DECISION 2 abstract | abstract | weakness | decision [This paper proposes a self-supervised contrastive learning method for few-shot learning.]←ABSTRACT [The idea is to learn the label embeddings using in-task data, and then use those for training a contrastive autoencoder for multi-label classification on the long-tail problem.]←ABSTRACT [The reviewers and AC note the following potential weaknesses: (1) limited novelty, (2) limited evaluation, and (3) lack of comparison to relevant baselines.]←WEAKNESS [AC agrees with the reviewers that the paper is not ready for publication in its current form.The paper is well-motivated and the idea is simple and straightforward.]←STRENGTH [However, the reviewers have raised several concerns about the evaluation and the clarity of the paper.]←WEAKNESS [In particular, the lack of comparison to the baselines is a major concern.]←WEAKNESS [The authors have addressed some of these concerns in the rebuttal, but the reviewers are still not convinced about the significance of the results. ]←REBUTTAL PROCESS [The paper would be much stronger if the authors could compare their method to more baselines for zero-shot learning, such as matching networks and Siamese networks.]←SUGGESTION [Overall, the paper is not ready for publication at ICLR.]←DECISION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>adapts cycle GAN to the problem of decipherment with some success.]←ABSTRACT [it's still an early result, but all the reviewers have found it to be interesting and worthwhile for publication.]←RATING SUMMARY Control Sequence Generated Passage 1 abstract | rating summary [This paper proposes a method for using cycle GANs to decipher text encrypted with historical ciphers.]←ABSTRACT [The reviewers were unanimous in their vote to accept.]←RATING SUMMARY 2 abstract | abstract | rating summary [This paper proposes a method for using cycle GANs to decipher text encrypted with historical ciphers.]←ABSTRACT [The proposed method is based on replacing the log loss of the GAN with a quadratic loss and regularizing the Jacobian of the discriminator.]←ABSTRACT [The reviewers were unanimous in their vote to accept.]←RATING SUMMARY 3 strength | decision [The reviewers agree that the paper is well-written, the problem is interesting, and the proposed method is novel.]←STRENGTH [The paper should be accepted.]←DECISION 4 misc | abstract | strength | decision [Thanks for your submission to ICLR.]←MISC [This paper proposes a method for using cycle GANs to decipher text encrypted with historical ciphers.]←ABSTRACT [This is an interesting and fairly original problem, and the proposed method is shown to be very effective.]←STRENGTH [All reviewers agree that the paper is well written, and I'm happy to recommend acceptance.]←DECISION 5 weakness | rebuttal process[ The reviewers raised a number of concerns including the correctness of the proof, the lack of a simple baseline, and the presentation of the paper.]←WEAKNESS [The authors' rebuttal addressed some of these concerns, but not to the degree that the reviewers felt it should be.]←REBUTTAL PROCESS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics of MReD.</figDesc><table><row><cell>Categories</cell><cell>Definitions</cell></row><row><cell>abstract</cell><cell>A piece of summary about the contents of the submission</cell></row><row><cell>strength</cell><cell>Opinions about the submission's strengths</cell></row><row><cell>weakness</cell><cell>Opinions about the submission's weaknesses</cell></row><row><cell cols="2">rating summary A summary about reviewers' rating scores or decisions</cell></row><row><cell cols="2">ac disagreement Area chair (AC) shares different opinions to reviewers</cell></row><row><cell>rebuttal process</cell><cell>Contents related to authors' rebuttal with respect to reviews or discussions between reviewers in the rebuttal period</cell></row><row><cell>suggestion</cell><cell>Concrete suggestions for improving the submission</cell></row></table><note>decisionFinal decision (i.e., accept or reject) on the submission miscellaneous None of the above, such as courtesy expressions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Category definition of meta-review sentences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>00 .69 .08 .06 .08 .00 .02 .00 .02 .05 .00 .00 .00 .38 .32 .11 .00 .09 .01 .03 .05 .01 .00 .01 .00 .47 .06 .01 .14 .06 .11 .04 .10 .00 .00 .04 .00 .05 .01 .25 .17 .22 .05 .21 .00 .01 .10 .21 .00 .01 .13 .07 .21 .05 .22 .00 .01 .07 .10 .06 .00 .09 .10 .27 .11 .20 .00 .01 .06 .09 .09 .01 .00 .11 .36 .06 .21 .00 .00 .02 .03 .02 .00 .05 .00 .15 .05 .68 .00 .01 .02 .04 .01 .00 .01 .09 .00 .04 .79 .00 .03 .14 .20 .07 .01 .09 .06 .14 .00 .26</figDesc><table><row><cell>&lt;start&gt;</cell></row><row><cell>abstract</cell></row><row><cell>strength</cell></row><row><cell>weakness</cell></row><row><cell>rating_summary</cell></row><row><cell>ac_disagreement</cell></row><row><cell>rebuttal_process</cell></row><row><cell>suggestion</cell></row><row><cell>decision</cell></row><row><cell>misc</cell></row><row><cell>&lt;end&gt;</cell></row></table><note>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Upper: example for the review combination method. S i represents the score given by reviewer Ri. &lt;REVBREAK&gt; is the special separator used to concatenate different review texts. Lower: examples of control methods. [TEXT INPUT] refers to the obtained text from the upper section.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Meta-review generation results on MReD.</figDesc><table><row><cell>1 ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Meta-review uncontrolled generation results for different review combination methods.</figDesc><table><row><cell cols="2">review cccccccccccccc 32.07 7.86 19.00</cell></row><row><cell>concat</cell><cell>32.88 8.58 19.63</cell></row><row><cell>merge</cell><cell>33.19 8.77 19.31</cell></row><row><cell>rate-concat</cell><cell>33.31 8.63 19.67</cell></row><row><cell>rate-merge</cell><cell>33.05 8.54 19.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Varied generation outputs by giving different control sequences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Occurrence of different categories for accepted and rejected papers, breakdown by average scores. Low for scores ≤ 5.5, high for scores ≥ 6.5, and med for borderline scores in between.</figDesc><table><row><cell></cell><cell></cell><cell>Accept</cell><cell></cell><cell></cell><cell>Reject</cell><cell></cell></row><row><cell></cell><cell cols="6">Low Med High Low Med High</cell></row><row><cell>abstract</cell><cell cols="2">79 75</cell><cell>74</cell><cell cols="2">69 69</cell><cell>74</cell></row><row><cell>strength</cell><cell cols="2">64 71</cell><cell>70</cell><cell cols="2">26 43</cell><cell>50</cell></row><row><cell>weakness</cell><cell cols="2">49 44</cell><cell>32</cell><cell cols="2">79 84</cell><cell>88</cell></row><row><cell>rating summary</cell><cell cols="2">25 33</cell><cell>32</cell><cell cols="2">29 25</cell><cell>24</cell></row><row><cell>ac disagreement</cell><cell>1</cell><cell>6</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell cols="3">rebuttal process ccccccccc 52 47</cell><cell>37</cell><cell cols="2">35 39</cell><cell>39</cell></row><row><cell>suggestion</cell><cell cols="2">29 26</cell><cell>23</cell><cell cols="2">23 32</cell><cell>38</cell></row><row><cell>decision</cell><cell cols="2">56 53</cell><cell>46</cell><cell cols="2">53 53</cell><cell>56</cell></row><row><cell>miscellaneous</cell><cell cols="2">19 19</cell><cell>14</cell><cell cols="2">24 35</cell><cell>45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Results of other common Transformers summarization models using source truncation of 1024. * represents our selected model in the main paper.</figDesc><table><row><cell>Pretrained Model</cell><cell>R 1</cell><cell>R 2</cell><cell>R L</cell></row><row><cell>Uncontrolled Generation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">facebook/bart-large-cnn* cccccccccc 33.20 8.55 19.62</cell></row><row><cell>facebook/bart-large</cell><cell cols="3">28.86 6.20 19.02</cell></row><row><cell>t5-large</cell><cell cols="3">30.75 8.44 20.23</cell></row><row><cell>google/pegasus-cnn_dailymail</cell><cell cols="3">28.76 6.37 16.79</cell></row><row><cell>Controlled Generation, sent-ctrl</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">facebook/bart-large-cnn* cccccccccc 38.39 10.60 22.86</cell></row><row><cell>facebook/bart-large</cell><cell cols="3">38.05 10.66 23.39</cell></row><row><cell>t5-large</cell><cell cols="3">35.90 10.18 23.92</cell></row><row><cell>google/pegasus-cnn_dailymail</cell><cell cols="3">33.48 8.68 21.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 :</head><label>15</label><figDesc>Micro F 1 Macro F 1 abstract strength weakness rating ACdisagree rebuttal suggestion decision misc MReD generic sentence baseline results on various score subsets.</figDesc><table><row><cell>BERT-base-cased + CRF</cell><cell>85.27</cell><cell>76.71</cell><cell>94.58</cell><cell>86.12</cell><cell>86.21 85.21</cell><cell>30.77</cell><cell>73.80</cell><cell>73.89</cell><cell>91.30 68.49</cell></row><row><cell cols="2">BERT-large-cased + CRF 84.68</cell><cell>77.84</cell><cell>93.93</cell><cell>86.71</cell><cell>84.36 84.07</cell><cell>40.00</cell><cell>72.60</cell><cell>74.35</cell><cell>91.60 72.96</cell></row><row><cell>RoBERTa-base + CRF</cell><cell>85.83</cell><cell>79.99</cell><cell>94.47</cell><cell>86.43</cell><cell>86.73 84.56</cell><cell>54.84</cell><cell>74.44</cell><cell>72.79</cell><cell>93.08 72.54</cell></row><row><cell>RoBERTa-large + CRF</cell><cell>85.72</cell><cell>79.34</cell><cell>94.42</cell><cell>85.61</cell><cell>87.09 85.40</cell><cell>50.00</cell><cell>73.97</cell><cell>75.63</cell><cell>90.93 71.00</cell></row><row><cell></cell><cell></cell><cell cols="5">Table 14: MReD sentence classification results.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">R 1 R 2 R L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Generic</cell><cell></cell><cell cols="3">27.58 3.97 14.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Source High Score cccccccccccccccc 26.95 4.38 15.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Low Score</cell><cell></cell><cell cols="3">25.82 4.14 14.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target Generic</cell><cell></cell><cell cols="3">27.98 5.52 15.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target High Score</cell><cell></cell><cell cols="3">31.10 5.76 16.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target Low Score</cell><cell></cell><cell cols="3">32.04 7.21 19.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 :</head><label>16</label><figDesc>Generation examples of alternative control sequences on the same review inputs using the sent-ctrl method.</figDesc><table><row><cell cols="2">Data Split cccccccccccccccccccccccc max med avg</cell></row><row><cell>train</cell><cell>7276 1482 1368</cell></row><row><cell>validation</cell><cell>3762 1427 1352</cell></row><row><cell>test</cell><cell>5144 1454 1352</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 17 :</head><label>17</label><figDesc>Source length statistics on all data splits. Max for maximum source length, med for median source length, and avg for average source length.</figDesc><table><row><cell>length</cell><cell>R 1</cell><cell>R 2</cell><cell>R L</cell></row><row><cell cols="4">1024 cccccccccccccccccccccccccc 38.39 10.60 22.86</cell></row><row><cell cols="4">2048 cccccccccccccccccccccccccc 38.73 10.82 23.05</cell></row><row><cell cols="4">3072 cccccccccccccccccccccccccc 38.30 10.34 22.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 18 :</head><label>18</label><figDesc>Meta-review sent-ctrl generation results of different source truncation lengths.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://openreview.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://huggingface.co/models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We use the Hugging Face Transformers' Rouge evaluation script, which has the field "use_stemmer" enabled. We include the evaluation script in our code.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>abstract</head><p>"The paper presents/explores/describes/addresses/proposes ..."</p><p>strength "The reviewers found the paper interesting." "The method and justification are clear." "The quantitative results are promising."</p><p>weakness "The paper is somewhat incremental ..." "... claims are confusing" "The main concern is ..." "... unfair experimental comparisons ..." rating summary "R1 recommends Accept." "All four reviewers ultimately recommended acceptance." "Reviews were somewhat mixed, but also with mixed confidence scores."</p><p>ac disagreement "The area chair considers the remaining concerns by Reviewer 3 as invalid." "I do not agree with the criticism about ..." "I disagree with the second point .  A Data Annotation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Category definitions</head><p>We show category examples in Table <ref type="table">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional annotation rules</head><p>The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information. For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, even though that sentence itself can be considered as "misc", it should still be labeled as "weakness" to be consistent in context. Second, not every sentence can be strictly classified into a single category. When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose. One example is: "Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited." Although the first half of the sentence discusses the strength of the submission, the primary purpose of this sentence is to point out its weakness, and therefore it should be labeled as weakness.</p><p>Furthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories. We then define a priority order of these 9 categories according to the importance of each category for annotators to follow: decision &gt; rating summary &gt; strength ? = weakness &gt; ac disagreement &gt; rebuttal process &gt; abstract &gt; suggestion &gt; miscellaneous. We use the sign " ? =" because there are some rare cases where a sentence contains both "strength" and "weakness" while there is no obvious emphasis on either, and it is hard to tell whether "strength" should have a priority over "weakness" or the other way round. We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as "strength", and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Borderline papers</head><p>We further analyze the category distribution in borderline papers. As shown in Table <ref type="table">11</ref>, for submissions within the score range of <ref type="bibr">[4.5,6)</ref>, there are 713 accepted submissions and 2,588 rejected submissions. One clear difference is the percentage of "strength" and "weakness". Another difference is the percentage of "ac disagreement", where the accepted papers have four times the value than rejected ones. This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Percentage of each category for accepted and rejected papers across score ranges</head><p>We further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table <ref type="table">12</ref>. For accepted papers, as the score increases, the percentage of meta-reviews having "weakness" and "suggestion" drops because the high-score submissions are more likely to be accepted. Even the percentage of "decision" drops following the same</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metagen: An academic meta-review generation system</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tribikram</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukomal</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-SIGIR</title>
				<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-SIGIR</title>
				<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Argument pair extraction from peer review and rebuttal via multi-task learning</title>
		<author>
			<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pre-train and plug-in: Flexible conditional text generation with variational autoencoders</title>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
				<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Richard Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WNGT</title>
				<meeting>WNGT</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multi-layered annotated corpus of scientific papers</title>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Fisas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ronzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>and Horacio Saggion</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from rdf data</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
				<meeting>INLG</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Kociskỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory</title>
				<meeting>NIPS. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Argument mining for understanding peer reviews</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitko</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Badugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentence-level content planning and style specification for neural text generation</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A dataset of peer reviews (peerread): Collection, insights and nlp applications</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving abstraction in text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpora for the conceptualisation and zoning of scientific papers</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Batchelor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">QuaSE: Sequence editing under quantifiable guidance</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Politeness transfer: A tag and generate approach</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrith</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Gulçehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGNLL</title>
				<meeting>SIGNLL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AKBC-WEKEX</title>
				<meeting>AKBC-WEKEX</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An introduction to duc-2004</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DUC</title>
				<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overview of the tac 2011 summarization track: Guided task and aesop task</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC</title>
				<meeting>TAC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><surname>La Ramshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Third Workshop on Very Large Corpora</title>
				<meeting>Third Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
				<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-supervised text style transfer: Cross projection in latent space</title>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long and diverse text generation with planning-based hierarchical variational model</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Controlling target features in neural machine translation via prefix constraints</title>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Takeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WAT</title>
				<meeting>WAT</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Targetguided open-domain conversation</title>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An annotation scheme for discourse-level argumentation in research articles</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Harnessing pre-trained neural networks with rules for formality style transfer</title>
		<author>
			<persName><forename type="first">Yunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic generation of citation texts in scholarly papers: A pilot study</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosheng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
