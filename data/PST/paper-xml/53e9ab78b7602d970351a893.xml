<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retinex method based on adaptive smoothing for illumination invariant face recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-02-15">15 February 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Seok</roleName><forename type="first">Young</forename><forename type="middle">Kyung</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<addrLine>300, Chun-Chun-Dong, Chang-An-Ku</addrLine>
									<postCode>440-746</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lai</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<addrLine>300, Chun-Chun-Dong, Chang-An-Ku</addrLine>
									<postCode>440-746</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Joong</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
							<email>jkkim@skku.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<addrLine>300, Chun-Chun-Dong, Chang-An-Ku</addrLine>
									<postCode>440-746</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retinex method based on adaptive smoothing for illumination invariant face recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-02-15">15 February 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">84C1587CCA0CF67A17EC657AD69DDBBA</idno>
					<idno type="DOI">10.1016/j.sigpro.2008.01.028</idno>
					<note type="submission">Received 7 August 2007; received in revised form 25 January 2008; accepted 25 January 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>Illumination normalization</term>
					<term>Adaptive smoothing</term>
					<term>Iterative convolution</term>
					<term>Retinex</term>
					<term>Anisotropic diffusion</term>
					<term>Conduction function</term>
					<term>Smoothing constraint</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose the Retinex method for illumination invariant face recognition developed on the basis of adaptive smoothing technology. By the well-known Retinex theory, illumination is generally estimated and normalized by smoothing the input image first and then dividing the estimate into the original input image. Therefore, performance mainly depends on how good the estimated illumination is. The proposed method estimates illumination by iteratively convolving the input image with a 3 Â 3 smoothing mask weighted by a coefficient via combining two measures of the illumination discontinuity at each pixel. We address a couple of additional concepts, which are designed to be suitable especially for face images. One is the new conduction function for adaptive smoothing, and the other is the smoothing constraint for more accurate description of real environments. In this way, we can achieve an efficient illumination normalization in which face images with even strong shadows are normalized efficiently. The proposed method is evaluated based on Yale face database B, CMU PIE database and AR face database by applying PCA. The comparative results indicate that the proposed method present consistent and promising results even when images under harsh illumination conditions are used as a training set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last decade, significant progress has been achieved in face recognition area. Principle component analysis (PCA) <ref type="bibr" target="#b0">[1]</ref>, linear discriminant analysis (LDA) <ref type="bibr" target="#b1">[2]</ref>, discriminative common vector (DCV) <ref type="bibr" target="#b2">[3]</ref>, etc. have been developed successfully for face recognition, the product of which has mostly been commercialized. However, for reliable face recognition in unconstrained environment, there still remain many tasks to be solved in the area. Among many factors affecting the performance of face recognition systems, illumination is known to be the one of the most significant. For example, ambient lighting varies greatly during the course of the day, and from one day to another, as well as between indoor and outdoor environments. Moreover, strong shadows cast from a direct light source can make certain facial features invisible. Therefore, illumination normalization is a major requirement in the face recognition process and also is a central topic in the field of computer vision.</p><p>In recent years, numerous approaches have been proposed to solve illumination problems in face recognition. Georghiades et al. <ref type="bibr" target="#b3">[4]</ref> showed that the illumination cones of human faces can be approximated well by low dimensional linear subspaces. Therefore, the set of face images in fixed pose but under different illumination conditions can be efficiently represented using an illumination cone. Recently, Basri and Jacobs <ref type="bibr" target="#b4">[5]</ref> showed that the set of images of a convex Lambertian object obtained under a variety of lighting conditions can be well approximated using a 9D linear subspace which is formed by nine harmonic images. However, this method requires knowledge of the object's surface normals and albedos before the harmonic subspace can be computed. As nine images are not real images, Lee et al. <ref type="bibr" target="#b5">[6]</ref> showed that the linear subspaces could be directly generated using real nine images captured under a particular set of illumination conditions. Based on these discoveries, some relighting methods have also been proposed to generate photo-realistic images under new lighting conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. These methods provide the new insight to facilitate face recognition under different lighting conditions. However, the above methods still either require certain assumptions to be made about the light source or need a large number of training sets, and these requirements are not considered practical in real applications. On the other hand, there are alternative methods available which are based on Retinex theory although these methods are not designed for face recognition initially. The Retinex theory motivated by Land <ref type="bibr" target="#b8">[9]</ref> is based on the physical imaging model, in which an image Iðx; yÞ is regarded as the product Iðx; yÞ ¼ Rðx; yÞLðx; yÞ where Rðx; yÞ is the reflectance and Lðx; yÞ is the illumination at each pixel ðx; yÞ. Here, the nature of Lðx; yÞ is determined by the illumination source, whereas Rðx; yÞ is determined by the characteristics of the imaged objects. Therefore, the illumination normalization for face recognition can be achieved by estimating the illumination L and then dividing the image I by it. However, it is impossible to estimate L from I, unless something else is known about either L or R. Hence, various assumptions and simplifications about L, or R, or both are proposed to solve this problem <ref type="bibr" target="#b9">[10]</ref>. A common assumption is that edges in the scene are edges in the reflectance, while illumination spatially changes slowly in the scene. Thus, in most Retinex methods, the reflectance R is estimated as the ratio of the image I and its smooth version which serves as the estimate of the illumination L, and many smoothing filters to estimate the illumination have been proposed for illumination invariant face recognition. Single scale Retinex (SSR), the latest version of Land's Retinex that was implemented and tested by Jobson et al. <ref type="bibr" target="#b10">[11]</ref>, employed a simple linear filter with Gaussian kernel. However, strong shadows cast from a direct light source violate the assumption that the illumination slowly varies, and halo effects are often visible at large illumination discontinuities in I. To solve this problem, Jobson extended SSR to multiscale Retinex (MSR) <ref type="bibr" target="#b11">[12]</ref> by combining several low-pass filtered copies of the logarithm of I using different cut-off frequencies for each low-pass filter. Recently, Gross and Brajovie <ref type="bibr" target="#b9">[10]</ref> introduced an anisotropic filter to reduce these halo effects to some extent. More recently, selfquotient image (SQI) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> has been proposed with impressive improvement of performance for illumination problem. SQI employs the weighed Gaussian filter in which the convolution region is divided into two sub-regions with respect to a threshold, and separate values of weights are applied in each sub-region. These Retinex methods have common advantages that they do not require training images and has relatively low computational complexity. However, these methods still cannot completely remove cast shadows because they lack in adaptability which can preserve discontinuities efficiently. As a result, they ultimately cannot avoid the decrease in the recognition rate. To solve these problems, efficient discontinuity preserving filter must be employed to estimate L. Though not developed for face recognition, many discontinuity preserving filters such as adaptive smoothing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, low curvature image simplifier (LCIS) <ref type="bibr" target="#b16">[17]</ref>, bilateral filtering <ref type="bibr" target="#b17">[18]</ref> have been proposed.</p><p>In this paper, we propose a novel Retinex method for illumination invariant face recognition applying recently developed concepts of discontinuity preserving filters. Our method is mainly based on adaptive smoothing using iterative convolution employing the idea that combines two discontinuity measures. For estimation of illumination especially suitable to face recognition, we introduce two additional concepts: first, we propose a new form of conduction function that is used to determine a discontinuity level in each pixel ðx; yÞ, and then we apply additional constraint for more accurate description of real environments. The paper is organized as follows. In Section 2, we review the general framework of Retinex method and adaptive smoothing. In Section 3, we describe the proposed method in detail. In Section 4, the experimental results are presented. Finally a conclusion is made in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General framework and adaptive smoothing</head><p>In this section, we review the general framework of Retinex method and then introduce adaptive smoothing which is the core procedure in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">General framework of Retinex method</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, Retinex method mainly consists of two steps: estimation and normalization of illumination. As mentioned in Section 1, illumination L is estimated as a smooth version of input image I. Smoothing should especially be carried out among pixels which have homogeneous illumination because illumination discontinuities such as cast shadow violate the assumption that the illumination slowly varies. This robustness requirement implies that the estimated illumination must be discontinuous at locations where the input image I has strong discontinuities of intensity <ref type="bibr" target="#b18">[19]</ref>. This is to take into account real world scenes more accurately, where illumination discontinuities such as shadows frequently occur. Once the estimation is completed, illumination is normalized by taking the difference between the logarithms of the input image and the estimated illumination. The logarithmic function removes noise from the image and makes it possible to promote useful signals more. In this framework, the estimation of illumination is the core procedure and computational issue because various smoothing methods can be applied. Therefore, the proper smoothing method must be carefully selected to fulfill robust requirement. For this purpose, we select adaptive smoothing proposed by Saint-Marc et al. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adaptive smoothing</head><p>The key idea of adaptive smoothing is to iteratively convolve the input image to be smoothed with the 3 Â 3 averaging mask whose coefficients reflect the discontinuity level of the input image at each point. General adaptive smoothing can be formulated as follows. Since we estimate illumination L as smoothed version of input image I, the initial value of the estimated illumination, i.e., L ð0Þ ðx; yÞ, must be the same as Iðx; yÞ. Then, the estimated illumination L ðtþ1Þ ðx; yÞ at the ðt þ 1Þth iteration is given by L ðtþ1Þ ðx; yÞ ¼ 1 N ðtÞ ðx; yÞ</p><formula xml:id="formula_0">X 1 i¼À1 X 1 j¼À1 ÂL ðtÞ ðx þ i; y þ jÞw ðtÞ ðx þ i; y þ jÞ (1) with N ðtÞ ðx; yÞ ¼ X 1 i¼À1 X 1 j¼À1 w ðtÞ ðx þ i; y þ jÞ,<label>(2)</label></formula><p>where w ðtÞ ðx; yÞ ¼ gðd ðtÞ ðx; yÞÞ.</p><p>N ðtÞ ðx; yÞ in (2) represents a normalizing factor. A conduction function g is a nonnegative monotonically decreasing function such that gð0Þ ¼ 1 and gðd ðtÞ ðx; yÞÞ approaches 0 as d ðtÞ ðx; yÞ increases, where d ðtÞ ðx; yÞ represents the amount of discontinuity at each pixel ðx; yÞ. As verified in <ref type="bibr" target="#b14">[15]</ref>, this adaptive smoothing process is the approximation of anisotropic diffusion <ref type="bibr" target="#b19">[20]</ref>. The diffusion effect of smoothing makes it possible to preserve discontinuities efficiently during smoothing. Both the discontinuity measure d and the conduction function g are key factors which decide the smoothing characteristics in adaptive smoothing. Saint-Marc et al. <ref type="bibr" target="#b14">[15]</ref> originally proposed to use gradient magnitude and exponential function for d and g, respectively. However, gradient magnitude is insufficient for normalizing face images because cast shadow and facial features may have same gradient magnitude. New discontinuity measure, which can complement the gradient magnitude, is required. Moreover, in our case, the objective of smoothing is just preserving discontinuities to remove shadow in normalization procedure, not enhancing discontinuities. Therefore, the exponential function is not also suitable for illumination normalization because it has edge sharpening effect during the smoothing process. To solve these problems, we first introduce an additional discontinuity measure, followed by employing the idea that combines two discontinuity measures <ref type="bibr" target="#b15">[16]</ref>. Then, we propose the new conduction function without edge sharpening effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, the proposed method is described in detail. First, two measurements of discontinuities, which determine the level of discontinuity at each pixel, are described. Then, the new conduction function is introduced and analyzed in the framework of adaptive smoothing for face recognition. Finally, the entire process of the proposed method is explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Discontinuity measures</head><p>Although there are many discontinuity measures, it is very hard to measure discontinuities of illumination accurately in an image. Especially, in the case of face recognition, discontinuities of shadows and facial features must be discriminated. This means that facial features must be smoothed while discontinuities of shadows are preserved. If discontinuities of facial features are preserved, facial features may be ambiguous after normalization process. We try to solve this problem by using the method that combines two measurements of discontinuities inspired by Chen <ref type="bibr" target="#b15">[16]</ref>: spatial gradient and local inhomogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Spatial gradient</head><p>Spatial gradient is a common local discontinuity measure in image processing. The spatial gradient of an image Iðx; yÞ at pixel ðx; yÞ is defined as the first partial derivatives of its image intensity function with respect to x and y:</p><formula xml:id="formula_2">rIðx; yÞ ¼ ½G x ; G y ¼ qIðx; yÞ qx ; qIðx; yÞ qy , (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where the derivatives are approximated by</p><formula xml:id="formula_4">G x ¼ Iðx þ 1; yÞ À Iðx À 1; yÞ,<label>(5)</label></formula><formula xml:id="formula_5">G y ¼ Iðx; y þ 1Þ À Iðx; y À 1Þ. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>The magnitude of the gradient vector in (4), on the other hand, is given by</p><formula xml:id="formula_7">jrIðx; yÞj ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi G 2 x þ G 2 y q . (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Local inhomogeneity</head><p>In addition to spatial gradient, inhomogeneity as another measure of discontinuity was proposed in <ref type="bibr" target="#b15">[16]</ref>. Although this measure is very efficient in the contextual term, it is very time consuming. Therefore, we simplify this measure for real time application. Another measure that we propose is just the average of local intensity differences for each pixel ðx; yÞ in the face image. We call this measure local inhomogeneity. This measure provides the degree of uniformity between all the pixels in the small neighborhood and current pixel. Therefore, if local inhomogeneity at pixel ðx; yÞ is large, we can expect that discontinuity occurs at pixel ðx; yÞ. The average of local intensity differences at pixel ðx; yÞ is tðx; yÞ ¼</p><formula xml:id="formula_8">P P ðm;nÞ2O j Iðx; yÞ À Iðm; nÞj jOj , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where O is a local neighborhood of the pixel ðx; yÞ, and ðm; nÞ indicates the locations of pixels in the neighborhood O. We only consider the 3 Â 3 neighborhood of the pixel ðx; yÞ. Then, tðx; yÞ at each pixel ðx; yÞ is normalized by</p><formula xml:id="formula_10">tðx; yÞ ¼ tðx; yÞ À t min t max À t min ,<label>(9)</label></formula><p>where t max and t min , respectively, are the maximal and minimal values of t across the entire face image.</p><p>To emphasize higher values of tðx; yÞ that more likely correspond to cast shadow as in <ref type="bibr" target="#b15">[16]</ref>, we also adopt a nonlinear transformation as follows:</p><formula xml:id="formula_11">tðx; yÞ ¼ sin p 2 tðx; yÞ ; 0ptðx; yÞp1. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Although this measure is calculated based only on its nearest neighborhood, we can show that discontinuities of large scale features is properly represented by the propagation effect of local inhomogeneity incurred by iterative convolution. In Section 4, it will be shown that local inhomogeneity efficiently complement spatial gradient through various experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">New conduction function</head><p>To utilize two discontinuity measures, the proper conduction function g must be defined. As mentioned in Section 2, g must be a nonnegative monotonically decreasing function because a large weight should be assigned to a pixel that involves low discontinuity, and vice versa for discontinuity preserving. Among possible selections of g, a couple of the most representative forms applied in past work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> are given as</p><formula xml:id="formula_13">g 1 ðd; KÞ ¼ exp À d 2 2K 2 , (<label>1 1 )</label></formula><formula xml:id="formula_14">g 2 ðd; KÞ ¼ 1 1 þ ðd=KÞ 2 .</formula><p>(</p><p>Here, d is the amount of discontinuity as mentioned in Section 2.2, and K is a parameter that determines the level of discontinuities which must be preserved. The influence of the parameter K is discussed later. These two functions cause two different operations in adaptive smoothing as iteration proceeds: one is the smoothing within homogeneous regions, and the other is the sharpening of strong discontinuities (e.g. edges) that will be preserved. In order to see these two operations in more detail, let us recall that the adaptive smoothing is the approximation of the anisotropic diffusion and consider a 1D signal IðxÞ without loss of generality. Here, we adopt g 2 in <ref type="bibr" target="#b11">(12)</ref> given that the same analysis can be extended to g 1 in <ref type="bibr" target="#b10">(11)</ref>. Then, strong discontinuities can be regarded as a blurred step edge as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In this case, IðxÞ increases as x increases at the neighborhood of the edge, i.e., I x 40. Also I xx ¼ 0 and I xxx o0 at the point of inflection. I x , I xx and I xxx are, respectively, the first, second and third partial derivatives with respect of x. Especially, we approximate the response of two discontinuity measures (jrIðx; yÞj and tðx; yÞ) to a blurred step edge by the magnitude of the derivative at that point. Therefore, we can substitute I x for d, and then the adaptive smoothing can be approximated as the following anisotropic diffusion:</p><formula xml:id="formula_16">qI qt ¼ q qx ðfðI x ÞÞ<label>(13)</label></formula><p>with</p><formula xml:id="formula_17">fðI x Þ ¼ 1 1 þ ðI x =KÞ 2 I x .<label>(14)</label></formula><p>Here, we call fðI x Þ the flow magnitude function. In this paper, we are interested in looking at the variation of the slope of the edge in time: ðq=qtÞðI x Þ. Since I is differentiable, the order of differentiation can be inverted as follows:</p><formula xml:id="formula_18">q qt ðI x Þ ¼ q qx qI qt ¼ q qx q qx ðfðI x ÞÞ ¼ q 2 fðI x Þ qI 2 x I 2 xx þ qfðI x Þ qI x I xxx . (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>Applying the property of I xx ¼ 0 at the point of inflection and inserting ( <ref type="formula" target="#formula_17">14</ref>) into (15) yields Since I xxx o0 and a square term of the denominator is always positive, the sign of ðq=qtÞðI x Þ is critically determined by the sign of ð1 À</p><formula xml:id="formula_20">q qt ðI x Þ ¼ 1 À I 2 x =K 2 ð1 þ ðI x =KÞ 2 Þ 2 I xxx . (<label>16</label></formula><formula xml:id="formula_21">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_22">I 2 x =K 2 Þ q qt ðI x Þ40 when I x 4K<label>(17)</label></formula><p>and q qt ðI x Þo0 when I x oK.</p><p>(</p><p>Therefore, we see how K determines the level of discontinuities which must be preserved. That is, a step edge will always be sharpened as the iterations progress when I x 4K. On the other hand, if I x oK, a step edge will be smoothed eventually. Among these operations, edge sharpening operation is very efficient in the view of either feature extraction or scale space approach. However, this edge sharpening operation is not proper to our objective because unrealistic results can be produced as shown in Fig. <ref type="figure">3(c</ref>). This is because sharpening effects of preserved discontinuities, which are strictly determined by K, makes the boundary between shadow region and nonshadow region more visible after normalization. In this case, facial features with discontinuities above K can be also distorted, and selecting the proper K is very difficult. Therefore, we propose a new form of g without edge sharpening effect as follows:</p><formula xml:id="formula_24">g new ðd; KÞ ¼ 1 1 þ ffiffiffiffiffiffiffiffiffi d=K p . (<label>1 9 )</label></formula><p>In order to see that this function does not have the edge sharpening effect, we repeat the above analysis using g new instead of g 2 . Then, the flow magnitude function of ( <ref type="formula" target="#formula_17">14</ref>) is changed as follows:</p><formula xml:id="formula_25">fðI x Þ ¼ 1 1 þ ffiffiffiffiffiffiffiffiffiffiffi I x =K p I x . (<label>2 0 )</label></formula><p>Then, we can acquire the following result by applying (20) into (15):</p><formula xml:id="formula_26">q qt ðI x Þ ¼ 1 þ 1 2 ffiffiffiffiffiffiffiffiffiffiffi I x =K p ð1 þ ffiffiffiffiffiffiffiffiffiffiffi I x =K p Þ 2 I xxx . (<label>2 1 )</label></formula><p>Since I xxx o0 and the square root term is positive, the sign of ðq=qtÞðI x Þ is always negative irrespective of I x . Because the edge sharpening happens only when the sign of ðq=qtÞðI x Þ is positive, we can say that the proposed conduction function does not have edge sharpening operation. This can also be explained by the shape of the flow magnitude function f. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the flow magnitude function with the previous conduction function g 2 has finite optimum that f is monotonically increasing for jI x joK while it is monotonically decreasing for jI x j4K. However, the flow magnitude function with the proposed conduction function g new is a throughout-increasing function and possesses no finite optimum. After all, ðq=qtÞðI x Þ is always negative because the first derivative of the flow magnitude function is always positive. On the other hand, this means that discontinuities will be smoothed eventually. However, we are not concerned about this problem because smoothing of homogeneous region is much faster than smoothing of discontinuities. We apply the proposed conduction function g new to both spatial gradient and local inhomogeneity as follows: aðx; yÞ ¼ g new ðtðx; yÞ; hÞ, (</p><p>bðx; yÞ ¼ g new ðjrIðx; yÞj; SÞ.</p><p>(</p><p>Here, parameters h ð0oho1Þ and S ðS40Þ are used to determine the level of discontinuities which must be preserved. We automatically select the proper values of S and h by some experiments in Section 4. Now, the corresponding weights of the convolution mask wðx; yÞ are determined using aðx; yÞ and bðx; yÞ as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>wðx; yÞ ¼ aðx; yÞbðx; yÞ.</p><p>(</p><p>Note that wðx; yÞ are computed only once prior to smoothing unlike the previous adaptive smoothing <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Smoothing constraint</head><p>For more accurate description of real environments, we address an additional constraint that surfaces cannot reflect more light than what is shed on them. Thus, the reflectance R is restricted to be in the range R 2 ½0; 1, and LXS <ref type="bibr" target="#b18">[19]</ref>. We call this constraint the smoothing constraint. After applying both this smoothing constraint and weights wðx; yÞ determined by combining two discontinuity mea-sures to our method, (1) can be modified as follows: features are efficiently smoothed while the illumination discontinuities are preserved as iteration proceeds. Note that the signal variation under light region is much larger than that under shadow region, and the facial features (in this case, the two eyes) have far different scales in these two regions. For a good and stable performance of face recognition, therefore, it is necessary that the small scale signal in the shadow region must be amplified so that both the light and shadow regions have similar signal scales after illumination normalization. By taking the difference between the logarithms of the input image and the estimated illumination as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we can achieve this purpose and ultimately acquire the normalized face image. This procedure can be formulated as follows: R 0 ðx; yÞ ¼ logðIðx; yÞ þ 1Þ À logðL ðTÞ ðx; yÞ þ 1Þ.</p><formula xml:id="formula_30">L 0 ðtþ1Þ ðx; yÞ ¼ 1 N ðtÞ X 1 i¼À1 X 1 j¼À1 ÂL ðtÞ ðx þ i; y þ jÞwðx þ i; y þ jÞ, L<label>ðtþ1Þ</label></formula><p>(</p><formula xml:id="formula_31">)<label>26</label></formula><p>Here, T is the maximal iteration number. Note that the output is always negative (R 0 ðx; yÞp0) because of the smoothing constraint. Therefore, we bring R 0 ðx; yÞ back into the range ½0; 1 by</p><formula xml:id="formula_32">Rðx; yÞ ¼ R 0 ðx; yÞ À R 0 min R 0 max À R 0 min ,<label>(27)</label></formula><p>where R 0 max and R 0 min are the maximal and minimal values of R 0 ðx; yÞ across the entire image. Table <ref type="table" target="#tab_0">1</ref> summarizes the proposed Retinex method for estimating and normalizing illumination, where both smoothing constraint and adaptive weighting are applied. As shown in Table <ref type="table" target="#tab_0">1</ref>, aðx; yÞ and bðx; yÞ are only calculated once. Therefore, the inherent characteristics of illumination discontinuities are applied uniformly to all steps of iteration. Moreover, aðx; yÞ and bðx; yÞ are jointly used to determine weights wðx; yÞ. bðx; yÞ mainly reflects the overall characteristic of illumination in the face image, and thus bðx; yÞ would play a dominant role for representing large illumination variation when all the nearest neighborhood pixels of ðx; yÞ have similar local inhomogeneity values. On the other hand, aðx; yÞ plays the complementary role of bðx; yÞ. Particulary, shadows with small scale can be efficiently removed by combining aðx; yÞ and bðx; yÞ, and this will be shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results and discussion</head><p>In this section, the proposed method is analyzed and evaluated by various experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Test databases</head><p>In order to evaluate the proposed method in diverse environments, we use the images from three databases: Yale face database B <ref type="bibr" target="#b3">[4]</ref>, CMU PIE database <ref type="bibr" target="#b20">[21]</ref> and AR face database <ref type="bibr" target="#b21">[22]</ref>.</p><p>The Yale face database B contains 5760 images taken from 10 subjects under 576 viewing conditions (9 poses Â 64 illumination conditions). Since we are only concerned with the illumination problem in this paper, we select 640 images for 10 subjects representing 64 illumination conditions under the frontal pose. Images in the database are divided into five subsets based on the angle of the light source directions. The five subsets are subset 1 (0 -12 ), subset 2 (13 -25 ), subset 3 (26 -50 ), subset 4 (51 -77 ), and subset 5 (above 78 ) <ref type="bibr" target="#b3">[4]</ref>. We refer to images in both subsets 4 and 5 as images with harsh illumination conditions. Among the 640 images, we discarded seven corrupted images and finally constructed total 633 images (70, 118, 118, 138, 189  Rðx; yÞ ¼ R 0 ðx; yÞ À R 0 min R 0 max À R 0 min images in subsets 1-5). Fig. <ref type="figure" target="#fig_4">6</ref> shows the representative images (0 ) of the 10 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_33">X 1 i¼À1 X 1 j¼À1 L ðtÞ ðx þ i; y þ jÞwðx þ i; y þ</formula><p>The CMU PIE database contains 41,368 images obtained from 68 subjects. We took the frontal face images with 21 different illumination conditions (see Fig. <ref type="figure" target="#fig_18">19</ref>) which are captured without room lights. Thus, the total number of images we used for our test is 1428.</p><p>The AR face database includes 26 frontal images with different facial expressions, illumination conditions, and occlusion for 126 subjects. Images were recorded in two different sessions 14 days apart. Among all images in two sessions, we only select images with six different illumination conditions for 120 subjects. Among six images of each subject, first three images were recorded in the first session and the remainder were recorded in the second session. Thus, the total number of images we used for our test is 720.</p><p>All images in three databases are manually aligned based on the positions of the eyes and resized to 100 Â 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameter selection</head><p>In the proposed method, there are three parameters S, h and T. Among these parameters, S and h determine level of discontinuities which must be preserved. As in <ref type="bibr" target="#b15">[16]</ref>, an optimal set of S and h may be found from an empirical study by SNR objective function. However, our objective in this paper is the illumination normalization for face recognition, not the noise removal. Therefore, in order to find the proper set of S and h, we have evaluated recognition accuracies for all three databases by varying values of S and h while keeping the value of T as 10. By an exhausted search, we have found that the proper values of two parameters with highest recognition rate are almost in the ranges of 0pSp10 and 0php0:1, and we also have discovered that the proper values of S and h generally have nonlinear relationship with the mean of each discontinuity measure. Among many possible candidates, we empirically apply a simple exponential, i.e., nonlinear, function to determine specific values of S and h as follows: S ¼ 10 expðÀjrIðx; yÞj mean =10Þ, (28)</p><formula xml:id="formula_34">h ¼ 0:1 expðÀtðx; yÞ mean =0:1Þ, (<label>29</label></formula><formula xml:id="formula_35">)</formula><p>where jrIðx; yÞj mean and tðx; yÞ mean are, respectively, the mean of spatial gradient and local inhomogeneity. Finding out, theoretically, the optimal nonlinear function is left for future study for now.</p><p>On the other hand, T is the maximal iteration number for termination. To find an optimal value of T in the view of illumination normalization is also a nontrivial issue. If T is too small, the dynamic range would be very small while shadows is removed effectively. If T is too large, the reverse would be true. Note that there is close correlation between the dynamic range of an image and the recognition performance. If the dynamic range is very small, recognition algorithms cannot model variations of faces due to intrinsic factors such as facial expression and pose although shadows are removed perfectly. Therefore, to see the effects of T in detail, we evaluate the proposed method using PCA for three databases while T varies. Given T, recognition accuracies for each database are calculated by only using one image per subject in all different lighting conditions (Yale B: 58, CMU PIE: 21, AR: 6) as the training set, in turn, and averaging the results. In Yale face database B, images under only 58 lighting conditions are used as the training set excluding six lighting conditions which include corrupted images. Note that all following experiments for Yale face database B just use images under 58 lighting conditions as the training set.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, T with the highest recognition rate are different for each database. In Yale face database B, the highest recognition rate of 99:82% is achieved at T ¼ 18, and in CMU PIE database, the highest recognition rate of 99:06% at T ¼ 14 is achieved. Then, recognition rate decreases very slowly. On the other hand, AR face database does not achieve the highest recognition rate even until 50 iterations. These results seem to be caused by the fact that AR face database has small variations of illumination conditions, while it has relatively more variations of facial features than the other two databases. Fortunately, the performance increases very slowly as iterations are increased. Therefore, we set T as 25 with satisfactory performances for all three databases in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Efficiency of combining two discontinuity measures, the new conduction function and smoothing constraint</head><p>In this section, we show that combining two discontinuity measures, the new conduction function and smoothing constraint are very efficient in terms of recognition accuracies. For this purpose, some experiments are carried out in all three databases. As in the former section, we also use one image per each subject in all different lighting conditions (Yale B: 58, CMU PIE: 21, AR: 6) as the training set, in turn. For example, in case of Yale face database B, the first trial use 10 images under the first illumination condition (0 in subset 1) as a training set.</p><p>First, we compare the proposed method with using only a single respective discontinuity measure: spatial gradient or local inhomogeneity. From Figs. 8-10, it is clear that combining two discontinuity measures is superior to only use a single discontinuity measure alone. In the second experiment, the proposed method with the new conduction function g new is compared with using the previous conduction functions g 1 and g 2 . For fair comparison, we conduct same analysis as in Section 4.2 for g 1 and g 2 , and set S as the mean of spatial gradient while h is set as the mean of lowest 50% local inhomogeneity values across a whole image following a similar procedure as in <ref type="bibr" target="#b15">[16]</ref>. We already showed in Section 3 that the function g new is better suited for illumination   normalization both theoretically and visually. As shown in Figs. 12 and 14, the overall recognition rates for Yale face database B and AR face database can thus be improved by applying g new . However, we observe in Fig. <ref type="figure" target="#fig_12">13</ref> that there exist unexpected cases for CMU PIE database where g new reveals worse performance, compared to g 2 . Actually, average recognition rates for g new and g 2 are, respectively, 98:84% and 98:77%, and this close result clearly seems to be due to the 16th trial where images under 16th illumination condition are used for training. To explain the result of 16th trial, we analyze the characteristic of two conduction functions (g new and g 2 ) in regions with very low values of discontinuity measures. Let us recall that the behavior of the conduction functions can be described by the shape of their flow magnitude functions. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>    discontinuity measures of facial features in shadow regions have very low values. Therefore, the conduction function g 2 can smooth facial features more rapidly, and ultimately facial features are better enhanced after normalization. Although this characteristic of the conduction function g 2 increases the recognition accuracy, thisseems to be a very rare case corresponding just to the 16th trial. Moreover, normalization results for previous conduction functions tend to contain outlines of facial features (eyes, mouth, and nose) while other details such as pupils and skin are removed or distorted because of sharpening effects of strong discontinuities. Especially, this situation is worse in the case of g 1 , and consequently, g 1 reveals the lowest recognition rates among three conduction functions. Therefore, we can say that the new conduction function is generally superior to two previous conduction functions with the results for other two databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Finally, we verify the efficiency of applying the smoothing constraint by comparing the recognition rates between two cases; with and without the smoothing constraint. Figs. <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> show that the smoothing constraint is very efficient just for Yale face database B and CMU PIE database while AR database renders little difference. From these results, we can see that the smoothing constraint could be very efficient under harsh illumination conditions because AR face database has small variations of illumination conditions whereas other two databases contain images with wide spectrum of illumination condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with other methods of illumination normalization</head><p>In this section, we compare the proposed method with other methods of illumination normalization: SSR <ref type="bibr" target="#b10">[11]</ref>, MSR <ref type="bibr" target="#b11">[12]</ref>, and SQI <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. These three methods are also Retinex method as our proposed method. The main difference between the proposed method and SSR is that SSR uses the Gaussian kernel to estimate illumination, while in MSR, smoothing results which apply three Gaussian kernels with different sizes are averaged. Unlike SSR and MSR, SQI average smoothing results which apply three weighed Gaussian kernels in   which the convolution region is divided into two sub-regions with respect to a threshold. The threshold is calculated by averaging intensity values of all pixels in the convolution region. We decide the size of kernels which used in three methods by applying the same analysis as it to decide T of the proposed method in Section 4.2 (see Table <ref type="table" target="#tab_2">2</ref>). The recognition accuracies are compared by applying PCA <ref type="bibr" target="#b0">[1]</ref>. Only one image for each subject is selected as a training set as well in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>First, CMU PIE database is tested using the same evaluation method as in Section 4.3, i.e., images under all 21 illumination conditions are used as the training set. To be more precise, the training set of the ith trial consists of 68 images with the ith illumination condition as in Fig. <ref type="figure" target="#fig_18">19</ref>. Fig. <ref type="figure" target="#fig_17">18</ref> shows recognition rates according to each method. As shown in the result, the proposed method outperforms other methods and has recognition rates over 94% in all 21 trials. Fig. <ref type="figure" target="#fig_18">19</ref> shows the illumination normalization results of one subject in CMU PIE database. Note that our method has superior results even when images with bad illumination conditions (1, 2, 15 and 16th in Fig. <ref type="figure" target="#fig_18">19</ref>) are used as a training set.</p><p>For AR face database, we carry out the same experiment as CMU PIE database. Fig. <ref type="figure" target="#fig_19">20</ref> shows results according to each method. Unlike results in CMU PIE database, it is interesting that recognition rates of SQI are higher than them of MSR. This can be explained by the results of each methods near cast shadows in face images. Face images in CMU PIE database include many cast shadows by very harsh lighting conditions unlike those in AR face database. As shown in Fig. <ref type="figure" target="#fig_21">22</ref>, SQI produces more unnatural results than MSR on cast shadows. This may be the reason why MSR outperforms SQI in CMU PIE database although SQI is more sophisticated technique. Irrespective of the competition between other methods, the proposed method also outperforms other methods in all trials. Table <ref type="table" target="#tab_3">3</ref> summarizes recognition rates by averaging results of all trials for each database, respectively.</p><p>In Yale face database B, the proposed method is tested in another detailed way <ref type="bibr" target="#b22">[23]</ref>. We first use the 10 images (one image for each subject in subset 1) in Fig. <ref type="figure" target="#fig_4">6</ref> as a training set and test images of all subsets except for the training set. As shown in Fig. <ref type="figure" target="#fig_20">21</ref>, the proposed method achieves recognition rates of 100% in all subsets. In this test, SSR and SQI also have high recognition rates, but still relatively lower than our proposed method. Other than three methods that we have evaluated by ourself, illumination cone <ref type="bibr" target="#b3">[4]</ref> achieved 100%, 100% and 100%, and 9PL <ref type="bibr" target="#b5">[6]</ref> achieved 100%, 100% and 97.2% on subsets 2, 3 and 4. These results are from   experiments that 7-9 images for each subject have been used as the training set. Harmonic relighting <ref type="bibr" target="#b6">[7]</ref> reported 100%, 100% and 90% on subsets 2, 3 and 4 by using one image for each subject as the training set. Note that our method only reports results on subset 5 and achieve 100% recognition rates on all five subsets as in <ref type="bibr" target="#b22">[23]</ref>. Second, we use images in each of the other subsets   are tested. In each testX , results of all possible trials are averaged. As shown in Fig. <ref type="figure" target="#fig_22">23</ref>, the proposed method has always consistent and promising results over 98.87% in all testX outperforming the other methods. As shown in Fig. <ref type="figure" target="#fig_21">22</ref>, these results can be accepted clearly by checking illumination normalization results of each method visually. The average recognition rates are also computed and shown in Table <ref type="table" target="#tab_4">4</ref>. As shown in Table <ref type="table" target="#tab_4">4</ref>, the proposed method reach high average recognition rate of 99.77% and overwhelm other three methods. This result is even better than 99.42% of TVQI in <ref type="bibr" target="#b22">[23]</ref>. Especially, our method is very simple and fast as will be shown in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computation time</head><p>In this section, we evaluate the average computation time of the proposed method. The average computation time is acquired by averaging the computation times of 633 images in Yale face database B. Tests are conducted using Microsoft Visual C þ þ Version 6.0 on a personal computer system equipped with a 2.4 GHz Intel Core2 Quad processor, and parameter settings in Table <ref type="table" target="#tab_2">2</ref> are applied without any changes. As shown in Table <ref type="table" target="#tab_5">5</ref>, the proposed method takes very short computation time of 50.43 ms. Although SSR has a little shorter computation time than the proposed method, SSR has lower recognition rates than the proposed method in all databases. Thus, we can say that the proposed method is the most reasonable in terms of both recognition accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel Retinex method for illumination invariant face recognition. In general framework of Retinex method, adaptive smoothing, which convolves an input image iteratively with a 3 Â 3 smoothing mask, was employed to estimate illumination. Coefficients of a smoothing mask for each pixel are determined via combining two discontinuity measures. Adaptive smoothing was designed especially to be suited for face images by applying two additional concepts: the new conduction function and the smoothing constraint, and it turned out to be superior to existing smoothing methods in terms of illumination estimation. Using the proposed method, we showed that images with even strong cast shadow are effectively normalized, and consequently the proposed method notably outperformed the existing   methods from the viewpoint of recognition accuracy as well. Moreover, the proposed method presented consistent and promising results even when we used images under harsh illumination conditions as a training set. We believe that the proposed method has a wide range of applications in real time face recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Future work will address the following aspects. First, we will construct the database with outdoor images and analyze the behavior of our method using the database. Second, we plan to scrutinize the nonlinear function to determine the optimal values of the parameters S and h. Third, we will extend our method to color images and employ recently developed tensor techniques <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> to facilitate extraction of illumination invariant features.  Jung for his insightful discussions on various aspects of the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General framework of Retinex method.</figDesc><graphic coords="3,37.65,559.94,219.60,108.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Profile of a blurred step edge. Fig. 3. Comparison of normalization results for different conduction functions. (a) The original facial image of 100 Â 100 pixels. (b) Smoothing result by the conduction function g 2 . (c) Normalization result that is acquired by dividing the original image by (b). (d) Smoothing result by the conduction function g new . (e) Normalization result that is acquired by dividing the original image by (d).</figDesc><graphic coords="5,282.73,66.73,219.60,178.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of two flow magnitude functions. (a) Flow magnitude functions by g new . (b) Flow magnitude functions by g 2 .</figDesc><graphic coords="6,113.51,518.38,321.66,150.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>25 ) 3 . 4 .</head><label>2534</label><figDesc>Fig.5shows a result of illumination estimation using the adaptive smoothing with the smoothing constraint and weights determined by combining two discontinuity measures through the new conduction function g new . Each region A and B in the graph corresponds to left and right eyes, respectively, in the face. We can see that regions of facial</figDesc><graphic coords="7,109.24,305.14,321.84,363.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Representative images of 10 subjects in Yale face database B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 11 compares illumination normalization results when different discontinuity measures are applied to one of images under 28th illumination condition (subset 4) in Yale face database B. It is shown that cast shadow under the left eye is completely removed by combining spatial gradient and local inhomogeneity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Recognition rates (%) as maximal iteration number T varies for each database.</figDesc><graphic coords="10,41.95,66.73,219.60,165.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Recognition rates (%) of when using different discontinuity measures in Yale face database B: spatial gradient, local inhomogeneity and spatial gradient and local inhomogeneity. Results of 58 trials using training sets with different illumination conditions are shown.</figDesc><graphic coords="10,287.03,66.73,219.60,166.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Recognition rates (%) of when using different discontinuity measures in CMU PIE database: spatial gradient, local inhomogeneity and spatial gradient and local inhomogeneity. Results of 21 trials using training sets with different illumination conditions are shown.</figDesc><graphic coords="10,287.03,317.79,219.60,164.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 11. Comparison of normalization results for different discontinuity measures. (a) The original facial image of 100 Â 100 pixels. (b) Normalization result when using both spatial gradient and local inhomogeneity. (c) Normalization result when only using spatial gradient. (d) Normalization result when only using local inhomogeneity.</figDesc><graphic coords="11,108.60,559.10,322.92,89.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Recognition rates (%) of when using different discontinuity measures in AR face database: spatial gradient, local inhomogeneity and spatial gradient and local inhomogeneity. Results of six trials using training sets with different illumination conditions are shown.</figDesc><graphic coords="11,37.65,316.99,219.60,168.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Recognition rates (%) of when using different conduction functions in Yale face database B: g new , g 1 , and g 2 .</figDesc><graphic coords="11,282.73,138.46,219.60,164.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Recognition rates (%) of when using different conduction functions in CMU PIE database: g new , g 1 , and g 2 .</figDesc><graphic coords="11,282.73,347.71,219.60,167.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.14. Recognition rates (%) of when using different conduction functions in AR face database: g new , g 1 , and g 2 .</figDesc><graphic coords="12,41.95,66.73,219.60,167.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Recognition rates (%) of the proposed method with smoothing constraint and one without smoothing constraint in Yale face database B.</figDesc><graphic coords="12,287.03,66.73,219.60,166.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Recognition rates (%) of the proposed method with smoothing constraint and one without smoothing constraint in CMU PIE database B.</figDesc><graphic coords="12,287.03,293.88,219.60,166.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Recognition rates (%) of the proposed method with smoothing constraint and one without smoothing constraint in AR face database.</figDesc><graphic coords="13,37.65,66.73,219.60,170.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Recognition accuracies (%) on CMU PIE database.</figDesc><graphic coords="13,282.73,66.73,219.60,166.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Twenty one different illumination conditions for each subject in CMU PIE database and their illumination normalization results by the proposed method.</figDesc><graphic coords="14,113.54,66.73,321.84,317.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Recognition accuracies (%) on AR face database.</figDesc><graphic coords="14,41.95,437.34,219.60,169.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Recognition accuracies (%) on Yale face database B using only the representative images as the training set.</figDesc><graphic coords="15,37.65,66.73,219.60,167.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Illumination normalization comparison in Yale face database B.</figDesc><graphic coords="15,37.65,293.88,219.60,195.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Recognition accuracies (%) of (a) test2, (b) test3, (c) test4, and (d) test5 on the Yale face database B.</figDesc><graphic coords="16,41.56,66.73,465.84,387.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 The</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">proposed Retinex method</cell></row><row><cell cols="2">1. Initialization</cell></row><row><cell cols="2">Input a given image I</cell></row><row><cell cols="2">L ð0Þ ðx; yÞ ¼ Iðx; yÞ; 8ðx; yÞ</cell></row><row><cell cols="2">Set parameters S and h for smoothing and maximal iteration</cell></row><row><cell>number T</cell><cell></cell></row><row><cell cols="2">2. Computation of weights</cell></row><row><cell cols="2">For each pixel ðx; yÞ, compute both aðx; yÞ and bðx; yÞ by (22)</cell></row><row><cell>and (23)</cell><cell></cell></row><row><cell cols="2">Compute weights wðx; yÞ from aðx; yÞ and bðx; yÞ</cell></row><row><cell cols="2">wðx; yÞ ¼ aðx; yÞbðx; yÞ</cell></row><row><cell cols="2">3. Iteration until t ¼ T</cell></row><row><cell cols="2">Perform iterative convolution to update L ðtÞ ðx; yÞ</cell></row><row><cell>L 0 ðtþ1Þ ðx; yÞ ¼</cell><cell>1 Nðx; yÞ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Parameter setting of the proposed method and other three methods for comparison</figDesc><table><row><cell>Methods</cell><cell>Parameters</cell></row><row><cell>Proposed method</cell><cell>T ¼ 25</cell></row><row><cell>SSR</cell><cell>k ¼ 15</cell></row><row><cell>MSR</cell><cell>k ¼ 5; 9; 15</cell></row><row><cell>SQI</cell><cell>k ¼ 5; 9; 15</cell></row></table><note><p>k is the size of kernels used in other three methods.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 Average</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Proposed method</cell><cell>SSR</cell><cell>MSR</cell><cell>SQI</cell></row><row><cell>CMU PIE (%)</cell><cell>98.84</cell><cell>94.75</cell><cell>96.86</cell><cell>91.24</cell></row><row><cell>AR (%)</cell><cell>91.61</cell><cell>84.69</cell><cell>83.5</cell><cell>86.33</cell></row></table><note><p>recognition rate (%) comparison on CMU PIE database and AR face database</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Average recognition rate (%) comparison on Yale face database B</figDesc><table><row><cell></cell><cell>Proposed</cell><cell>SSR</cell><cell cols="2">MSR SQI</cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell></row><row><cell>Average recognition</cell><cell>99.77</cell><cell cols="2">95.48 98.43</cell><cell>97.28</cell></row><row><cell>rate (%)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Average computation time (ms) comparison</figDesc><table><row><cell></cell><cell>Proposed</cell><cell cols="3">SSR MSR SQI</cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average computation</cell><cell>50.43</cell><cell>33.5</cell><cell>66.1</cell><cell>137.67</cell></row><row><cell>time (ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Y.K. Park et al. / Signal Processing 88 (2008) 1929-1945</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for useful suggestions and detailed comments. The authors also wish to thank Dr. Cheolkon</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative common vectors for face</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neamtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barkana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="383" to="390" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nine points of light: acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition under generic illumination based on harmonic relighting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Pattern Recognition Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face relighting with radiance environment maps</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An alternative technique for the computation of the designator in the Retinex theory of color vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="3078" to="3080" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image preprocessing algorithm for illumination invariant face recognition, in: Fourth International Conference on Audio and Video Based Biometric Person Authentication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brajovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround Retinex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multiscale Retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized quotient image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face recognition under varying lighting conditions using self quotient image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive smoothing: a general tool for early vision</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saint-Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="514" to="529" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive smoothing via contextual and local discontinuities</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1552" to="1567" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LCIS: a boundary hierarchy for detailpreserving contrast reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Computer Vision</title>
		<meeting>the 1998 IEEE International Conference on Computer Vision<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robust recursive envelope operators for fast Retinex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keshet</surname></persName>
		</author>
		<idno>HPL-2002-74R1</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Hewlett-Packard Research Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The CMU pose illumination and expression (PIE) database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The AR face database</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>#24</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Center (CVC)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Illumination normalization for face recognition and uneven background correction using total variation based image models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IEEE CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">General tensor discriminant analysis and gabor features for gait recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1700" to="1715" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised tensor learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Inform. Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminant locally linear embedding with high order tensor data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems Man Cybernet</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian tensor approach for 3D face modelling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Systems Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
