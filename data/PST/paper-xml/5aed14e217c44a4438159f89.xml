<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Malware with an Ensemble Method Based on Deep Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-12">12 March 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinpei</forename><surname>Yan</surname></persName>
							<idno type="ORCID">0000-0002-2959-6165</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos; an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos; an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Qi</surname></persName>
							<idno type="ORCID">0000-0003-2386-4154</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos; an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos; an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qifan</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos; an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos; an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Malware with an Ensemble Method Based on Deep Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-12">12 March 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">D73B7BD58CCBB02A517247876D1D4EFE</idno>
					<idno type="DOI">10.1155/2018/7247095</idno>
					<note type="submission">Received 18 August 2017; Revised 3 December 2017; Accepted 6 February 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Malware detection plays a crucial role in computer security. Recent researches mainly use machine learning based methods heavily relying on domain knowledge for manually extracting malicious features. In this paper, we propose MalNet, a novel malware detection method that learns features automatically from the raw data. Concretely, we first generate a grayscale image from malware file, meanwhile extracting its opcode sequences with the decompilation tool IDA. Then MalNet uses CNN and LSTM networks to learn from grayscale image and opcode sequence, respectively, and takes a stacking ensemble for malware classification. We perform experiments on more than 40,000 samples including 20,650 benign files collected from online software providers and 21,736 malwares provided by Microsoft. The evaluation result shows that MalNet achieves 99.88% validation accuracy for malware detection. In addition, we also take malware family classification experiment on 9 malware families to compare MalNet with other related works, in which MalNet outperforms most of related works with 99.36% detection accuracy and achieves a considerable speed-up on detecting efficiency comparing with two state-of-the-art results on Microsoft malware dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, various kinds of software provide wealth resources for users but also bring a certain potential danger; thus malware detection is always a highly concerned issue in computer security field. According to the recent study, the number of malicious samples is rapidly increasing. For instance, 69,277,289 kinds of malicious objects (scripts, exploits, executable files, etc.) are detected by Kaspersky Lab in 2016 <ref type="bibr" target="#b0">[1]</ref>. The total number of malware samples increased 22% in the past four quarters to 670 million samples detected by McAfee Labs <ref type="bibr">[2]</ref> in 2017. The number of samples is too large, requiring a highly effective way to detect malwares.</p><p>A large number of researches have studied methods for analyzing and detecting malware. Traditional commercial antivirus products usually rely on signature-based method, which needs a local signature database to store patterns extracted from malware by experts. However, this approach has great limitations since specific minor changes to malware can change the signature, so more and more malware could easily evade signature-based detection by encrypting, obfuscating, or packing. Hence, many different malware detection approaches with machine learning technology have been proposed in recent years, such as static analysis which learns statistical characteristics like API calls, ğ‘-grams, and so on <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4]</ref> or dynamic behavior analysis <ref type="bibr" target="#b3">[5]</ref>. Though dynamic analysis does not require complex reverse engineering, it needs to simulate the operation environment for malwares, which is difficult to arouse all malware behaviors. At the same time, it is time-consuming for malware behavior monitoring since some malicious behaviors hide for a long time before attack. For static analysis, a great strength is that it can achieve rapid detection for massive malwares. However, various encryption and obfuscation techniques are the major issue for static analysis. Attackers can deliberately make various changes on malwares, hence static analysis is difficult to capture the characteristics of malware. Meanwhile, malware uses packing technologies to prevent reverse engineering which leads to high costs for static analysis.</p><p>At present, several machine learning methods <ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref> are paid the most attention for solving the above problems and have been applied to malware detection in the industry. However, many of them heavily rely on the relevant domain knowledge for malware analysis and artificial features extraction. These features are used to train a classification machine learning model and finally make the classification for a new file sample. But a serious problem is that malware is constantly being created, updated, and changed. To deal with this, a great deal of expert knowledge is required to catch up the changing malware environment, and the original well-designed features may not be applicable to a new malware family (a malware family refers to a malware variants group with homogeneous attack behaviors), resulting in heavy and inefficient feature engineering work. Thus, how to reduce the cost of artificial feature engineering and how to extract useful information from the raw data and let the model achieve features of self-learning to improve the accuracy and efficiency for malware detection are our main motivations.</p><p>In this paper, we present MalNet, a novel malware detection method for detecting whether a Windows executable file is malware. MalNet performs a comprehensive static analysis which includes two novel methods based on deep neural networks. One method is learning from grayscale images by Convolution Neural Network (CNN). The grayscale image is extracted from raw binary file in which CNN can get the structure features of a malware from its local image patterns. The other method is learning from opcode sequence by Long-Short Term Memory (LSTM). Opcode sequences are extracted by decompilation tool where LSTM can learn features about malicious code sequences and patterns. In reality, since malware often contains very long opcode sequences which cause the gradient vanishing problem of LSTM when training, we take truncated backpropagation algorithm based on subsequence to solve this problem in this paper which can also allow LSTM parallel computing on a bunch of subsequences to improve training efficiency. Meanwhile considering that malicious codes may be implanted into a normal file by attackers, in this case malicious features or behaviors only appear in some opcode subsequences; hence we come up with subsequence selection method to filter out benign subsequences of a malware which may mislead LSTM. Overall, MalNet uses these two networks to learn features from the raw data and then uses stacking ensemble to fuse two networks' discriminant result with extra metadata feature and finally generates a binary classification result for malware detection.</p><p>To verify the performance of MalNet, we perform evaluation experiments on a large dataset, which contains 21,736 malware samples from Microsoft and 20,650 benign samples collected by us. We choose 1/10 samples as validation dataset, where MalNet achieves detection accuracy of 99.88% and true positive rate of 99.14% with a false positive rate of 0.1%, much higher than the ğ‘-gram baseline result. Meanwhile we also make a malware family classification for 21,736 malware samples in 9 malware families to compare MalNet with other related works, where MalNet achieves 99.36% overall accuracy outperforming most of other methods. Besides, since rapid growing malware samples require a fast and efficient malware detection method, we evaluate the detection efficiency for MalNet. The result shows that since MalNet does not need to do special feature extraction, it only takes 0.03 s to give a prediction in detection phase which is superior comparing with two state-of-the-art methods only costing a little detection accuracy behind.</p><p>In summary, we make the following contributions in this paper:</p><p>(i) We propose a novel approach using deep neural networks for malware detection which takes CNN and LSTM networks to automatically learning features from the raw data to capture the malicious file structure patterns and code sequence patterns. It greatly reduces the cost of artificial features engineering.</p><p>(ii) We design and implement MalNet, a malware detection method, and solve practical problems such as grayscale image generation, very long sequences learning and gradient vanishing problem for LSTM, parallel computation for LSTM, and noise data processing. And we further use stacking ensemble for MalNet to combine networks' results to optimize the detection accuracy.</p><p>(iii) We make a series of evaluation experiments for MalNet including malware detection and malware family classification. The results show that MalNet outperforms most of other related approaches on malware detection accuracy and gets a superior detecting efficiency.</p><p>The rest of this paper is organized as follows. Related work is discussed in Section 2. MalNet detection methodology is introduced in Section 3. Experiments and analysis are presented in Section 4. Sections 5 and 6 discuss and conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Malware detection has always been a concern area of research in recent years. Several methods and techniques have been proposed to counter the growing amount and sophistication of malware.</p><p>Static Analysis for Malware Detection. Static analysis often uses lexical analysis, parsing, control flow, and data flow analysis techniques <ref type="bibr" target="#b6">[8]</ref> to mine the program. One common static malware detection method for the previous industry communities is signature-based method. For an unknown executable file, they can determine whether it is a known malware by searching whether there is a matching signature in the malicious code database. This detection method <ref type="bibr" target="#b7">[9]</ref> generated a unique signature identifier for a malware based on some specific manually designed features. However, signature-based methods are limited to detect unknown malwares, since an unknown malware may contain new features not captured by signatures. In addition, these signatures present a series of fixed malicious characteristics. So if the malware passes through the some encryption or obfuscation operation, it will get a high probability to evade the signaturebased detection.</p><p>The current situation has promoted the development of dynamic analysis. Moser et al. <ref type="bibr" target="#b6">[8]</ref> explored the shortcomings of static analysis methods and introduced a code obfuscation scheme that make it harder to complete the detection relying solely on static analysis. Since the dynamic analysis is not susceptible to code obfuscation conversion, it is an important complement to static analysis.</p><p>Dynamic Analysis for Malware Detection. Dynamic analysis is used by running a malware in a controlled environment (virtual machine, simulator, emulator, sandbox, etc.) and analyzing the behavior of malicious code (interaction with the system) <ref type="bibr" target="#b8">[10]</ref>. Before executing the malware sample, the corresponding monitoring tools are required to open first such as Process Monitor, Capture BAT (for monitoring file system and registry), Process Explorer, and Process Hackerreplace (for monitoring process), Wireshark (for monitoring network), and Regshot (for detecting system change).</p><p>In the process of dynamic analysis, malware detection result comes from behavior information (including system calls traces, network access, and file and memory modifications <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b9">11]</ref>) collection and analysis from the operating system (the execution environment of the program) through software runtime. These techniques have been widely studied as malware detection solutions, but they have also been noted to be less robust when exposed to large dataset <ref type="bibr" target="#b10">[12]</ref>. Since it is hard to simulate every situation that can arouse malware behavior, it is difficult to determine the effective time for monitoring malware activity and when to stop. Hence, simulating all malware behaviors needs continuous monitoring of malware behavior which results in colossal waste of computer resources, and it will be an arduous task when detecting mass malwares in present. However, antivirus engines today receive a flood of new malware samples each day, so an automated approach is needed to be fast and save the cost of extensive manual analysis. A variety of machine learning based techniques have been proposed and used for malware detection.</p><p>Machine Learning Based Malware Detection. Recently, machine learning methods (e.g., Support Vector Machines (SVM), Decision Trees (DT)) have been used to detect and classify unknown samples for malware family due to its scalability, rapidity, and flexibility. Schultz et al. <ref type="bibr" target="#b11">[13]</ref> first proposed to apply the data mining method to detect malware and used three different types of static features, respectively, PE head, string sequence, and byte sequence. Then a rulebased algorithm called Ripper <ref type="bibr" target="#b12">[14]</ref> is applied to DLL data mining and used naive Bayesian as a learning algorithm to find the character data and pattern feature information of byte sequence. It takes the malicious code data as input and obtains their best classification accuracy rate of 97.11%. Kolter and Maloof <ref type="bibr" target="#b13">[15]</ref> then achieved a better result by using ğ‘gram instead of nonoverlapping byte sequence features for data mining. Their conclusion suggests that the best decision can be obtained by using the boost decision tree.</p><p>Saxe and Berlin <ref type="bibr" target="#b14">[16]</ref> instead proposed a method to distinguish malware from benign one with a neural network. In their research, entropy histogram is calculated from binary data and the number of callings of the contextual byte data, and metadata of execution files and DLL import are extracted. Those four types of features are transformed to 256 dimensions vector one by one. Unknown samples are classified with feature vectors which are learned in a fourlayer neural network. Their TPR result is 95.2% while FPR is 0.1%.</p><p>And there are some novel ideas for malware detection. Nataraj et al. <ref type="bibr" target="#b15">[17]</ref> proposed a visualized malware classification approach through image processing. Specifically, the malware binary data is transformed into a grayscale image, and the classification was done by kNN model with Euclidean distance calculation. The experiments show that it is a fast malware detection method, but this method uses global image features so that attacker can use some local transformation for malware to evade. So in their follow-up paper <ref type="bibr" target="#b16">[18]</ref>, they compared two methods of image feature processing with dynamic analysis. The experimental results show that the method based on image feature is efficient and scalable and can obtain an accuracy closing to the dynamic analysis result. They also found that this improved method can perfectly deal with both packed and unpacked malware samples. Kong and Yan <ref type="bibr" target="#b17">[19]</ref> proposed a framework for automatic malware classification based on unsupervised clustering learning by structured information (function call graphs). After extracting the fine-grained feature for function call graph of the malware, it will calculate the similarity of the malware by the distance matrix based discrimination learning method to cluster samples with the same malware family. After that, these pairs of malware distances are used for classification by an ensemble classifier. Santos et al. <ref type="bibr" target="#b18">[20]</ref> proposed a method using ğ‘-gram features to distinguish malware from benignware. In their research, unknown malware is detected by ğ‘˜-nearest samples with most similar ğ‘-gram features. And there are more approaches with similar idea using ğ‘-gram based on byte, opcode, or API call frequency for identifying malware <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b19">21]</ref>.</p><p>Moreover, since it is difficult to accurately and efficiently complete malware detection from a single point of view of static or dynamic analysis, some studies have begun to integrate both dynamic and static features. Santos et al. <ref type="bibr" target="#b20">[22]</ref> proposed a hybrid malware detection tool based on machine learning algorithms called OPEM that utilizes a set of features obtained from static and dynamic analysis of malicious code. Static features are obtained by mining opcodes from the executable files, and dynamic features are obtained by monitoring system calls, operations, and exceptions. The maximum accuracy of their malware detection rate is 96.60% with SVM classifier. The experiments proved that the hybrid method could get a better performance compared with running static or dynamic analysis separately.</p><p>The above machine learning based malware detection has achieved pretty good results; however, most of these methods rely heavily on expert knowledge for the design of features. At the same time, as the malware continues to grow and change dynamically, the human-designed features face many challenges which require a significant cost for manually updating features in response to new malware. Therefore, this paper tries to extract useful information from massive raw data and reduce the cost of artificial feature engineering by automatic feature learning characteristic of deep neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detection Methodology</head><p>In this section, we introduce the proposed method for malware detection and come up with a malware detection method called MalNet which uses CNN and LSTM networks.</p><p>For malware detection, MalNet actually performs a binary classification task, receiving the raw file data as input, and outputs a discrimination probability indicating how likely it is a malware. Concretely, the detection process by MalNet can be divided into two stages (see in Figure <ref type="figure" target="#fig_0">1</ref>). The first stage is to preprocess malware sample data, it takes a binary form of a Windows executable file, generates a grayscale image from it, and extracts opcode sequence and metadata feature with decompilation tool. So this stage generates the appropriate data format as the input of the follow-up CNN and LSTM networks. The second stage applies the core process of MalNet, which takes CNN and LSTM networks, respectively, learning from the grayscale image and the opcode sequence. To optimize the detection performance, we use stacking ensemble to integrate two networks' output and metadata features and get final prediction result.</p><p>MalNet actually learns three different kinds of feature sets from the raw data; first MalNet learns malicious file structure features from the grayscale image by CNN and then learns malicious code pattern features from opcode sequence by LSTM. These two feature sets are reflecting the local pattern information; hence we add some simple metadata features as a description of the global information. The specific design of MalNet and the detection process are described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Malware Grayscale Image through CNN.</head><p>In this section, we introduce CNN networks and describe how to construct malware structure feature by learning malware grayscale image through CNN. This method is inspired by Nataraj et al. <ref type="bibr" target="#b15">[17]</ref>, which visualizes malware binaries as grayscale images and these images can clearly reflect the structural characteristics of malware files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Malware Grayscale Image Generation.</head><p>In order to generate malware grayscale image, the raw data requires being processed and transformed into an image format. We take an executable file as input data and treat it as raw .bytes binary stream file. The binary stream file can be regarded as a hexadecimal stream file by converting every 4 bits into a hexadecimal number. Considering that the range of a hexadecimal number is exactly from 0 to 16, and we combine every two hexadecimal numbers exactly corresponding to the gray value of a 256-level image pixel. So the raw data can be converted to a grayscale image by this simple mapping transformation. The whole bunch of binary stream sequence is segmented for every 8 bits which corresponds to gray level of each pixel, and it is arranged sequentially to form the corresponding gray image. The generated grayscale images are shown in Figure <ref type="figure" target="#fig_2">2(a)</ref>.</p><p>Incidentally, we can use a similar method to generate grayscale images from decompiled files and only need to decompile the executable file first, obtaining .asm decompiled file, and then also treat it as binary stream for the same mapping transformation. However, in the actual process we found that the grayscale image generated by the decompiled file lost a lot of structure patterns (the generated grayscale images are seen in Figure <ref type="figure" target="#fig_2">2(b)</ref>). Executable files with obvious differences still have very similar grayscale images presentations. The reason is that the decompiled file has a relatively fixed and organized structure since decompiled tool will output a normal format. For example, we use IDA Pro <ref type="bibr" target="#b21">[23]</ref> which generates a decompiled file whose beginnings of each line are the PE segment name and the starting file address, followed by the decompile instruction. Therefore, all the decompiled files tend to have similar generated grayscale images. Thus, the grayscale image generated by the decompiled file is not suitable for further learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Convolution Neural Network. MalNet takes a CNN</head><p>to learn from grayscale images. As a typical deep neural network, CNN is widely used in computer vision area and image related tasks. The most notable characteristic of CNN is that it reduces a huge amount of calculation by the idea of weights sharing, local field, and subsample in space. It shares the same weight between a group of CNN neurons, mining patterns on local fields by convolution operation. CNN directly takes the raw image as input and outputs the classification or regression result with an end-to-end structure. And the neuron weights of CNN are trained by backpropagation algorithm. A typical application of CNN <ref type="bibr" target="#b22">[24]</ref> is used for handwritten digital recognition through multiple convolution layers and pooling layers to handle the input data. Each convolution layer outputs a set of feature maps, while each feature map represents a high-level features extracted via one specific convolution filter. And the  pooling layer mainly uses the principle of local correlation to complete the downsampling, so the subsequent convolution layer can extract features from a more global perspective. These greatly reduce the number of weight parameters and calculation for training a deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Data Preprocessing for Grayscale Image.</head><p>In order to meet CNN requirements for input data, we first preprocess the grayscale image. When CNN performs a task like image classification, it takes the input image data with the same sizes. Generally, the image data should have the same length and width (length to width ratio is 1 : 1). It is for the convenience of subsequent convolution operation. Since executive files have different file sizes, various grayscale image sizes also have big differences. In fact, a large grayscale image can reach 1.04 MB (2048 Ã— 1036 pixels), while a small one is only 120 KB (512 Ã— 472 pixels). So it is necessary to normalize all grayscale images. We use bilinear interpolation algorithm, an image scaling method for normalization. It makes use of the four nearest pixels values in the original image to determine a virtual pixel value of the target image, which achieves better effect than the nearest neighbor interpolation. Also, the normalized size of grayscale image is a hyperparameter, which reflects the tradeoff between classification accuracy and calculation cost. The larger the normalized image size, the richer the information received by the CNN input; then with more complex network structure better detection result will be obtained, but the corresponding cost is longer time-consuming for network training. To this end, we finally choose 64 Ã— 64 as the normalized size of grayscale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Opcode Sequence through LSTM.</head><p>In this section, we introduce another important part of MalNet, which deals with opcode sequence with LSTM to learn malicious sequence features and patterns. Opcode sequences are extracted from decompiled files. These sequences actually reflect code logic and program execution logic of executive files. Hence, LSTM can mine malicious code sequence features corresponding to high-level malicious behavior from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Opcode Sequence Extraction.</head><p>To learn from opcode sequence, first we need to extract opcode sequence from raw executive files. We decompile the executive file through IDA Pro which generates .asm format decompiled file. IDA Pro is a common decompilation and debugging tool that resolves malware into Intel x86 assembly instructions.</p><p>Then for the .asm file, we traverse all lines and slice sentences through space character as a delimiter to match each phrase to our predefined opcode set which contains all common Intel x86 assembly instructions. If the matching is successful, we retain the opcode; otherwise, we delete the phrase. During this process we find that there are a large number of duplicate opcode subsequences on decompiled files, such as ğ‘‘ğ‘‘, ğ‘‘ğ‘‘, . . . , ğ‘‘ğ‘‘ or ğ‘‘ğ‘, ğ‘‘ğ‘, ğ‘‘ğ‘, . . . , ğ‘‘ğ‘. So it is required to filter these duplicate subsequences by adding some rules. The pseudocode of our opcode sequence extraction algorithm is shown in Algorithm 1.</p><p>In the process of opcode sequence extraction, the size of the opcode set will affect the average length of opcode sequences. Larger opcode set will accept more kinds of opcodes. Since there are many noise data and too long opcode sequence will cause difficult learning problem with LSTM, we need to limit the size of the opcode set in a reasonable range so that it only contains the most valid information. So we treat all decompiled .asm files as text and instructions as vocabularies. Then we make frequency statistics and filter out the low frequency vocabularies. After that we use each vocabulary frequency as a feature and perform a classification by a random forests model, random forests can give a ranking for all features importance. We choose the vocabularies which give the best feature importance. Finally we get opcode set including 185 elements and extract opcode sequences with that. By now these opcode sequences should be digitized before being used as input of neural network; we use one-hot encoding which simply takes a mapping transformation to get a sparse vector like [0, 0, 0, 1, 0, . . . , 0] whose ğ‘ binary status bits represent ğ‘ states only containing one nonzero element. And each opcode gets a unique one-hot representation. (2) for ğ‘– inğ‘“ğ‘–ğ‘™ğ‘’ğ‘ ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Very Long</head><p>(3) ğ‘“ğ‘–ğ‘™ğ‘’ = ğ‘œğ‘ğ‘’ğ‘›(ğ‘–.ğ‘ğ‘ ğ‘š); // Open the corresponding IDA pro decompiled file; (4) for ğ‘™ğ‘–ğ‘›ğ‘’ inğ‘“ğ‘–ğ‘™ğ‘’; // Read in line;</p><p>(5) ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘  = ğ‘™ğ‘–ğ‘›ğ‘’.ğ‘ ğ‘ğ‘™ğ‘–t(" "); // Cut the line into phrases by space character; <ref type="bibr" target="#b4">(6)</ref> for ğ‘¤ğ‘œğ‘Ÿğ‘‘ in ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ ; //To judge each phrase, it requires to meet the following two points at the same time:</p><p>(1) The current word belongs to opcode set opcode set;</p><p>(2) The last three words are not duplicated opcodes. <ref type="bibr" target="#b5">(7)</ref> if ğ‘¤ğ‘œğ‘Ÿğ‘‘ in ğ‘œğ‘ğ‘ğ‘œğ‘‘ğ‘’ ğ‘ ğ‘’ğ‘¡ and (ğ‘¤ğ‘œğ‘Ÿğ‘‘! = ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘ and ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘! = ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘) : <ref type="bibr" target="#b6">(8)</ref> ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘ = ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘; <ref type="bibr" target="#b7">(9)</ref> ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘ = ğ‘¤ğ‘œğ‘Ÿğ‘‘; <ref type="bibr" target="#b8">(10)</ref> ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ .ğ‘ğ‘‘ğ‘‘(ğ‘¤ğ‘œğ‘Ÿğ‘‘); <ref type="bibr" target="#b9">(11)</ref> end if <ref type="bibr" target="#b10">(12)</ref> end for <ref type="bibr" target="#b11">(13)</ref> end for <ref type="bibr" target="#b12">(14)</ref> end for Algorithm 1: Opcode sequence extraction algorithm for executive files.</p><p>Long-Short Term Memory (LSTM) <ref type="bibr" target="#b23">[25]</ref> is widely used for processing time series data, which is an improved model based on Recurrent Neural Networks (RNNs). RNN uses an internal state to represent previous input values which allows it to capture temporal context. Based on this, LSTM uses the Constant Error Carousel (CEC) and well-designed "gate" structures to ease the vanishing gradient problem during errors backpropagation. So loss can flow backwards through longer timestep, which enables LSTM to learn long-term dependency and context. In brief, LSTM adds three gates (input gate, forget gate, and output gate) to decide and control the CEC state. Here MalNet uses LSTM network to learn from opcode sequence for malware detection.</p><p>However, one existing problem for LSTM is that it is difficult to effectively train when the input sequence is too long, though LSTM can capture longer time series context than RNN. In our work, if the size of executive file is very large, then the length of extracted opcode sequence is very long. For example, the average opcode sequence length of Ramnit malware family samples reaches 36,000. But the performance of LSTM is reduced fast when the length of input sequence exceeds 200. So how to process very long sequence with LSTM network is critical.</p><p>One simple strategy for very long sequences processing with LSTM network is Truncating And Padding (TAP). In particular, TAP first sets a fixed length ğ‘, truncates and discards the part of long sequences exceeding length ğ‘, and pads short sequences to length ğ‘ with predefined identifier. It is convenient but it abandons a lot of information due to the truncation operation. Truncated Backpropagation Through Time (truncated BPTT) is another solution <ref type="bibr" target="#b24">[26]</ref>, which adds a time window constraint to limit the maximum distance for error backpropagation operation. So the error propagation and gradient calculation are only performed in the window, and the weights of nodes beyond the window are not updated.</p><p>It enhances the computational efficiency by sacrificing a small part of the accuracy comparing with standard BPTT (or full BPTT) since standard BPTT calculation is less effective when backpropagation distance is too long. In addition, truncated BPTT is more suitable for online learning, as it can quickly adapt to the newly generated part of a very long sequence. Overall, truncated BPTT is reasonable and effective since it learns all sequence information comparing with TAP strategy.</p><p>In our scenario, we come up with a practical implement based on truncated BPTT algorithm for LSTM network. Since gradients are only propagated in the window, we first divide an opcode sequence into multiple subsequences, where the length of each subsequence equals the window length of truncated BPTT. Then for each subsequence we just do a full BPTT which equals doing the truncated BPTT for the whole sequence with no intersection window division. Most importantly, this allows LSTM to train in parallel on a bunch of subsequences. One of the major problems with LSTM is that the recurrent structure restricts it to train a sequence serially, which is inefficient. However, with these subsequences, our LSTM training process can be 3 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Subsequence Selection and Subsequence Fusion.</head><p>With the above subsequence training strategy, subsequences become the input of LSTM network while the output is generated for each subsequence. It can be regarded as the discrimination result that LSTM thought how likely this subsequence is malicious. However, subsequences may contain a lot of noise especially for malware samples. Because, for many malware authors, they just implanted a malicious code snippet into a benignware, so many subsequences of this kind of sample are harmless. For this reason, it is crucial to take care of these subsequences in malware. Here we come up with a subsequence selection method for cleaning up Figure <ref type="figure">4</ref>: The process of subsequence fusion. Suppose that red, blue, and green represent three different prediction labels. By using relative majority weighted voting method, then final result follows the label ğ‘™ ğ‘– owning the most weighted votes. these subsequences and providing a higher quality dataset to LSTM.</p><p>We utilize LSTM network itself without additional model to complete subsequence selection. In binary classification task (for malware detection), the output layer of LSTM network uses a logistic regression giving a probability value to identify negative class or positive class. This is similar to a recent work <ref type="bibr" target="#b25">[27]</ref> using reconstruction error as the basis for subsequence anomaly detection by LSTM-based Encoder-Decoder. Specifically, the logistic regression layer takes the output of hidden layer and calculates the probability ğ‘(ğ‘¦ | ğ‘¥ ğ‘— ) of the current subsequence sample ğ‘¥ ğ‘— . ğ‘¦ is the corresponding labels where ğ‘¦ = 0 presents negative class (benign label) and ğ‘¦ = 1 presents positive class (malware label) and the actually logistic output can be presented as ğ‘(ğ‘¦ = 1 | ğ‘¥ ğ‘— ). The intuition is the higher or lower ğ‘(ğ‘¦ = 1 | ğ‘¥ ğ‘— ), the more confidence of LSTM for current sample. Since the benign subsequence in malware lacks malicious features but still with a malicious label, which will confuse LSTM, the confidence given by LSTM is relatively lower. Hence, we can use ğ‘(ğ‘¦ = 1 | ğ‘¥ ğ‘— ) to help subsequence selection.</p><p>In order to use the LSTM's output ğ‘(ğ‘¦ = 1 | ğ‘¥ ğ‘— ) to perform the subsequence selection for ğ‘¥ ğ‘— , we set a threshold ğ›¿ and compare it to the maximum likelihood probability. The formula is shown as follows:</p><formula xml:id="formula_0">ğ›¿ â‰¥ max {ğ‘ƒ (ğ‘¦ = 1 | ğ‘¥ ğ‘— ) , 1 -ğ‘ƒ (ğ‘¦ = 1 | ğ‘¥ ğ‘— )} .<label>(1)</label></formula><p>For current subsequence ğ‘¥ ğ‘— , if the above formula holds, it means that LSTM sets a low confidence level that current subsequence belongs to any category, indicating that subsequence ğ‘¥ ğ‘— cannot provide sufficient valid information for LSTM to judge or just with wrong label (benign subsequence of malware). So it can be seen as noisy subsequence and has been filtered out.</p><p>At the beginning of LSTM training process, since LSTM parameters are randomly initialized, the output of LSTM cannot be trusted so we use another threshold ğœ‚ to determine when the network has enough capacity to start subsequence selection after several iterations. Specifically we calculate the training error of a batch of inputs. If training errors of continuous ğ‘€ input batches are lower than the threshold ğœ‚, then LSTM triggers subsequence selection, where ğ‘€ is a hyperparameter we set to 5. All these hyperparameters are chosen through experiment. Moreover, since training error will always decrease through subsequence selection, we divide a validation set and use validation error to find the right time to stop subsequence selection. Once validation error cannot reduce anymore, it is considered to be the appropriate time for ending subsequence selection (the whole process is seen in Figure <ref type="figure">3</ref>).</p><p>By now LSTM output the classification results for subsequences; we need to use these results for completing the classification of original opcode sequence. We can use a simple fusion to solve it. One common fusion strategy is the voting method. In our scenario, we use the relative majority weighted voting method to fuse the discrimination result of subsequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ» (ğ‘¥) = ğ¶ argmax</head><formula xml:id="formula_1">ğ‘— ğ‘‡ âˆ‘ ğ‘–=1 ğ‘¤ ğ‘– â„ ğ‘— ğ‘– (ğ‘¥) ,<label>(2)</label></formula><p>where ğ‘¤ ğ‘– represents the weight of subsequence discrimination result â„ ğ‘— ğ‘– (ğ‘¥). Normally, ğ‘¤ ğ‘– â‰¥ 0 and âˆ‘ ğ‘‡ ğ‘–=1 ğ‘¤ ğ‘– = 1. Considering that the classification output result of each subsequence is a nonnormalized probability given from the logistic regression layer of LSTM, here we can directly use these nonnormalized probabilities as weights ğ‘¤ ğ‘– for each subsequence (see Figure <ref type="figure">4</ref>).</p><p>Figure <ref type="figure">4</ref> shows how the weighted voting method is applied to subsequence fusion. Each of the original sequence is divided into subsequences, which are the input for trained LSTM network and get corresponding classification results. Finally, we use these subsequence results and corresponding weights ğ‘¤ ğ‘– to get the prediction for the original opcode sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Data Augmentation Strategy Based on Sliding Window.</head><p>When dealing with classification tasks using neural networks, there is often a category imbalance problem in reality. For example, the number of benignware in reality is much more than malware and is easily accessible. While the distribution of different malware families is more uneven, those malware families which are widely spread have a bigger number of accessible samples and unpopular malware family only has few samples as dataset. In order to avoid these unpopular categories being ignored by the classifier resulting in poor recognition accuracy, data augmentation strategy is often used to solve category imbalance problem in the real world where oversamples on unpopular categories combined with negative sampling of popular categories to achieve a more balanced distribution of different category samples.</p><p>However, some data augmentation methods, such as mapping transformation (widely used in image data) and SMOTE algorithm (based on interpolation), are not suitable for sequence data. Here we propose a data augmentation strategy based on sliding window which is used for category imbalance problem on the subsequent malware family classification task. In the previous section, the way of subsequence segmentation for very long sequence has no intersections, which means there is no repetitive element between any two subsequences. For expanding data samples, we consider a segmentation method with intersection for very long sequence by using a sliding window. The window length is exactly equal to the window length of truncated BPTT, and then the number of generated subsequences is controlled by setting the step length of the sliding window.</p><p>So, the smaller the step length, the more the number of generated subsequences. To ensure that the original sequence of information is not lost, we add the constraint where the step length should be no more than the length of the sliding window. Suppose that the number of samples of the current category ğ‘™ ğ‘– is ğ›½ ğ‘– ; each sample length is ğ›¼ ğ‘–ğ‘— where ğ‘— âˆˆ (0, 1, . . . , ğ›½ ğ‘– ) and the length of the sliding window is set as ğœ.</p><p>To expand the data sample for current class ğ‘™ ğ‘– to ğ›¾, we first calculate the total length of the sequence of category ğ‘™ ğ‘– , as follows:</p><formula xml:id="formula_2">len (ğ‘™ ğ‘– ) = ğ›½ ğ‘– âˆ‘ ğ‘—=0 ğ›¼ ğ‘–ğ‘— .<label>(3)</label></formula><p>For current category ğ‘™ ğ‘– , the step length ğ‘‘ ğ‘– of the sliding window is calculated as follows:</p><formula xml:id="formula_3">ğ‘‘ ğ‘– = âˆ‘ ğ›½ ğ‘– ğ‘—=0 ğ›¼ ğ‘–ğ‘— -ğœ ğ›¾ , 1 â‰¤ ğ‘‘ ğ‘– â‰¤ ğœ.<label>(4)</label></formula><p>Since the minimum length of step length is set to 1, there is a corresponding upper limit on the number of samples ğ›¾ to be expanded:</p><formula xml:id="formula_4">ğ›¾ â‰¤ ğ›½ ğ‘– âˆ‘ ğ‘—=0 ğ›¼ ğ‘–ğ‘— -ğœ.<label>(5)</label></formula><p>In subsequent malware family classification task, this data augmentation strategy can achieve a relatively balanced distribution on the data sample numbers of different malware families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stacking Ensemble.</head><p>MalNet also extracts some metadata features of the malware apart from using LSTM and CNN. The main reason is that LSTM and CNN capture local feature and metadata feature in contrast can get the global description for malware. And these metadata features are easy to obtain such as the size of malware source files, the starting address of the byte file, the size of decompiled file, number of rows, and the length of different PE segments. Now MalNet has three parts of temp results which are LSTM discrimination result, CNN discrimination result, and metadata features. To achieve a final detection result, we integrate these three parts by stacking ensemble. A general procedure of a stacking ensemble method <ref type="bibr" target="#b26">[28]</ref> involves a learner trained to combine other heterogeneous learners' results. Here learner usually means a machine learning model. The individual learners are called the first-level learners which gives a temp result, while the combiner is called the second-level learner to stack the output of first-level learners for making a better predictions. The basic idea is to train the first-level learners using the original training set and then use their output to generate a new dataset for training the second-level learner.</p><p>By now the above three parts are designed to obtain malware features from different perspectives, containing local level and global level. We use CNN network, LSTM network, and feature extraction as first-level learners and take a logistic regression as the second-level learner for stacking ensemble. The process of stacking ensemble can be seen in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>And the objective function of logistic regression is defined as</p><formula xml:id="formula_5">ğ‘ƒ ğœƒ (ğ‘¥) = 1 1 + exp (-ğœƒ ğ‘‡ ğ‘¥) , (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where ğœƒ is the parameter. The range of ğ‘ƒ ğœƒ (ğ‘¥) value is (0, 1).</p><p>For the training data (ğ‘¥ (ğ‘—) , ğ‘¦ (ğ‘—) = ğ‘–), the maximum likelihood probability is calculated and the loss is used to optimize ğœƒ through backpropagation algorithm. We use the Stochastic Gradient Decent (SGD) to train the logistic regression model as the second-level learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Evaluations</head><p>In this section, we will describe the experiment environment and the concrete implementation of MalNet.   system taking more than 500 GB space. For each sample, two file formats are provided, the malware source files (binary stream files without the PE head) and the corresponding decompiled files by IDA Pro, respectively. And the benignware source files are collected by us from some software providers, such as Cnet <ref type="bibr" target="#b27">[30]</ref> and Baidu Software Center <ref type="bibr" target="#b28">[31]</ref>.</p><p>In evaluation experiments, we measure the following performance metrics: accuracy, true positive rate (TPR), false positive rate (FPR), equal error rate (EER), and receiver operating characteristic (ROC). EER is the same as FPR value when operating threshold is adjusted such that FPR and false reject rate (FRR) become equal. The main metrics are defined as follows:</p><p>(i) TPR is defined as the probability that the current malicious sample is correctly identified:</p><formula xml:id="formula_7">TPR = True Positives True Positives + False Negatives . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>(ii) FPR is defined as the probability that the benign sample is wrongly identified as malware:</p><formula xml:id="formula_9">FPR = False Positives False Positives + True Negatives . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Note that TPR reflects the usability of MalNet while FPR shows the security. Here, due to the bias for security considerations, we evaluate MalNet TPR value when FPR equals to 0.1%, which keeps a very low FPR as a prerequisite. In addition, to determine the detection efficiency of MalNet, we calculate the time consumptions on training phase and detecting phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">MalNet CNN.</head><p>We build two CNN network structures, called BaseNet and VGGNet. For BaseNet, we use the 5 Ã— 5 convolution kernel and stride size 1. Also, to simplify the design of CNN network, the edges of the image are padded so that the output feature map size of the convolution operation is consistent with the input image. At the same time, BaseNet uses 2 Ã— 2 max pooling operation and stride size 2, so the length and the width of sampled images are reduced to half of the original. BaseNet totally uses 2 convolution layers, 2 maxpooling layers, and 1 fully connected layer and adds Dropout layer <ref type="bibr" target="#b29">[32]</ref> for each pooling layer and fully connected layer to prevent overfitting.</p><p>Since BaseNet uses a large convolution window, it cannot support deeper network structure when the input image size is small. To this end, we build VGGNet according to Simonyan and Zisserman work <ref type="bibr" target="#b30">[33]</ref>, which uses small window convolution filter to apply a deeper network. Specifically VGGNet uses 3 Ã— 3 convolution kernel, stride size 1, and 1pixel edge padding. Also, 3 Ã— 3 max-pooling operation is used and stride size is set to 2. So it can deal with input images with larger size under the same size of feature map, only requiring a small amount of additional calculation. VGGNet has deeper network structure, a total of 3 convolution layers, 2 pooling layers, and 2 fully connected layers. Also, we use Leaky ReLU activation function <ref type="bibr" target="#b31">[34]</ref>, uniform distribution weight initialization, and batch normalization <ref type="bibr" target="#b32">[35]</ref> which </p><p>enhance CNN network convergence performance. The specificity of BaseNet and VGGNet structures and parameters is concluded in Table <ref type="table" target="#tab_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">MalNet LSTM.</head><p>LSTM network consists of two layers, and each layer has 185 neuron nodes. We also use Dropout as regularization mechanism to reduce overfitting. We add Dropout layer for two hidden layers of LSTM and set the probability value ğœŒ (the selection of ğœŒ determines the intensity of Dropout) of 0.5. Besides, Dropout mechanism only takes effect in the training phase. It can be regarded that Dropout helps training many subnetworks during the training phase and the predicting phase; it combines all subnetworks to make an ensemble prediction. Moreover, we use SGD and set the batch size (=30) for training. And Adam optimization algorithm <ref type="bibr" target="#b33">[36]</ref> is used as the optimizer; it combines momentum factor with Adagrad optimization algorithm, which provides a fine-grained control of the learning rate decay. Adam optimizer only needs an initial learning rate (=2ğ‘’ -3) as a hyperparameter, and the learning rate during the training process can be adjusted adaptively without manually setting weight decay. The summary settings and parameters for LSTM networks are listed in Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of MalNet CNN.</head><p>The corresponding grayscale images of 21,736 malware samples and 20,650 benignware samples are first generated and normalized to the size of 64 Ã— 64. And we use 6-fold cross validation to evaluate two CNN networks, BaseNet and VGGNet. For the above two CNN network structures we do a comparative experiment to evaluate their detection performance. We use their output discriminant result for grayscale images as corresponding malware detection result. Apart from network structure, the rest of the basic settings are consistent for BaseNet and VGGNet, such as the Leaky ReLU activation function, Adam optimization algorithm, and the initial learning rate. Figures <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_6">7</ref> represent ROC curves of validation result for two networks and the list of 6 groups of experiment results corresponding to 6fold cross validation, respectively. The results show that VGGNet achieves an average AUC of 0.99952 and average  False positive rate detection accuracy of 98.14%, which are better than the average AUC of 0.99896 and average classification accuracy of 96.82% for BaseNet. Since the grayscale image contains a wealth of local slight information, we consider that VGGNet can achieve a better performance due to its deeper network structure which can capture more localized image association. And although deeper network often requires longer training time-consuming, VGGNet greatly reduces the number of nodes required for the fully connected layer by using maxout mechanism, resulting in no increase of network parameters and no decrease on training efficiency. Hence VGGNet is used as the CNN network part for MalNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance of MalNet LSTM.</head><p>In data preprocessing phase, we first define an opcode set containing 185 candidate opcodes and use this to extract opcode sequences from the decompiled files. And then we divide all opcode sequences One important task is to evaluate and optimize our proposed subsequence selection strategy. The main function of subsequence selection is to filter noisy subsequences, especially the benign part of a malware to ensure high quality input data for LSTM network. Here a quantitative indicator for subsequence selection is required to identify the degree of tolerance for noise in opcode sequences. We define a hyperparameter called selection rate ğœ whose value refers to the proportion of training subsequences pass through subsequence selection. The choice for ğœ in fact reflects the trade-off between MalNet generalization performance and data quality. In our experiment, we compare with five different selection rates ğœ (=100%, 99%, 97.5%, 95%, 90%).</p><p>In Figure <ref type="figure" target="#fig_7">8</ref>, we can see that the AUC value is the lowest when ğœ = 90% and the highest when ğœ = 97.5%. The AUC values of the rest subsequence selection rate are better than the benchmark results without subsequence selection (ğœ = 100%), which proves that subsequence selection strategy is effective. However, the detection performance declines rapidly when ğœ decreases to 90%. We think the reason is that LSTM network usually has a certain antinoise ability, which means the input data with certain noise (not too much) only has little interference on the discriminant result of LSTM and LSTM can automatically perceive and resist noise from the data. In contrast, filtering out too many subsequences will cause LSTM to lack enough data for learning. So we choose ğœ = 97.5% as optimized hyperparameter for our subsequence selection strategy.</p><p>In the later experiment, we further compare the detection performance of TAP strategy and truncated BPTT on different BPTT length ğ›¼, where TAP strategy just truncates the part of sequence exceeding ğ›¼ before training and truncated BPTT divides opcode sequence into subsequence with length ğ›¼ for training. The experiment result (seen in Figure <ref type="figure">9</ref>) shows that truncated BPTT is superior to TAP strategy in Besides, we reproduce an ğ‘-gram based malware detection method according to relevant studies <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21]</ref> and use it as a baseline in our dataset. It extracts 1-gram, 2-gram, and 3-gram features on both bytes and opcodes, discards low frequency features, makes a further feature selection according to the feature importance from random forest, and finally constructs a 12,834-dimensional feature vector for each sample and trains a SVM classifier for malware detection.</p><p>We make a malware detection experiment using 21,736 malware samples and 20,650 benignware samples. After training all models to make a prediction whether it is a malware on test set containing 2,000 malware samples and 2,000 benignware samples, the comparison results are seen in Table <ref type="table" target="#tab_7">5</ref> (MF represents metadata feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Other Works.</head><p>Due to the sensitivity of malware data, many datasets used by other related works are not public, which increased the difficulty for comparison of different malware detection methods. Fortunately, our paper  <ref type="bibr" target="#b34">[37]</ref> 99.83 72 13.33 Novel Features <ref type="bibr" target="#b35">[38]</ref> 99.77 21.86 4 Linear kNN <ref type="bibr" target="#b36">[39]</ref> 96.6 --Random Forest <ref type="bibr" target="#b37">[40]</ref> 95.62 --One-class SVM <ref type="bibr" target="#b38">[41]</ref> 92 --tGAN <ref type="bibr" target="#b39">[42]</ref> 96.39 --Strand Gene Sequence <ref type="bibr" target="#b40">[43]</ref> 98 For CNN this grayscale image does not have rotational invariance, since the pixels of the grayscale image are extracted from the binary stream line by line, and the rotate transformation would break these textures. Similarly, grayscale image does not support tilt transformation of a certain angle. Hence we customize some mapping transformations for data augmentation including horizontal rollover, horizontal shift (randomly shift -10 to 10 pixels to the right), and longitudinal stretch (randomly cut into 3 to ğ‘ parts with vertical, each part stretches in ratio with 1/1.3 to 1.3).</p><p>So, we conduct the same malware family classification experiment with MalNet using the same Microsoft malware dataset as other related works; the results are summarized as shown in Table <ref type="table" target="#tab_8">6</ref>. It can be seen that MalNet achieves 99.36% classification accuracy and outperforms most of related works, which closes to Kaggle Winner Solution <ref type="bibr" target="#b34">[37]</ref> with 99.83% accuracy. It is noteworthy that although there are two approaches <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b35">38]</ref> having slightly better detection accuracy over MalNet, both approaches rely on a large number of feature engineering works. And the biggest problem with this is the potential inefficiency performance on both training and detecting phase from the experiment results they claimed.</p><p>As we can see Kaggle Winner Solution takes almost 3 days to train their model with a relatively good hardware environment including Google Compute Engine with 16 CPUs, 104 GB RAM, and 1 TB of disk space. And the authors claimed that the real model training time is only 1 hour and the remaining time is fully used for feature engineering. This is because they extracted massive features (around 70 K original features) to make up for the lack of expert knowledge in this area. The work of Novel Feature <ref type="bibr" target="#b35">[38]</ref> performs feature engineering more effectively through a certain expert knowledge and avoids huge time-consuming calculation like 3-gram and 4-gram feature extraction. However, it still takes approximately 1 day to extract features from the training data. More seriously, this has a greater impact on the efficiency of the detection phase. Novel Features take about 4 seconds to predict a sample, while Kaggle Winner Solution takes around 13 seconds. These seem difficult to satisfy the efficiency for a real antivirus scenario.</p><p>On detecting phase, the model prediction is often very fast and makes the slowness of feature extraction more obvious. MalNet is able to detect one sample in 0.03 seconds (note that the computation time for decompiling is not taken into account as this step is required by all methods), which is faster than other related works, especially comparing with Novel Features and Kaggle Winner Solution. This is because the most time-consuming part for deep neural network is in the training phase in which backpropagation needs to calculate plenty of gradients to complete the training of the mass parameters. However, the prediction phase only needs one forward propagation and it is even more efficient with GPU acceleration. Actually, since the training phase is offline, the key factor is detection efficiency rather than training efficiency. Nowadays, the emergence of massive amounts of malware per day is a challenge to the malware detection efficiency. Therefore, although MalNet is slightly behind the best detection accuracy, its detection efficiency gets greatly enhanced in return, more suitable for the real-life application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Some recent works try to evade the detection of machine learning based malware classifiers by adversarial learning <ref type="bibr" target="#b41">[44,</ref><ref type="bibr" target="#b42">45]</ref>. Their experiments show that it is possible to generate adversarial samples based on a trained machine learning classifier. The core of adversarial sample crafting is to find a small perturbation ğœ on feature vectors ğ‘‹ of original malware sample to change the classification results ğ¹ to benign. Formally, they compute the gradient of ğ¹ with respect to ğ‘‹ to estimate the direction in which a perturbation ğœ in ğ‘‹ would maximally change ğ¹'s output. The basic idea is shown in Figure <ref type="figure" target="#fig_8">10</ref>. This attack scene is mainly caused by the characteristics of discriminative model and lacking of sufficient data. When dealing with a classification task with discriminative model, since it is almost impossible to have enough data to help model make decision in whole feature space, discriminative model will try to expand the distance between samples and decision boundary for better classification result and, meanwhile, expand the area of each category in feature space. The benefit of this is to make the classification easier, but the downside is that it also includes a lot of feature spaces that do not clearly belong to current category, which enables attackers generating adversarial samples from this feature space.</p><p>The earliest work of this topic came from Nguyen et al. <ref type="bibr" target="#b43">[46]</ref> which found that a slight change in the image could trick the image classifier, and then it has been introduced into the security area in recent years to attack security systems that rely on machine learning model. According to the conclusions of some of these related works, we make several changes to MalNet to prevent adversarial crafting attacks as much as possible. First is to add regularization so that the model does not get too overfitting to the training set and promote enclosure of the feature space of benign category. Here we add ğ¿2 regularization to MalNet which keeps a conservative discrimination result to unknown feature space to prevent adversarial samples using these feature spaces fooling malware detection classifier. Second, we try adversarial training which crafts adversarial samples in advance and let MalNet train these samples by online learning which enhances MalNet from resisting adversarial crafting attack. Third, as Biggio et al. <ref type="bibr" target="#b42">[45]</ref> discovered that ensemble learning with different classifiers can generate a more robust classifier for adversarial crafting attack, here we use a stacking ensemble for MalNet. Fourth, Biggio et al. believed some of features are not easily evaded, such as ğ‘-gram; here we use LSTM to mine features like ğ‘-gram from raw opcode sequences. Finally we take the idea of "gradient masking" <ref type="bibr" target="#b44">[47]</ref> in our real system, which let model output hard decision (the predicted target category) rather than probabilities of different categories, so it is hard for the attackers to obtain a useful gradient to build adversarial samples (a minor change cannot affect the output result).</p><p>Nevertheless, there is no detailed analysis in this paper of whether MalNet is susceptible to adversarial crafting attacks or a quantitative assessment of the effects of above changes we take. Besides, some defenses to adversarial attack are claimed not robust enough in last few years <ref type="bibr" target="#b45">[48,</ref><ref type="bibr" target="#b46">49]</ref> and other methods came up <ref type="bibr" target="#b47">[50]</ref>. In general, adversarial crafting attack is a big, important, and popular topic; we have not given a complete analysis to MalNet for adversarial attack, and in the future we will consider conducting some exploration and detailed analysis with relevant experiments for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a malware detection method called MalNet, which uses two deep neural networks CNN and LSTM to, respectively, learn from grayscale image and opcode sequence extracted from raw binary executive files, followed by a stacking ensemble to fuse them. We use MalNet to complete a malware detection experiment for 42,386 samples (1/10 samples for validation) and it achieves 99.88% accuracy and 99.14% of TPR with FPR of 0.1%. We also make a malware family classification experiment for comparison to other related works, and MalNet outperforms most of other works with 99.36% accuracy and raises detecting efficiency a lot comparing with two state-of-the-art results on Microsoft malware dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of our proposed malware detection process. MalNet is the core malware detection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The generated grayscale images. (a) Grayscale images are generated from the raw binary file. (b) Grayscale images are generated from the decompiled file.</figDesc><graphic coords="5,436.05,153.41,58.06,57.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The learning process of stacking ensemble model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>expr1, lr = 3e -4; BaseNet expr2, lr = 3e -4; BaseNet expr3, lr = 3e -4; BaseNet expr4, lr = 3e -4; BaseNet expr5, lr = 3e -4; BaseNet expr6, lr = 3e -4; BaseNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The ROC curve of BaseNet classification result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The ROC curve of VGGNet classification result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>î‹µFigure 8 :</head><label>8</label><figDesc>Figure 8: The classification results for different subsequence selection rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The adversarial crafting attack on malware detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Opcode sequence (1) ğ‘“ğ‘–ğ‘™ğ‘’ğ‘  = ğ‘”ğ‘’ğ‘¡ ğ‘“ğ‘–ğ‘™ğ‘’ğ‘ (); // Get all executive files;</figDesc><table /><note><p>Sequence Learning by LSTM. We first briefly introduce LSTM network. As a deep neural network, Input: Executive file Output:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The platforms environment for MalNet.</figDesc><table><row><cell>Platforms</cell><cell>Content</cell></row><row><cell>Hardware dependencies</cell><cell>NVIDIA GPU Maxwell-based GTX980 16 GB of memory</cell></row><row><cell></cell><cell>Ubuntu 14.04 LTS</cell></row><row><cell></cell><cell>Python 2.7.6</cell></row><row><cell></cell><cell>Numpy 1.8.2</cell></row><row><cell></cell><cell>Scipy 0.13.3</cell></row><row><cell></cell><cell>Tensorflow 0.7.0</cell></row><row><cell>Software dependencies</cell><cell>Theano 0.9.0.dev</cell></row><row><cell></cell><cell>Lasagne 0.1</cell></row><row><cell></cell><cell>Nolearn 0.6.0.dev</cell></row><row><cell></cell><cell>Scikitlearn 0.15.2</cell></row><row><cell></cell><cell>Pandas 0.15.2</cell></row><row><cell></cell><cell>Mahotas 1.2.4</cell></row><row><cell></cell><cell>NVIDIA GPU driver</cell></row><row><cell>GPU components</cell><cell>CUDA 7.5</cell></row><row><cell></cell><cell>cuDNN V4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The list of two CNN network structures parameters.</figDesc><table><row><cell></cell><cell>Network layer type</cell><cell>Size</cell><cell>Output dimension</cell></row><row><cell></cell><cell>Input layer</cell><cell>-</cell><cell>(1, 1, 64, 64)</cell></row><row><cell></cell><cell>Convolutional Layer</cell><cell>32 5 Ã— 5 Convolution kernel</cell><cell>(1, 32, 64, 64)</cell></row><row><cell></cell><cell>Max pooling layer</cell><cell>2 Ã— 2, stride 1</cell><cell>(1, 32, 32, 32)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(1, 32, 32, 32)</cell></row><row><cell>BaseNet</cell><cell>Convolutional layer Max pooling layer</cell><cell>64 5 Ã— 5 Convolution kernel 2 Ã— 2, stride 1</cell><cell>(1, 64, 32, 32) (1, 64, 16, 16)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(1, 64, 16, 16)</cell></row><row><cell></cell><cell>Fully connected layer</cell><cell>Logistic regression</cell><cell>(1024, 1)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(1024, 1)</cell></row><row><cell></cell><cell>Output layer</cell><cell>-</cell><cell>1</cell></row><row><cell></cell><cell>Input layer</cell><cell>-</cell><cell>(1, 1, 64, 64)</cell></row><row><cell></cell><cell>Convolutional layer</cell><cell>32 3 Ã— 3 Convolution kernel</cell><cell>(1, 32, 64, 64)</cell></row><row><cell></cell><cell>Convolutional layer</cell><cell>16 3 Ã— 3 Convolution kernel</cell><cell>(1, 16, 64, 64)</cell></row><row><cell></cell><cell>Max pooling layer</cell><cell>2 Ã— 2, stride 1</cell><cell>(1, 16, 32, 32)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(1, 16, 32, 32)</cell></row><row><cell></cell><cell>Convolutional layer</cell><cell>32 3 Ã— 3 Convolution kernel</cell><cell>(1, 32, 32, 32)</cell></row><row><cell>VGGNet</cell><cell>Max pooling layer</cell><cell>2 Ã— 2, stride 1</cell><cell>(1, 32, 16, 16)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(1, 32, 16, 16)</cell></row><row><cell></cell><cell>Fully connected layer</cell><cell>512 maxout unit</cell><cell>(32, 512)</cell></row><row><cell></cell><cell>Dropout layer</cell><cell>-</cell><cell>(32, 512)</cell></row><row><cell></cell><cell>Fully connected layer</cell><cell>Logistic regression</cell><cell>(</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The list of LSTM network parameters.</figDesc><table><row><cell>Model parameters</cell><cell>LSTM</cell></row><row><cell>Maximum iterations</cell><cell>6.40ğ¸ + 04</cell></row><row><cell>Weights initialization</cell><cell>[-0.04, +0.04]</cell></row><row><cell>Truncated BPTT length</cell><cell>120</cell></row><row><cell>Batch training samples</cell><cell>30</cell></row><row><cell>Initial learning rate</cell><cell>2.00ğ¸-03</cell></row><row><cell>Dropout probability</cell><cell>0.5</cell></row><row><cell>Gradient regularization factor</cell><cell>10</cell></row><row><cell>Activation function</cell><cell>Tanh</cell></row><row><cell>Optimization algorithm</cell><cell>Adam</cell></row><row><cell>Propagation direction of time series</cell><cell>One-way</cell></row><row><cell>Hidden layers</cell><cell>2</cell></row><row><cell>Hidden nodes</cell><cell>185</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The experiment results for different LSTM networks. So even if ğ›¼ is small, it can still get considerable detection performance. But we can see its performance only gets slight improvement when ğ›¼ increases and even begins to decrease when ğ›¼ is over 180; this is because when ğ›¼ is too large, truncated BPTT on each subsequence will encounter gradient vanishing problem and in this case the subsequence should be further divided and trained separately. So we finally choose ğ›¼ = 120 for our MalNet LSTM.Finally, we conclude all experiments about LSTM network in Table4. The optimal LSTM network setting for MalNet is containing 2 hidden layers of 185 neurons nodes combined with truncated BPTT and subsequence selection strategies, where the truncated BPTT length ğ›¼ is 120 and subsequence selection rate ğœ is 97.5%. The training process for this takes 1.34 hours with GPU and the detection result achieves 99.13% accuracy and TPR reaches to 98.69% when FPR is 0.1%.</figDesc><table><row><cell>Strategy</cell><cell>Models</cell><cell>Accuracy (%)</cell><cell>AUC</cell><cell cols="2">TPR (FPR = 0.1%) EER (%)</cell><cell>Training times (h)</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 30)</cell><cell>71.43</cell><cell>0.8863</cell><cell>52.03</cell><cell>-</cell><cell>0.41</cell></row><row><cell>TAP</cell><cell>LSTM (ğ›¼ = 60)</cell><cell>87.13</cell><cell>0.9791</cell><cell>81.67</cell><cell>6.45</cell><cell>0.54</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 80)</cell><cell>91.56</cell><cell>0.9854</cell><cell>85.39</cell><cell>4.88</cell><cell>0.89</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 120)</cell><cell>94.86</cell><cell>0.9931</cell><cell>91.53</cell><cell>3.17</cell><cell>-</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 30)</cell><cell>94.37</cell><cell>0.9928</cell><cell>91.11</cell><cell>3.10</cell><cell>-</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 60)</cell><cell>96.83</cell><cell>0.9950</cell><cell>93.37</cell><cell>-</cell><cell>-</cell></row><row><cell>Truncated BPTT</cell><cell>LSTM (ğ›¼ = 80)</cell><cell>98.08</cell><cell>0.9989</cell><cell>95.13</cell><cell>-</cell><cell>1.24</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 120)</cell><cell>98.47</cell><cell>0.9993</cell><cell>96.81</cell><cell>-</cell><cell>1.36</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 180)</cell><cell>97.82</cell><cell>0.9987</cell><cell>95.42</cell><cell>-</cell><cell>1.53</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 120, ğœ = 90%)</cell><cell>97.98</cell><cell>0.9988</cell><cell>95.01</cell><cell>1.34</cell><cell>1.16</cell></row><row><cell>Truncated BPTT + subsequence selection</cell><cell>LSTM (ğ›¼ = 120, ğœ = 95%)</cell><cell>98.83</cell><cell>0.9997</cell><cell>97.22</cell><cell>0.84</cell><cell>-</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 120, ğœ = 99%)</cell><cell>98.66</cell><cell>0.9996</cell><cell>96.69</cell><cell>0.92</cell><cell>-</cell></row><row><cell></cell><cell>LSTM (ğ›¼ = 120, ğœ = 97.5%)</cell><cell>99.13</cell><cell>0.9999</cell><cell>98.69</cell><cell>0.54</cell><cell>1.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The malware detection experiment results for different methods.</figDesc><table><row><cell>Models</cell><cell>Accuracy (%)</cell><cell>AUC</cell><cell cols="5">TPR (%) FPR (%) EER (%) Training time (h) Detection time (ms)</cell></row><row><cell>ğ‘-gram</cell><cell>93.21</cell><cell>0.9864</cell><cell>89.22</cell><cell>0.1</cell><cell>3.94</cell><cell>4.18</cell><cell>2304</cell></row><row><cell>LSTM</cell><cell>99.13</cell><cell>0.9999</cell><cell>98.69</cell><cell>0.1</cell><cell>0.54</cell><cell>1.34</cell><cell>13.62</cell></row><row><cell>CNN</cell><cell>98.14</cell><cell>0.9989</cell><cell>96.92</cell><cell>0.1</cell><cell>1.55</cell><cell>1.46</cell><cell>15.97</cell></row><row><cell>LSTM + CNN + MF (MalNet)</cell><cell>99.88</cell><cell>0.9999</cell><cell>99.14</cell><cell>0.1</cell><cell>0.37</cell><cell>2.91</cell><cell>30.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The comparison with other works.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell><cell>Training time (h)</cell><cell>Detection time (s)</cell></row><row><cell>Kaggle Winner Solution</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the National Natural Science Foundation of China under Grant no. 61672421.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>The authors declare that they have no conflicts of interest. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overall statistics for 2016</title>
		<ptr target="https://securelist.com/kaspersky-security-bulletin-2016-execu-tive-summary/76858/" />
	</analytic>
	<monogr>
		<title level="j">Kaspersky Security Bulletin</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">N-gram analysis for computer virus detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Pujari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Virology and Hacking Techniques</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DLLMiner: Structural mining for malware detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Narouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Security and Communication Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3311" to="3322" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward automated dynamic malware analysis using CWSandbox</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Freiling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security and Privacy</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic analysis of malware behavior using machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trinius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Mannheim</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A static heuristic approach to detecting malware targets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Faraji</forename><surname>Daneshgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbaspour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Security and Communication Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3015" to="3027" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Limits of static analysis for malware detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kruegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kirda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Computer Security Applications Conference (ACSAC &apos;07)</title>
		<meeting>the 23rd Annual Computer Security Applications Conference (ACSAC &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007-12">December 2007</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatically inferring malware signatures for anti-virus assisted attacks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM Asia Conference on Computer and Communications Security<address><addrLine>UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="587" to="598" />
		</imprint>
	</monogr>
	<note>ASIA CCS &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A layered architecture for detecting malicious behaviors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martignoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Symposium on Recent Advances in Intrusion Detection (RAID &apos;08)</title>
		<meeting>eeding of the International Symposium on Recent Advances in Intrusion Detection (RAID &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for constructing features and models for intrusion detection systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="227" to="261" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On challenges in evaluating malware clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Recent Advances in Intrusion Detection</title>
		<meeting>the International Symposium on Recent Advances in Intrusion Detection</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6307</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data mining methods for detection of new malicious executables</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy (S &amp; P)</title>
		<meeting>the IEEE Symposium on Security and Privacy (S &amp; P)</meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 12th International Conference on Machine Learning</title>
		<meeting>eeding of the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect and classify malicious executables in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maloof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2721" to="2744" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep neural network based malware detection using two dimensional binary program features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Malicious and Unwanted Software, (MALWARE &apos;15)</title>
		<meeting>the 10th International Conference on Malicious and Unwanted Software, (MALWARE &apos;15)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">October 2015</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Malware images: Visualization and automatic classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nataraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Visualization for Cyber Security</title>
		<meeting>the 8th International Symposium on Visualization for Cyber Security<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comparative assessment of malware classification using binary texture analysis and dynamic analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nataraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Yegneswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Porras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence (AISec &apos;11)</title>
		<meeting>the 4th ACM Workshop on Security and Artificial Intelligence (AISec &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminant malware distance learning on structural information for automated malware classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (KDD &apos;13)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (KDD &apos;13)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="page" from="1357" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N-gramsbased file signatures for malware detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Penya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Bringas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICEIS 2009 -11th International Confeence on Enterprise Information Systems</title>
		<meeting>the ICEIS 2009 -11th International Confeence on Enterprise Information Systems</meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting unknown malicious code by applying classification techniques on opcode patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shabtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moskovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Security Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OPEM: A static-dynamic approach for machine-learningbased malware detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brezo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Bringas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference CISIS&apos;12-ICEUTE12-SOCO12 Special Sessions</title>
		<meeting>the International Joint Conference CISIS&apos;12-ICEUTE12-SOCO12 Special Sessions</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ida Pro</surname></persName>
		</author>
		<ptr target="http://www.hexrays.com/products/ida/support/down-loadfreeware.shtml" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for on-line training of recurrent network trajectories</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">LSTMbased encoder-decoder for multi-sensor anomaly detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anand</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.00148" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensemble methods foundations and algorithms</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Taylor &amp; Francis, London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Windows Software</title>
		<author>
			<persName><surname>Pc</surname></persName>
		</author>
		<ptr target="http://download.cnet.com/windows/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Software</forename><surname>Baidu</surname></persName>
		</author>
		<author>
			<persName><surname>Center</surname></persName>
		</author>
		<ptr target="http://rj.baidu.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1207.0580" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations (ICLR &apos;15)</title>
		<meeting>the 3rd International Conference for Learning Representations (ICLR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities, improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 30th International Conference on Machine Learning (ICML &apos;13)</title>
		<meeting>eeding of the 30th International Conference on Machine Learning (ICML &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML &apos;15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations (ICLR &apos;2015)</title>
		<meeting>the 3rd International Conference for Learning Representations (ICLR &apos;2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/xiaozhouwang/kaggleMicrosoftMalware/blob/master/Saynotooverfitting.pdf" />
		<title level="m">Microsoft Malware Classification Challenge (BIG 2015) First Place Team: Say No To Overfitting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Novel feature extraction, selection and fusion for effective malware family classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Semenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM Conference on Data and Application Security and Privacy, (CODASPY &apos;16)</title>
		<meeting>the 6th ACM Conference on Data and Application Security and Privacy, (CODASPY &apos;16)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Performance analysis of machine learning and pattern recognition algorithms for Malware classification</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Djaneye-Boundjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kebede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE National Aerospace and Electronics Conference and Ohio Innovation Summit, (NAECON-OIS &apos;16)</title>
		<meeting>the 2016 IEEE National Aerospace and Electronics Conference and Ohio Innovation Summit, (NAECON-OIS &apos;16)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Random forest for malware classification</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Muga</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.07770" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">One-class SVM with privileged information and its application to malware detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smolyakov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.08039" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Malware detection using deep transferred generative adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing</title>
		<meeting>the International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Polymorphic malware detection using sequence classification methods and ensembles: BioSTAR 2016 Recommended Submission -EURASIP Journal on Information Security</title>
		<author>
			<persName><forename type="first">J</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adversarial perturbations against deep neural networks for malware classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.04435" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<meeting>the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.1897" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
	<note>ASIA CCS &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Defensive distillation is not robust to adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.04311" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.07263" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.02976" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
