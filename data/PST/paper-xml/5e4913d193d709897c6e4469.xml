<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
							<email>msjunbozhang@outlook.com</email>
						</author>
						<author>
							<persName><forename type="first">Junkai</forename><forename type="middle">â€¢ K</forename><surname>Sun</surname></persName>
							<email>junkaisun@outlook.com</email>
						</author>
						<author>
							<persName><forename type="first">Dekang</forename><forename type="middle">K</forename><surname>Qi</surname></persName>
							<email>dekangqi@outlook.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<country>China. Y. Zheng</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit1">Xidian Univer-sity</orgName>
								<orgName type="institution" key="instit2">Chinese Academy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75E4D413617A16C7F99649BA11239F3D</idno>
					<idno type="DOI">10.1109/TKDE.2019.2891537</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Spatio-temporal Data</term>
					<term>Urban Computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting flows (e.g. the traffic of vehicles, crowds and bikes), consisting of the in-out traffic at a node and transitions between different nodes, in a spatio-temporal network plays an important role in transportation systems. However, this is a very challenging problem, affected by multiple complex factors, such as the spatial correlation between different locations, temporal correlation among different time intervals, and external factors (like events and weather). In addition, the flow at a node (called node flow) and transitions between nodes (edge flow) mutually influence each other. To address these issues, we propose a multitask deep-learning framework that simultaneously predicts the node flow and edge flow throughout a spatio-temporal network. Based on fully convolutional networks, our approach designs two sophisticated models for predicting node flow and edge flow respectively. These two models are connected by coupling their latent representations of middle layers, and trained together. The external factor is also integrated into the framework through a gating fusion mechanism. In the edge flow prediction model, we employ an embedding component to deal with the sparse transitions between nodes. We evaluate our method based on the taxicab data in Beijing and New York City. Experimental results show the advantages of our method beyond 11 baselines, such as ConvLSTM, CNN, and Markov Random Field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S Patio-temporal networks (ST-networks), like transporta- tion networks and sensor networks, are widely available in the real world, with each node incorporating a spatial coordinate and each edge being associated with dynamic properties. Flows in such ST-networks have two representations (see Figure <ref type="figure" target="#fig_0">1</ref>): 1) node flow, i.e., the in-and out-flows at a node, and 2) edge flow, namely, the transitions between nodes. In a transportation system, these two types of flows can be measured by 1 the number of cars driven nearby roads, 2 the number of people traveling by metro/bus, 3 the number of pedestrians, or 4 all of them together if data is available. Figure <ref type="figure" target="#fig_0">1</ref>(b) presents an illustration. Taking node r 1 as an example, we can calculate the inflow as 3, and outflow as 3 according to the mobile phone signals and the GPS trajectories of vehicles, respectively. In detail, we can see the transition from r 3 to r 1 is 3, and the transition from r 1 to r 2 and r 4 are 2 and 1, respectively. Therefore, we can get two levels of flows: node-level and edge-level, as shown in Figure <ref type="figure" target="#fig_0">1(c)</ref>, of which the inflow and outflow of four nodes (r 1 , r 2 , r 3 , r 4 ) are (3, 3, 0, 5) and (3, 2, 5, 1), respectively, with transitions over all edges being viewed as a directed graph.</p><p>Predicting these types of flows in a ST-network is of great importance to public safety, traffic management and Yu Zheng and Junbo Zhang are corresponding authors. Partial work were done when authors are researchers at Microsoft Research. network optimization <ref type="bibr" target="#b33">[34]</ref>. Taking the crowd flow <ref type="bibr" target="#b32">[33]</ref> as an example, amounts of people streamed into a strip region at the 2015 New Year's Eve celebrations in Shanghai, resulting in a catastrophic stampede that killed 36 people. If one can predict the transitions between regions and the crowd flow in each region, such tragedies can be prevented or mitigated by utilizing emergency mechanisms (e.g., sending out warnings, evacuating people, or conducting traffic control). However, simultaneously predicting in/out flows at all nodes and transitions over edges of a ST-network is very challenging because of the following aspects: 1) Scale and complexity: The in/out flow of a location depend on that of its near neighbors as well as distant neighbors in geographical spaces, as people can transit between any locations, particularly when some events take place in a city. Given a big city with a large number (N ) of locations, there are N 2 possibility of transitions, though these transitions may not occur simultaneously at a time interval. Thus, to predict the flow of location, either the in/out flow or transition flow, we need to consider the dependence between the location and others throughout a city. In addition, the prediction is also concerned with the flow at past time intervals. Moreover, we cannot predict the flow of each location individually and respectively, because locations in a city are connected, correlated, and mutually influence each other. The complexity and scale have posed huge challenges to traditional machine learning models like probabilistic graphical models.</p><p>2) Model multiple correlations and external factors: There are three types of correlations we need to model when dealing with such a prediction problem. The first one is the spatial correlation between flows of different locations, including the correlation between near locations and that between distant locations. The second one is the temporal correlation between flows of a location at different time intervals, consisting of the temporal closeness, periodic and trend properties. Third, the in/out flows and transition flow are highly correlated and mutually reinforced. The sum of transitions streaming into a location is the in-flow of the location. Likewise, an accurate prediction of the total outflow in a location can help predict the transition flows from the location to other places more accurately, vice versa. Additionally, these flows are affected by external factors, such as events, weather, and accidents. How to integrate them into the predictive model is non-trivial.</p><p>3) Dynamics and sparsity: Because of the N 2 possibility, the flow of transitions between locations changes over time much more tremendously than the in/out flow. The transitions (between a location and the rest of places) that will really occur at the next time interval may be a very small portion of the N 2 possibilities (i.e. very sparse). Predicting such a sparse transition in such a high dimensional space is a very challenging task.</p><p>To tackle the aforementioned challenges, we propose a Multitask Deep-Learning (MDL, see Figure <ref type="figure" target="#fig_2">4</ref>) framework to predict the flows at nodes and on edges collectively and simultaneously. The contributions of the research are threefold:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The MDL devises a deep neural network for predicting the flow at nodes (entitled NODENET) and that on edges (entitled EDGENET) respectively. These two deep neural networks are coupled through a concatenation of their latent layers, and trained together. In addition, the correlation between these two types of flows are modeled by a regularization in the loss function. The deep learning-based model can handle the complexity and scale problem in the prediction, while the multitask framework mutually reinforces the prediction of each type of flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>Both NODENET and EDGENET are three-stream fully convolutional networks (3S-FCNs), where closenessstream, period-stream, and trend-stream capture three different temporal correlations. Each stream FCN also captures spatial correlations between both near and distant locations. A gating component is employed to fuse the external factors with the spatiotemporal correlations. To deal with the transition sparsity problem, in the EDGENET we design an embedding component, which encodes the sparse (and high dimensional) input with a latent and lowdimensional representation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>Definition 1 (Node). A spatial map is divided into I Ã— J grids based on the longitude and latitude, denoted by V = {r 1 , r 2 , ..., r IÃ—J }, each of which represents a spatial node, as shown in Figure <ref type="figure" target="#fig_5">2(a)</ref>.</p><p>Let (Ï„, x, y) be a temporal geospatial coordinate, of which Ï„ denotes timestamp, and (x, y) denotes geospatial point. The movement of an object can be recorded as a timeordered spatial trajectory, among which the start point and end point (i.e. start-end pair), denoted by s = (Ï„ s , x s , y s ) and e = (Ï„ e , x e , y e ), represent the source and destination, respectively. Let P be all start-end (i.e. (s, e)) pairs. Definition 2 (In/out flows). Given a set of start-end pairs P.</p><p>Let T = {t 1 , â€¢ â€¢ â€¢ , t T } be a sequence of time intervals. For a node r ij that lies at the i th row and the j th column of the map, the outflow and inflow during the interval t are defined respectively as</p><formula xml:id="formula_0">X t (0, i, j) = |{(s, e) âˆˆ P : (x s , y s ) âˆˆ r ij âˆ§ Ï„ s âˆˆ t}| (1) X t (1, i, j) = |{(s, e) âˆˆ P : (x e , y e ) âˆˆ r ij âˆ§ Ï„ e âˆˆ t}| (2)</formula><p>where X t (0, :, :) and X t (1, :, :) mean outflow and inflow matrices, respectively. (x, y) âˆˆ r ij means the point (x, y) lies within the node r ij , and Ï„ e âˆˆ t means the timestamp </p><formula xml:id="formula_1">t = t 1 , â€¢ â€¢ â€¢ , t T )</formula><p>, there exists a counterpart weight matrix S t âˆˆ R N Ã—N that represents the weighted directed edges between nodes during the t th time interval. In our study, the weight of the edge from node r s to node r e at time t is a non-negative scalar representing the transition from r s to r e in the corresponding time interval. In a case where there is no connection between two nodes at time t, the corresponding element in S t should be 0. Definition 3 (Transition). Given a set of start-end pairs P.</p><p>Let T = {t 1 , â€¢ â€¢ â€¢ , t T } be a sequence of time intervals. Let S t be the transition matrix during the interval t.</p><p>The transition from node r s to r e , denoted S t (r s , r e ), is defined as</p><formula xml:id="formula_2">S t (r s , r e ) = |{(s, e) âˆˆ P : (x s , y s ) âˆˆ r s âˆ§ (x e , y e ) âˆˆ r e âˆ§ Ï„ s âˆˆ t âˆ§ Ï„ e âˆˆ t}|<label>(3)</label></formula><p>where r s , r e âˆˆ V are the start and end nodes, respectively. (x, y) âˆˆ r means the point (x, y) lies within the grid r. Ï„ s âˆˆ t and Ï„ e âˆˆ t mean that the timestamp Ï„ s and Ï„ e are both in the time interval t. Here we consider the transitions that only happen at a certain time interval. Therefore, for a real-world application, we can predict a real transition whose start and end points are both in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Converting time-varying graphs into tensors</head><p>To apply deep neural networks to time-varying graphs, we propose converting each graph at time t into a tensor first. Given a directed graph G t = (V, E t ) at time t, we unroll it first, then compute the directed weight matrix (e.g., transition matrix S t ), and finally get a tensor M t âˆˆ R 2N Ã—IÃ—J . Figure <ref type="figure">3</ref> presents an illustration. (a) Given a graph consisting of 4 nodes and 6 edges at time t. (b) We first unroll it that is a directed graph. (c) For each node, there are incoming and outgoing transitions, represented by a vector (dimension = 8). Taking Node r 1 for example, its outgoing and incoming transition vectors are respectively [0, 2, 0, 1] and [0, 0, 3, 0], which are further concatenated into one vector [0, 2, 0, 1, 0, 0, 3, 0], containing both outgoing and incoming information. (d) Finally, we can reshape the matrix into a tensor, among which each node has a fixed spatial position according to the original map segmentation, protecting the spatial correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Flow Prediction Problem</head><p>Flow prediction, generally speaking, is a time series problem, which aims to predict the citywide flows in each region at time interval T+1 given the historical observations until time T. But the flows in our paper contain two perspectives, which are inflow,outflow in regions and transition flows between regions, as defined above. Our goal in this paper is to predict all these flows at the same time. In addition, we also integrate some external factors such as holidays information, weather conditions, temperature and so on. These external features can be collected and provide some extra useful information. The related notations are listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Problem 1. We here define the goal of our paper. Given the historical flow observations</p><formula xml:id="formula_3">{X t , M t |t = t 1 , â€¢ â€¢ â€¢ , t T } and</formula><p>external features E T , we propose a model to collectively predict X t T +1 and M t T +1 in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTITASK DEEP LEARNING</head><p>Figure <ref type="figure" target="#fig_2">4</ref> presents our MDL framework, consisting of three components, which are used for data converting, node flow modeling, and edge flow modeling, respectively. As illustrated in the left part of Figure <ref type="figure" target="#fig_2">4</ref>, we first convert the trajectory (or trip) data over a map along time into two types of flows: i) node flow that is a time-ordered sequence of tensors</p><formula xml:id="formula_4">{X t |t = t 1 , â€¢ â€¢ â€¢ , t T } (Step (1a)); ii) edge flow that is a time-ordered sequence of graphs (transition matrices) {S t |t = t 1 , â€¢ â€¢ â€¢ , t T } (Step (2a)), which is further converted into a sequence of tensors {M t |t = t 1 , â€¢ â€¢ â€¢ , t T } (Step (2b))</formula><p>according to the method introduced in Section 2.1. These two types of video-like data are then fed into NODENET and EDGENET, respectively. Taking NODENET as an example, it selects three different types of fragments, and feed them into a 3S-FCN, which can model the temporal correlations, including closeness, period, and trend. Among them, each steam FCN can capture spatial near and distant correlations via multiple convolutions. The latent representations of middle layers of NODENET and EDGENET are coupled by a BRIDGE component, and trained together. We employ a embedding layer (called Em) to handle transition sparsity problem. A gating fusion component is used to integrate the external factors. In addition, the correlation between the node flow and edge flow are modeled by a regularization between Xt and Mt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EDGENET</head><p>According to the aformentioned converting method, the transition graph at each time interval can be converted into a tensor M t âˆˆ R 2N Ã—IÃ—J . For each node r ij , it has up to 2N transition possibility, including N incomings and N outgoings. However, for a certain time interval, the transition between nodes may be very sparse. Inspired by the embedding method of natural language processing <ref type="bibr" target="#b22">[23]</ref>, we propose employing a spatial embedding method, to tackle such sparse and high-dimensional (2N , depending on the number of nodes in the ST-network) problem. In detail, the spatial embedding tends to learn a function that maps a 2N -dimension vector of node r ij into a k-dimension space as follows:</p><formula xml:id="formula_5">Z t (:, i, j) = W m M t (:, i, j) + b m , 1 â‰¤ i â‰¤ I, 1 â‰¤ j â‰¤ J (4)</formula><p>where W m âˆˆ R kÃ—2N and b m âˆˆ R k are the learnable parameter matrix and vector, respectively. All I Ã— J nodes share these parameters. M t (:, i, j) âˆˆ R 2N means the vector located at (i, j).</p><p>The flows, like the traffic of crowds in a city <ref type="bibr" target="#b32">[33]</ref>, are always affected by spatio-temporal dependencies. To capture different temporal dependencies (closeness, period, and trend), Zhang et al. proposed a deep spatio-temporal residual network that selects different key frames along the time.</p><p>Inspired by this, we here selects recent, near, and distant key frames to predict the time interval t, respectively denoted</p><formula xml:id="formula_6">M dep t = {M close t , M period t , M trend t</formula><p>}, as follows:</p><p>â€¢ Closeness dependents:</p><formula xml:id="formula_7">M close t = {Z t-lc , â€¢ â€¢ â€¢ , Z t-1 }. â€¢ Period dependents: M period t = {Z t-lpâ€¢p , Z t-(lp-1)â€¢p , â€¢ â€¢ â€¢ Z t-p }.</formula><p>â€¢ Trend dependents:</p><formula xml:id="formula_8">M trend t = {Z t-lqâ€¢q , Z t-(lq-1)â€¢q , â€¢ â€¢ â€¢ Z t-q }.</formula><p>where p and q are the period and trend span, respectively. l c , l p , and l q are the lengths of these three parts of sequences. The output (i.e. the prediction at next time interval) has the same resolution as the inputs. Such task is very similar to the well-known image segmentation problem, which can be handled by a fully convolutional network (FCN) <ref type="bibr" target="#b21">[22]</ref>. Inspired by this, we here propose a three-stream FCN (3S-FCN, see Figure <ref type="figure" target="#fig_2">4</ref>) to capture temporal closeness, period, and trend dependencies. Among that, each stream is a FCN, consisting many convolutions (see Figure <ref type="figure" target="#fig_3">5</ref>). According to the property of convolution, one convolutional layer can capture spatial near dependencies. As the number of convolutional layers increases, FCN can capture farther and farther dependencies, even citywide spatial dependencies. However, such deep convolution network become very hard to train. Therefore, we employ residual connections <ref type="bibr" target="#b11">[12]</ref> to help the training. Similar to the residual block used in the residual network <ref type="bibr" target="#b12">[13]</ref>, we use a block that consists of Batch Normalization (BN, <ref type="bibr" target="#b15">[16]</ref>), Rectified Linear Unit (ReLU, <ref type="bibr" target="#b18">[19]</ref>), and Convolution (Conv). Let the outputs of closeness-, period-, and trend-stream FCNs be M c , M p , M q , respectively. Different nodes may have different properties of closeness, period, and trend. To address this issue, we propose using a parametric-matrix-based fusion <ref type="bibr" target="#b32">[33]</ref> (PM fusion in Figure <ref type="figure" target="#fig_2">4</ref>), to merge them,</p><formula xml:id="formula_9">M f cn = W c M c + W p M p + W q M q (5)</formula><p>where is the Hadamard product (i.e., element-wise multiplication), W c , W p , W q are the learnable parameters that adjust the degrees affected by temporal closeness, period and trend, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NODENET and BRIDGE</head><p>Similar to EDGENET, NODENET is also a 3S-FCN, we select recent, near, and distant key frames as the closeness, period, and trend dependents. The difference is that NODENET doesnot have the embedding layer because the number of channels of inputs is only 2. These three different sets of dependents are fed into three different stream FCNs, whose outputs are further merged by a PM fusion component (see Figure <ref type="figure" target="#fig_2">4</ref>), too. Then, we can get the output of 3S-FCN, denoted X f cn âˆˆ R CxÃ—IÃ—J .</p><p>Considering that node flow is correlated with edge flow, so the representations learned from NODENET and ED-GENET should be connected. To connect NODENET and ED-GENET, assuming two latent representations of NODENET and EDGENET are X f cn and M f cn respectively. We here propose two fusion methods. SUM Fusion: The sum fusion method directly sum up these two representations, the output map at the same spatial node r ij across channel c is as follows :</p><formula xml:id="formula_10">H(c, :, :) = X f cn (c, :, :) + M f cn (c, :, :), c = 0, â€¢ â€¢ â€¢ , C -1 (6)</formula><p>where C is the number of channels of X f cn and M f cn , and H âˆˆ R CÃ—IÃ—J . It's obvious that this fusion method is subjected to the fact that both representations of two tasks should have a same shape, i.e. X f cn and M f cn have a same size at channel dimension. CONCAT Fusion: In order to be free from the restraint. We propose an another fusion method called CONCAT.Formally, the concatenation of two latent representation maps X f cn and M f cn at the same spatial node r ij across channel c as follows:</p><formula xml:id="formula_11">H(c, :, :) = X f cn (c, :, :), c = 0, â€¢ â€¢ â€¢ , C x -1 (7) H(C x + c, :, :) = M f cn (c, :, :), c = 0, â€¢ â€¢ â€¢ , C m -1<label>(8)</label></formula><p>where C x and C m are the numbers of channels of X f cn and M f cn , respectively, and H âˆˆ R (Cx+Cm)Ã—IÃ—J . CONCAT fusion actually can better integrates two levels of node and edge flows by mutually reinforcing. We also discuss another fusion method as BRIDGE (see Section 4.3).</p><p>After CONCAT fusion, we append a convolutional layer into NODENET and EDGENET, respectively. The convolution is used to map combined latent feature maps H into different-size-channel outputs, i.e., X res âˆˆ R 2Ã—IÃ—J and M res âˆˆ R 2N Ã—IÃ—J , see Figure <ref type="figure" target="#fig_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusing External Factors Using a Gating Mechanism</head><p>External factors, such as events and weather, that can affect the flows in the different parts of a ST-network. For example, an accident may block the traffic of a certain area locally, and a rainstorm may reduce the citywide flows globally. Such an external factor just like a switch, the flows would be tremendously changed if it happen. Based on this insight, we here develop a gating-mechanism-based fusion, as shown in Figure <ref type="figure" target="#fig_4">6</ref>. At time t, one can obtain the corresponding external features in the ST-network, denoted E t âˆˆ R leÃ—IÃ—J , of which E t (:, i, j) âˆˆ R le represents the feature vector of a particular node. Formally, we can obtain the following gating values for EDGENET as follows, F m (i, j) = Ïƒ (W e (:, i, j) â€¢ E t (:, i, j) + b e (i, j)) ,</p><formula xml:id="formula_12">1 â‰¤ i â‰¤ I, 1 â‰¤ j â‰¤ J<label>(9)</label></formula><p>where W e âˆˆ R leÃ—IÃ—J and b e âˆˆ R IÃ—J are learnable parameters. F m âˆˆ R IÃ—J is the output of GATING, of which Then we employ a PRODUCT fusion based on the gating mechanism as follows:</p><formula xml:id="formula_13">Mt (c, :, :) = tanh(F m M Res (c, :, :)), c = 0, â€¢ â€¢ â€¢ , 2N -1 (10)</formula><p>where tanh is a hyperbolic tangent that ensures the output values are between -1 and 1. Similarly, the final prediction of NODENET at time t is Xt (c, :, :) = tanh(F x X Res (c, :, :)), c = 0, 1</p><p>where F x âˆˆ R IÃ—J is another output of GATING. One reason of using different GATING values (i.e. F) for node and edge flows is that the external factors can affect the in/out flows and transitions of different locations differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Losses</head><p>Let Ï† be all the learnable parameters in EDGENET, we intend to learn them by minimizing the following objective function between predicted transitions M and true transitions M,</p><formula xml:id="formula_15">arg min Ï† J edge = tâˆˆT 2N-1 c=0 Q c t</formula><p>Mt (c, :, :) -M t (c, :, :)</p><p>(12) where Q c t is an indication matrix for all the non-zero entries in M t (c, :, :), i.e., Q c t (i, j) = 1 if and only if M t (c, i, j) &gt; 0. T is a set of available time intervals. â€¢ F is the Frobenius Norm of a matrix. Similarly, let Î¸ be all the learnable parameters in NODENET. t is an indication matrix for all the non-zero entries in X t (c, :, :), i.e., P c t (i, j) = 1 if and only if X t (c, i, j) &gt; 0. We know that the sum of transitions streaming into node r i j is the inflow of the node, and the sum of transitions streaming out is the outflow. From Definition 2, Xt (0, :, :) and Xt (1, :, :) are outflow and inflow matrices, respectively.</p><p>According to the transition tensor constructed method introduced in Section 2.1, we know that the first N channels represent outgoing transitions, and the last N channels represent incoming transitions. Therefore, it yields the following loss function:</p><formula xml:id="formula_17">arg min Î¸,Ï† tâˆˆT i j Xt (0, i, j) - N -1 c=0 Mt (c, i, j) 2 + Xt (1, i, j) - 2N -1 c=N Mt (c, i, j) 2 (14)</formula><p>Or, equivalently, it can be written as</p><formula xml:id="formula_18">arg min Î¸,Ï† J mdl = tâˆˆT ï£« ï£¬ ï£¬ ï£¬ ï£¬ ï£­ Xt (0, :, :) outflow - N -1<label>c=0</label></formula><p>Mt (c, :, :)</p><formula xml:id="formula_19">outgoing transitions 2 F + Xt (1, :, :) inflow - 2N -1 c=N</formula><p>Mt (c, :, :)</p><formula xml:id="formula_20">incoming transitions 2 F ï£¶ ï£· ï£· ï£· ï£· ï£¸<label>(15)</label></formula><p>Finally, we obtain the combined loss as follows:</p><formula xml:id="formula_21">arg min Î¸,Ï† Î» node J node + Î» edge J edge + Î» mdl J mdl<label>(16)</label></formula><p>where Î» node , Î» edge , and Î» mdl are adjustable hyperparameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We consider two kinds of datasets: TaxiBJ and TaxiNYC, see Table <ref type="table" target="#tab_4">2</ref>. To evaluate the prediction performance, we consider the Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).   â€¢ LSTM: Long-Short-Term-Memory network <ref type="bibr" target="#b14">[15]</ref>. The setting is same to RNN.</p><p>â€¢ GRU: Gated-Recurrent-Unit network <ref type="bibr" target="#b5">[6]</ref>. The setting is same as RNN.</p><p>â€¢ ST-ANN: Spatio-Temporal Artificial Neural Network, which takes spatial (nearby 8 regions) and temporal (8 previous time intervals) values as input features.</p><p>â€¢ ConvLSTM: Convolutional LSTM <ref type="bibr" target="#b28">[29]</ref>, a state-of-the-art model for precipitation nowcasting using the radar echo dataset (image sequence). The crowd flow data used in this paper can be viewed as a sequence of images, each of which is crowd flows at a time interval. Previous 3 frames are used to predict the next frame. The model consists of two ConvLSTM layers and a convolutional layer, in which the kernel size is (3, 3) and the filter number is 32. Other hyperparameters are same to RNN.</p><p>â€¢ ST-ResNet: Spatio-Temporal Residual Convolutional Network <ref type="bibr" target="#b32">[33]</ref>, showing state-of-the-art performance on node flow prediction.</p><p>â€¢ MRF: Markov-Random-Field-based citywide flow prediction model <ref type="bibr" target="#b13">[14]</ref>, that leverages flows in all individual regions and transitions between regions as well as external factors (e.g., weather).</p><p>For both datasets, we select last four weeks (i.e. 672 time intervals) as the test set, and the others as the training set. MDL is implemented using TensorFlow <ref type="bibr" target="#b1">[2]</ref> and Keras <ref type="bibr" target="#b6">[7]</ref>, and trained via backpropagation and the Adam <ref type="bibr" target="#b17">[18]</ref> optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Preprocessing</head><p>In the output of the MDL, we use tanh as our final activation, whose range is between -1 and 1. Here, we use the Min-Max normalization method to scale the data into the range [-1, 1]. In the evaluation, we re-scale the predicted value back to the normal values, compared with the ground truth. For external factors, we use one-hot encoding to transform metadata (i.e., DayOfWeek, Weekend/Weekday), holidays and weather conditions into binary vectors, and use Min-Max normalization to scale the Temperature and Wind speed into the range [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Hyperparameters</head><p>We here introduce the hyperparameter settings of our MDL. By default, we set Î» node = 1 and Î» edge = 1, which means two tasks are equally important, and Î» mdl as 0.0005. p and q are empirically fixed to one-day and one-week, respectively. For lengths of the three dependent sequences, we set them as: l c âˆˆ {1, 2, 3}, l p âˆˆ {1, 2, 3}, l q âˆˆ {1, 2, 3}. We set the number of convolutions of FCN as 5 by default. We select 90% of the training data for training each model, and the remaining 10% is chosen as the validation set, which is used to early-stop our training algorithm for each model based on the best validation score. Afterwards, we continue to train the model on the full training data for a fixed number of epochs (e.g., 10 epochs). Network parameters are trained from a random start<ref type="foot" target="#foot_1">1</ref> , using the Adam <ref type="bibr" target="#b17">[18]</ref> optimization to perform all weight updated with a fixed learning rate. The batch size is 32. The learning rate is set as one of {0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Evaluation Metrics</head><p>We measure the accuracy of our methods and baselines by Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) <ref type="foot" target="#foot_2">2</ref> for both node-level (i.e., inflow/outflow) and edgelevel (i.e., transition) prediction as where y and Å· are the available ground truth and the corresponding predicted value, respectively; n is the number of all available ground truths.</p><formula xml:id="formula_22">RM SE = 1 n i (y i -Å·i ) 2 , M AE = 1 n i |y i -Å·i |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Node Flow Prediction. We first compare various methods on the task of predicting in/out flows in the test test, given observed trained data. Table <ref type="table" target="#tab_6">4</ref> shows the RMSE and MAE of node flow prediction on TaxiBJ and TaxiNYC. We can observe that, MDL and MRF consistently outperforms all other baselines. In detail, our MDL performs apparently better than MRF on TaxiNYC. On the dataset TaxiBJ, MDL has a competitive result against MRF. The reason may be that TaxiNYC is 3 times bigger (T in Table <ref type="table" target="#tab_4">2</ref>) than TaxiBJ.</p><p>In other words, our MDL has better performance on larger data than MRF. We also notice that it is time-consuming to train MRF, which takes about one week to finish the whole training process on TaxiBJ using the code provided in <ref type="bibr" target="#b13">[14]</ref>. In detail, taking the inflow prediction of TaxiNYC as an example, the results of RMSE demonstrate that MDL is relatively 85% better than HA, 42% better than ARIMA, 34% better than SARIMA, 39% better than VAR, 11% better than ST-ANN, 47% better than RNN, 8% better than ST-ResNet, and 5% better than ConvLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Edge Flow Prediction.</head><p>We next compare the methods on the task of forecasting transitions. Table <ref type="table" target="#tab_8">6</ref> presents the RMSE and of edge flow prediction on TaxiBJ and TaxiNYC. The experiments on the transition prediction task is very time-consuming. We mainly run the experiments on MDL and HA, ARIMA, SARIMA, ST-ANN, and ST-ResNet, demonstrating that MDL outperforms others. The results show that our MDL significantly outperforms 5 baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Fusing Mechanisms</head><p>In this section, we present the empirical experiments on different fusing mechanisms. To couple NODENET and ED-GENET, we introduce the CONCAT fusion in Section 3.2. A straight-forward fusion method is to use the SUM fusion by H = X f cn + M f cn . Note that SUM requires two latent feature maps have the same shape. For fusing external factors, one can choose one of the following ways: the GATED fusion introduced in Section 3.3, SIMPLE fusion (the sum fusion in <ref type="bibr" target="#b32">[33]</ref>), or not use (i.e. w/o). Therefore, there are a total of 6 variants of MDL, as shown in      Many works are trying to find some patterns and correlations from spatio-temporal datasets <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. There are some previously published works on predicting an individual's movement based on their location history <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>. They mainly forecast millions, even billions, of individuals' mobility traces rather than the aggregated crowd flows in a region. Such a task may require huge computational resources, and it is not always necessary for public safety situations. Some other researchers aim to predict travel speed and traffic volume on the road <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Most of them are predicting single or multiple road segments, rather than citywide ones <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Recently, researchers have started to focus on city-scale traffic flow prediction <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Both work are different from ours where the proposed methods naturally focus on the individual region not the city, and they do not partition the city using a grid-based method which requires a more complex method to find irregular regions first. Deng et al. proposed a latent space model for predicting time-varying traffic <ref type="bibr" target="#b7">[8]</ref> on the fixed graph (i.e. road network), which is different from ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classical Models for Time Series Prediction.</head><p>Forecasting flow in a spatio-temporal network can be viewed as a time series prediction problem. Existing timeseries models, like the auto-regressive integrated moving average model (ARIMA, <ref type="bibr" target="#b2">[3]</ref>), seasonal ARIMA <ref type="bibr" target="#b25">[26]</ref>, and the vector autoregressive model <ref type="bibr" target="#b3">[4]</ref> can capture the temporal dependencies very well, yet it fails to handle spatial correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neural Networks for Sequence Prediction.</head><p>Neural networks and deep learning <ref type="bibr" target="#b9">[10]</ref> have gained numerous success in the fields such as compute vision <ref type="bibr" target="#b18">[19]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, and natural language understanding <ref type="bibr" target="#b19">[20]</ref>. Recurrent neural networks (RNNs) have been used successfully for sequence learning tasks <ref type="bibr" target="#b27">[28]</ref>. The incorporation of long short-term memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> or gated recurrent unit (GRU) <ref type="bibr" target="#b5">[6]</ref> enables RNNs to learn longterm temporal dependency. However, these neural network models can only capture spatial or temporal dependencies.</p><p>Recently, researchers have combined the above networks and proposed a convolutional LSTM network <ref type="bibr" target="#b28">[29]</ref> that learns spatial and temporal dependencies simultaneously. Such a network cannot model very long-range temporal dependencies (e.g., period and trend), and training becomes more difficult as depth increases. Zhang et al. proposed a spatio-temporal residual network <ref type="bibr" target="#b32">[33]</ref>, capable of capturing spatio-temporal dependencies as well as external factors, yet it may be not suited to deal with transitions over large dynamic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We proposed a novel multitask deep learning (MDL) framework for simultaneously predicting in/out flows (node flow) and transitions (edge flow) in a spatio-temporal network. MDL can not only handle the complexity and scale problem in the prediction, but also mutually reinforce the prediction of each type of flow. In addition, MDL is capable of capturing the spatial correlations (near and distant), temporal correlations (closeness, period, trend), and external factors (like events and weather). We evaluate our MDL on two real-world datasets in Beijing and NYC, achieving performances which are significantly better than 11 baseline methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flows in a simple spatio-temporal network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spatial nodes (regions) and flow matrices</figDesc><graphic coords="2,404.16,47.19,100.88,69.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. MDL framework. Em: embedding; Conv: convolution; FCN: fully convolutional network.</figDesc><graphic coords="4,73.80,43.70,464.40,217.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. FCN with residual connections.</figDesc><graphic coords="4,312.00,304.22,252.00,60.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. MDL using CONCAT fusion</figDesc><graphic coords="5,312.00,43.70,252.00,150.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 F</head><label>2</label><figDesc>For the square loss it yields the following optimization problem, , :, :) -X t (c, :, :)<ref type="bibr" target="#b12">(13)</ref> where P c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3</head><label></label><figDesc>put an training instance ({X dep t , M dep t , Et}, Xt) into D train 4 end // train the model 5 initialize the parameters Î¸, Ï† 6 repeat 7 randomly select a batch of instances D batch from D train 8 find Î¸, Ï† by minimizing the objective (16) with D batch 9 until stopping criteria is met 10 output the learned MDL model Algorithm 1 outlines the MDL training process. We first construct training instances from the original sequence of observations (lines 1-4). During each iteration, we optimize the objective (16) on the selected batch of training instances D batch (lines 7-8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .Fig. 9 .Fig. 10 .</head><label>78910</label><figDesc>Fig. 7. Training curves on TaxiNYC of various fusions. The vertical axis corresponds to training and validation (valid) losses, and the horizontal axis corresponds to the number of epochs.</figDesc><graphic coords="9,312.20,254.49,251.80,104.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Predictions of MDL against the ground truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>lists the mathematical notation used in this paper.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Description of notation</cell></row><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>V = {r Xt âˆˆ R 2Ã—IÃ—J</cell><cell>tensor of inflow/outflow at nodes at time t</cell></row><row><cell>St âˆˆ R N Ã—N</cell><cell>matrix of transition over edges at time t</cell></row><row><cell>Mt âˆˆ R 2N Ã—IÃ—J</cell><cell>tensor of transition converted from St</cell></row><row><cell>Et âˆˆ R le</cell><cell>external features at time t</cell></row><row><cell>Xt(:, i, j), Mt(:, i, j)</cell><cell>vector of node r ij</cell></row><row><cell>Xt(c, :, :), Mt(c, :, :)</cell><cell>matrix of c-th channel</cell></row><row><cell>X dep t M dep t</cell><cell>dependent set of Xt dependent set of Mt</cell></row><row><cell>2</cell><cell># channels of node flow Xt</cell></row><row><cell>2N</cell><cell># channels of edge flow Mt</cell></row></table><note><p><p>ij } spatial node set, 1 â‰¤ i â‰¤ I, 1 â‰¤ j â‰¤ J N</p>number of nodes, i.e., I Ã— J T available time interval set</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>t1 , â€¢ â€¢ â€¢ , G t T . A particular graph G t = (V, E t )captures the topological state of the spatio-temporal system during the t th time interval. For each graph G t (where</figDesc><table><row><cell>3 r 1 r 3</cell><cell>2 1 2</cell><cell>2 r 2 r 4</cell><cell>1</cell><cell>t-1 r 4 r 1 r 2 r 3</cell><cell>t r 4 r 4 r 1 r 1 r 2 r 2 r 3 r 3</cell><cell>2 1 Outgoing Incoming 1 2 3 2 3 2 1 1 2 2 r 2 r 1 r 3 r 4 r 2 r 1 r 3 r 4</cell><cell>â€¦ 8 c h a n n e l s r 3 r 4 r 2 r 1</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell>(c)</cell><cell>(d)</cell></row><row><cell cols="7">Fig. 3. Converting a time-varying graph into a tensor.</cell><cell></cell></row><row><cell></cell><cell cols="7">Ï„ e is in the time interval t. The inflow and outflow</cell></row><row><cell></cell><cell cols="7">matrices at a certain time are shown in Figure 2.</cell></row><row><cell cols="8">Considering two types of flows (i.e. inflow and outflow),</cell></row><row><cell cols="8">a time-varying spatial map is conventionally represented</cell></row><row><cell cols="8">as a time-ordered sequence of tensors, with each tensor</cell></row><row><cell cols="8">corresponding to a snapshot of the map during a certain</cell></row><row><cell cols="8">time interval. In detail, each tensor consists of two matrices:</cell></row><row><cell cols="8">inflow matrix and outflow matrix, as shown in Figure 2.</cell></row><row><cell cols="8">Let V denote the set of all nodes in a ST-network under</cell></row><row><cell cols="8">study, and N A temporal graph consisting of T discrete non-overlapping |V | = I Ã— J be the number of nodes.</cell></row><row><cell cols="8">time intervals is represented by the time-ordered sequence</cell></row><row><cell cols="6">of directed graphs G</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Data statistics Taxi trip records are taken from the NYC from 2011 to 2014. Trip data includes: pick-up and drop-off dates/times, pick-up and drop-off locations. Among the data, the last four weeks are chosen as the test set, and the others as the training set.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>TaxiBJ</cell><cell>TaxiNYC</cell></row><row><cell></cell><cell># time intervals</cell><cell>35064</cell><cell>11472</cell></row><row><cell></cell><cell>Shape of Xt</cell><cell>16 Ã— 16</cell><cell>16 Ã— 16</cell></row><row><cell></cell><cell>Shape of Mt</cell><cell>512 Ã— 16 Ã— 16</cell><cell>512 Ã— 16 Ã— 16</cell></row><row><cell cols="2">4.1 Settings</cell><cell></cell></row><row><cell cols="2">4.1.1 Datasets</cell><cell></cell></row><row><cell cols="4">We use two different sets of data as shown in Table 3.</cell></row><row><cell cols="4">Each dataset contains two sub-datasets: trajectories/trips,</cell></row><row><cell cols="4">and external factors, detailed as follows.</cell></row><row><cell>â€¢</cell><cell cols="3">TaxiBJ: Trajectory data is the taxicab GPS data and meteorology data in Beijing from four time intervals:</cell></row><row><cell></cell><cell cols="3">1st Jul. 2013 -30th Oct. 2013, 1st Mar. 2014 -30th Jun.</cell></row><row><cell></cell><cell cols="3">2014, 1st Mar. 2015 -30th Jun. 2015, 1st Nov. 2015</cell></row><row><cell></cell><cell cols="3">-10th Apr. 2016. We choose data from the last four</cell></row><row><cell></cell><cell cols="3">weeks as the test set, and all data before that as the</cell></row><row><cell></cell><cell>training set.</cell><cell></cell></row></table><note><p><p>â€¢</p>TaxiNYC:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Datasets (holidays include adjacent weekends).</figDesc><table><row><cell>Dataset</cell><cell>TaxiBJ</cell><cell>TaxiNYC</cell></row><row><cell>Data type</cell><cell>Taxi GPS</cell><cell>Taxi Trip</cell></row><row><cell>Location</cell><cell>Beijing</cell><cell>New York</cell></row><row><cell></cell><cell>7/1/2013 -10/30/2013</cell><cell></cell></row><row><cell>Time Span</cell><cell cols="2">3/1/2014 -6/30/2014 1/1/2011 -3/1/2015 -6/30/2015 12/30/2014</cell></row><row><cell></cell><cell>11/1/2015 -4/10/2016</cell><cell></cell></row><row><cell>Time interval</cell><cell>1 hour</cell><cell>1 hour</cell></row><row><cell>Gird map size</cell><cell>(16, 16)</cell><cell>(16, 16)</cell></row><row><cell cols="2">Trajectory data</cell><cell></cell></row><row><cell>Average sampling rate (s)</cell><cell>âˆ¼ 60</cell><cell>\</cell></row><row><cell># taxis</cell><cell>34,000+</cell><cell>\</cell></row><row><cell># available time interval</cell><cell>11,472</cell><cell>35,064</cell></row><row><cell cols="3">External factors (holidays and meteorology)</cell></row><row><cell># holidays</cell><cell>106</cell><cell>451</cell></row><row><cell cols="2">Weather conditions 16 types (e.g., Sunny, Rainy) Temperature / â€¢ C [-24.6, 41.0]</cell><cell>\ \</cell></row><row><cell>Wind speed / mph</cell><cell>[0, 48.6]</cell><cell>\</cell></row><row><cell>4.1.2 Baselines</cell><cell></cell><cell></cell></row><row><cell cols="3">â€¢ HA: Historical Average model that uses the average of historical values in corresponding periods.</cell></row><row><cell cols="3">â€¢ ARIMA: Auto-Regressive Integrated Moving Average model.</cell></row><row><cell cols="2">â€¢ SARIMA: Seasonal ARIMA model.</cell><cell></cell></row></table><note><p><p><p><p>â€¢ VAR: Vector Auto-Regressive that can capture the pairwise relationships among all flows.</p>â€¢ RNN: Recurrent Neural Network</p><ref type="bibr" target="#b9">[10]</ref></p>. We selected previous L frames to predict the next frame. Hyperparameters: L is set as one of {3, 6, 12}, the hidden units is set as one of {32, 64}, learning rate set as one of {0.1, 0.01, 0.001, 0.0001}, .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Comparisons with baselines on TaxiBJ and TaxiNYC in node flow prediction.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RMSE</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">TaxiBJ</cell><cell cols="2">TaxiNYC</cell><cell cols="2">TaxiBJ</cell><cell cols="2">TaxiNYC</cell></row><row><cell></cell><cell cols="4">inflow outflow inflow outflow</cell><cell cols="4">inflow outflow inflow outflow</cell></row><row><cell>HA</cell><cell>21.23</cell><cell>22.49</cell><cell>417.49</cell><cell>401.33</cell><cell>13.49</cell><cell>13.98</cell><cell>85.51</cell><cell>86.68</cell></row><row><cell>ARIMA</cell><cell>10.83</cell><cell>11.41</cell><cell>108.83</cell><cell>100.42</cell><cell>7.03</cell><cell>7.28</cell><cell>25.14</cell><cell>26.46</cell></row><row><cell>SARIMA</cell><cell>11.00</cell><cell>11.14</cell><cell>96.07</cell><cell>85.95</cell><cell>6.98</cell><cell>7.12</cell><cell>21.70</cell><cell>23.09</cell></row><row><cell>VAR</cell><cell>10.05</cell><cell>10.38</cell><cell>104.29</cell><cell>93.84</cell><cell>6.74</cell><cell>6.86</cell><cell>24.19</cell><cell>22.53</cell></row><row><cell>RNN</cell><cell>8.68</cell><cell>8.48</cell><cell>118.61</cell><cell>108.06</cell><cell>5.39</cell><cell>5.24</cell><cell>29.37</cell><cell>30.24</cell></row><row><cell>LSTM</cell><cell>9.39</cell><cell>9.06</cell><cell>121.01</cell><cell>110.16</cell><cell>5.64</cell><cell>5.44</cell><cell>28.28</cell><cell>29.12</cell></row><row><cell>GRU</cell><cell>9.37</cell><cell>9.30</cell><cell>124.12</cell><cell>106.89</cell><cell>5.66</cell><cell>5.55</cell><cell>28.95</cell><cell>27.51</cell></row><row><cell>ST-ANN</cell><cell>8.71</cell><cell>8.59</cell><cell>73.50</cell><cell>68.20</cell><cell>5.46</cell><cell>5.45</cell><cell>19.69</cell><cell>20.26</cell></row><row><cell>ConvLSTM</cell><cell>8.95</cell><cell>8.55</cell><cell>66.57</cell><cell>55.70</cell><cell>5.73</cell><cell>5.47</cell><cell>18.56</cell><cell>19.91</cell></row><row><cell>ST-ResNet</cell><cell>8.21</cell><cell>7.89</cell><cell>69.00</cell><cell>55.50</cell><cell>5.18</cell><cell>5.15</cell><cell>19.28</cell><cell>18.28</cell></row><row><cell>MRF</cell><cell>7.35</cell><cell>7.08</cell><cell>87.86</cell><cell>76.98</cell><cell>4.57</cell><cell>4.50</cell><cell>18.30</cell><cell>18.35</cell></row><row><cell>MDL [ours]</cell><cell>7.71</cell><cell>7.15</cell><cell>53.68</cell><cell>47.44</cell><cell>4.95</cell><cell>4.75</cell><cell>13.98</cell><cell>14.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>Baselines</cell></row><row><cell>Model</cell><cell>Temporal Spatial External Transition</cell></row><row><cell>HA</cell><cell></cell></row><row><cell>ARIMA</cell><cell></cell></row><row><cell>SARIMA</cell><cell></cell></row><row><cell>VAR</cell><cell></cell></row><row><cell>RNN</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell></row><row><cell>GRU</cell><cell></cell></row><row><cell>ST-ANN</cell><cell></cell></row><row><cell>ConvLSTM</cell><cell></cell></row><row><cell>ST-ResNet</cell><cell></cell></row><row><cell>MRF</cell><cell></cell></row><row><cell>MDL [ours]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Transition prediction results. RMSE/ MAE for each method.</figDesc><table><row><cell>Model</cell><cell>TaxiBJ</cell><cell>TaxiNYC</cell></row><row><cell>HA</cell><cell>1.05/ 0.68</cell><cell>45.03/ 10.14</cell></row><row><cell>ARIMA</cell><cell>0.98/ 0.69</cell><cell>16.06/ 4.89</cell></row><row><cell>SARIMA</cell><cell>1.26/ 0.77</cell><cell>16.21/ 5.06</cell></row><row><cell>ST-ANN</cell><cell>0.92/ 0.63</cell><cell>12.87/ 4.18</cell></row><row><cell>ST-ResNet</cell><cell>0.72/ 0.37</cell><cell>14.75/ 4.82</cell></row><row><cell>MDL [ours]</cell><cell>0.65/ 0.32</cell><cell>9.89 / 3.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The same hyperparameter setting (e.g. number of training iterations) is used for all variants. We can observe that the CONCAT + GATING method outperforms other methods based on RMSE and MAE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>RMSE and MAE on the TaxiNYC test set using MDL with different types of fusions. -year data from TaxiNYC. Experiments are run on the same MDL model with l c = 3, l p = 1, l q = 1. Figure8presents the results. We can observe that more data always has better results on both node flow and edge flow prediction.4.4.2 Effect of Network DepthFigure9presents the effect of network depth on TaxiNYC (3-month data). As the network goes deeper (i.e. the number of convolutions increases), the RMSE of the model first decreases, demonstrating that the deeper network often has a better result because it can capture not only spatial near dependencies but also distant ones. However, the RMSE increases when the network becomes much deeper, showing that the training process becomes much more difficult.</figDesc><table><row><cell cols="2">Fusing type</cell><cell></cell><cell>RMSE/ MAE</cell><cell></cell></row><row><cell>Bridge</cell><cell>External</cell><cell>inflow</cell><cell>outflow</cell><cell>transition</cell></row><row><cell>CONCAT</cell><cell>GATING</cell><cell cols="2">53.68/ 13.98 47.44/ 14.63</cell><cell>9.89/ 3.48</cell></row><row><cell>CONCAT</cell><cell>SIMPLE</cell><cell cols="3">55.68/ 14.48 49.03/ 15.00 10.12/ 3.55</cell></row><row><cell>CONCAT</cell><cell>w/o</cell><cell cols="3">55.70/ 14.64 47.81/ 14.82 10.10/ 3.57</cell></row><row><cell>SUM</cell><cell>GATING</cell><cell cols="3">55.77/ 14.24 48.32/ 14.88 10.10/ 3.54</cell></row><row><cell>SUM</cell><cell>SIMPLE</cell><cell cols="3">55.81/ 14.50 49.53/ 15.17 10.29/ 3.62</cell></row><row><cell>SUM</cell><cell>w/o</cell><cell cols="3">54.85/ 14.14 49.32/ 15.12 10.11/ 3.57</cell></row><row><cell cols="4">4.4 Evaluation on Model Hyper-parameters</cell><cell></cell></row><row><cell cols="3">4.4.1 Effect of Training Data Size</cell><cell></cell><cell></cell></row><row><cell cols="5">To demonstrate the effectiveness of training data size for</cell></row><row><cell cols="5">deep learning model, here we select 3-month, 6-month, 1-</cell></row><row><cell cols="4">year and 34.4.3 Effect of multi-task component</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 and</head><label>8</label><figDesc>Figure 10  demonstrate the influence our multitask component on the final experiments performance. From the table and figure, we can find that transition flow prediction task can be improved in most cases, and when the Î» node = Î» edge = 1 and Î» mdl =0.1, our multi-task model achieves best performance against others, under this circumstance, both tasks get better results compared with two single tasks, which proves the effectiveness and reliability of the fact that our multi-task part can mutually promote the performance of each task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">single-task vs multi-task</cell><cell></cell></row><row><cell cols="3">Hyper-Parameters</cell><cell></cell><cell>RMSE / MAE</cell><cell></cell></row><row><cell>Î» node</cell><cell>Î» edge</cell><cell>Î» mdl</cell><cell>inflow</cell><cell>outflow</cell><cell>transition</cell></row><row><cell>0</cell><cell>1</cell><cell>0</cell><cell>/</cell><cell>/</cell><cell>10.53/3.63</cell></row><row><cell>1</cell><cell>0</cell><cell>0</cell><cell cols="3">56.66/14.60 51.30/15.34 10.16/3.55</cell></row><row><cell>1</cell><cell>1</cell><cell>0.1</cell><cell>53.68/13.98</cell><cell>47.44/14.63</cell><cell>9.89/3.48</cell></row></table><note><p><p><p><p><p><p>4.5 Flow Predictions</p>Figure</p>11</p>depicts two nodes' predictive results of our MDL over the next one hour against the ground truth in New York City (NYC) in the last 4 weeks of 2014. In detail, Node (10, 1) always have higher flow than Node</p><ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b2">3)</ref></p>. We can observe that our model is very accurate in tracing the ground truth</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The learnable parameters are initialized using a uniform distribution with the default parameter in Keras<ref type="bibr" target="#b6">[7]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>2. The smaller the better for RMSE and MAE.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported by the National Natural Science Foundation of China (Grant No. 61672399, No. U1609217 and No. U1401258), and the China National Basic Research Program (973 Program, No. 2015CB352400).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Traffic flow prediction for road transportation networks with limited traffic data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rajabioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Ioannou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="653" to="662" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictions of freeway traffic speeds and volumes using vector autoregressive models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Deek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road traffic congestion monitoring in social media with hinge-loss markov random fields</title>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Â¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1179.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent space model for road networks to predict time-varying traffic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Citymomentum: an online approach for crowd behavior prediction at a citywide level</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="559" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<idno type="DOI">10.1007/978-3-319-46493-038</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46493-038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FCCF: forecasting citywide crowd flows based on big data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatiotemporal periodical pattern mining in traffic data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505821.2505837</idno>
		<ptr target="http://doi.acm.org/10.1145/2505821.2505837" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd ACM SIGKDD International Workshop on Urban Computing, ser. UrbComp &apos;13</title>
		<meeting>the 2Nd ACM SIGKDD International Workshop on Urban Computing, ser. UrbComp &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Traffic prediction in a bike-sharing system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Next place predictions based on user mobility traces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Porta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)</title>
		<imprint>
			<date type="published" when="2015-04">April 2015</date>
			<biblScope unit="page" from="93" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting traffic volumes and estimating the effects of shocks in massive transportation systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5643" to="5648" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparison of parametric and nonparametric models for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="321" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prediction of human emergency behavior and their mobility following largescale disaster</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Accurate and interpretable bayesian mars for traffic flow prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2457" to="2469" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pred: Periodic region detection for mobility modeling of social media users</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2017 -Proceedings of the 10th ACM International Conference on Web Search and Data Mining</title>
		<imprint>
			<publisher>Association for Computing Machinery, Inc</publisher>
			<date type="published" when="2017-02">2 2017</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regions, periods, activities: Uncovering urban dynamics via cross-modal representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052601</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052601" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, ser. WWW &apos;17. Republic and Canton of</title>
		<meeting>the 26th International Conference on World Wide Web, ser. WWW &apos;17. Republic and Canton of<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14501" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Urban computing: concepts, methodologies, and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
