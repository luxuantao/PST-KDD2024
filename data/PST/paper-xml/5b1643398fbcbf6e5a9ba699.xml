<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Deep Convolutional Neural Network Architectures for Object Classification and Detection within X-ray Baggage Security Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samet</forename><surname>Akcay</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mikolaj</forename><forename type="middle">E</forename><surname>Kundegorski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
						</author>
						<title level="a" type="main">Using Deep Convolutional Neural Network Architectures for Object Classification and Detection within X-ray Baggage Security Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E20BF0D621DF9802BCF47C4BC64CA522</idno>
					<idno type="DOI">10.1109/TIFS.2018.2812196</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2018.2812196, IEEE Transactions on Information Forensics and Security 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep convolutional neural networks</term>
					<term>transfer learning</term>
					<term>image classification</term>
					<term>detection</term>
					<term>X-ray baggage security</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the use of deep Convolutional Neural Networks (CNN) with transfer learning for the image classification and detection problems posed within the context of X-ray baggage security imagery. The use of the CNN approach requires large amounts of data to facilitate a complex end-to-end feature extraction and classification process. Within the context of Xray security screening, limited availability of object of interest data examples can thus pose a problem. To overcome this issue, we employ a transfer learning paradigm such that a pre-trained CNN, primarily trained for generalized image classification tasks where sufficient training data exists, can be optimized explicitly as a later secondary process towards this application domain.</p><p>To provide a consistent feature-space comparison between this approach and traditional feature space representations, we also train Support Vector Machine (SVM) classifier on CNN features. We empirically show that fine-tuned CNN features yield superior performance to conventional hand-crafted features on object classification tasks within this context. Overall we achieve 0.994 accuracy based on AlexNet features trained with Support Vector Machine (SVM) classifier. In addition to classification, we also explore the applicability of multiple CNN driven detection paradigms such as sliding window based CNN (SW-CNN), Faster RCNN (F-RCNN), Region-based Fully Convolutional Networks (R-FCN) and YOLOv2. We train numerous networks tackling both single and multiple detections over SW-CNN/F-RCNN/R-FCN/YOLOv2 variants. YOLOv2, Faster-RCNN, and R-FCN provide superior results to the more traditional SW-CNN approaches. With the use of YOLOv2, using input images of size 544×544, we achieve 0.885 mean average precision (mAP) for a six-class object detection problem. The same approach with an input of size 416×416 yields 0.974 mAP for the two-class firearm detection problem and requires approximately 100ms per image. Overall we illustrate the comparative performance of these techniques and show that object localization strategies cope well with cluttered X-ray security imagery where classification techniques fail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>X RAY baggage security screening is widely used to main- tain aviation and transport security and poses a significant image-based screening task for human operators reviewing compact, cluttered and highly varying baggage contents within limited time-scales. The increased passenger throughput, in the global travel network, and the increased focus on broader aspects of extended border security (e.g., freight, shipping, S.Akcay * , C.G.Willcocks, T.P.Breckon are with Department of Computer Science, Durham University, Durham, UK (e-mail: { samet.akcay, christopher.g.willcocks, toby.breckon }@durham.ac.uk).</p><p>M.E.Kundegorski was with Department of Computer Science, Durham University, Durham, UK.</p><p>Asterisk indicates corresponding author. Previous work within this context is primarily based on the bag of visual words model (BoVW) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> although there is some limited research using other techniques such as sparse representations <ref type="bibr" target="#b5">[6]</ref>. Convolutional neural networks (CNN), a state-of-the-art paradigm for contemporary computer vision problems, were introduced into the field of X-ray baggage imagery by <ref type="bibr" target="#b6">[7]</ref>, comparing CNN to a BoVW approach with conventional hand-crafted features trained with a Support Vector Machine (SVM) classifier. Following the work of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> also studies X-ray baggage object classification with CNN similarly comparing it against traditional classifiers.</p><p>Motivated by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we conduct an extensive set of experiments to evaluate the strength of CNN features and traditional hand-crafted features (SIFT, SURF, FAST, KAZE <ref type="bibr" target="#b3">[4]</ref>).</p><p>As with <ref type="bibr" target="#b6">[7]</ref>, we perform layer freezing by fixing parameters from the source domain without any further optimization to observe how fixing the layer parameters at varying points in the network influences the overall performance of the transfer learning based tuning of the end-to-end CNN. Furthermore, in contrast to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> comparing end-to-end CNN classification with traditional feature-driven pipelines, we additionally present results whereby we extract the output of the last layer of a given CNN (f c 7 of AlexNet <ref type="bibr" target="#b8">[9]</ref>) as a feature map itself. We subsequently train an SVM classifier, generally used as the final classification stage of feature-driven approaches <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, to provide a consistent feature-space comparison between both learned (CNN) and traditional feature representations.</p><p>In addition to the proposed classification scheme, we explore object detection within this problem domain by investigating both the use of a sliding window paradigm (akin to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>) and evaluate contemporary approaches to learn efficient object localization via R-CNN <ref type="bibr" target="#b10">[11]</ref>, R-FCN <ref type="bibr" target="#b11">[12]</ref> and YOLOv2 <ref type="bibr" target="#b12">[13]</ref> approaches. As shown in previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> the challenging and cluttered nature of object detection in X-ray security imagery often poses additional challenges for established contemporary classification and detection approaches, such as RCNN/R-FCN <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>The main contributions of this paper are: (a) the exhaustive evaluation of classification architectures of <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> against prior work in the field from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, (b) the feature-space comparison of the end-to-end CNN classification results of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> against the final stage SVM classification on the extracted CNN features, (c) the comparison of the region based object detection/localization strategies of <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> against the prior strategies proposed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Contrasting performance results are obtained against the prior published studies of <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> over a comprehensive dataset of 11, 627 examples making this the largest combined Xray object detection and classification study in the literature to date. Moreover, the evaluation is strengthened further by using UK government evaluation dataset <ref type="bibr" target="#b18">[19]</ref> (available upon request from UK Home Office Centre for Applied Science and Technology (CAST)). Overall, we identify classification approaches and detection strategies that outperform the prior work of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Aviation security screening systems are of interest and have been studied for decades <ref type="bibr" target="#b19">[20]</ref>. Computer Aided Screening (CAS) performs automated threat detection in the generalized sense, however this largely remains an unsolved problem. Previous work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> has focused on image enhancement <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>, segmentation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref> or detection <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> tasks in order to further investigate the real time applicability of CAS to automatize aviation security screening. For a detailed overview the reader is directed to Rodgers et al. <ref type="bibr" target="#b21">[22]</ref> and Mouton et al. <ref type="bibr" target="#b20">[21]</ref>. Our focus is based on addressing the object classification and detection tasks presented in the following sections. Classification: For the classification of X-ray objects, the majority of prior work proposes traditional machine learning approaches based on a Bag-of-Visual-Words (BoVW) feature representation scheme, using hand-crafted features together with a classifier such as a Support Vector Machine (SVM) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>The work of <ref type="bibr" target="#b0">[1]</ref> considers the concept of BoVW within X-ray baggage imagery using SVM classification with several feature representations (DoG, DoG+SIFT, DoG+Harris) achieving a performance of 0.7 recall, 0.29 precision, and 0.57 average precision. Turcsany et al. <ref type="bibr" target="#b1">[2]</ref> followed a similar approach and extended the work presented in <ref type="bibr" target="#b0">[1]</ref>. Using a BoVW with SURF descriptors and an SVM classifier, together with a modified version of codebook generation, yields 0.99 true positive and 0.04 false positive rates <ref type="bibr" target="#b1">[2]</ref>. BoVW approaches with feature descriptor and SVM classification are also used in <ref type="bibr" target="#b2">[3]</ref> for the classification of single and dual-view X-ray images, with optimal average precisions achieved for firearms (0.95) and laptops (0.98). Mery et al. <ref type="bibr" target="#b16">[17]</ref> propose a recognition approach that applies detection to single-view images to find objects of interest, and then matches these across multiple view X-ray images yielding 0.96 precision and 0.93 recall for 120 objects. A BoVW approach is further employed in <ref type="bibr" target="#b5">[6]</ref> where a dictionary is formed for each class that consists of feature descriptors of randomly cropped image patches. Performance of the model is evaluated by fitting a sparse representation classification to the extracted feature descriptors of randomly cropped test patches, and adaptive dictionaries are obtained from the training stage. The experimental procedure demonstrates promising results for classification of the patches.</p><p>Kundegorski et al. <ref type="bibr" target="#b3">[4]</ref> exhaustively explore the use of various feature point descriptors as visual word variants within a BoVW model. This is for image classification based threat detection within baggage security X-ray imagery, using a FAST-SURF feature detector and descriptor combination giving a maximal performance with an SVM classification (2 class firearm detection: 94.0% accuracy).</p><p>The study of <ref type="bibr" target="#b6">[7]</ref> compares a BoVW approach and a CNN approach, exploring the use of transfer learning by finetuning weights of different layers transferred from another network trained on a different task. Experiments show that the CNN outperforms the BoVW method, even when features are abstractly transferred from another classification problem. Following the earlier work of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> exhaustively explores the use of varying classification approaches within the Xray baggage domain using ten different techniques, including BoVW, sparse representations, and CNN. Experiments show parallel results with <ref type="bibr" target="#b6">[7]</ref>, supporting the generalized superiority of CNN features but without any further consideration of the initial object detection (localization) problem, or exhaustive exploration of CNN performance in the broader sense. Detection: Object classification is a significant task for the identification (semantic labeling) of particular objects against others, i.e., being a threat or non-threat. However, a vital remaining task within this problem domain is that of detection in which objects of interest are localized within the overall Xray image, commonly denoted with a bounding box or shape outline. Since detection is a challenging problem, detection based models within X-ray baggage imagery are significantly more limited in the literature.</p><p>In <ref type="bibr" target="#b27">[28]</ref>, detection of regions of interest (ROI) within X-ray images is performed via a geometric model of the object, by estimating structure from motion. Potential regions obtained from segmentation step are then tracked based on their similarity, achieving 0.943 true positive and 0.056 false positive rates on a small, uncluttered dataset.</p><p>Franzel et al. <ref type="bibr" target="#b9">[10]</ref> propose a sliding window detection approach with the use of a linear SVM classifier and histogram of oriented gradients (HOG) <ref type="bibr" target="#b30">[31]</ref>. As HOG is not fully rotationally invariant, they supplement their approach by detection of varying orientations. As a next step, called multi-view integration, detections from single view X-ray images, taken from multiple viewpoints in a modern X-ray scanner machine, are fused to avoid false detections and find the intersection of the true detections. Multi-view detection is shown to provide superior detection performance to single-view detection for handguns (mAP: 0.645). Similarly, <ref type="bibr" target="#b4">[5]</ref> explores object detection in X-ray baggage imagery by evaluating various handcrafted feature detector and descriptor combinations with the use of a branch and bound algorithm and structural SVM classifier (mAP: 0.881 for 6400 images of handguns, laptops  <ref type="bibr" target="#b29">[30]</ref>) of VGG16 <ref type="bibr" target="#b14">[15]</ref> trained on X-ray data. The first column of each convolution box demonstrates grayscale Grad-CAM, while the second column is Grad-CAM heatmap on an input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and glass bottles).</head><p>A related body of work also targets the use of BoVW techniques within the highly related task object detection within 3D computed tomography (CT) baggage security imagery <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. An extensive review is presented in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>By contrast to the predominance of BoVW techniques <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and the limited evaluation of recent developments from the CNN literature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> within this problem domain, we explicitly evaluate multiple CNN classification architectures <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> across multiple contemporary detection (object localization) paradigms. Uniquely, we consider a side-by-side comparison of multiple CNN variants and detection paradigms against traditional BoVW for reference across varied and challenging X-ray security images datasets, which are highly representative of operational conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLASSIFICATION</head><p>Automated threat screening task in X-ray baggage imagery can be considered as a classical image classification problem. Here we address this task using convolutional neural networks and transfer learning approaches based on the prior work of <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>, and expanding the earlier preliminary studies of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. To these ends, we initially outline a brief generalized background for convolutional neural networks and transfer learning, and explain our approach to applying these techniques to object classification within X-ray baggage images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Neural Networks</head><p>Deep convolutional neural networks have been widely used in many challenging computer vision tasks such as image classification <ref type="bibr" target="#b15">[16]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> and segmentation <ref type="bibr" target="#b37">[38]</ref> Krizhevsky et.al. (AlexNet) <ref type="bibr" target="#b8">[9]</ref> propose a network (ie., similar to <ref type="bibr" target="#b38">[39]</ref> but deeper and wider, having 5 conv layers with 11 × 11 receptive filters and 3 f c layers, and 60 million parameters in total). This high-level of parametrization, and hence representational capacity, make the network susceptible to over-fitting in the traditional machine learning sense. The use of dropout, whereby hidden neurons are randomly removed during the training process, is introduced to avoid over-fitting such that performance dependence on individual network elements is reduced in favor of cumulative error reduction and representational responsibility for the problem space. In addition to dropout which increases the robustness of the networks to over-fitting, ReLu <ref type="bibr" target="#b8">[9]</ref> is another novel approach in this work introduced as an activation function for non-linearity. By following the success of this work, Zeiler and Fergus <ref type="bibr" target="#b39">[40]</ref> design a similar architecture with smaller receptive fields (ZFNet). Furthermore, the work also introduces a new approach for the visualization of feature representations within networks <ref type="bibr" target="#b39">[40]</ref>.</p><p>Inspired by the favorable outcome of <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b39">[40]</ref>, network width is thoroughly explored in <ref type="bibr" target="#b13">[14]</ref> via the comparison of three networks with varying width. By following this, Simonyan and Zisserman (VGG) <ref type="bibr" target="#b14">[15]</ref> study the importance of network depth on classification accuracy by stacking convolutional layers with small 3 × 3 receptive fields with a stride of 1. Not only does the use of small receptive filters increase non-linearity but also decrease the total number of parameters of the network. It is empirically shown that stack of 3 × 3 convolutional filters within a network with varying depth between 11 to 19 layers can significantly improve the state-of-the-art.</p><p>He et al. (ResNet) <ref type="bibr" target="#b15">[16]</ref> propose a simple yet powerful network by following the work in <ref type="bibr" target="#b40">[41]</ref>. Input is first fed into two stacked conv layers, then is added to the output of the conv layers before non-linearity is applied. This approach is used up to 34 layers. For deeper networks such as 50, 101, 152, filter factorization is employed such that conv layers are stacked using 1 × 1, 3 × 3 and 1 × 1 filters (bottleneck layer). The proposed approach significantly reduces the number of parameters needed for a deep network and outperforms the previous state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer Learning</head><p>Modern CNN architectures such as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b36">[37]</ref> are trained on huge datasets such as ImageNet <ref type="bibr" target="#b41">[42]</ref> which contains approximately a million of data samples and 1000 distinct class labels. However, the limited applicability of such training and parameter optimization techniques to problems where such large datasets are not available gives rise to the concept of transfer learning <ref type="bibr" target="#b34">[35]</ref>. The work of <ref type="bibr" target="#b35">[36]</ref> illustrated that each hidden layer in a CNN has distinct feature representation related characteristics among of which the lower layers provide general feature extraction capabilities (akin to Gabor filters and alike), while higher layers carry information that is increasingly more specific to the original classification task. Figure <ref type="figure" target="#fig_1">2</ref>, for instance, demonstrates Gradient-based class activation map (Grad-CAM <ref type="bibr" target="#b29">[30]</ref>) of VGG16 <ref type="bibr" target="#b14">[15]</ref> for an example Xray classification object. Lower layers -i.e. conv 1-2 and conv 2-2 , behave as edge detectors, while higher layers like conv 4-3 and conv 5-3 provides more specific representations belonging to the input image. This finding facilitates the verbatim re-use of the generalized feature extraction and representation of the lower layers in a CNN, while higher layers are fine-tuned towards secondary problem domains with related characteristics to the original. Using this paradigm, as demonstrated in Figure <ref type="figure" target="#fig_2">3</ref>, we can leverage the a priori CNN parametrization of an existing fully trained network on a generic 1000+ object class problem <ref type="bibr" target="#b41">[42]</ref> (Figure <ref type="figure" target="#fig_2">3A</ref>), as a starting point for optimization towards to the specific problem domain of limited object class detection within X-ray images (Figure <ref type="figure" target="#fig_2">3B</ref>). Instead of designing a new CNN with random weight initialization, we instead adopt a pre-trained CNN, preoptimized for generalized object recognition, and fine-tune its weights towards our specific classification domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application to X-ray Security Imagery</head><p>To investigate the applicability of convolutional neural networks in object classification in X-ray baggage imagery, we address two specific target problems:-a) binary classification problem that performs firearm detection (i.e., gun vs. nogun) akin to that of the prior work of <ref type="bibr" target="#b3">[4]</ref> to compare CNN features to conventional hand crafted attributes; b) a multiclass X-ray object classification problem (6 classes: firearm, firearm-components, knives, ceramic knives, camera and laptop), which further investigates the performance of CNN for the classification of multiple X-ray objects. The following subsection describes the datasets we use in our experiments.</p><p>1) Datasets: To perform classification tasks, we use four types of datasets described below: Dbp 2 : Our data-set (11, 627 X-ray images) are constructed using single conventional X-ray imagery with associated false color materials mapping from dual-energy <ref type="bibr" target="#b20">[21]</ref>. To generate a dataset for firearm detection, we manually crop baggage objects, and label each accordingly (e.g., Figure <ref type="figure" target="#fig_3">4</ref> ) -on the assumption an in-service detection solution would perform scanning window search through the whole baggage image.</p><p>In addition to manual cropping, we also generate a set of negative images by randomly selecting 256 × 256 fixed-sized overlapping image patches from a large corpus of baggage images that do not contain any target objects. Following these approaches, our evaluation datasets consist of In addition to these datasets, we also use UK government evaluation dataset <ref type="bibr" target="#b18">[19]</ref>, which is available upon request from UK Home Office Centre for Applied Science and Technology (CAST). This dataset comprises of both expertly concealed firearm (threat) items and operational benign (non-threat) imagery from commercial X-ray security screening operations on the UK (baggage/parcels). From this dataset, we define two evaluation problems based on the provided annotation for the presence of firearms threat items. Full Firearm vs. Operational Benign -(FFOB): comprising 4, 680 firearm threat and 5,000 non-threat images, and is denoted as FFOB. Firearm Parts vs. Operational Benign -(FPOB): contains 8, 770 firearm and parts threat and 5, 000 non-threat images (denoted FPOB, comprising of annotations as any of {bolt carrier assembly, Pump action, Set, Shotgun, Sub-Machine-Gun}).</p><p>We split the datasets into training (60%), validation (20%) and test sets (20%) such that each split has similar class distribution but unseen test set contains somewhat challenging samples never trained before. Besides, we also weight the data when sampling to cope with class imbalances. We also perform random flipping, random cropping, and rotation to each sample to augment the datasets.</p><p>2) Classification: Using transfer learning paradigm explained in Section III-B, this work leverages the a priori CNN parametrization of an existing fully trained network, on a generic 1000 object class problem <ref type="bibr" target="#b41">[42]</ref>, as a starting point for optimization towards another problem domain of limited object class detection within X-ray images.</p><p>For the binary classification problem, we specifically make use of the CNN configuration designed by Krizhevsky et al. <ref type="bibr" target="#b8">[9]</ref>, having 5 convolutional layers (conv), 3 fully-connected layers (f c), and trained on the ImageNet dataset on a 1000 class image classification problem, denoted as AlexNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>The first step is to fine tune all of the conv and f c layers of the network via transfer learning on the training set of the target classification problem. In addition to this, we also perform layer freezing, meaning that instead of updating layer parameters for our task, we use the original unmodified weights from the initial trained CNN parametrization of <ref type="bibr" target="#b8">[9]</ref>. This allows us to observe how fine-tuning each layer impacts the overall performance.</p><p>Also, having fine-tuned the parameters via this transfer learning approach, we extract the features of the last fully connected layer (f c 7 ) to train on an SVM classifier. This allows us to additionally compare the internal feature space representation of the CNN model to alternative more traditional (handcrafted) BoVW features as used in prior work <ref type="bibr" target="#b3">[4]</ref>.</p><p>Evaluation of our proposed approach is performed against the prior SVM-driven work of Kundegorski et. al. <ref type="bibr" target="#b3">[4]</ref> within a BoVW framework. SVM are trained using Radial Basis Function (RBF) kernel {SV M RBF } with a grid search over kernel parameter, γ = 2 x : x ∈ {-15, 3}, and model fitting cost, c = 2 x : x ∈ {5, 15}, using k-fold cross validation (k = 5) with F-score optimization (being more representative then accuracy for unbalanced datasets). The results for the best performing parameter set are reported for each feature configuration.</p><p>The second set of experiments is the classification of multiple baggage objects, a more complex six class object problem. Here the lesser performing SVM with handcrafted features are not considered (Table <ref type="table" target="#tab_3">I</ref>), in favor of the CNN approach. Instead, we fine-tune AlexNet <ref type="bibr" target="#b8">[9]</ref>, VGG <ref type="bibr" target="#b14">[15]</ref> and ResNet <ref type="bibr" target="#b15">[16]</ref>, each of which are top performing entries of ImageNet <ref type="bibr" target="#b41">[42]</ref> competition. By doing so, we aim to evaluate the feasibility of CNN for this problem domain further.</p><p>To update the parameters of all the networks during training, we use cross-entropy for the loss function, and utilize Adam <ref type="bibr" target="#b43">[44]</ref> optimizer with a learning rate of 10 -3 , and a weight decay of 0.005 since we observe that it achieves superior accuracy to SGD and RMS for this task. Our stopping criterion is to terminate optimization where validation starts to reduce, while training accuracy continues to improve. This fork between training and validation performance usually takes 30 epochs for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>The performance is evaluated by the comparison of True Positive Rate (TP) (%), False Positive Rate (FP) (%) together with Precision (P), Accuracy (A) and F-score (F) (harmonic mean of precision and true positive rate).</p><p>Results for the two class problem is given in Table <ref type="table" target="#tab_3">I</ref>, which is divided into four sections: -first section lists the performance of the CNN approach, notated as AlexN et a b , meaning that the network is fine-tuned from layer a to layer b, while the rest of the layers are frozen (Table <ref type="table" target="#tab_3">I</ref>, top). This means, for instance, AlexN et 4-8 is trained by fine-tuning the layers {4, 5, 6, 7, 8} and freezing the layers {1, 2, 3} (i.e. remain unchanged from the pre-trained model of <ref type="bibr" target="#b8">[9]</ref>). The second section has the results of an SVM classifier trained on the output of the last layer of CNN (Table <ref type="table" target="#tab_3">I</ref>, middle upper). Similar to the first section, we again perform layer freezing here for a consistent comparison of CNN features and BoVW features. The third section shows fine tuning results based on contemporary end to end CNN architectures (VGG M <ref type="bibr" target="#b13">[14]</ref>, VGG 16 <ref type="bibr" target="#b14">[15]</ref>, ResNet 18 <ref type="bibr" target="#b15">[16]</ref>, ResNet 50 <ref type="bibr" target="#b15">[16]</ref>, ResNet 101 <ref type="bibr" target="#b15">[16]</ref>, Table <ref type="table" target="#tab_3">I</ref>, middle lower). The last section lists the best performing BoVW feature detector/descriptor variants trained with SVM in the work of <ref type="bibr" target="#b3">[4]</ref> (Table <ref type="table" target="#tab_3">I</ref>, bottom).</p><p>Table <ref type="table" target="#tab_3">I</ref> shows the performance results of firearm detection. We see that true and false positives have a general trend to decrease as the number of fine-tuned layers reduces. Likewise, freezing lower layers reduces the accuracy of the models.</p><p>Training an SVM classifier on CNN features with layer freezing yields relatively better performance than the standard end to end CNN results. Here, We see a performance pattern such that fine-tuning more layers has a positive impact on the overall performance. For instance, SVM trained on fully finetuned CNN has the highest performance on all of the metrics, outperforming the prior work of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref> (Table <ref type="table" target="#tab_3">I</ref>).</p><p>For an end to end fine-tuning using contemporary architectures, we observe the direct proportion of performance and network complexity. ResNet 101 <ref type="bibr" target="#b15">[16]</ref>, for instance, is the best performing network among all of the end to end CNN networks (TableI).</p><p>It is also significant to note that the performance of the best feature detector/descriptor combination of BoVW approach (FAST/SURF <ref type="bibr" target="#b3">[4]</ref>) is worse than any of the CNN features   <ref type="bibr" target="#b14">[15]</ref> 99.08 1.14 0.997 0.990 0.985 ResNet 18 <ref type="bibr" target="#b15">[16]</ref> 99.38 1.43 0.996 0.992 0.988 ResNet 50 <ref type="bibr" target="#b15">[16]</ref> 99.54 1.00 0.998 0.995 0.992 ResNet 101 <ref type="bibr" target="#b15">[16]</ref>   given in Table <ref type="table" target="#tab_3">I</ref>. Further comparison of BoVW+SVM against CNN+SVM proves the superiority of CNN features to traditional handcrafted features (Table <ref type="table" target="#tab_3">I</ref>).</p><p>Table <ref type="table" target="#tab_6">II</ref> shows the overall performance of the networks fine-tuned for multiple class problem. Like Table <ref type="table" target="#tab_3">I</ref>, finetuning the entire network yields the best performance. A conclusion can be reached from these results that fine-tuning higher level layers and freezing lower ones have a detrimental impact on the performance of the CNN model. Similar to Table <ref type="table" target="#tab_3">I</ref>, performance and network complexity are also directly proportional. With relatively lower complexity than the rest, AlexNet <ref type="bibr" target="#b8">[9]</ref> has the lowest accuracy of 0.924. ResNet 101 <ref type="bibr" target="#b15">[16]</ref>, on the other hand, achieves the highest on all metrics (P=96.0% R=96.6%  <ref type="bibr" target="#b14">[15]</ref> 0.931 0.943 0.940 0.936 ResNet 18 <ref type="bibr" target="#b15">[16]</ref> 0.933 0.943 0.936 0.937 ResNet 50 <ref type="bibr" target="#b15">[16]</ref> 0.934 0.910 0.923 0.917 ResNet 101 <ref type="bibr" target="#b15">[16]</ref>    <ref type="bibr" target="#b14">[15]</ref> 99.010 0.000 1.000 0.995 0.995 VGG 16 <ref type="bibr" target="#b14">[15]</ref> 99.831 0.000 1.000 0.999 0.999 ResNet 18 <ref type="bibr" target="#b15">[16]</ref> 99.472 0.000 1.000 0.997 0.997 ResNet 50 <ref type="bibr" target="#b15">[16]</ref> 100.00 0.923 0.990 0.995 0.995 ResNet 101 <ref type="bibr" target="#b15">[16]</ref> 100.00 0.311 0.996 0.998 0.998 A=97.5% F=96.1%).</p><p>In addition, results are presented on the UK government evaluation dataset <ref type="bibr" target="#b18">[19]</ref> in Tables <ref type="table" target="#tab_7">III</ref> and<ref type="table" target="#tab_11">IV</ref> . Within Table <ref type="table" target="#tab_7">III</ref> and IV we present results for classification only (following the approach of Section III-B), where we can see comparable performance to the earlier results presented in Tables <ref type="table" target="#tab_3">I</ref> and<ref type="table" target="#tab_6">II</ref>  <ref type="bibr" target="#b14">[15]</ref> 95.864 0.919 0.990 0.974 0.974 VGG 16 <ref type="bibr" target="#b14">[15]</ref> 97.238 4.217 0.954 0.965 0.964 ResNet 18 <ref type="bibr" target="#b15">[16]</ref> 95.725 0.744 0.992 0.975 0.974 ResNet 50 <ref type="bibr" target="#b15">[16]</ref> 99.411 1.060 0.988 0.991 0.991 ResNet 101 <ref type="bibr" target="#b15">[16]</ref> 99.608 0.000 1.000 0.998 0.998  Fig. <ref type="figure">6</ref>: Confusion matrices for AlexNet <ref type="bibr" target="#b8">[9]</ref>, VGG16 <ref type="bibr" target="#b14">[15]</ref> ResNet-50 <ref type="bibr" target="#b15">[16]</ref> fine tuned for multi class problem Figure <ref type="figure" target="#fig_4">5</ref> depicts the t-SNE <ref type="bibr" target="#b42">[43]</ref> visualization of feature maps of the down-projected internal feature space representation extracted from VGG 16 <ref type="bibr" target="#b14">[15]</ref> fine-tuned for binary (A) and multi-class (B) problems. In both cases, classes are well separated, showing the capability of CNN features within this problem domain (Figure <ref type="figure" target="#fig_4">5</ref>). Figure <ref type="figure">6</ref> depicts per-class accuracy obtained via the use of AlexNet <ref type="bibr" target="#b8">[9]</ref> and ResNet 101 <ref type="bibr" target="#b15">[16]</ref>, the worst and best performing networks within this task. We see that laptop and camera object classes are straightforward to classify. In contrast, networks have relatively lower classification confidence for knife, ceramic knife vs. firearm, firearm parts, which obviously stems from the similarity of the objects.</p><p>Limitations: Due to the cluttered nature of the input dataset, there are certain cases where CNN based classification fails to classify threats. Figure <ref type="figure" target="#fig_6">7</ref>, for instance, demonstrates that CNN labels these image examples as laptops with high confidence, as the predominant object signature present in the image patch, while failing to detect the foreground objects of interest (yellow highlights, Figure <ref type="figure" target="#fig_6">7</ref>). This results in a significant increase in false negative occurrences (Table <ref type="table" target="#tab_6">II</ref>). We consider this primarily as an object detection problem, and hence explore the contemporary object detection strategies in the subsequent part of this study. IV. OBJECT DETECTION From Section III, the approach of CNN based classification via transfer learning yields promising performance especially for single and non-occluded X-ray image patches. When it comes to classifying multiple objects (Figure <ref type="figure" target="#fig_6">7</ref>), however, more sophisticated approaches are needed to perform joint localization. Here we give a brief introduction to CNN based object detection algorithms for an exhaustive evaluation within X-ray baggage domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>Sermanet et al. (OverFeat) <ref type="bibr" target="#b17">[18]</ref> uses a sliding window approach to generate the region proposals, which is then fed into a convolutional neural network for the classification. The key idea here is that bounding box regression is performed with an extra regression layer which shares the weights with the main network. Subsequent work <ref type="bibr" target="#b44">[45]</ref> proposes a detection algorithm (RCNN), based on three main stages: region proposal generation, feature extraction, and classification. The first stage employs an external region proposal generator, followed by a fine-tuned CNN in the next stage for feature extraction. The final stage performs classification with an SVM classifier. Even though it outperforms previous work by a large margin, this model is not considered to be real-time applicable due to runtime and memory issues. In contrast, SPPNet <ref type="bibr" target="#b45">[46]</ref> contains variable-sized spatial pooling layer between the convolutional and fully connected layers, which allows the to handle images of arbitrary scales and aspect ratios. With this design, image representations can be computed once in SPPNet, which makes the network significantly faster than RCNN. Like RCNN, however, the network has several separate stages, which is computationally expensive. Fast RCNN by Girshick <ref type="bibr" target="#b46">[47]</ref> combines feature extraction, classification and bounding box regression stages by designing a partially end to end CNN network, significantly outperforming <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> regarding speed and accuracy. The novelty of the work is to employ a region of interest pooling layer (RoI) before fully connected layers (f c) to fix the size of the region proposals generated by the region proposal algorithm. These fixed sized object localization proposals are then classified via f c layers. In addition to the classification, bounding box regression is also performed via a multi-task loss function to localize objects of interests with a bounding rectangle. The limitation, however, is that the network still needs an external region proposal algorithm such as selective search <ref type="bibr" target="#b47">[48]</ref>. Inspired by the strong and weak points of <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>, Ren et al. <ref type="bibr" target="#b10">[11]</ref> propose a model, named Faster RCNN (F-RCNN) performing all the aforementioned stages in an end to end deep neural network. This approach not only reduces time complexity and required memory but also significantly boosts overall performance. Further optimization of this concept by <ref type="bibr" target="#b11">[12]</ref> proposes a fully convolutional detection framework (R-FCN), which yields faster training and testing performance with competitive accuracy compared to F-RCNN <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this work, we adapt F-RCNN, R-FCN and YOLOv2 each of which provide a significant boost in accuracy, for use within an X-ray baggage object detection context, and compare with previous object detection approaches primarily based on traditional sliding window detection frameworks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection Strategies</head><p>Within this work, we consider a number of competing contemporary detection frameworks and explore their applicability and performance for generalized object detection in X-ray baggage imagery. Sliding Window Based CNN detection consists of two main stages, one of which is to generate objects of interests, while the other one performs classification. To create objects of interest, a fixed sized n × n window slides over the image horizontally and vertically denoting the current region o interest. The disadvantage of using fixed sized window is that large objects may not fit within the window, resulting in weaker proposal generation. The use of image pyramids addresses this issue via the use of multi-scale sampling of the image and subsequent image interpolation of window regions at differing scale to a fixed size classification region input size. First two stages of Figure <ref type="figure" target="#fig_8">8A</ref> demonstrate region proposal generation process for a sliding window approach. After generating this region of interest proposals, each is evaluated by the second stage of classification (here using a CNN as per Section III, Figure <ref type="figure" target="#fig_8">8A</ref>). As described in Section III, with the use of transfer learning approach, CNN extracts convolutional features and performs classification via fullyconnected layers. This method is similar to an external region proposal generator (sliding window traversal of the image) followed by CNN classification. Faster RCNN (F-RCNN) is based on two subnetworks, containing a unique region proposal network (RPN) and Fast RCNN network together <ref type="bibr" target="#b10">[11]</ref>. Instead of utilizing an external region proposal algorithm as in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref>, this model has its region proposal network (the main differentiator from Fast RCNN <ref type="bibr" target="#b46">[47]</ref>). The RPN consists of convolutional layers that generate set of anchors with different scales and aspect ratios, and predict their bounding box coordinates together with a probability score denoting whether the region is an object or  background. Anchors are generated by spatially sliding a 3x3 window through the feature maps of the last convolutional layers of the Fast RCNN network. These features are then fed to objectness classification and bounding box regression layers. Objectness classification layer classifies whether a region proposal is an object or a background while bounding box regression layer predicts the coordinates of the area. An RoI pooling layer resizes these regions to fixed sized dimensions. f c layers then create feature vectors to be used by bounding box regression and softmax layers (see Figure <ref type="figure" target="#fig_8">8B</ref>). R-FCN, proposed by Dai et al. <ref type="bibr" target="#b11">[12]</ref>, points out the main limitation of Faster RCNN in that each region proposal within RoI pooling layer is computed numerous of times due to the two subsequent fully connected layers, which is computationally expensive (Figure <ref type="figure" target="#fig_8">8B</ref>). They propose a new approach by removing fully-connected layers after RoI pooling, and employing a new variant denoted as "position sensitive score map" <ref type="bibr" target="#b11">[12]</ref>, which handles translation variance issue in detection task (Figure <ref type="figure" target="#fig_8">8C</ref>). Removing fully connected subnetworks leads to much faster convergence both in training and test stages, while achieving similar detection performance results to Faster RCNN <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv Features</head><note type="other">Input Grid Cell</note><p>1556-6013 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2018.2812196, IEEE Transactions on Information Forensics and Security</p><p>YOLOv2 <ref type="bibr" target="#b12">[13]</ref> is a fully CNN that achieves state-of-theart results for object detection. It uses specific techniques to improve its performance against the prior work. Its initial novelty stems from the fact that it performs detection in a single forward-pass, while region-based approaches utilize sub-network for region generation. Like Faster RCNN, it also employs anchors. The main difference here, however, is that instead of fixing the anchor parameters, this approach makes use of k-means clustering over the input data to learn the anchor parameters of the ground truth bounding boxes. In addition to anchors, YOLOv2 performs batch normalization after each layer, resulting in an improvement in the overall performance. Another strategy is the use of higher resolution input images together with multi-scale training. Unlike classification networks that inputs smaller size images such as 224 × 224, YOLOv2 accepts inputs with higher resolution varying between 350 × 350 to 600 × 600. Besides, the model randomly resizes input images during the training, which allows the network to work with objects with varying scales, and hence handles scaling issue. The above strategies yield significant performance improvements, and the approach achieves the state-of-the-art.</p><p>The way YOLOv2 works is rather novel. It divides the input into 13 × 13 grid cells, each of which predicts 5 bounding box coordinates for each anchor. Moreover, for individual predicted bounding boxes, the network outputs confidence score showing the similarity between the bounding boxes and the ground truth. Finally, the output also includes the probability distribution of the classes that the predicted bounding boxes belong. Performing regression and classification within a single network makes YOLOv2 significantly faster, achieving real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application to X-ray Security Imagery</head><p>We compare four localization strategies for our object detection task within X-ray security imagery: a traditional sliding window approach <ref type="bibr" target="#b9">[10]</ref> coupled with CNN classification <ref type="bibr" target="#b17">[18]</ref>, Faster RCNN (F-RCNN) approach of <ref type="bibr" target="#b10">[11]</ref> (a contemporary architecture within recent object recognition challenge results <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b48">[49]</ref>), R-FCN approach of <ref type="bibr" target="#b11">[12]</ref> (comparable to F-RCNN in performance yet offering significant computational efficiency gains over the former), and YOLOv2 <ref type="bibr" target="#b12">[13]</ref>, which currently achieves the best detection performance on PASCAL VOC benchmark while keeping the computation in real-time. Dataset: Instead of using multi-view conventional X-ray patches that we manually crop for the classification task in Section III, here we use full X-ray images to perform binary and multiple class object detection. Detection: For sliding window CNN (SW-CNN) we employ 800 × 800 input image, 50 × 50 fixed size window with a step size of 32 to generate region proposals. We also use image pyramids to fit the window to varying sized objects using 9 pyramid levels. For the classification of the proposed regions we use AlexNet <ref type="bibr" target="#b8">[9]</ref>, VGG M, 16 <ref type="bibr" target="#b14">[15]</ref>, and ResNet-{50, 101} <ref type="bibr" target="#b15">[16]</ref> networks. Although <ref type="bibr" target="#b17">[18]</ref> employs an extra bounding box regression layer within their SW-CNN approach, we do not perform regression as none of the prior work within this domain does so <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>For Faster RCNN <ref type="bibr" target="#b10">[11]</ref> we use the original implementation with a few modifications, and train Faster RCNN with AlexNet <ref type="bibr" target="#b8">[9]</ref>, VGG M, 16 <ref type="bibr" target="#b14">[15]</ref>, and ResNet-{50, 101} <ref type="bibr" target="#b15">[16]</ref> architectures. Since R-FCN is fully convolutional by design, we only use ResNet-{50, 101} <ref type="bibr" target="#b15">[16]</ref> networks for R-FCN to train and test.</p><p>For the training of the detection strategies explained here, we employ transfer learning approach and use the networks pre-trained on ImageNet dataset <ref type="bibr" target="#b41">[42]</ref>. In so doing not only increases performance but also reduces training time significantly. We use stochastic gradient descent (SGD) with momentum and weight decay of 0.9 and 0.0005, respectively. The initial learning rate of 0.001 is divided by 10 with step down method in every 10, 000 iteration. For F-RCNN/R-FCN, batch size is set to 256 for the RPN. All of the networks are trained by using dual-core Intel Xeon E5-2630 v4 processor and Nvidia GeForce GTX Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>Performance of the models is evaluated by mean average precision (mAP), used for PASCAL VOC object detection challenge <ref type="bibr" target="#b49">[50]</ref>. To calculate mAP, we perform the following: we first sort n d detections based on their confidence scores. Next, we calculate the area of intersection over union for the given ground truth and detected bounding boxes for each detection as</p><formula xml:id="formula_0">Ψ(B gt i , B dti ) = Area(B gt i ∩ B dti ) Area(B gt i ∪ B dti ) ,<label>(1)</label></formula><p>where B gt i and B dti are ground truth and detected bounding boxes for detection i, respectively. Assuming each detection as unique, and denoting the area as a i , we then threshold it by θ = 0.5 giving a logical b i , where</p><formula xml:id="formula_1">b i = 1 a i &gt; θ; 0 otherwise.<label>(2)</label></formula><p>This is followed by a prefix-sum giving both true positives t and false positives f , where</p><formula xml:id="formula_2">t i = t i-1 + b i ,<label>(3)</label></formula><formula xml:id="formula_3">f i = t i-1 + (1 -b i ).</formula><p>The precision p and recall r curves are calculated as</p><formula xml:id="formula_4">p i = t i t i + f i ,<label>(4)</label></formula><formula xml:id="formula_5">r i = t i n p ,</formula><p>where n p is the number of positive samples. For a smoother curve, precision vector is then interpolated by using</p><formula xml:id="formula_6">p i = max(p i , p i+1 ).<label>(5)</label></formula><p>We then calculate average precision (AP) based on the area under precision ( p) recall ( r) curve  As shown in Eq 7, we finally find mAP by averaging AP values that we calculate for C classes.</p><formula xml:id="formula_7">AP = n d i p i ∆r.<label>(6)</label></formula><formula xml:id="formula_8">mAP = 1 C C c=1 AP c<label>(7)</label></formula><p>Tables V and VI show binary and multi-class detection results for SW-CNN, F-RCNN, R-FCN with varying networks, and a fixed sized number of region proposals of 300, and for YOLOv2 with a fixed network with varying input image size. For completeness, we additionally present the comparative results for Fast R-CNN (RCNN) <ref type="bibr" target="#b46">[47]</ref> (detection architecture pre-dating that of F-RCNN <ref type="bibr" target="#b10">[11]</ref> and R-FCN <ref type="bibr" target="#b10">[11]</ref>).</p><p>As a general trend, we observe that performance increases with overall network complexity such that superior performance is obtained with VGG16 and ResNet 101 for the regionbased approaches. This observation holds for both the 2class and 6-class problems considered here. Overall, YOLOv2 yields the leading performance for both 2-class and 6-class problems. In addition to this set of experiments, we also train the detection approaches using the pre-trained weights of Dbp6 dataset introduced in Section III-C1. Since not observing significant nuances in results, we do not include them here.</p><p>For the multi-class detection task (Table <ref type="table" target="#tab_11">V</ref>) we see a similar performance pattern to that seen in the earlier firearm detection task. Here, SW-CNN performs worse than any network trained using a Faster RCNN or R-FCN architecture. Similirwise, overall mAP of RCNN is lower than any R-FCN and R-FCN architecture. For comparison of F-RCNN and R-FCN, we observe that Faster RCNN achieves its highest peak using with higher mAP than ResNet-50 and ResNet101. R-FCN with ResNet-50 and ResNet 101 yields slightly worse performance, (mAP: 0.846, 0.856) , than that of the best of Faster-RCNN. For the overall performance comparison, YOLOv2 with an input size of 544×544 shows superior performance (mAP: 0.885). <ref type="table" target="#tab_13">VI</ref> shows that SW-CNN, even with a complex second stage classification CNN such as VGG16 and ResNet 101 , performs poorly compared to any other detection approaches. This poor performance is primarily due to lacking a bounding box regression layer (Figure <ref type="figure" target="#fig_8">8</ref>), a significant performance booster as shown in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Likewise, the best performance of RCNN with VGG16 (mAP: 0.854) is worse than any F-RCNN or R-FCN. This is because the RPN within F-RCNN and R-FCN provides superior object proposals than the selective-search approach used in RCNN. For overall performance on the binary firearm detection task, R-FCN with YOLOv2 with an input image of size 416×416 yields the highest mAP of 0.974.  Faster RCNN (F-RCNN) <ref type="bibr" target="#b10">[11]</ref>, R-FCN <ref type="bibr" target="#b11">[12]</ref> and YOLOv2 <ref type="bibr" target="#b12">[13]</ref> for firearm detection problem (300 region proposals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For firearm detection Table</head><p>Figure <ref type="figure">9</ref> illustrates the impact on the number of region proposals and input image sizes on both detection performance and runtime. Figure <ref type="figure">9A</ref>-B demonstrate detection performance of the approaches on 2-class and 6-class detection tasks, respectively. Increase in the number of region proposals and input image size lead to a rise in detection performance. Overall, YOLOv2 achieves the highest detection on both tasks. Figure <ref type="figure">9C</ref> shows mean runtime in frame per second (fps) where we can see YOLOv2 significantly outperforms the rest of the detection approaches. The lowest fps YOLOv2 achieves (50fps) is still considerably better than the best runtime R-FCN <ref type="bibr" target="#b19">(20)</ref>, F-RCNN (2.9) and SW-CNN (0.8) achieve. Figure <ref type="figure" target="#fig_0">10</ref> illustrates qualitative examples extracted from the statistical performance analysis of Table <ref type="table" target="#tab_11">V</ref>. We see that detection approaches can cope with cluttered datasets where classification methods can fail as shown in Figure <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we exhaustively explore the use of CNN in the tasks of classification and detection within X-ray baggage imagery. For the classification problem, we make a comparison between CNN and traditional BoVW approaches based on features. To do so, we perform layer freezing to observe the relative performance of fixed and fine-tuned sets of CNN feature maps. In addition to this, we train SVM classifier on top of the last layer of the network to have a consistent comparison between CNN and handcrafted features. We also explore various CNN to see the impact of network complexity on overall performance.</p><p>Experimentation demonstrates that CNN features achieve superior performance to handcrafted BoVW features. Fine tuning the entire network for this problem yields 0.996% True Positive (TP), 0.011 False Positive (FP) and 0.994 accuracy (A), a significant improvement on the best performing handcrafted feature detector/descriptor (FAST/SURF, 0.830 TP, 0.033 FP, 0.940 A). For the classification of multiple Xray baggage objects, ResNet-50 achieves 0.986 (A), clearly demonstrating the applicability of CNN within X-ray baggage imagery, and outperforming prior reported results in the field <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>In addition to classification, we also study object detection strategies to improve the performance of cluttered datasets further, where classification techniques fail. Hence, we examine the relative performance of traditional sliding window driven detection with CNN model <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref> against contemporary region-based <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b46">[47]</ref> and single forward-pass based <ref type="bibr" target="#b12">[13]</ref> CNN variants. We show that contemporary Faster RCNN, R-FCN, and YOLOv2 approaches outperform SW-CNN, which is already empirically shown to outperform handcrafted features, regarding both speed and accuracy. YOLOv2 yields 0.885 and 0.974 mAP over 6-class object detection and 2-class firearm detection problems, respectively. This result illustrates the real-time applicability and superiority of such integrated region based detection models within this X-ray security imagery context. Future work will consider exploiting multi-view X-ray security imagery in an end to end design. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Exemplar X-ray baggage imagery multiple objects.</figDesc><graphic coords="1,374.57,220.23,111.85,83.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Gradient-based class activation map (Grad-CAM<ref type="bibr" target="#b29">[30]</ref>) of VGG16<ref type="bibr" target="#b14">[15]</ref> trained on X-ray data. The first column of each convolution box demonstrates grayscale Grad-CAM, while the second column is Grad-CAM heatmap on an input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Transfer learning pipeline. (A) shows classification pipeline for a source task, while (B) is a target task, initialized by the parameters learned in the source task.</figDesc><graphic coords="4,329.16,223.25,225.67,167.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Exemplar X-ray baggage image with extracted data set regions including background samples. Type of baggage objects in the dataset is as follows: (A) Firearm Component, (B) Ceramic Knife, (C) Laptop, (D) Camera , (E) Firearm , (F) Knife</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: t-SNE<ref type="bibr" target="#b42">[43]</ref> visualization of feature maps extracted from the last f c layer of VGG16<ref type="bibr" target="#b14">[15]</ref> fine-tuned for binary (A) and (B) problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Exemplar image cases where CNN (only) classification fails to detect an object in the presence of clutter and other confusing items of interest (here: background laptop detected, knives/guns missed).</figDesc><graphic coords="7,213.34,617.57,87.34,93.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Schematics for the CNN driven detection strategies evaluated. A. Sliding Window based CNN (SW-CNN) [4], [7], B. Faster RCNN (F-RCNN)<ref type="bibr" target="#b10">[11]</ref>, C. R-FCN<ref type="bibr" target="#b11">[12]</ref>, D. YOLOv2<ref type="bibr" target="#b12">[13]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Impact of number of box proposals on performance. (A) for binary class (B) for multi-class (C) Runtime. Models are trained using ResNet101</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Results of CNN and BoVW on Dbp2 dataset for firearm detection. AlexNetab denotes that the network is fine tuned from layer a to layer b.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="6">: Statistical evaluation of CNN architectures (AlexNet,</cell></row><row><cell cols="6">VGG, and ResNet) on Dbp6 dataset for multi-class problem.</cell></row><row><cell></cell><cell>TP%</cell><cell>FP%</cell><cell>P</cell><cell>A</cell><cell>F</cell></row><row><cell>AlexNet [9]</cell><cell>99.830</cell><cell>0.943</cell><cell>0.990</cell><cell cols="2">0.994 0.994</cell></row><row><cell>VGG M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Statistical evaluation of varying CNN architectures (AlexNet, VGG, and ResNet) on FFOB dataset<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell>TP%</cell><cell>FP%</cell><cell>P</cell><cell>A</cell><cell>F</cell></row><row><cell>AlexNet [9]</cell><cell>95.088</cell><cell>3.527</cell><cell>0.960</cell><cell cols="2">0.958 0.958</cell></row><row><cell>VGG M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V :</head><label>V</label><figDesc>Detection results of SW-CNN, Fast-RCNN (F-RCNN)<ref type="bibr" target="#b46">[47]</ref>, Faster RCNN (F-RCNN)<ref type="bibr" target="#b10">[11]</ref>, R-FCN<ref type="bibr" target="#b11">[12]</ref> and YOLOv2<ref type="bibr" target="#b12">[13]</ref> for multi-class problem (300 region proposals). Class names indicates corresponding average precision (AP) of each class, and mAP indicates mean average precision of the classes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VI :</head><label>VI</label><figDesc></figDesc><table /><note><p><p><p>Detection results of SW-CNN, Fast-RCNN (RCNN)</p><ref type="bibr" target="#b46">[47]</ref></p>,</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the UK Home Office for partially funding this work. Views contained within this paper are not necessarily those of the UK Home Office.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visual Words on Baggage X-Ray Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baştan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-23672-3_44</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-23672-3_441,2" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Berlin Heidelberg</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving feature-based object recognition for x-ray baggage security screening using primed visualwords</title>
		<author>
			<persName><forename type="first">D</forename><surname>Turcsany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Industrial Technology</title>
		<imprint>
			<date type="published" when="2013-02">Feb 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object recognition in multiview dual energy x-ray images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On using feature descriptors as visual words for object detection within x-ray baggage security screening</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kundegorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akçay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Imaging for Crime Detection and Prevention]</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>1, 2, 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view object detection in dual-energy x-ray images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baştan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2009">2015. 1, 2, 8, 9</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Svec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arias</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-29451-3_56</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-29451-3_561" />
		<title level="m">Object Recognition in Baggage Inspection Using Adaptive Sparse Representations of X-ray Images</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer learning using convolutional neural networks for object classification within x-ray baggage security imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akçay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kundegorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1057" to="1061" />
		</imprint>
	</monogr>
	<note>Image Processing. 1, 2, 3, 5, 8</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modern computer vision techniques for x-ray testing in baggage inspection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Svec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Riffo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="682" to="692" />
			<date type="published" when="2017">2017. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2012. 1, 2, 3, 5, 6, 7, 9</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection in multi-view X-ray images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Franzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">11</biblScope>
			<pubPlace>1, 2, 8, 9</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>1, 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via regionbased fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1605.06409</idno>
		<ptr target="http://arxiv.org/abs/1605.06409" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>1, 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">Dec. 2016. 1, 3, 8, 9</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1405.3531</idno>
		<ptr target="http://arxiv.org/abs/1405.35312" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2009">2014. 1556 2, 3, 4, 5, 6, 7, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object recognition in x-ray testing using an efficient search algorithm in multiple views</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Riffo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zuccar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pieringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Insight-Non-Destructive Testing and Condition Monitoring</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<ptr target="http://arxiv.org/abs/1312.62292" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UK Home Office Centre for Applied Science and Technology (CAST)</title>
		<idno>Number: 146/16</idno>
		<ptr target="https://www.gov.uk/government/collections/centre-for-applied-science-and-technology-information2" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>OSCT Borders X-ray Image Library</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explosives detection systems (eds) for aviation security</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="55" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of automated image understanding within 3D baggage computed tomography security screening</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of X-ray science and technology</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated x-ray image analysis for cargo security: Critical review and future promise</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaccard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of X-ray science and technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A combinational approach to the fusion, de-noising and enhancement of dual-energy x-ray luggage images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference On</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving weapon detection in single energy x-ray images through pseudocoloring</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Gribok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="784" to="796" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using image processing methods to improve the explosive detection accuracy</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Conners</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="750" to="760" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image segmentation optimisation for xray images of airline luggage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence for Homeland Security and Personal Safety</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2004 IEEE International Conference</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object separation in x-ray image sets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2010 IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2093" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated detection in complex objects using a tracking algorithm in multiple x-ray views</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual cortex inspired features for object detection in x-ray images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2573" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno>abs/1610.02391</idno>
		<ptr target="http://arxiv.org/abs/1610.02391" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparison of 3D interest point descriptors with application to airport baggage object detection in complex CT imagery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Flitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megherbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2420" to="2436" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object classification in 3D baggage security computed tomography imagery using visual codebooks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Flitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2489" to="2499" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Materials-based 3D segmentation of unknown objects from dual-energy computed tomography imagery in baggage security screening</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1961" to="1978" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<ptr target="http://arxiv.org/abs/1409.48423" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<ptr target="http://arxiv.org/abs/1511.005613" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">Nov 1998. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<ptr target="http://arxiv.org/abs/1311.29013" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<ptr target="http://arxiv.org/abs/1512.005673" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2009">2015. 3, 4, 5, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.69805" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.47297" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">4729</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<ptr target="http://arxiv.org/abs/1504.080837" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coco</forename><surname>Microsoft</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10602-1_489" />
	</analytic>
	<monogr>
		<title level="s">Common Objects in Context. Cham</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
