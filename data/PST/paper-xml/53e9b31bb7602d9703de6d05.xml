<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Informed source separation through spectrogram coding and data embedding $</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-09-19">19 September 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
							<email>antoine@liutkus.net</email>
							<affiliation key="aff0">
								<orgName type="department">Institut Telecom</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">CNRS LTCI</orgName>
								<address>
									<addrLine>37/39 rue Dareau</addrLine>
									<postCode>75014</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Pinel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Grenoble Institute of Technology</orgName>
								<address>
									<postCode>38402</postCode>
									<settlement>Grenoble Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Badeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Telecom</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">CNRS LTCI</orgName>
								<address>
									<addrLine>37/39 rue Dareau</addrLine>
									<postCode>75014</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Grenoble Institute of Technology</orgName>
								<address>
									<postCode>38402</postCode>
									<settlement>Grenoble Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaël</forename><surname>Richard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Telecom</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">CNRS LTCI</orgName>
								<address>
									<addrLine>37/39 rue Dareau</addrLine>
									<postCode>75014</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Informed source separation through spectrogram coding and data embedding $</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-09-19">19 September 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">D043C95E911589E60BA04BD56E2BD8FA</idno>
					<idno type="DOI">10.1016/j.sigpro.2011.09.016</idno>
					<note type="submission">Received 9 March 2011 Received in revised form 18 July 2011 Accepted 5 September 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Audio source separation Wiener filtering Data embedding NTF</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the issue of underdetermined source separation in a particular informed configuration where both the sources and the mixtures are known during a so-called encoding stage. This knowledge enables the computation of a side-information which is small enough to be inaudibly embedded into the mixtures. At the decoding stage, the sources are no longer assumed to be known, only the mixtures and the extracted sideinformation are processed for source separation. The proposed system models the sources as independent and locally stationary Gaussian processes (GP) and the mixing process as a linear filtering. This model allows reliable estimation of the sources through generalized Wiener filtering, provided their spectrograms are known. As these spectrograms are too large to be embedded in the mixtures, we show how they can be efficiently approximated using either Nonnegative Tensor Factorization (NTF) or image compression. A high-capacity embedding method is used by the system to inaudibly embed the separation side-information into the mixtures. This method is an application of the Quantization Index Modulation technique applied to the time-frequency coefficients of the mixtures and permits to reach embedding rates of about 250 kbps. Finally, a study of the performance of the full system is presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Source separation has been a popular field of research in signal processing for about 20 years (see e.g. <ref type="bibr" target="#b0">[1]</ref> for a review and <ref type="bibr" target="#b1">[2]</ref> for fundamental principles). Its goal is to recover several signals called sources that were mixed together in one or several signals called mixtures.</p><p>One possible classification of audio source separation systems is given by the value of the ratio r between the number of observed mixture signals, K, and the number of sources, M. If this ratio is greater than 1, the problem is called overdetermined and source separation systems may then rely on microphone array algorithms and can often reach excellent performance. When the ratio decreases, the problem becomes determined (r¼ 1) and can be solved efficiently, for example through Independent Component Analysis <ref type="bibr" target="#b2">[3]</ref>. When r o 1, the problem gets underdetermined and systems' performances are hardly predictable and possibly very poor because the source separation problem is then particularly difficult. However, this case is of particular interest in music processing since most music mixtures are composed of more than two sources, while the number of observations K is often limited to one or two (respectively for mono and stereo recordings). Separating source signals from such music mixtures is of great interest because it would enable to isolate the different elements of the audio scene, leading to karaoke applications, or to separately manipulate them, e.g. by modifying the volume, the color or the spatialization of an instrument, a process referred to as active listening or remixing. In the present paper, we will focus on the underdetermined source separation (USS) of music signals.</p><p>In the case of USS, there are less observable signals than necessary to solve the underlying mixing equations and separation cannot be achieved without some prior information about the signals that permits to eliminate ambiguity. Research on the subject is hence largely focused on how to model any such knowledge and take it into account in the inference process. We can roughly distinguish between three kinds of commonly used prior information.</p><p>First, inspired by Auditory Scene Analysis <ref type="bibr" target="#b3">[4]</ref>, some authors have proposed generative models for the signals of interest, ranging from parametric harmonic models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> to acoustic generative models inspired from the speech processing community <ref type="bibr" target="#b6">[7]</ref>. In this case, source separation consists in estimating the best parameters of these generative models from the observed mixtures. In the Bayesian framework, informative prior distributions are furthermore assigned to these parameters. Such priors can be obtained from query signals as in <ref type="bibr" target="#b7">[8]</ref> or from third party Music Information Retrieval (MIR) methods such as fundamental frequency estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> or onsets detection.</p><p>Second, much research has also focused on algorithms using prior knowledge about the mixing process. In particular, it is common practice in musical production to assign different spatial locations to the sources in multichannel mixtures. Algorithms can then be designed that exploit this spatial distribution of the sources to perform separation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. A fundamental property of audio sources is that they are often sparse in the Time-Frequency (TF) domain (see <ref type="bibr" target="#b12">[13]</ref> and chapter Sparse Component Analysis in <ref type="bibr" target="#b0">[1]</ref>), which means that few sources are usually active in the same time-frequency bin. Therefore, they can be distinguished by different directions of arrival (DOA), and a filtering of the mixtures depending on the estimated DOA can lead to very good separation performance.</p><p>Finally, other methods were proposed in the last few years that decompose the spectrograms of the mixtures into additive elements corresponding to the sources. Decomposition through Nonnegative Tensor Factorization (NTF) techniques <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> was proved to be adequate to this end. In this framework, the spectrograms of the mixtures are decomposed as a time-weighted sum of fixed spectral basis corresponding to the sources. Extensions of NTF methods were subsequently proposed in a Bayesian framework that make use of informative conjugate priors for parameters estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In any case, all these methods for USS rely on a TF representation of the mixtures and their objective can be understood as determining the relative contribution of each source in each time-frequency bin. Estimates of the sources are then obtained either through a binary <ref type="bibr" target="#b18">[19]</ref> or a soft <ref type="bibr" target="#b19">[20]</ref> TF masking strategy. The latter is equivalent to a Wiener filter applied in each frame of the mixtures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, thus generalizing classical denoising methods to the case of more than only two sources.</p><p>In this study, we focus on a special case of USS, called Informed Source Separation (ISS) and depicted in Fig. <ref type="figure" target="#fig_1">1</ref>. ISS can be understood as an encoding/decoding framework in which both the sources and the mixtures are available at the encoder, but only the mixtures are available at the decoder, as well as some side-information that has been generated by the encoder and transmitted along with the mixtures to assist the separation process. ISS thus aims at making source separation robust by providing adequate and case-specific prior knowledge to the separation algorithms. Its main advantage is that it permits to reliably recover the separated tracks from mixtures with only a reasonable amount of side-information. This approach was initially proposed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> for monophonic linear instantaneous mixtures using Modified Discrete Cosine Transform (MDCT) domain matrix quantization techniques. It has been extended to stereo linear instantaneous mixtures in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, using local inversion of the mixtures in the TF domain. Even if these studies settle the foundations of the informed approach for source separation within a two-step coding-decoding process, they are confronted to at least two notable limitations: first, they Encodingstage.</p><p>•An informed conf guration where both the sources and the mixtures are known •A small side-information is computed and watermarked into the mixtures. are limited to linear instantaneous mixtures, which is too restrictive if professional music production is to be considered; and second, their performance relies either on a proprietary (i.e. over-specific) source encoding technique or on a ''weak form'' of the sparsity assumption (in <ref type="bibr" target="#b25">[26]</ref>, at most two sources are assumed to be predominant in each TF bin) which may fail for a large number of sources or for sources that significantly overlap in the TF plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M sources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decodingstage</head><p>In the present paper, we introduce a new framework for ISS which is based on modeling the sources as independent and locally stationary GPs that are mixed together through linear filtering into the mixtures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, thus addressing the more general case of convolutive mixtures. Moreover, the sources are not assumed to be sparse and can possibly significantly overlap in the TF domain. We will show in Section 2 that source separation can be achieved very reliably in this framework via generalized Wiener filtering <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref>, provided the power spectrograms of the sources that are used to build those Wiener filters are available. As those spectrograms are too large to be used directly as the side-information to be transmitted to the decoder, we propose to use dimension reduction techniques such as Nonnegative Tensor Factorization (NTF) or image compression to encode them in a more concise way. This new framework enables to perform ISS with any kind of sources and realistic mixing processes, thus significantly extending the range and potential impact of the informed approach of audio source separation as compared to previously proposed ISS methods.</p><p>As for the side-information embedding, we use a highcapacity data embedding technique based on the combination of Quantization Index Modulation (QIM) <ref type="bibr" target="#b29">[30]</ref> applied on time-frequency coefficients of the mixture signals, and a Psycho-Acoustic Model (PAM) used to control the inaudibility of the process. This embedding technique has been presented in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, and already exploited for ISS in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Therefore, we only provide here a general overview of this technique and focus on its use within the new ISS framework. In particular, a thorough evaluation section allows assessing the consequences of the embedding process on source separation quality. Note that this study only focuses on uncompressed, PCM signals.</p><p>This paper is organized as follows. In Section 2, we detail the model used for the sources and the mixing process as well as the corresponding source separation method. In Section 3, we introduce two different approaches for the choice of the parameters to be inaudibly hidden in the mixtures through the high-capacity data embedding technique which is presented in Section 4. Finally, we give some experimental results in Section 5 in which we study the influence of both the dimensionality reduction technique and data embedding on source separation quality. Finally, we draw some conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Source separation model</head><p>In this section, we make use of the Gaussian Process framework for Source Separation (GPSS) that is presented in full generality in <ref type="bibr" target="#b21">[22]</ref>. In the case of audio processing and locally stationary signals, its parameters reduce to the spectrograms of the sources and separation is equivalent to generalized Wiener filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations</head><p>Locally Stationary Gaussian Processes (LSGP) <ref type="bibr" target="#b21">[22]</ref> are a common model for source separation. An LSGP is a signal whose restriction to a small portion of time called frame is a stationary Gaussian process and is independent from the other frames. It is important to note that an LSGP is not necessarily stationary, but only its restrictions to small durations are stationary.</p><p>Let f9½f ðx 1 Þ Á Á Á f ðx n Þ &gt; be an LSGP observed on n samples. f is split into a set ff t g t ¼ 1ÁÁÁN T of N T overlapping frames of length L. The Short Term Fourier Transform (STFT) of f is defined as the matrix F whose tth column contains the Fourier transform of f t . Since the different frames are supposed independent, 1 and since the coefficients of the Fourier transform of each frame t are asymptotically independent because f t is stationary, all coefficients of the matrix F are assumed independent. In the following, F o,t will denote the frequency bin o of frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us define the (power</head><formula xml:id="formula_0">) spectrogram S of f as S o,t ¼ 9F o,t 9 2 . It is readily shown that if f is an LSGP, F o,t</formula><p>follows a centered complex Gaussian distribution of variance S o,t <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Separation of one mixture of locally stationary GPs</head><p>Suppose we observe the sum y(x) of M locally stationary signals f m (x): yðxÞ ¼ P M m ¼ 1 f m ðxÞ for n samples. The STFT Y of the mixture is hence the sum of the STFTs fF m g m ¼ 1ÁÁÁM of the sources. Since we suppose that the sources are LSGPs, each time-frequency bin of Y is the sum of M independent complex coefficients with Gaussian distribution and can thus be modeled as a complex Gaussian Mixture Model. This model is equivalent to the Gaussian Scaled Mixture Model introduced by Benaroya <ref type="bibr" target="#b19">[20]</ref> and further studied by many others <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Let S m be the power spectrogram of source m. It can be shown that the Minimum Mean Squared Error (MMSE) estimate Fm0 of the STFT F m0 of source m 0 is</p><formula xml:id="formula_1">Fm 0 ¼ S m 0 P m S m Á Y<label>ð1Þ</label></formula><p>where A=B and A Á B respectively stand for componentwise division and multiplication of matrices A and B. In audio source separation, this result is referred to as generalized -or adaptive -Wiener filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Separation of multichannel mixtures</head><p>Considering now the more general multichannel case where K mixtures y k È É k ¼ 1ÁÁÁK are available (in practice we 1 Assuming overlapping frames to be independent is classical in the source separation literature. As demonstrated in <ref type="bibr" target="#b21">[22]</ref>, it leads to overconfidence in the estimates, but provides smooth transitions between the frames. Recent studies such as <ref type="bibr" target="#b32">[33]</ref> focus on this issue, which is out of the scope of this paper.</p><p>generally have two channels, for stereo signals). We can assume that each mixture y k is the sum of filtered versions of the M sources <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_2">y k ðxÞ ¼ X M m ¼ 1 f km ðxÞ ð 2Þ</formula><p>where f km ðxÞ is called the contribution of source m to mixture k for time x and is defined as the convolution of source m with a stable linear filter a km , that is called the mixing filter from source m to mixture k:</p><formula xml:id="formula_3">f km ðxÞ9 a km nf m À Á ðxÞ9 X P t ¼ 0 a km t ð Þf m xÀt ð Þ<label>ð3Þ</label></formula><p>We will only consider causal and Finite Impulse Response (FIR) filters of order P here. Provided that P is sufficiently small compared to the length L of each frame, (3) can be simply written in the frequency domain. Typical values for P are 150 in the following, whereas L % 3000 in our implementation. Let A km be the Fourier transform of the mixing filter a km , F km be the STFT of f km and S m be the power spectrogram of the source m. It is readily shown that the MMSE estimate Fkm 0 of F km 0 is approximately given by<ref type="foot" target="#foot_0">2</ref> </p><formula xml:id="formula_4">Fkm0 ¼ ðdiag9A km 0 9 :2 ÞS m0 P M m ¼ 1 ðdiag9A km 9 :2 ÞS m :Y k<label>ð4Þ</label></formula><p>This permits to efficiently compute the estimates of the contributions of all the sources within the mixtures provided the mixing filters a km and the spectrograms S m are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Strategies for the side-information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Introduction</head><p>After having introduced source separation for locally stationary GPs, we now focus on the particular informed configuration depicted in Fig. <ref type="figure" target="#fig_1">1</ref>, where the source signals f m as well as the mixtures y k are perfectly known at the encoder. We are then left to produce a set of parameters Y that we can inaudibly embed into the mixtures to assist source separation at the decoder. In all the following, the signals are assumed locally stationary as in Section 2.</p><p>The model we introduced in Section 2 provides simple ways to estimate the sources in the mixtures. To this end, the separation method given in ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_4">4</ref>) requires the mixing filters fa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM , the spectrograms fS m g m ¼ 1ÁÁÁM of the sources.</p><p>In the remaining of this section, we will thus devise different possible strategies that allow recovering estimates of fa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM and fS m g m ¼ 1ÁÁÁM given Y at the decoder. Note that the informed approach highly contrasts with blind or semi-blind source separation or spectral subtraction methods based on Wiener filtering where the parameters of the Wiener filter (mixing filters and power spectrograms of the sources) have to be estimated from the observations only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Definition or estimation of the mixing filters</head><p>First of all, there are basically two possibilities for the mixing filters at the encoder. Those filters are either defined during the encoding process of the informed mixtures and thus perfectly known, or the mixing process is made separately (e.g. by a professional sound engineer) and they have to be estimated at the encoder before the encoding process. Previous studies in the automatic mixing literature <ref type="bibr" target="#b34">[35]</ref> have focused on the latter case. Given some fixed length P þ1 for their impulse response, estimation of the mixing filters fa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM at the encoder is straightforward. Indeed, for every mixture k, everything is available at the encoder to estimate the filters fa km g m ¼ 1ÁÁÁM that minimize the mean squared error between y k and P M m ¼ 1 a km nf m through standard least squares method.</p><p>It is noticeable that in real-world scenarios, the mixing process may include very long mixing filters such as reverberations or even nonlinear processing such as compression. In this study, we nonetheless approximate the mixing as a linear filtering of the sources by finite impulse response filters of length P % 150. As demonstrated in Section 5, this approximation yields good results even in nonlinear mixtures, thus suggesting that it is a reasonable simplifying assumption.</p><p>In the following discussion, we will thus simply assume that the mixing filters are readily available at the encoder and included in the side information Y. Note also that in the present study, the mixing filters are assumed to be constant over each whole piece of music to process, hence the amount of side-information reserved to encode those filters is very reasonable (at least compared to the spectrograms information that is computed for each frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Oracle configuration</head><p>The proposed separation method requires the spectrograms of the sources at the decoder. A first idea is to simply embed the whole ''raw'' set of source spectrograms. This would result in</p><formula xml:id="formula_5">Y oracle ¼ ffa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM ,fS m g m ¼ 1ÁÁÁM g</formula><p>As this setting of Y is the one that guarantees MMSE estimates, we will call it the Oracle configuration in the following. Unfortunately, such a scenario is actually not an option, since we cannot afford to embed the complete spectrograms fS m g m ¼ 1ÁÁÁM inaudibly within the mixtures. More precisely, the total number #Y oracle of parameters included in Y in this case is</p><formula xml:id="formula_6">#Y oracle ¼ M Â N o Â N T |fflfflfflfflffl{zfflfflfflfflffl} for each source þ M Â K Â ðP þ 1Þ |fflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflffl ffl} for A<label>ð5Þ</label></formula><p>which clearly leads to an excessive bitrate considering the embedding capacity: the typical bitrate necessary to embed the parameters in this configuration is 20,000 kbps, 3  which is larger than available: in the embedding method presented in Section 4, typical capacity is about 200 kbps per mixture signal, which is much insufficient for this purpose. Still, as the Oracle configuration is guaranteed to produce the best results the model is capable of, it will serve as a reference method for evaluation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dimension reduction techniques</head><p>As the spectrograms fS m g m ¼ 1ÁÁÁM of the sources cannot be directly embedded into the mixtures because of insufficient capacity, realistic but effective models have to be devised to encode them concisely. For that purpose, a simple idea we proposed in <ref type="bibr" target="#b26">[27]</ref> is to approximate the 3-dimensional tensor S containing the stacked spectrograms of the sources. 4  A first solution is to approximate S as the product of low-rank nonnegative matrices, thus leading to an NTF model <ref type="bibr" target="#b15">[16]</ref> that greatly reduces the number of parameters to include in Y. More specifically, we can chose the Canonical Polyadic (CP) decomposition 5 and approximate S as</p><formula xml:id="formula_7">S o,t,m % Ŝ o,t,m ¼ X R r ¼ 1 Q mr W or H rt<label>ð6Þ</label></formula><p>where Q , W and H are the new parameters Y NTF of the model, i.e.</p><formula xml:id="formula_8">Y NTF ¼ ffa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM ,Q ,W,Hg.</formula><p>The approximation is graphically illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, we have shown that this technique is equivalent to modeling the source signals as a sum of R independent LSGPs with constant normalized power spectrum called latent components. This approach leads to finding Y NTF such that the Itakura-Saito distance 6   D IS ðS9 Ŝ Þ between the spectrograms and their reconstruction is minimal. The main advantage of this approach over the Oracle solution as presented in Section 3.3 is that the number #Y NTF of parameters included in Y NTF becomes</p><formula xml:id="formula_9">#Y NTF ¼ N o Â R |fflfflffl ffl{zfflfflffl ffl} for W þN T Â R |fflfflffl{zfflfflffl} for H þ M Â R |fflffl ffl{zfflffl ffl} for Q þ M Â K Â ðP þ1Þ |fflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflffl ffl} for A<label>ð7Þ</label></formula><p>which is much smaller than #Y oracle given in (5) as we show in Section 5. Typical bitrates necessary to convey Y NTF indeed drop to approximately 150 kbps. It is very important to note that the latent variables W and H are the same for all sources, whereas the specific structure of a given source is modeled by Q , hence the very efficient compression power of this representation. This model has been thoroughly discussed in <ref type="bibr" target="#b28">[29]</ref>. For only one source, it is equivalent to the NMF approach that was popularized by <ref type="bibr" target="#b35">[36]</ref> when using IS divergence as a cost function, which is a special case of b-divergence for b ¼ 0 (see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> on this point). Algorithms in the aforementioned papers can be generalized to the case of 3-dimensional tensors of M channels and the corresponding update rules for the parameters are summarized in Algorithm 1 for any b-divergence. 7 The main idea with those iterative algorithms is to randomly initialize the parameters of the model with nonnegative values, and then iteratively update each matrix by multiplying it componentwise in order to ensure a diminution of the cost function. In Algorithm 1, we have also included component-wise exponentiation of the update matrices through an exponent stepsize ZðbÞ as suggested by recent convergence analysis of tensor decompositions <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. ZðbÞ is defined the following way:</p><formula xml:id="formula_10">ZðbÞ ¼ 1 2Àb if bo1 1 i f 1o br2 1 bÀ1 if bZ2 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; :</formula><p>Algorithm 1. Update rules for the parameters Q , W and H of the source model ( <ref type="formula" target="#formula_7">6</ref>) for one iteration.</p><formula xml:id="formula_11">Q m: 'diag diagðQ m: Þ: W &gt; ð Ŝ:bÀ2 m :SmÞH &gt; W &gt; Ŝ:bÀ1 m H &gt; ! ÁZðbÞ ! W'W: P M m ¼ 1 ð Ŝ:bÀ2 m :SmÞðdiagðQ m: ÞHÞ &gt; P M m ¼ 1 Ŝ:bÀ1 m ðdiagðQ m: ÞHÞ &gt; " #<label>ÁZðbÞ</label></formula><p>H'H:</p><formula xml:id="formula_12">P M m ¼ 1 ðWdiagðQ m: ÞÞ &gt; ð Ŝ:bÀ2 m :SmÞ P M m ¼ 1 ðWdiagðQ m: ÞÞ &gt; Ŝ:bÀ1 m " # ÁZðbÞ</formula><p>When the parameters have been estimated at the encoder and transmitted to the decoder, separation can then be performed by replacing S m in (1) or (4) by its CP approximation <ref type="bibr" target="#b5">(6)</ref>. Evaluation of this technique for several orders R of the CP approximation is given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Spectrogram compression</head><p>Alternatively to NTF decomposition, another original idea that we propose in the present paper to concisely encode the source spectrograms is to compress them using appropriate compression techniques borrowed from the image processing literature <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. Indeed, each spectrogram S m for a given source m is a matrix of nonnegative numbers and can hence be understood (and compressed) as a large image.</p><p>Since 8-bit image compression cannot encode more than 256 levels for a grayscale image and since the dynamic range found in typical spectrograms is generally much larger, applying compression algorithms directly on S m leads to poorly encoded spectrograms. It was found that instead of encoding S m , one could rather encode log S m , which exhibits much less dynamic variations. In addition, the log-spectrograms can be normalized between maxðlog S m Þ and maxðlog S m ÞÀX dB where X is 3 This approximation is obtained using M ¼5, No ¼ 1024 and N T ¼ 100 frames=s. 4 That is, ½S o,t,m ¼ ½Sm o,t . 5 CP is also called CANDECOMP or PARAFAC <ref type="bibr" target="#b15">[16]</ref>. 6 D IS ðA9BÞ9 P o,t,m ½Ao,t,m=Bo,t,mÀlog ðAo,t,m=Bo,t,mÞÀ1. typically set to 80 dB for music spectra (values below the new min value are set to this new min value). Corresponding normalization factors are transmitted within the side-information and used to denormalize the spectrograms at the decoder, but these factors occupy a very limited embedding capacity. Of course, at the decoding stage, log-spectra are converted back to linear scale.</p><p>In any case, we consider a compression algorithm denoted C in the following that produces a buffer of data y m which permits to concisely encode the spectrogram S m of each source through an image compression algorithm:</p><formula xml:id="formula_13">y m ¼ CfS m g ð 8Þ</formula><p>An estimate Ŝm of the spectrogram is then obtained by applying the inverse transformation:</p><formula xml:id="formula_14">Ŝm ¼ C À1 fy m g ð<label>9Þ</label></formula><p>Once such an estimate has been obtained, separation can be performed by replacing S m in ( <ref type="formula" target="#formula_1">1</ref>) or ( <ref type="formula" target="#formula_4">4</ref>) by its estimate <ref type="bibr" target="#b8">(9)</ref>. In Fig. <ref type="figure" target="#fig_4">3</ref>, we show an excerpt of an original logspectrogram and its corresponding estimation using the classical JPEG algorithm <ref type="bibr" target="#b40">[41]</ref> with a very low quality setting.</p><p>When using such an Image Compression (IC) technique, the side-information Y IC to be inaudibly embedded in the mixtures becomes</p><formula xml:id="formula_15">Y IC ¼ ffy m g m ¼ 1ÁÁÁM ,fa km g k ¼ 1ÁÁÁK,m ¼ 1ÁÁÁM g ð<label>10Þ</label></formula><p>and the corresponding number of parameters and bitrate then depends on the setting of the compression quality in the chosen image compression routine C. In Section 5, we see that the capacity required for the transmission of Y IC lies between 20 kbps and 200 kbps depending on the Quality setting of the image compression algorithm. In the following, we will use the standard JPEG compression algorithm for still images <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Conclusion</head><p>In this section, we have presented two methods that can be used by the encoder to encode the parameters needed by the decoder to perform source separation in the framework introduced in Section 2. These methods lead to a sufficiently small amount of side-information parameters to allow efficient inaudible embedding within the mixtures. The uncompressed, full-accuracy, version of the parameters can be used as a reference oracle configuration providing upper bounds for the separation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">High-capacity data embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Introduction</head><p>In this subsection, we present the high-capacity embedding technique that we use to embed the side-information Y that contains all the hyperparameters needed to perform separation of the mixtures. As mentioned in the introduction, this method has already been presented in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, and we thus only provide the general lines here. We refer the reader to these references for technical details. The basic principle of the method is that if time-frequency coefficients  can be quantized with a limited amount of binary resource without noticeable quality impairments in perceptual audio coding, they can also be modified to embed data. Note that the complete side-information Y to transmit has to be split and spread across the different TF bins of the different mixtures depending on capacity values, so that each of them carries a small part of the complete message. Conversely, the decoded elementary messages have to be concatenated to recover the complete side-information. For simplicity of presentation, we do not focus here on such implementation details, that are trivial and arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Time-frequency transform</head><p>As briefly mentioned in Section 1, the data hiding process used at the encoder to embed the side-information Y needed by the decoder to perform source separation is an application of the Quantization Index Modulation (QIM) technique introduced in <ref type="bibr" target="#b29">[30]</ref> and is applied to the coefficients of the TF representation of the mixture.</p><p>In <ref type="bibr" target="#b30">[31]</ref> the Modified Discrete Cosine Transform (MDCT) was used, and in <ref type="bibr" target="#b31">[32]</ref> an integer version of the MDCT (IntMDCT, see <ref type="bibr" target="#b41">[42]</ref>) has shown to provide improved embedding rates. In addition, IntMDCT maps integer values in the time-domain with integer values in the frequency domain. Therefore, it has the advantage to provide an embedded mix signal which is fully compliant with the PCM format, while an explicit conversion to this format was necessary with the MDCT and introduced noise robustness issues. For these reasons, we use IntMDCT in the present system. In our implementation, the frames are W¼2048 samples long (46.5 ms for a sampling frequency f s ¼ 44:1 kHz), with a 50% overlap between consecutive frames. This results in matrices of IntMDCT coefficients of 1024 frequency bins (denoted by o) times n=1024 time bins (denoted by t; n is the total length of each signal). The time-domain signals are recovered from embedded IntMDCT matrices by frame-wise inverse transformation followed by overlap-add.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Embedding through IntMDCT quantization</head><p>We now present the principle of the embedding process. Let Cðo,tÞ denote the capacity at TF bin ðo,tÞ, i.e. the size of the binary code to be embedded in the IntMDCT coefficient at that TF bin (under inaudibility constraint). We will see below how Cðo,tÞ is determined for each TF bin. For each TF bin ðo,tÞ, a set of 2 Cðo,tÞ uniform quantizers is defined, whose quantization levels are intertwined, and each quantizer represents a Cðo,tÞbit binary code. Embedding a given binary code on a given IntMDCT coefficient is done by quantizing this coefficient with the corresponding quantizer (i.e. the quantizer indexed by the code to transmit; see Fig. <ref type="figure" target="#fig_5">4</ref>).</p><p>At the decoder, recovering the code is done by comparing the transmitted coefficient with the 2 Cðo,tÞ quantizers, and selecting the quantizer with the quantization level closest to the transmitted coefficient. Note that because the capacity values depend on ðo,tÞ, those values must also be transmitted in order to select the right set of quantizers. To this purpose, a fixed-capacity embedding ''reservoir'' is allocated in the higher frequency region of the spectrum, and the capacity values are actually defined within subbands (see <ref type="bibr" target="#b31">[32]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Embedding rate and performance</head><p>The embedding rate r is given by the average total number of embedded bits per second of signal. It is obtained by summing the capacity Cðo,tÞ over the embedded region of the TF plan and dividing the result by the signal duration. The performance of the embedding process is determined by the inaudibility constraint. <ref type="foot" target="#foot_1">8</ref>Here, the inaudibility constraint induces an upper bound on the number of quantizers, hence a corresponding upper bound on the capacity Cðo,tÞ <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. More specifically, we constraint the power of the embedding error in the worst case to remain under the masking threshold Mðo,tÞ provided by a psycho-acoustic model (PAM) inspired from Perceptual Audio Compression <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. It can be shown that the optimal capacity is given by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> Cðo,tÞ ¼ 1 2 log 2 ðMðo,tÞÞ þ1</p><formula xml:id="formula_16">Ä Å :<label>ð11Þ</label></formula><p>where b:c denotes the floor function. The PAM leads to a masking threshold which is signal-dependant and calculated for each signal frame. This masking threshold is inspired from the MPEG-AAC model <ref type="bibr" target="#b44">[45]</ref> and was adapted to the present data hiding problem. In particular, it is possible to control the embedding rate by translating the masking threshold by a scaling factor a (in dB), i.e. using the following variant of (11):</p><p>C a ðo,tÞ ¼ 1 2 log 2 ðMðo,tÞ Á 10 a=10 Þþ1</p><formula xml:id="formula_17">j k :<label>ð12Þ</label></formula><p>Similarly to the rate-distortion theory in source coding, signal quality is expected to decrease as the embedding rate increases, and vice-versa. When a40 dB, the masking threshold is raised, allowing for larger values of the quantization error and thus larger capacities and embedding rates, at the price of potentially lower quality. On the contrary, when ao0 dB, the masking threshold is lowered, leading to a ''safety margin'' for the inaudibility of the embedding process, at the price of lower embedding rate. An end-user of the proposed system can thus look for the best trade-off between rate and quality for a given application. It can be shown that the embedding rate r a corresponding to C a and the basic rate r ¼ r 0 are related by 9</p><formula xml:id="formula_18">r a C r 0 þ a Á log 2 ð10Þ 10 Á F u<label>ð13Þ</label></formula><p>where F u is the bandwidth of the embedded frequency region. This linear relation enables to easily control the embedding rate by the setting of a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Conclusion</head><p>With this data hiding technique, maximum embedding rates of about 250 kbps can be obtained for many musical signals of different styles such as rock, pop, jazz, funk, metal, electro, bossa, fusion, etc. Note that for stereo signals, this embedding capacity is to be understood per channel, and is approximately one third of the 16-bit 44.1 kHz PCM bitrate necessary to convey the original signals (705 kbps/channel). 10 Such rates generally correspond to the higher level of the masking curve allowed by the PAM and the limit of masking power can hence be reached. More ''comfortable'' rates can be set between 150 and 200 kbps in each channel, to guarantee transparent quality for the embedded signals <ref type="bibr" target="#b31">[32]</ref>. This flexibility is used in the present ISS system to fit the embedding capacity with the size of the side-information. Indeed, we see in Section 5 that the required capacities to convey the side-information vary from 30 kbps to 250 kbps, depending on the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data and metrics</head><p>In order to perform objective evaluation of source separation in general (and not only of informed source separation), the original sources are needed and quantitative evaluation can hence only be performed for mixtures whose constitutive sources are known beforehand. Fortunately, thanks to the rapidly growing community of musicians working with the Creative Commons licenses, 11  such material is now readily available. In this study, experiments were carried out with the internal source separation corpus gathered for the Quaero programme, 12   from which 15 different excerpts were chosen of various musical styles along with their constitutive separated tracks. The corpus includes excerpts composed of 5-11 separated tracks, which are of many kinds, including acoustic instruments such as piano or guitar, male and female singers, distorted sounds/voices, digital effects, etc. All sampling rates were set to 44,100 Hz and all signals are approximately 30 s long.</p><p>Since state of the art methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> only allow for linear instantaneous mixtures, we created a set of 7 such mixes from our corpus for comparison with the proposed method. For the rest of the evaluation, all mixing was done either in mono (5 excerpts) or in stereo (10 excerpts) using Digital Audio Workstations (DAW). It includes equalizing, panning and digital effects such as reverberation and compression on some excerpts. The real contributions of the different sources into their respective mixtures have also been obtained as the separate outputs of the DAW mixing tools. This permits to quantitatively evaluate the estimation of the sources contributions f km .</p><p>Objective criteria to evaluate the quality of the separation were used as defined in the BSSEVAL toolbox <ref type="bibr" target="#b45">[46]</ref>. BSSEVAL is a popular evaluation toolbox that is used for the international Signal Separation Evaluation Campaign (SiSEC <ref type="bibr" target="#b46">[47]</ref>) and that produces several metrics assessing separation quality for each source. For each estimated source the metrics produced by BSSEVAL are the Source to Distortion Ratio (SDR), the Source to Artifact Ratio (SAR) and the Source to Interference Ratio (SIR). All these metrics are expressed in dB. Whereas the SDR is a global measure of separation performance, the SAR and SIR respectively measure the amount of separation/reconstruction artifacts and the amount of energy from the other interfering sources. In any case, higher is better. It was shown in <ref type="bibr" target="#b47">[48]</ref> that the metrics from BSSEVAL are indeed representative of perceptual separation quality. In the whole evaluation section, averages of the metrics within a mixture are done by weighting the results for its constitutive sources according to the logarithm of their total energy. When comparing with the state of the art, we also use the PEASS toolkit, which is a complementary toolbox for perceptual evaluation.</p><p>We first compare the proposed method with the state of the art in Section 5.2. Then, we evaluate the impact of the embedding process on separation in Section 5.3 and finally, in Section 5.4, we compare the different techniques that we introduced to encode the spectrograms of the mixtures in the side-information, namely dimension reduction (see Section 3.4) and image compression (see Section 3.5). Sounds from these evaluations can be listened to on our webpage. 13   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with state of the art</head><p>We performed a complete test of the state of the art method <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, called PARVAIX in the following, on a subset of our corpus, composed of linear instantaneous 9 Actually, the approximation is an exact equality for a multiple of 10 log 10 ð4Þ, and the approximation is very good for other values of a, since the embedding rate results from the averaging on a large number of capacity values. 10 Note that the data embedding technique is not robust to lossy audio compression such as MP3 or AAC. Some perspectives on ISS for compressed music signals are discussed in the conclusion section. 11 www.creativecommons.org 12 www.quaero.org 13 See http://www.telecom-paristech.fr/ $ liutkus/iss/.</p><p>mixtures only. The same bitrates (75 kbps) were used in all cases. Results can be found in Fig. <ref type="figure">5</ref> (left) for BSSEVAL.</p><p>As can be seen in Fig. <ref type="figure">5</ref> (left), results are very good with both methods. SDR values systematically range between 5 dB and 15 dB, largely outperforming underdetermined blind source separation. Furthermore, SIR is seen to lie between 10 dB and 30 dB for PARVAIX and between 10 dB and 20 dB with the proposed method, thus clearly confirming that the sources are very well separated in both cases. However, it is noticeable that PARVAIX performs better with respect to the BSSEVAL criteria, which are close to signal to noise ratios. Still, it does not perform as well on perceptive grounds and the difference in performance seen on Fig. <ref type="figure">5</ref> (left) may be explained by several facts. First, it is not surprising that PARVAIX obtains very good BSSEVAL performance. Indeed, this method aims at optimizing the output signal to noise ratio of the different sources given the assumption that only two sources are active in each TF bin. Doing so, it leads to musical noise in the separated sources, caused by setting many TF bins to zero, contrary to the Wiener filter used in this study. The fact that musical noise is not well accounted for by BSSEVAL has already been noticed in the literature <ref type="bibr" target="#b48">[49]</ref>. Second, it is well known that Wiener filtering, which uses the phase of the mixtures, may lead to slight desynchronizations of the signals, that are not perceived perceptually by the human auditory system but that may cause signal to noise ratios to drop dramatically in some cases.</p><p>For these reasons, it was demonstrated by EMIYA et al. in <ref type="bibr" target="#b48">[49]</ref> that further evaluation metrics based on perceptual features may provide better assessment of separation quality. For that purpose, they introduce a toolkit for Perceptual Evaluation of Audio Source Separation (PEASS), available from their website. We therefore evaluated the separated sources with this toolkit <ref type="foot" target="#foot_3">14</ref> and obtained the results given in Fig. <ref type="figure">5</ref> (right), which are slightly better for the proposed method, especially when considering artifacts. If the technique proposed by PARVAIX handles well sources that have a sparse power spectrogram, it produces poorer estimates of sources with non sparse spectrograms, such as distorted or noisy sounds. On the contrary, the proposed method achieved good performance in every case.</p><p>Nevertheless, this evaluation also confirms that objective metrics for the evaluation of source separation is a delicate and open issue, especially for ISS where quality of the estimates is often very good. As recent studies showed that strong connections exist between source coding and ISS <ref type="bibr" target="#b49">[50]</ref>, evaluation for comparing different techniques in ISS may even need to switch from a source separation paradigm as is the case here to perceptual evaluations that are classical in source coding. Still, for the remaining of this study, we use BSSEVAL for evaluation, because within the same technique, it gives scores that match perceptive observations. In any case, the reader is encouraged to visit the webpage of this study and listen to the separated tracks himself (see footnote <ref type="bibr" target="#b12">13)</ref>.</p><p>Anyway, it must be reminded that state of the art is limited to linear instantaneous mixtures only, which is a serious drawback when considering practical applications that involve mixing done by a professional sound engineer. On the contrary, the proposed method proves to be efficient even for mixtures including reverberation of the sources or processed through digital dynamic compressors as demonstrated in the remaining of this evaluation. Comparison of the method of PARVAIX (red) with proposed method (blue). BSSEVAL (left) does not account for musical noise whereas PEASS (right) does. OPS, TPS, IPS and APS respectively stand for Overall/Target/Interference/Artifacts Perceptual Score. For all metrics, higher is better. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Impact of the embedding process</head><p>Since the ISS system presented in Section 1 and depicted in Fig. <ref type="figure" target="#fig_1">1</ref> can lead to several kinds of artifacts coming either from data embedding or imperfect Wiener filtering, two evaluations have been performed on the complete dataset. The goal of the first evaluation presented in this section is to assess the impact of the embedding process on separation quality. The second, presented in Section 5.4, is to compare the different strategies we proposed for encoding the spectrograms.</p><p>To evaluate the impact of embedding, we have performed Oracle separation, i.e. using the original spectrograms of the sources (see Section 3.3), on the original and embedded mixtures for various bitrates. Since embedding can only decrease performance, we computed the average loss in BSSEVAL criteria induced by embedding. Results are given in Fig. <ref type="figure">6</ref>, where each line corresponds to one of the 10 stereo signals of our dataset.</p><p>Several remarks can be made when considering results in Fig. <ref type="figure">6</ref>. First, the performance obtained by the Oracle configuration is very good. Perceptually, it is very hard to distinguish the extracted sources from the original. Second, we observe that compared to separation of the unembedded mixtures, the embedding process does not lead to any significant loss in performance up to bitrates of approximately 200 kbps for all signals and approximately 250 kbps for most signals. 15 Perceptually, it is very hard to notice any difference between the sources extracted through this method and the original source signals, which confirms that the proposed complete model is adequate for the ISS problem. Second, when very high bitrates are required for the embedding of the side-information, Oracle performance drops rapidly after 250 kbps. This occurs sometimes with the NTF method on mono mixtures for R¼150. These results are very similar to those obtained when assessing the perceptual quality of the embedded mixtures in <ref type="bibr" target="#b31">[32]</ref>. This suggests that above 250 kbps the mixtures are too degraded for both listening and source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison of image compression and dimension reduction</head><p>The second evaluation performed on the complete dataset concerns comparison of the different methods  <ref type="figure">6</ref>. Oracle performance without embedding (above) and loss of performance due to embedding (below) for all excerpts. Higher is better. 15 Loud music allows for more embedding.</p><p>we suggested in Section 3 to encode the spectrograms of the mixtures. The first of these techniques is dimension reduction-called NTF in the following-and has been presented in Section 3.4. The number of its parameters Y NTF is mainly controlled by the number R of components used in the tensor decomposition. The second method uses direct image compression of the spectrograms and it is called IC in the following. It has been presented in Section 3.5 and the size of its corresponding side information Y IC is controlled by the quality parameter of the lossy compression algorithm.</p><p>Both techniques were evaluated with various sizes for their corresponding side information. Since their performance is bounded above by the Oracle technique, we computed the difference between the results given by BSSEVAL on each case with the corresponding Oracle performance (with the same embedded mixtures). Results are given in Fig. <ref type="figure" target="#fig_8">7</ref>.</p><p>Considering Fig. <ref type="figure" target="#fig_8">7</ref>, we see that performance obtained by IC or NTF is directly controlled by the quality setting considered: the quality parameter of the image compression algorithm for IC and R for NTF. <ref type="foot" target="#foot_4">16</ref> This can be explained by the fact that sophisticated models permit very reliable estimates of the spectrograms whereas small models only permit crude modeling. Still, large models also lead to higher bitrates, and thus to a degradation of the performance as demonstrated in Section 5.3. This suggests that a trade-off is to be found between the improvement of spectrograms modeling through higher quality settings for side-information, and the corresponding loss in performance induced by embedding more data. For example, Fig. <ref type="figure" target="#fig_8">7</ref> shows that the loss in SDR induced by using IC rather than Oracle is lower than 1 dB at 200 kbps and Fig. <ref type="figure">6</ref> shows that the loss induced by embedding at this rate is negligible. It is not interesting to increase the side information rate to 300 kbps because the loss due to embedding in this case is higher than the gain induced by better encoding of the spectrograms.</p><p>In any case, we see that both techniques are very close to Oracle performance, which is very satisfactory. Furthermore, considering Fig. <ref type="figure">8</ref> on which boxplots <ref type="foot" target="#foot_5">17</ref> of the encoding/decoding times <ref type="foot" target="#foot_6">18</ref> over all experiments are displayed, we see that source separation at the decoder can be done extremely rapidly and does not require large computational resources. Whatever the chosen method, the complete system thus permits to successfully model the spectrograms of the sources and embed the corresponding information within the mixtures. The sources recovered through generalized Wiener filtering at the decoder are seen to be very well separated, with almost no interferences and only a very small amount of artifacts. Active listening applications such as karaoke or remastering are hence made possible with such a system on realistic mixtures. Now, comparing the respective performance of IC and NTF, we see that in our evaluation, IC always yields better results than NTF for a given bitrate and a given excerpt. However, this fact may be tempered by noting that the parameters for NTF are not optimally coded in this study, leading to an overestimation of the bitrate necessary to convey Y NTF . Future work may hence demonstrate better performance for NTF than obtained here, notably through appropriate compression of Y NTF . Still, considering encoding times given in Fig. <ref type="figure">8</ref> we see that IC performs approximately 10 times faster for coding than the NTF method. Even if the image compression method we used is implemented in an extremely mature and optimized library, contrarily to our Matlab implementation of NTF, this difference of encoding time between IC and NTF is mostly explained by the fact that the computations presented in Algorithm 1 are performed many times for NTF, thus leading to heavy computational costs, whereas JPEG encoding of a spectrogram only requires quantization of a cosine transform and can be performed very rapidly <ref type="bibr" target="#b40">[41]</ref>. Computational complexity of NTF is hence inherently much greater than IC. Altogether, spectrograms compression through the IC method leads to lower necessary bitrates, higher separation quality and lower computational costs. It is thus the preferred method to perform ISS. 19   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Informed source separation consists in providing valuable prior knowledge to a source separation algorithm. This study considered the case where this knowledge has been computed at an encodingstage where both the mixtures and the original sources are known, and then inaudibly embedded into the PCM mixtures through an adequate high-capacity data embedding technique. At the decoding stage, this side-information is extracted and separation is performed using the side-information.</p><p>The statistical framework presented in this study models the sources as locally stationary Gaussian processes that are mixed using linear FIR filters. We have shown that very reliable source separation can be performed in this case when the spectrograms of the sources are known at the decoder. Since these spectrograms are much too large matrices to be possibly embedded into the mixtures, several approaches were proposed to approximate them, including dimension reduction through tensor factorization and image compression.</p><p>In this study, we have performed a thorough evaluation of the proposed ISS method. The corpus we considered includes sources that are highly non-sparse in the frequency domain, such as distorted drums or guitars and mixtures that were obtained with professional DAW, including nonlinear compressions. These settings correspond to use cases that were not possibly handled by previous methods proposed for ISS. In any case, performance was extremely encouraging and the proposed model based on generalized Wiener filtering of the mixtures is hence adequate for the ISS problem and allows active listening applications of musical content such as karaoke or musical remixing.</p><p>Interesting features of the proposed statistical method for informed source separation are first that the embedding process at appropriate rate does not perceptually nor quantitatively lead to significant degradation during the source separation step and second that it makes it possible to use any algorithm suitable for image compression as a candidate for side-information. Possible extensions to this work would include synchronized data embedding to allow real-time separation at the decoder and embedding methods that are either robust to compression algorithms such as MP3 or that benefit from dedicated adjunct channels as for example defined in MPEG-SAC/SAOC <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. Preliminary experiments show that the proposed separation method is quite robust to audio compression of the mixture signals. Decoding Time Fig. <ref type="figure">8</ref>. Boxplots of encoding/decoding times over all experiments, depending on the method (IC or NTF). 19 Other lossy image compression techniques such as JPEG2000 have also been tested, but because of the lack of space, we decided to only report on JPEG, since it is a free, readily available algorithm. Note however that JPEG2000, based on wavelet transforms, exhibited better separation performance than JPEG for very low side-information bitrates: SDR, SAR and SIR were respectively approximately 0.3 dB, 1.5 dB and 1.5 dB better for JPEG2000 than for JPG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/sigpro Signal Processing 0165-1684/$ -see front matter &amp; 2011 Elsevier B.V. All rights reserved. doi:10.1016/j.sigpro.2011.09.016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•Fig. 1 .</head><label>1</label><figDesc>Fig.1. General architecture of an ISS method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Bm: is the mth row of matrix B, Sm is the observed spectrogram of source m, Ŝm ¼ W diagðQ m: ÞH is the estimated spectrogram of source m with current model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Canonical Polyadic model. The spectrograms of the sources are jointly modeled as an NTF model.</figDesc><graphic coords="6,123.50,454.76,325.18,78.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of estimated spectrogram using the JPEG algorithm with quality ¼ 10.</figDesc><graphic coords="6,123.50,575.00,325.12,78.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Example of QIM using a set of quantizers for Cðo,tÞ ¼ 2 with their respective gray code index and a global grid. The binary code 01 is embedded into the IntMDCT coefficient Xðo,tÞ by quantizing it to X w ðo,tÞ using the quantizer indexed by 01.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig.5. Comparison of the method of PARVAIX (red) with proposed method (blue). BSSEVAL (left) does not account for musical noise whereas PEASS (right) does. OPS, TPS, IPS and APS respectively stand for Overall/Target/Interference/Artifacts Perceptual Score. For all metrics, higher is better. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig.6. Oracle performance without embedding (above) and loss of performance due to embedding (below) for all excerpts. Higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Evaluation with embedding on the 15 excerpts of our corpus and with four quality settings for each excerpt and each technique. 0 corresponds to the Oracle performance in each case. Solid and dashed lines are for stereo and mono excerpts respectively.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>If V is a vector, diag V is the matrix whose diagonal elements are composed of the elements of V . If M is a matrix, diag M is the column vector containing the diagonal elements of M. A. Liutkus et al. / Signal Processing 92 (2012) 1937-1949</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1"><p>In<ref type="bibr" target="#b30">[31]</ref>, where the MDCT was used, robustness to the 16-bit PCM conversion of the embedded signal was also considered as a constraint. This is no more the case with the IntMDCT, since a 16-bit PCM embedded signal is directly generated. A. Liutkus et al. / Signal Processing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="92" xml:id="foot_2"><p>(2012) 1937-1949  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_3"><p>Very special thanks to A. OZEROV at IRISA (Rennes, France) that assisted us on this point.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_4"><p>In NTF, we used the same number of 60 iterations for Algorithm 1 for all cases in our evaluation, which may not be sufficient for very large R, explaining why high bitrates do not necessarily lead to better performance for NTF.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_5"><p>See e.g. http://en.wikipedia.org/wiki/Box_plot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_6"><p>Experiments were done on a Quad Core computer with 4 GB RAM.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>$ This work is partly funded by the French National Research Agency (ANR) as a part of the DReaM project (ANR-09-CORD-006-03) and partly supported by the Quaero Programme, funded by OSEO, French State agency for innovation.</p><p>n Corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Handbook of Blind Source Separation: Independent Component Analysis and Blind Deconvolution</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blind signal separation: statistical principles, Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="2009" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Independent Component Analysis</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Hyv Ärinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley and Sons</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis, The perceptual Organization of Sound</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian harmonic models for musical signal analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithm for the separation of harmonic sounds with time-frequency smoothness constraint</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Digital Audio Effects (DAFx-03)</title>
		<meeting>the Sixth Conference on Digital Audio Effects (DAFx-03)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An iterative approach to monaural musical mixture de-soloing</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP&apos;09)</title>
		<meeting>the IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP&apos;09)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Separation by humming&apos;&apos;: User guided sound extraction from monophonic mixtures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ryyn änen, Combining pitch-based inference and non-negative spectrogram factorization in separating vocals from polyphonic music</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA Tutorial and Research Workshop on Statistical and Perceptual Audition</title>
		<meeting>ISCA Tutorial and Research Workshop on Statistical and Perceptual Audition<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Source separation by score synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ganseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Abel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Computer Music Conference (ICMC&apos;2010)</title>
		<meeting>the International Computer Music Conference (ICMC&apos;2010)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blind separation of speech mixtures via timefrequency masking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1830" to="1847" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Under-determined reverberant audio source separation using local observed covariance and auditory-motivated time-frequency representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Latent Variable Analysis and Signal Separation, LVA/ICA&apos;10</title>
		<meeting>the Ninth International Conference on Latent Variable Analysis and Signal Separation, LVA/ICA&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse representations in audio and music: from coding to source separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="995" to="1005" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence. With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fe ´votte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general modular framework for audio source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ ICA 2010)</title>
		<meeting>Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ ICA 2010)<address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wiley Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised single-channel source separation using Bayesian NMF</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Workshop on Applications of. Signal Processing to Audio and Acoustics (WASPAA&apos;09)</title>
		<meeting>the 2009 IEEE Workshop on Applications of. Signal Processing to Audio and Acoustics (WASPAA&apos;09)<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="93" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gamma Markov random fields for audio source modelling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="589" to="601" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="793" to="799" />
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio source separation with a single sensor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="199" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prior structures for time-frequency energy distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA&apos;07)</title>
		<meeting>the 2007 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA&apos;07)<address><addrLine>New Paltz, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="151" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gaussian processes for underdetermined source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3155" to="3167" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A watermarking-based method for single-channel audio source separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parvaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Brossier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;09)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;09)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A watermarking-based method for informed source separation of audio signals with a single sensor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parvaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Brossier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1464" to="1475" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Informed source separation of underdetermined instantaneous stereo mixtures using source index embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parvaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Informed source separation of linear instantaneous under-determined audio mixtures by source index embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parvaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1721" to="1733" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Informed source separation using latent components</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2010)</title>
		<meeting>Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2010)<address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non negative sparse representation for Wiener based source separation with a single sensor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;03)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;03)<address><addrLine>Hong-Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="613" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multichannel nonnegative matrix factorization in convolutive mixtures for audio source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fe ´votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="563" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantization index modulation: a class of provably good methods for digital watermarking and information embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1423" to="1443" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A high-capacity watermarking technique for audio signals based on MDCT-domain quantization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Congress on Acoustics</title>
		<meeting>the 20th International Congress on Acoustics<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-rate data embedding in uncompressed music signals using QIM and IntMDCT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Digital Audio Effects Workshop (DAFx)</title>
		<meeting>Digital Audio Effects Workshop (DAFx)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Consistent Wiener filtering: generalized time-frequency masking respecting spectrogram consistency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mizuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2010)</title>
		<meeting>Ninth International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2010)<address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proper complex random processes with applications to information theory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Neeser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1293" to="1302" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic target mixing using least-squares optimization of gains and equalization settings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barchiesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Digital Audio Effects (DAFx-09)</title>
		<meeting>the 12th Conference on Digital Audio Effects (DAFx-09)<address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stability analysis of multiplicative update algorithms and application to nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Emmanuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1869" to="1881" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convergence-guaranteed multiplicative algorithms for non-negative matrix factorization with beta-divergence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2010)</title>
		<meeting>2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization with the beta-divergence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fe ´votte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Multidimensional Signal, Image, and Video Processing and Coding</title>
		<meeting><address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The JPEG still picture compression standard</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">IntMDCT-a link between perceptual and lossless audio coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brandenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;93)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;93)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">p. II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overview of MPEG audio: current and future standards for low bit-rate audio coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brandenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Perceptual coding of digital audio</title>
		<author>
			<persName><forename type="first">T</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="451" to="515" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Information technology Generic coding of moving pictures and associated audio information</title>
		<idno>ISO/IEC JTC1/SC29/WG11 MPEG</idno>
		<imprint/>
	</monogr>
	<note>Part 7: Advanced Audio Coding (AAC) (IS13818-7(E), 2004</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fe ´votte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The 2008 signal separation evaluation campaign: a community-based approach to large-scale evaluation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bofill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICA &apos;09: Proceedings of the Eighth International Conference on Independent Component Analysis and Signal Separation</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Comparison of subjective and objective evaluation methods for audio source separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kornycky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ünel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Kondoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meetings on Acoustics</title>
		<meeting>Meetings on Acoustics</meeting>
		<imprint>
			<publisher>ASA</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Subjective and objective quality assessment of audio source separation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Emiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hohmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASL.2011.2109381</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2046" to="2057" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Informed source separation: source coding meets source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA&apos;11)</title>
		<meeting><address><addrLine>Mohonk, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kj örling, J. Breebaart, colleagues, MPEG surround-the ISO/MPEG standard for efficient and compatible multi-channel audio coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AES 122nd Convention</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MPEG spatial audio object coding-the ISO/MPEG standard for efficient coding of interactive audio scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engdeg Ård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Falch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hellmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ölzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Purnhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Resch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Villemoes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Audio Engineering Society Convention</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
