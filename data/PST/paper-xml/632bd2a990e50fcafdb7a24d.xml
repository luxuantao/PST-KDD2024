<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENERATE RATHER THAN RETRIEVE: LARGE LANGU-AGE MODELS ARE STRONG CONTEXT GENERATORS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-21">21 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<country>IN</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
							<email>iterdan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cognitive Services Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cognitive Services Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cognitive Services Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingxuan</forename><surname>Ju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<country>IN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cognitive Services Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cognitive Services Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<country>IN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Microsoft Cognitive Services Research group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GENERATE RATHER THAN RETRIEVE: LARGE LANGU-AGE MODELS ARE STRONG CONTEXT GENERATORS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-21">21 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.10063v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GENREAD), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require access to a large amount of world or domain knowledge <ref type="bibr" target="#b37">(Petroni et al., 2021)</ref>. These tasks are even challenging for humans without access to an external knowledge source such as Wikipedia. A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from Wikipedia and then conditions the prediction of the answer on these documents along with the question <ref type="bibr" target="#b21">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b29">Lewis et al., 2020;</ref><ref type="bibr" target="#b17">Izacard &amp; Grave, 2021)</ref>. Modern dense retrieval models typically calculate similarity scores between question and document representations <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>. Nevertheless, these methods mainly suffer from three drawbacks. First, the representations of questions and documents are obtained independently, leading to only shallow interactions captured between them <ref type="bibr" target="#b22">(Khattab &amp; Zaharia, 2020;</ref><ref type="bibr" target="#b23">Khattab et al., 2021)</ref>. Second, the question or document representation is embedded into one single vector, potentially missing fine-grained information when computing the similarity between the two vector representations. Third, document retrieval over a large corpus requires the retriever model to first encode all candidate documents and store representations for each document. This operation is highly expensive, limiting the parameter size of dense retrievers, limiting the potential benefits of using large language models <ref type="bibr" target="#b28">(Levine et al., 2022)</ref>.</p><p>Early retrieval methods mainly employed sparse retrievers, such as BM25 <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>. Recently, ORQA <ref type="bibr" target="#b27">(Lee et al., 2019)</ref> and DPR <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref> have revolutionized the field by utilizing dense contextualized vectors for document indexing, leading to superior performance to traditional approaches. Such models can be trained using weak supervision on question-answer pairs or pre-trained using self-supervised tasks. For the reader, current state-of-the-art models such as FiD, leveraged encoder-decoder models to generate answers <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GENERATOR AS RETRIEVER FOR PRODUCING CONTEXTUAL DOCUMENTS</head><p>Recent works have investigated using auto-regressive language models to generate identifier strings for documents, as an intermediate target for retrievals, such as Wikipedia page titles <ref type="bibr" target="#b7">(De Cao et al., 2020)</ref>, root-to-leaf paths in a hierarchical cluster tree <ref type="bibr" target="#b45">(Tay et al., 2022)</ref>, or distinctive n-grams that can be mapped to full passages <ref type="bibr" target="#b2">(Bevilacqua et al., 2022)</ref>. As a downside, if appropriate metadata (e.g. titles) is not available, one needs to create the identifiers, hence the structure, which has not been thoroughly evaluated on a large-scale benchmark <ref type="bibr" target="#b2">(Bevilacqua et al., 2022)</ref>.</p><p>Another line of work has demonstrated that knowledge stored in the parameters of these models can be retrieved to some extent by directly generating evidence given a query <ref type="bibr" target="#b36">(Petroni et al., 2019;</ref><ref type="bibr" target="#b41">Roberts et al., 2020)</ref>. However, previous work proposes to only use generation for query expansion in traditional search engines <ref type="bibr" target="#b32">(Mao et al., 2021)</ref>, but these solutions do not exploit the full potential of auto-regressive architecture and still lag behind dense retrieval approaches <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>. Different from the above approaches that aimed to train a generator model to produce contextual document identifiers (which is still using the original Wikipedia text) or provide data augmentation to the retriever, our work directly generates a contextual document based on the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NLP MODELS ENHANCED BY LARGE LANGUAGE MODEL OUTPUTS</head><p>Large language models present a research breakthrough for natural language research, particularly for their state-of-the-art performance and impressive generative capabilities <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b6">Chowdhery et al., 2022)</ref>. A line of recent work has shown that relevant knowledge can be elicited from large language models, especially for those domains that lack appropriate knowledgebases with sufficient coverage <ref type="bibr" target="#b31">(Liu et al., 2022;</ref><ref type="bibr" target="#b11">Fang et al., 2022;</ref><ref type="bibr">Yu et al., 2022b)</ref>. For example, <ref type="bibr" target="#b31">Liu et al. (2022)</ref> proposed leveraging GPT-3 to generate relevant contexts, then providing the contexts as additional input when answering a question. Another line of work focused on prompting a large language model to generate a series of intermediate reasoning steps, often referred to as chain-of-thought <ref type="bibr">(Wei et al., 2022b;</ref><ref type="bibr" target="#b24">Kojima et al., 2022;</ref><ref type="bibr" target="#b30">Li et al., 2022)</ref>. The prompt consists of an instruction (e.g., Let's think step by step!), a few demonstrations that are fixed for each task, and a new-question placeholder. The demonstrations are human-written, and each consists of a question in the style of the task and a series of intermediate reasoning steps that is helpful for answering the question.</p><p>Our work is different from the above works in two aspects. First, all existing work used large language models to generate intermediate reasoning steps for commonsense reasoning tasks. Instead, we aim to "retrieve" relevant knowledge from large language models, because of the nature of knowledge-intensive tasks which require large amounts of world or domain knowledge. Second, instead of generating one output of reasoning process, the main challenge of our task lies in retrieving diverse documents with rich knowledge, hence we proposed a clustering-based prompt method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this section, we present details of our proposed novel Generate-then-Read (GENREAD) pipeline for solving various knowledge-intensive tasks. Specifically, it first prompts a large language model (e.g., GPT-3) to generate contextual documents with respect to a given query, then reads the generated documents to predict the final answer. The reader can either be a large model (e.g., GPT-3) used for the zero-shot setting, or a small one (e.g., FiD) fine-tuned with generated documents on the training split of the target dataset. We introduce the zero-shot setting in ?3.1 and supervised setting in ?3.2. </p><formula xml:id="formula_0">Q 1 ,D 1 ;?Q 5 ,D 5 ; Large Lang- uage Model Q 1 ,D 1 ;?Q 5 ,D 5 ; Q 1 ,D 1 ;?Q 5 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ZERO-SHOT SETTING</head><p>Under the zero-shot setting, there is no training data -neither questions nor contextual documents.</p><p>When tested on the open-domain QA task, most existing large language models directly encode the given question and predict the answer <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b9">Du et al., 2022;</ref><ref type="bibr" target="#b6">Chowdhery et al., 2022)</ref>. Specifically, the question q, associated with some text prompt, is input to the model, which then generates the answer, denoted as p(a|q, ?), where ? represents the pre-trained model parameters. In practice, the maximum a posteriori estimation (MAP) is the final answer, i.e., ? = arg max a p(a|q, ?). However, this way of directly asking large language models to output answers often leads to poor performance, as it leaves a considerable amount of additional world knowledge unexploited <ref type="bibr" target="#b28">(Levine et al., 2022)</ref>. On the contrary, the zero-shot retrieve-then-read pipeline first uses an off-the-shelf retriever to fetch relevant documents from an external knowledge source such as Wikipedia, then asks the large language model to read the documents and predict the answer.</p><p>In this work, we improve the performance by introducing an additional auxiliary generated document variable d, and then extend the model to have the form p(a, d|q) = p(a|d, q)p(d|q), where each condition is computed using a large language model such as GPT-3 which includes its conditional variables as part of its input. In practice, we cannot sum over all possible documents d. Therefore, the most common approach is to compute the MAP estimate d = arg max p(d) using beam search, and then to approximate the sum over d with this single value. This two step approach, which we label as generate-then-read is described in detail below.</p><p>STEP1: GENERATE. In this step, we first prompt a large language model to generate documents based on the given question. For example, the input to the language model could be "Provide a background document from Wikipedia to answer the given question. {a question placeholder}". We can use any decoding strategy (e.g., greedy decoding, beam search), but we used greedy decoding throughout the zero-shot experiments for simplicity and reproducibility.</p><p>STEP 2: READ. In the second step, we use generated sentence d along with the input question to produce the final answer from the large language model. This is actually the same setting as "zeroshot" reading comprehension, as widely studied in existing works <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b26">Lazaridou et al., 2022)</ref>. Concretely, we first choose relevant prompts from P3 <ref type="bibr" target="#b0">(Bach et al., 2022)</ref>, which includes over 2,000 open-source prompts for roughly 170 datasets. Finally, the language model is fed the prompted text to generate the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SUPERVISED SETTING</head><p>Although large language models demonstrate impressive performance on zero-shot learning abilities, their performance still lag behind the supervised setting. Therefore, we also explore how the generated documents from large language models can benefit the supervised setting. As directly fine-tuning large language models on downstream datasets could be prohibitively expensive, we leverage a small reader model to peruse the generated documents under the supervised setting. A small reader model can also encode a much longer sequence through the fusion-in-decoder manner, which could be more than 20,000 tokens <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>.</p><p>Under the supervised setting, scaling the size of retrieved documents can lead to better performance <ref type="bibr" target="#b21">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b17">Izacard &amp; Grave, 2021)</ref>. This is mainly because retrieving more documents can cover more relevant information and knowledge, i.e., a higher recall score. Nevertheless, asking a large language model to generate multiple high-quality contextual documents is a challenging task. Dense retrieval methods can fetch multiple documents covering different perspectives of the answer. Compared to dense retrievers, simply prompting a large language model to generate contextual documents often leads to low knowledge coverage, mainly due to the high coincidence of token distributions in the decoding process under a single prompt. Though sampling decoding methods, such as nucleus sampling <ref type="bibr" target="#b15">(Holtzman et al., 2020)</ref> can diversify the generation process to some extent, the knowledge content of generated texts tends to be highly repetitive when used to generate documents for question answering. We further propose two novel solutions, including diverse human prompts and clustering-based prompts, which will be elaborated on in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">NUCLEUS SAMPLING</head><p>Sampling-based methods are one of the dominant approaches to increasing the diversity of generated documents <ref type="bibr" target="#b15">(Holtzman et al., 2020)</ref>. Nucleus sampling is one of the state-of-the-art sampling methods that focus on the smallest possible sets of high-likelihood words, denotes as V, such that the sum of their probability is larger than p, where p is a hyperparameter. Then, the vocabulary that are not in V (p) are set to 0; the rest are re-scaled to ensure that they sum to 1. We refer to more details in the nucleus sampling paper <ref type="bibr" target="#b15">Holtzman et al. (2020)</ref>. In the experiments, we set p = 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">DIVERSE HUMAN PROMPTS</head><p>In order to avoid similar token distributions under a single prompt, we ask human annotators to provide different prompts, in order to make the generated document diverse. This method is simple, but can effectively vary the token distribution during generation. In the experiments, we empirically found this method can bring improvement to the retrieval performance. However, this method suffers from two drawbacks. On one hand, it requires human annotators to write different prompts, which cannot be easily generalized to different knowledge-intensive tasks. On the other hand, different large language models might be sensitive to different prompt words, which might cause a set of good prompt words not work on a different large language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">CLUSTERING-BASED PROMPTS</head><p>To better increase the knowledge coverage in generated documents, we propose a novel clusteringbased prompt method. It first clusters the representations of a set of documents, where the number of classes is equal to the number of documents that need to be generated in the end. Next, it randomly selects k documents from each cluster. Lastly, a large language model presents the different k document sets as in-context demonstrations for generating documents to a given question. In this way, large language models are based on different distributions of examples, hence resulting in documents covering different perspectives. We show this in Figure <ref type="figure" target="#fig_0">1</ref> illustrates the details of each step as follows.</p><p>STEP 1: RETRIEVE OR GENERATE ONE DOCUMENT PER QUESTION. Similar to the zero-shot setting, we first ask a large language model to generate one contextual document for each given question. As for the supervised setting, we use the training set to do this. Alternatively, we can use an unsupervised retriever (e.g., BM25) to obtain a document from Wikipedia.</p><p>STEP 2: USE K-MEANS TO CLUSTER THE DOCUMENTS. We then use a large language model (i.e., GPT-3) to encode the generated documents, which is a 12,288-dimensional vector per document. Then, we use K-means to cluster the documents into K sets, in which K is the number of documents to be generated. We vary the number of K in the experiments, which will be illustrated in Figure <ref type="figure">2</ref>.</p><p>STEP 3: SAMPLE AND GENERATE K DOCUMENTS. Lastly, we sample n documents from each cluster, in which n is a hyperparameter<ref type="foot" target="#foot_0">1</ref> . Then, the large language model generates a document for each given question based on different selected document examples. In other words, the n sampled documents from the same cluster serve as in-context demonstrations for the large language model to generate a contextual document for the given question. By enumerating the sampled documents in these K clusters, we can finally get K-generated documents. By conditioning on different sampled in-context demonstrations collected from different clusters, the language model has been biased for different perspectives. Although these perspectives exist in a latent manner, we empirically show it works well in practice by comparing with sampling methods, diverse human prompts (Figure <ref type="figure">2</ref> and Table <ref type="table">2</ref>) and just randomly sampling n documents from the entire dataset (  <ref type="bibr" target="#b47">(Wei et al., 2021)</ref>) under this setting without using any external document. Our GENREAD can achieve comparable or even better performance than zero-shot retrieve-then-read models that use a retriever or search engine to first obtain contextual documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct comprehensive experiments on three knowledge-intensive NLP tasks, including open-domain QA (NQ <ref type="bibr" target="#b25">(Kwiatkowski et al., 2019)</ref>, TriviaQA <ref type="bibr" target="#b20">(Joshi et al., 2017)</ref> and WebQ <ref type="bibr" target="#b1">(Berant et al., 2013)</ref>), fact checking (FEVER <ref type="bibr" target="#b46">(Thorne et al., 2018)</ref> and FM2 <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref>) and open-domain dialogue system (WoW <ref type="bibr" target="#b8">(Dinan et al., 2019)</ref>). We explore the same dataset splits (denoted as open split in the experimental tables) for the open-domain QA setting as used by <ref type="bibr" target="#b21">Karpukhin et al. (2020)</ref>; <ref type="bibr" target="#b17">Izacard &amp; Grave (2021)</ref>. For the FEVER and WoW datasets, we use the dataset splits from KILT challenge <ref type="bibr" target="#b37">(Petroni et al., 2021)</ref>. For the FM2 dataset, we use its official dataset splits. More detailed dataset information can be found in Appendix A.1.</p><p>To evaluate the model performance, we use exact match (EM) score for evaluating open-domain QA <ref type="bibr" target="#b56">(Zhu et al., 2021</ref>). An answer is considered correct if and only if its normalized form<ref type="foot" target="#foot_1">2</ref> has a match in the acceptable answer list. We also employ Recall@K (R@K) as an intermediate evaluation metric, measured as the percentage of top-K retrieved or generated documents that contain the answer. This metric is commonly used in evaluations of previous works <ref type="bibr" target="#b21">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b16">Izacard &amp; Grave, 2020;</ref><ref type="bibr" target="#b43">Sachan et al., 2022)</ref>. For other knowledge-intensive tasks, we follow the KILT benchmark <ref type="bibr" target="#b37">(Petroni et al., 2021)</ref> to use accuracy (ACC) as the primary metric for fact checking and F1 / Rouge-L (R-L) score as the primary metric for open-domain dialogue system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ZERO-SHOT SETTING EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">BASELINE METHODS</head><p>We first compare our proposed GENREAD approach with various large language models proposed in recent years, including GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020</ref><ref type="bibr">), Gppher (Rae et al., 2021)</ref>, FLAN <ref type="bibr" target="#b47">(Wei et al., 2021)</ref>, GLaM <ref type="bibr" target="#b9">(Du et al., 2022)</ref>, Chinchilla <ref type="bibr" target="#b14">(Hoffmann et al., 2022)</ref>, PaLM <ref type="bibr" target="#b6">(Chowdhery et al., 2022)</ref> and InstrcutGPT <ref type="bibr" target="#b35">(Ouyang et al., 2022)</ref>. Due to the space limitation, we only put the best performance on each dataset in Table <ref type="table" target="#tab_0">1</ref>, in which the line is called previous SoTA methods. In addition, their corresponding model parameters and performance are listed in  Figure <ref type="figure">2</ref>: Recall@K score on test sets, measured as the percentage of top-K retrieved or generated documents that contain the answer. Our proposed clustering-based prompting method can outperform DPR and Google search, also two variants of using LLMs to generate contextual documents.</p><p>GENREAD , i.e., [prompt words; contextual document; question]. BM25 is a traditional sparse retrieval method. Contriever <ref type="bibr">(Izacard et al., 2022a</ref>) is a state-of-the-art unsupervised dense retrieval model. DPR <ref type="bibr" target="#b21">(Karpukhin et al., 2020</ref>) is a supervised dense retrieval model directly trained on NQ, TriviaQA and WebQ datasets. We note that comparing with above three methods is challenging because our method only relies on the large language model itself, without using any external corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">EXPERIMENTAL RESULTS</head><p>In the experiments, we use InstructGPT as our backbone model. As shown in Table <ref type="table" target="#tab_0">1</ref>, compared with state-of-the-art large language models, our proposed GENREAD with the InstructGPT reader improves its performance by generating contextual documents and conditioning on the generated documents, even though no new data is introduced, and the generator and reader have the exact same parameters. Specifically, GENREAD can improve the EM score by +6.9 on three open-domain QA benchmarks, compared to the original InstructGPT. We also make a similar observation on fact checking and open-domain dialogue system. Our proposed GENREAD can consistently outperform the baseline InstructGPT model without retrieving any contextual documents.</p><p>To further validate the effectiveness of GENREAD , we compare against zero-shot retrieve-then-read pipeline models, which first use a retrieval model or the Google search engine to get a relevant contextual document, then use InstructGPT to read the texts and produce the final answer. As shown in Table <ref type="table" target="#tab_0">1</ref>, GENREAD can achieve on-par performance with zero-shot retrieve-then-read pipeline models on the NQ and FM2 datasets, and outperform them on all other benchmarks. The knowledge learned by the large language models can be retrieved via autoregressive text generation. Without seeing any examples from these datasets, GENREAD can outperform using the supervised retrieval model (i.e., DPR) to recover relevant contextual documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SUPERVISED SETTING KNOWLEDGE-INTENSIVE TASKS</head><p>The supervised setting uses training data to train the "reader" model. Unlike zero-shot, where there is only an inference phase, this setting can use both the question-answer pairs and external knowledge for each dataset to train a reader specific to each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">BASELINE METHODS</head><p>We compare our proposed GENREAD with two groups of baselines. The first group includes closedbook models, where no external supporting document is provided during both training and inference: T5-11B <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> and <ref type="bibr">EaE (F?vry et al., 2020)</ref>. The second group contains models under the retrieve-then-read pipeline: ORQA <ref type="bibr" target="#b27">(Lee et al., 2019)</ref>, REALM <ref type="bibr" target="#b13">(Guu et al., 2020)</ref>, DPR <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>, RAG <ref type="bibr" target="#b29">(Lewis et al., 2020)</ref>, and FiD <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>. These methods first employ a retriever to retrieve a handful of relevant documents with respect to a given question from a large collection of documents such as Wikipedia, and then a reader to infer a final answer from the received documents. We also compared obtaining relevant documents from the internet using the Google Table <ref type="table">2</ref>: Supervised open-domain QA performance. By only using generated documents from GPT-3, our GENREAD with FiD reader (named GENREAD (FiD)) can achieve better performance than baseline methods on TriviaQA and WebQ. Through our detailed analysis of NQ, we found the performance gap mainly due to the temporality and label incompleteness, elaborated in ?4.2.4.</p><p>search engine. We exclude recently proposed reader models such as UnitedQA <ref type="bibr" target="#b5">(Cheng et al., 2021)</ref> and KG-FiD <ref type="bibr">(Yu et al., 2022a)</ref>, because We believe that our approach is orthogonal to reader models.</p><p>Employing more advanced readers in our framework is likely to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">EXPERIMENTAL SETUP</head><p>For our proposed method, we replace the retriever with a large language model to directly generate contextual documents. In the experiments, we use InstructGPT <ref type="bibr" target="#b35">(Ouyang et al., 2022)</ref>. After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.</p><p>We note that we only use 10 documents during reading for the following reasons.</p><p>WHY DO WE CHOOSE TO USE ONLY 10 DOCUMENTS INSTEAD OF 100 WHEN READING?</p><p>As noted in Section 6.2 in DPR <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref> and Figure <ref type="figure">3</ref> in FiD <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>, increasing the number of documents can lead to better model performance and achieve state-of-the-art when using 100 documents. However, there are two major drawbacks to using 100 documents during the reading step. First, the operation is very expensive, leading to a significant increase in memory consumption and training time. As reported by <ref type="bibr" target="#b17">Izacard &amp; Grave (2021)</ref>, the training process requires 64 Tesla V100 32GB running for around one day. Second, generating documents by using a large language model is slow and expensive, so only using 10 documents can be a significant cost saving in our method. Therefore, in our experiments, we choose to use 10 documents during the reading process. When using FiD-770M (i.e., FiD-large), the training process can be easily performed even on a single Tesla V100 32GB GPU. Meanwhile, when only using 10 documents, we can also increase the size of FiD model from 770M to 3B, which takes about the same amount of GPU memory as using 100 documents on a 770M model, but at the same time significantly shortens the training time.</p><p>We note that training T5-3B model needs a bigger cluster such as 8 Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">EXPERIMENTAL RESULTS ON OPEN-DOMAIN QA</head><p>We first use Recall@K to compare the retrieval accuracy of different models. As shown in Figure <ref type="figure">2</ref>, GENREAD can significantly outperform DPR and Google search for under 10 retrieved or generated documents. Compared to different GENREAD variants, including nucleus sampling, human written prompts, and clustering-based prompts, clustering-based prompts achieve the best performance. At the same time, we notice that the language model inevitably has the problem that the slope of the curve decreases as the number of generated documents increases. On one hand, this is due to the similarity of token distributions when large language models generate multiple documents. On the other hand, due to the shallow interaction characteristics of the dense retrieval model itself, the retrieved documents might not be completely relevant to the given question, so that the increase in recall might come from false positive documents, as also mentioned by <ref type="bibr" target="#b43">Sachan et al. (2022)</ref>.</p><p>We then use EM score to evaluate the answer accuracy, which is shown in Table <ref type="table">2</ref>. First, we can observe the FiD model performs the best among all baseline models. Using FiD-xl with only 10 documents achieves comparable performance with using FiD-l with 100 documents. The average gap is less than 1% on three benchmarks. Compared with both close-book models and Wikipediabased retrieve-then-read pipelines, our proposed GENREAD can achieve state-of-the-art performance. Furthermore, compared with using sampling methods to generate documents, the clustering-based prompt method can improve the EM score by +2.2 on average. This indicates that the clustering-based prompt method is effectively increasing the knowledge coverage of generated documents, and also leading to better downstream QA performance. We also show that GENREAD can outperform Google search on all benchmarks. We observe both our method and Google search perform worse than DPR, mainly due to the significant portion of time-dependent questions in the dataset, which is described in the following analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">ERROR ANALYSIS ON THE NQ DATASET</head><p>As stated in <ref type="bibr" target="#b54">Zhang &amp; Choi (2021)</ref>, NQ contains a significant proportion, roughly 16.5%, of questions that have time-dependent answers. Given the up-to-date answers, our GENREAD outperforms the reported performance by about 3.5, reaching 49.1. We also analyze the answers of Google search and have similar observations. We provide case studies in Appendix 19. Similarly, <ref type="bibr">Izacard et al. (2022b)</ref> observed using the latest version of Wikipedia (12 / 2021) could lead to 4.4 drops of the EM score, compared to the Wikipedia version (12 / 2018) that the NQ questions are created from.</p><p>Besides, some answer labels provided in the NQ dataset are not complete. For example, person names in the NQ dataset usually consist of first, middle, and last names, but most names in the generated documents are first and last names. For the question "who played lionel in as time goes by?", the labeled answer is "Geoffrey Dyson Palmer". DPR-FiD produces "Geoffrey Dyson Palmer" but GENREAD produces "Geoffrey Palmer", both of which should be considered correct. We manually checked 100 examples and found that 7 of the DPR-FiD outputs were incorrectly evaluated, while 14 of the GENREAD outputs were incorrectly evaluated. As the temporal effects and incomplete answer issue on the NQ dataset have been observed in many other recent papers <ref type="bibr">(Min et</ref>   <ref type="table"></ref>and<ref type="table">FM2</ref>) and open-domain dialogue system (WoW).</p><p>We demonstrate the experimental results in Table <ref type="table" target="#tab_3">3</ref>. Under the supervised setting, GENREAD can achieve on par performance on the fact checking task and superior performance on the dialogue system task, indicating that large language model can be seen as a strong knowledge generator.</p><p>The main reason that GENREAD performs worse than the dense retriever for fact checking is that the task provides sufficient semantic information to reach strong performance on this binary decision task. So, Figure <ref type="figure">3</ref>: Combining DPR retrieved documents and large language model (LLM) generated documents can achieve significantly better performance than using DPR retrieved documents only. For a fair comparison, instead of adding LLM generated documents to the model, we replace 10 documents retrieved by DPR with 10 documents generated by LLM so the total number of documents is the same.</p><p>In this experiment, we use FiD-l (i.e., FiD-large) as the reader model because when the documents scale to more than 20, FiD-xl (i.e., FiD-3B) causes out-of-memory issues on A100 GPUs.</p><p>there is a smaller semantic gap between the given factual statement and contextual documents than that of question and document pairs in open-domain QA and dialogue system tasks, which is an easier retrieval setting for modern dense retrieval methods that are mainly based on vector similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OBSERVATIONS AND EXPERIMENTAL ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">COMPLEMENTARITY OF GENERATED AND RETRIEVED DOCUMENTS</head><p>Generated documents can be combined with retrieved documents to outperform both. Even with a very large number of retrieved documents, including few samples of generated knowledge leads to large improvements. As shown in Table <ref type="table">2</ref>, merging retrieved documents with generated documents can achieve state-of-the-art performance compared to all baseline methods listed in the table. Specifically, it can improve +5.7 averagely on three open-domain QA benchmarks compared to DPR alone, and improve +4.4 averagely compared to the large language model alone.</p><p>In addition, we show the performance when scaling the number of DPR retrieved documents with a fixed number of generated documents (i.e., 10) in Figure <ref type="figure">5</ref>. We can observe that merging retrieved documents with generated documents can consistently outperform DPR documents alone on all three open-domain QA benchmarks, suggesting that the two are strongly complementary.  <ref type="table" target="#tab_4">14</ref><ref type="table" target="#tab_6">15</ref><ref type="table" target="#tab_0">16</ref><ref type="table" target="#tab_7">17</ref>.</p><p>The improvement in open-domain QA performance is due to the fact that correct answers are included more frequently in the generated text than in the retrieved documents. Recall@K is the most commonly used metric in existing works to measure the retrieval performance, which computes the percentage of top-K retrieved or generated documents that contain any possible answer at least once. However, as many questions contain multiple correct answers, recall@K cannot fully reflect the diversity of generated or retrieved documents. Each question in the WebQ dataset has 2.39 correct answers, 1.79 correct answers in NQ and 14.02 (including all entity alias) in the TriviaQA dataset. NQ and WebQ do not include alias names in the labels.</p><p>In this section, we also demonstrate the answer coverage performance of different models in  <ref type="bibr" target="#b55">(Zhang et al., 2022)</ref> 62.1 51.8 GPT-3 <ref type="bibr" target="#b35">(Ouyang et al., 2022)</ref> 71.3 54.5 Codex <ref type="bibr">(OpenAI, 2022)</ref> 72.6 55.4</p><p>Table <ref type="table" target="#tab_6">5</ref>: Exact match (EM) score with using DPR and different open-source large language models such as OPT and Codex to generate contextual documents.</p><p>We note that reproducing experiments on the GPT-3 API, though publicly available, costs money. For this reason, we further add an evaluation on two open-source large language models OPT <ref type="bibr" target="#b55">(Zhang et al., 2022)</ref> and <ref type="bibr">Codex (OpenAI, 2022)</ref>. As shown in  Figure <ref type="figure" target="#fig_3">4</ref> shows the scaling of performance with GPT-3 generator parameters, including Ada-150M, Babbage-1.3B, Curie-6.7B and Davinci-175B. We note that for both FiD and our GEN-READ , we use the FiD-xl with 10 input documents either retrieved from Wikipedia or generated by GPT-3. The performance of both Triv-iaQA and WebQ continues to improve as the generator model parameters increase, as does the slope. Besides, only with the largest GPT-3 model, GENREAD can outperform the dense retrieval model. This indicates using large language model to generate contextual documents is an "emergent ability" of scaling, which is not present in smaller models but is present in larger models <ref type="bibr">(Wei et al., 2022a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we present a novel perspective for solving knowledge-intensive tasks (e.g., open-domain QA) by replacing the dense retrieval models with large language model generators. We call it generate-then-read (GENREAD), which first prompts a large language model to generate contextual documents based on a given question, then read the generated document to produce the final answer. We perform evaluations on three different knowledge-intensive tasks, and address the challenge of low knowledge coverage, showing that it can be improved by our proposed novel clustering-based prompt method. Notably, GENREAD reaches 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the current retrieval-reader model DPR-FiD by +4.0 and +3.9, even without retrieving any documents from external knowledge sources such as Wikipedia. We demonstrate this performance can be further improved by fusing retrieval and generation.  <ref type="table">2</ref>), fact checking and open-domain dialogue system (numbers reported in Table <ref type="table" target="#tab_3">3</ref>). The upper part numbers are from GENREAD (FiD-l) and the lower part numbers are from GENREAD (FiD-xl). (c) WebQ-test (retrieval)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Figure <ref type="figure">5</ref>: Additional retrieval performance evaluation of experiments on combining DPR retrieved documents and large language model generated documents. Merging documents from two sources achieved significantly better performance than using DPR retrieved documents only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>TriviaQA WebQ NQ R@1 R@10 R@20 R@1 R@10 R@20 R@1 R@10 R@20 BM25 <ref type="bibr" target="#b42">(Robertson et al., 2009)</ref> 46.2 71.7 76.4 19.1 51.8 62.6 22.8 55.6 63.9 Contriever <ref type="bibr">(Izacard et al., 2022a)</ref> 34.0 67.9 74.3 18.2 55.7 65.7 18.8 54.8 65.1 DPR <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref> 53.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 READABILITY ANALYSIS</head><p>After we manually compare some retrieved documents from DPR and generated documents from GPT-3, we observe that the readability of different documents, when they contain the correct answer string, is different. In other words, documents that contain answers may contain a certain amount of noisy information that affects both the model and human reading.</p><p>In order to further validate the readability of retrieved documents and generated documents, we extracted a subset of data examples from NQ, TriviaQA and WebQ, in which both retrieved and generated documents hit the correct answer. As shown in Table <ref type="table" target="#tab_11">11</ref>, when both retrieved and generated documents hit the correct answer, the FiD reader can produce more correct answers when reading the generated documents from large language models.</p><p>We also provide some case studies in Table <ref type="table" target="#tab_4">14</ref> to Table <ref type="table" target="#tab_7">17</ref>. For example, in Table <ref type="table" target="#tab_0">16</ref>, the question is "What city was Zeus the patron god of?". The first document retrieved by DPR is "Many were specific only to a particular deity or city-state. For example, the festival of Lykaia was celebrated in Arcadia in Greece, which was dedicated to the pastoral god Pan. Like the other Panhellenic Games, the ancient Olympic Games were a religious festival, held at the sanctuary of Zeus at Olympia.". Although the document contains the correct answer span, it is hard to read and find it. On the contrary, GPT-3 generates the document "Zeus was the patron god of the city of Olympia, which was located in the northwestern Peloponnese region of Greece. Olympia was the site of the Olympic Games, which were held every four years in honor of Zeus.", which is much easier to read.</p><p>No </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HUMAN PROMPT ANNOTATIONS</head><p>In order to get a better prompt for large language models to generate better contextual documents, we asked 30 students in the computer science department to write different prompts. We first constructed a small validation set with 200 examples by combining 50 random question-answer pairs from NQ, 100 random pairs from TriviaQA and 50 random pairs from WebQ. When an annotator wrote down a prompt, our system can immediately evaluate the prompt by using the validation set and return the performance to the annotator. Then, the annotator can modify the previous prompt until the recall performance reaches a threshold, which is set as 50 in our experiments. Finally, we got 29 prompts from human annotators due to two of them are the same. We used the top-10 prompts (shown in Among top-10 generated documents: opera: 2 times chamber music: 7 times classical music: 1 time Table <ref type="table" target="#tab_4">14</ref>: Case study of retrieved documents and generated documents from language models (1/4).</p><p>Query: What city was Zeus the patron god of? Answers: Olympia, Elis DPR #1: Many were specific only to a particular deity or city-state. For example, the festival of Lykaia was celebrated in Arcadia in Greece, which was dedicated to the pastoral god Pan. Like the other Panhellenic Games, the ancient Olympic Games were a religious festival, held at the sanctuary of Zeus at Olympia. Other festivals centred on Greek theatre, of which the Dionysia in Athens was the most important. More typical festivals featured a procession, large sacrifices and a feast to eat the offerings, and many included entertainments and customs such as visiting friends, wearing fancy dress and unusual behaviour in the ... (Wikipedia id: 1945872 -"Ancient Greek religion") DPR #2: Theatre of Dionysus The Theatre of Dionysus Eleuthereus is a major theatre in Athens, considered to be the world's first theatre, built at the foot of the Athenian Acropolis. Dedicated to Dionysus, the god of plays and wine (among other things), the theatre could seat as many as 17,000 people with excellent acoustics, making it an ideal location for ancient Athens' biggest theatrical celebration, the Dionysia. It was the first theatre ever built, cut into the southern cliff face of the Acropolis, and supposedly the birthplace of Greek tragedy. The remains of a restored and redesigned Roman version can still ... (Wikipedia id: 3732515 -"Theatre of Dionysus") Among top-10 retrieved documents: Olympia: 1 time GPT nucleus #1: Zeus was the patron god of the city of Olympia in Greece.</p><p>GPT nucleus #2: The city of Olympia in Greece was the site of the ancient Olympics, and it was also the home of Zeus, the patron god of the Olympics. Among top-10 generated documents: Olympia: 6 times GPT cluster #1: Zeus was the patron god of the city of Olympia, which was located in the northwestern Peloponnese region of Greece. Olympia was the site of the Olympic Games, which were held every four years in honor of Zeus.</p><p>GPT cluster #2: Zeus was the patron god of the city of Olympia. Olympia was a sanctuary of ancient Greece in Elis on the Peloponnesos peninsula, in south-western Greece. The sanctuary was known as the Panhellenic Sanctuary of Zeus, and the altar of Zeus was located in the Temple of Zeus. The Olympic Games were held in Olympia every four years from 776 BC to 393 AD.</p><p>GPT cluster #3: Zeus was the patron god of the city of Olympia. Among top-10 generated documents: Olympia: 9 times Elis: 1 time Table <ref type="table" target="#tab_0">16</ref>: Case study of retrieved documents and generated documents from language models (3/4).</p><p>Query: What is the collection of the districts to the east of the Jordan River? Answers: Jordan, Golan Heights DPR #1: Water politics in the Jordan River basin Water politics in the Jordan River basin refers to political issues of water within the Jordan River drainage basin, including competing claims and water usage, and issues of riparian rights of surface water along transnational rivers, as well as the availability and usage of ground water. Water resources in the region are scarce, and these issues directly affect the five political subdivisions (Israel, the West Bank, Lebanon, Syria and Jordan) located within and bordering the basin, which were created since the collapse, , during World War I, of the former single controlling entity, the ... (Wikipedia id: 12822413 -"Water politics in the Jordan River basin") DPR #2: Jordan's western border follows the bottom of the rift. Although an earthquake-prone region, no severe shocks had been recorded for several centuries. By far the greatest part of the East Bank is desert, displaying the land forms and other features associated with great aridity. Most of this land is part of the Syrian Desert and northern Arabian Desert. There are broad expanses of sand and dunes, particularly in the south and southeast, together with salt flats. Occasional jumbles of sandstone hills or low mountains support only meager and stunted vegetation that ... (Wikipedia id: 210052 -"Geography of Jordan") Among top-10 retrieved documents: Jordan: 8 times GPT nucleus #1: The collection of the districts to the east of the Jordan River is known as the West Bank. The West Bank is a landlocked territory situated in Western Asia. It is bordered by Jordan to the east, Egypt to the southwest, and Israel to the west and north.</p><p>GPT nucleus #2: The collection of the districts to the east of the Jordan River is the West Bank. The West Bank is a landlocked territory near the Mediterranean coast of Western Asia, bordered by Lebanon to the north, Syria to the northeast, Jordan to the east, and Egypt to the southwest. It is the smaller of two political entities that comprise the Palestinian territories, the other being the Gaza Strip. Among top-10 generated documents: Jordan: 10 times GPT cluster #1: The collection of the districts to the east of the Jordan River is known as the West Bank. The West Bank is a landlocked territory situated in Western Asia. It is bordered by Jordan to the east, Egypt to the southwest, and Israel to the west and north GPT cluster #2: The Jordan River is a major river in the Middle East, the source of which is in the northern part of Israel. The river flows southward through Israel and then Jordan, emptying into the Dead Sea. East of the river is the collection of districts known as the East Bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT cluster #3:</head><p>There is no single answer to this question as the east bank of the Jordan River is home to a number of different districts and regions, each with its own unique history, culture, and customs. However, some of the more well-known districts on the east bank include the West Bank, the Gaza Strip, and the Golan Heights. Among top-10 generated documents: Jordan: 10 times Golan Heights: 2 times Table <ref type="table" target="#tab_7">17</ref>: Case study of retrieved documents and generated documents from language models (4/4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: GENREAD leverages distinct question document pairs (Q 1 ,D 1 ;...Q 5 ,D 5 ) sampled from each embedding cluster (in different colors) as in-context demonstrations to prompt a large language model to generate diverse documents (Doc1 to Doc3), then read the documents to predict an answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>WebQ-test (open-domain split)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model performance on TriviaQA and WebQ with different size of GPT-3 as generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>prompts) 69.6 82.9 85.1 54.5 73.3 75.4 48.0 70.9 74.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Zero-shot open-domain QA performance. Our proposed GENREAD with the InstructGPT reader (named GENREAD (InstructGPT)) can significantly outperform the original InstructGPT, achieving new state-of-the-art performance on three open-domain QA benchmarks (previous SoTA:</figDesc><table><row><cell>Models</cell><cell cols="7">Open-domain QA NQ TriviaQA WebQ FEVER FM2 WoW (F1 / R-L) Fact Checking Dialogue System</cell></row><row><cell cols="4">*with retriever, AND directly trained on these datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPR + InstructGPT*</cell><cell>29.9</cell><cell>55.3</cell><cell>20.1</cell><cell>79.8</cell><cell>65.9</cell><cell>15.4</cell><cell>13.7</cell></row><row><cell cols="4">*with retriever, BUT NOT trained on these datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25 + InstructGPT</cell><cell>20.5</cell><cell>53.3</cell><cell>16.0</cell><cell>78.7</cell><cell>65.2</cell><cell>15.7</cell><cell>13.7</cell></row><row><cell cols="2">Contriever + InstructGPT 19.1</cell><cell>52.4</cell><cell>16.8</cell><cell>80.4</cell><cell>66.6</cell><cell>15.5</cell><cell>14.0</cell></row><row><cell>Google + InstructGPT</cell><cell>27.8</cell><cell>58.7</cell><cell>19.9</cell><cell>82.9</cell><cell>66.0</cell><cell>14.8</cell><cell>13.2</cell></row><row><cell cols="4">*without retriever, and not using external documents</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Previous SoTA methods</cell><cell>24.7 1</cell><cell>56.7 2</cell><cell>19.0 1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>InstructGPT (no docs.)</cell><cell>20.9</cell><cell>52.6</cell><cell>18.6</cell><cell>77.6</cell><cell>59.4</cell><cell>15.4</cell><cell>13.8</cell></row><row><cell cols="2">GENREAD (InstructGPT) 28.2</cell><cell>59.3</cell><cell>24.8</cell><cell>80.4</cell><cell>65.5</cell><cell>15.8</cell><cell>14.2</cell></row></table><note><p>Table 10). 1 GLaM (Du et al., 2022), 2 FLAN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>al., 2020;<ref type="bibr" target="#b54">Zhang &amp; Choi, 2021)</ref>, we conduct ablation studies mainly on the TriviaQA and WebQ datasets, to avoid the bias of the experimental results caused by temporality and incompleteness. Supervised performance on fact checking (FEVER</figDesc><table><row><cell>4.2.5 EXPERIMENTAL RESULTS ON OTHER TASKS</cell><cell></cell></row><row><cell>Models</cell><cell>FEVER FM2 Acc. Acc. F1 / R-L WoW</cell></row><row><cell>RAG (Lewis et al., 2020)</cell><cell>86.3 71.1 13.1 / 11.6</cell></row><row><cell>FiD (Izacard &amp; Grave, 2021)</cell><cell>90.2 77.6 17.5 / 16.1</cell></row><row><cell>GENREAD (FiD-xl) (sampling)</cell><cell>89.0 76.3 18.9 / 16.7</cell></row><row><cell cols="2">GENREAD (FiD-xl) (clustering) 89.6 77.8 19.1 / 16.8</cell></row><row><cell>merge two source docs.</cell><cell>91.8 78.9 20.1 / 17.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Answer coverage over 10 retrieved or generated documents. Case studies are provided in Tables</figDesc><table><row><cell>4.3.2 ANSWER COVERAGE ANALYSIS OVER ALL DOCUMENTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Documents obtained by ?</cell><cell cols="3">NQ TriviaQA WebQ</cell></row><row><cell cols="2">BM25 (Robertson et al., 2009) 0.84</cell><cell>1.83</cell><cell>0.72</cell></row><row><cell>Google search engine 3</cell><cell>0.99</cell><cell>1.92</cell><cell>1.01</cell></row><row><cell>DPR (Karpukhin et al., 2020)</cell><cell>1.19</cell><cell>1.90</cell><cell>1.10</cell></row><row><cell cols="2">GENREAD (nucleus sampling) 0.97</cell><cell>2.01</cell><cell>1.08</cell></row><row><cell cols="2">GENREAD (10 human prompts) 1.00</cell><cell>2.12</cell><cell>1.18</cell></row><row><cell cols="2">GENREAD (clustering prompts) 1.04</cell><cell>2.17</cell><cell>1.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Answer coverage measures the percentage of accepted answers that are contained in the documents. Coverage analysis showed that generated text tends to have lower coverage than retrieved documents because generated knowledge tends to have little diversity compared to retrieved documents. To improve coverage, we propose GENREAD with clustering, where we include examples in the prompt from different clusters of the training data to elicit more diverse knowledge generations.</figDesc><table><row><cell cols="2">4.3.3 REPRODUCIBILITY VIA OPEN SOURCE LARGE LANGUAGE MODELS</cell><cell></cell></row><row><cell>Documents obtained by ?</cell><cell cols="2">TriviaQA WebQ</cell></row><row><cell>DPR (Karpukhin et al., 2020)</cell><cell>66.3</cell><cell>50.8</cell></row><row><cell>OPT</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, OPT performed worse than GPT-3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>but still achieved comparable performance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with DPR; OpenAI Codex achieved the best</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>performance on both TriviaQA and WebQ.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4.3.4 SCALING WITH NUMBER OF LARGE MODEL PARAMETERS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(69.6)</cell></row><row><cell></cell><cell></cell><cell>DPR+FiD (66.3)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EM score</cell><cell>45 55</cell><cell>(40.3) DPR+FiD (50.8)</cell><cell>(44.6)</cell><cell>(52.6)</cell></row><row><cell></cell><cell>(37.4)</cell><cell></cell><cell>(41.7)</cell><cell></cell></row><row><cell></cell><cell>35</cell><cell>(29.8)</cell><cell cols="2">TriviaQA</cell></row><row><cell></cell><cell>(27.6)</cell><cell></cell><cell cols="2">WebQ</cell></row><row><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1e8</cell><cell>1e9</cell><cell>1e10</cell><cell>1e11</cell></row><row><cell></cell><cell></cell><cell cols="2"># GPT-3 model parameters</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Zero-shot open-domain QA performance, compared to recent large language models. All models in the table do not leverage any external corpus for document retrieval. Compared to InstructGPT, our proposed GENREAD with the InstructGPT reader can improve the EM score by +6.9 on average. GENREAD can achieve state-of-the-art performance on open test sets of all three benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell># total</cell><cell>NQ</cell><cell cols="2">TriviaQA</cell><cell>WebQ</cell></row><row><cell></cell><cell cols="6">parameters open test open test wiki split open test</cell></row><row><cell>GPT-3 (Brown et al., 2020)</cell><cell></cell><cell>175B</cell><cell>14.6</cell><cell>49.2</cell><cell>64.3</cell><cell>14.4</cell></row><row><cell>Gopher (Rae et al., 2021)</cell><cell></cell><cell>280B</cell><cell>10.1</cell><cell>43.5</cell><cell>52.8</cell><cell>-</cell></row><row><cell>FLAN (Wei et al., 2021)</cell><cell></cell><cell>137B</cell><cell>20.7</cell><cell>56.7</cell><cell>68.1</cell><cell>-</cell></row><row><cell>GLaM (Du et al., 2022)</cell><cell></cell><cell>64B</cell><cell>21.5</cell><cell>-</cell><cell>68.0</cell><cell>15.5</cell></row><row><cell cols="2">Chinchilla (Hoffmann et al., 2022)</cell><cell>70B</cell><cell>16.6</cell><cell>55.4</cell><cell>67.0</cell><cell>-</cell></row><row><cell cols="2">PaLM (Chowdhery et al., 2022)</cell><cell>540B</cell><cell>21.2</cell><cell>-</cell><cell>76.9</cell><cell>10.9</cell></row><row><cell cols="2">InstructGPT (Ouyang et al., 2022)</cell><cell>175B</cell><cell>20.9</cell><cell>52.6</cell><cell>62.9</cell><cell>18.6</cell></row><row><cell>GENREAD (InstructGPT)</cell><cell></cell><cell>175B</cell><cell>28.2</cell><cell>59.3</cell><cell>70.3</cell><cell>24.8</cell></row><row><cell>Settings / Datasets</cell><cell>NQ</cell><cell cols="4">TriviaQA WebQ FEVER FM2</cell><cell>WoW</cell></row><row><cell>Peak learning rate</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>5e-5</cell></row><row><cell>Total batch size</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>16</cell></row><row><cell>Total training steps</cell><cell cols="6">15,000 10,000 10,000 10,000 10,000 20,000</cell></row><row><cell>Best validation steps</cell><cell>6,000</cell><cell>500</cell><cell>8,500</cell><cell>5,000</cell><cell cols="2">6,000 20,000</cell></row><row><cell cols="2">Validation performance 43.27</cell><cell>69.47</cell><cell>60.33</cell><cell>88.97</cell><cell cols="2">73.57 18.60</cell></row><row><cell>Best validation ? test</cell><cell>70.24</cell><cell>53.35</cell><cell>87.80</cell><cell>74.64</cell><cell cols="2">69.46 18.49</cell></row><row><cell>Peak learning rate</cell><cell>5e-5</cell><cell>6e-5</cell><cell>3e-5</cell><cell>5e-5</cell><cell>5e-5</cell><cell>3e-5</cell></row><row><cell>Total batch size</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>8</cell></row><row><cell>Total training steps</cell><cell cols="6">20,000 15,000 15,000 15,000 15,000 20,000</cell></row><row><cell>Best validation steps</cell><cell>14,000</cell><cell>8,500</cell><cell cols="4">11,500 10,000 6,000 16,500</cell></row><row><cell cols="2">Validation performance 44.83</cell><cell>70.61</cell><cell>61.00</cell><cell>90.53</cell><cell cols="2">76.30 19.12</cell></row><row><cell>Best validation ? test</cell><cell>45.55</cell><cell>71.55</cell><cell>51.00</cell><cell>77.75</cell><cell cols="2">69.46 18.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparaters settings and validation performance for open-domain QA (numbers reported in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Retrieval performance evaluated by Recall@K of baselines and different GENREAD variants. Some numbers in the table are overlapped with those in Figure2. This table aims to show the performance of more methods, and to provide accurate recall numbers for future research comparisons.</figDesc><table><row><cell>R@10</cell><cell cols="3">TriviaQA WebQ NQ</cell></row><row><cell>Sample 5 documents from entire data</cell><cell>82.3</cell><cell>72.6</cell><cell>68.8</cell></row><row><cell>Sample 5 documents from each cluster</cell><cell>82.9</cell><cell>73.3</cell><cell>70.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation study on the strategy of sampling documents as in-context demonstrations.</figDesc><table><row><cell>EM score</cell><cell>TriviaQA</cell><cell>WebQ</cell><cell>NQ</cell></row><row><cell>DPR-FiD</cell><cell>80.2</cell><cell>63.3</cell><cell>63.1</cell></row><row><cell>GENREAD</cell><cell>86.8</cell><cell>66.7</cell><cell>64.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Readability study on retrieved and generated documents. See A.3 for more details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Top-10 human prompts, evaluated on merged validation set of NQ, TriviaQA and WebQ.</figDesc><table><row><cell>. Prompts</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Performance on NQ, TriviaQA and WebQ test sets of top-10 human prompts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 and</head><label>12</label><figDesc>Table13) in the human prompt setting, as described in ?3.2.2. The following is a list of surviving works that Mozart composed for performance at gatherings of Masons. The story and music of his opera "The Magic Flute" is also considered to have strong Masonic influences. The following is a partial list of family members, patrons, and colleagues who were Masons. Mozart's grandfather Johann Georg, a bookbinder, was raised among the extended Mozart family in Augsburg, in the house of Johann's own grandfather David Mozart. David and his children were distinguished architects and master ... (Wikipedia id: 10988168 -"Mozart and Freemasonry") DPR #2: Progressively, and in large part at the hands of Mozart himself, the contrapuntal complexities of the late Baroque emerged once more, moderated and disciplined by new forms, and adapted to a new aesthetic and social milieu. Mozart was a versatile composer, and wrote in every major genre, including symphony, opera, the solo concerto, chamber music including string quartet and string quintet, and the piano sonata. These forms were not new, but Mozart advanced their technical sophistication and emotional reach. He almost single-handedly developed and popularized the Classical piano concerto. He wrote a great deal ... (Wikipedia id: 451968 -"Wolfgang Amadeus Mozart") Among top-10 retrieved documents: opera: 2 times chamber music: 1 time GPT nucleus #1: Mozart was a prolific composer and wrote in many different genres. He composed symphonies, operas, concertos, chamber music, and more. Many of his works are considered some of the greatest ever written in those genres. GPT nucleus #2: Mozart composed a variety of music, including operas, symphonies, chamber music, concertos, and sonatas. His early works were influenced by the music of Haydn and Bach, and his later works show the influence of the French and Italian composers of his time. Mozart's music is characterized by its beauty, elegance, and precision, and he is considered one of the greatest composers of all time. GPT nucleus #3: Mozart composed a wide variety of music, including operas, symphonies, concertos, solo pieces, and chamber music. He also wrote a number of religious works, including masses and motets. Mozart found work in Vienna as a piano teacher and composer for opera and other works. There he met a fellow Mason, Joseph Haydn, who was to become an important mentor. Mozart married Constanze Weber in 1782; their marriage is thought to have been happy, in contrast to his earlier unhappy arranged marriage to his cousin, Maria Anna Nannerl Mozart. Mozart composed more than 600 works, including works widely. GPT cluster #2: Mozart was a prolific composer, and wrote in a wide variety of genres. He composed over 600 works, including operas, symphonies, concertos, solo works, and chamber music. His operas include The Marriage of Figaro, Don Giovanni, and Cosi fan tutte. His symphonies include the well-known Symphony No. 40 in G minor, and his concertos include the Piano Concerto No. 21 in C major, known as the ?lvira Madiganconcerto. GPT cluster #3: Mozart was a prolific composer who wrote music in a wide variety of genres. He is best known for his work in the classical genre, which includes such works as his Symphony No. 40 in G minor and his piano concertos. He also wrote operas, including The Marriage of Figaro and The Magic Flute. In addition to his work in classical music, Mozart also composed works in the popular genre, such as his Eine kleine Nachtmusik.</figDesc><table><row><cell>Query: What type of music did Mozart composed?</cell></row><row><cell>Answers: opera, classical music, art song, chamber music, ballet</cell></row><row><cell>DPR #1:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the experiments, we set n = 5 and found increasing n does not bring extra improvement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use the same normalization procedure as introduced in<ref type="bibr" target="#b21">Karpukhin et al. (2020)</ref>.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TriviaQA <ref type="bibr" target="#b20">(Joshi et al., 2017)</ref> open domain 78,785 8,837 11,313 public wikipedia split 7,993 public WebQ <ref type="bibr" target="#b1">(Berant et al., 2013)</ref> open domain 3,478 300 2,032 public NQ <ref type="bibr" target="#b25">(Kwiatkowski et al., 2019)</ref> open domain 79,168 8,757 3,610 public FEVER <ref type="bibr" target="#b46">(Thorne et al., 2018)</ref> kilt challenge 104,966 10,444 10,100 hidden FM2 <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref> official split 10,149 1169 1380 public WoW <ref type="bibr" target="#b8">(Dinan et al., 2019)</ref> kilt challenge 63,734 3,054 2,944 hidden Table <ref type="table">6</ref>: Datasets splits and statistics. For FEVER and WoW, labels in the test are hidden, so the model performance should be evaluated at https://ai.facebook.com/tools/kilt/.</p><p>A.1 DATASETS AND SPLITS -TRIVIAQA (TQA) <ref type="bibr" target="#b20">(Joshi et al., 2017)</ref> contains a set of trivia questions with answers that were originally scraped from trivia and quiz-league websites.</p><p>-WEBQUESTIONS (WebQ) <ref type="bibr" target="#b1">(Berant et al., 2013</ref>) consists of questions selected using Google Suggest API, where the answers are entities in Freebase.</p><p>-NATURAL QUESTIONS (NQ) <ref type="bibr" target="#b25">(Kwiatkowski et al., 2019)</ref> were mined from real Google search queries and the answers are spans in Wikipedia articles identified by human annotators.</p><p>We explore the same train / dev / test splits for the open-domain QA setting as used by <ref type="bibr" target="#b17">Izacard &amp; Grave (2021)</ref>; <ref type="bibr" target="#b21">Karpukhin et al. (2020)</ref>. For TriviaQA, GPT-3 / GLaM / PaLM <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b9">Du et al., 2022;</ref><ref type="bibr" target="#b6">Chowdhery et al., 2022)</ref> evaluate on the Wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with GPT-3's performance.</p><p>-FEVER <ref type="bibr" target="#b46">(Thorne et al., 2018)</ref> is one of the largest datasets for fact checking that requires retrieving evidence from external corpus to support if a statement is supported or refuted.</p><p>-FOOL ME TWICE (FM2) <ref type="bibr" target="#b10">(Eisenschlos et al., 2021</ref>) is a challenging fact checking dataset collected by gamification. Players write challenging claims either entailed or refuted by evidence from Wikipedia. They are then tasked to spot the refuted claim among a group.</p><p>-WIZARD OF WIKIPEDIA (WoW) <ref type="bibr" target="#b8">(Dinan et al., 2019)</ref> is an open-domain dialogue task for training agents that can converse knowledgeably about open-domain topics. One speaker in the conversation must ground their utterances in a specific knowledge sentence from a Wikipedia page.</p><p>We use the same train / dev / test splits in KILT challenge <ref type="bibr" target="#b37">(Petroni et al., 2021)</ref> for the FEVER and WoW datasets. Their test labels are hidden, so the performance can only be evaluated through https://ai.facebook.com/tools/kilt. For FM2, we use its official dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 IMPLEMENTATION DETAILS</head><p>We use T5-770M <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> and T5-3B as our backbone models to implement FiD <ref type="bibr" target="#b17">(Izacard &amp; Grave, 2021)</ref>. We use AdamW as the optimizer, with 2,000 warm-up steps. We set the dropout probability to 0.1 and weight decay to 0.01. We use one A100 for running T5-770M and set the batch size of 16. We use 8 A100 for running T5-3B and set the per GPU batch as 2, leading to the total batch size as 16. We searched different learning rates, ranging from 5e-6 to 4e-5, and we found 3e-5 to 6e-5 performed the best under the T5-3B setting and 5e-5 to 1e-4 performed the best under the T5-770M setting. We refer to more individual implementation details in Table <ref type="table">8</ref>.</p><p>We implement other baseline methods by using repositories:</p><p>-BM25: GPT cluster #2: Monsanto is a biotechnology and agriculture company that specializes in genetically modified organisms (GMOs). The company owns a number of patents on GMOs, and it produces a range of products that contain GMOs, including herbicides, pesticides, and seeds.</p><p>GPT cluster #3: Monsanto is a multinational agricultural biotechnology corporation. It is one of the world's leading producers of genetically engineered seed and Roundup, a glyphosate-based herbicide. The company also manufactures other agricultural chemicals, such as insecticides and fungicides. Among top-10 generated documents: agriculture: 2 times seed: 5 times agricultural chemicals: 1 time  The labeled answer is "Geoffrey Dyson Palmer", however, "Geoffrey Palmer" is also correct.</p><p>Q: How many cracker barrels in the united states? 645 over 630 Explanation: The labled answer is "639" or "over 600", so "over 630" is also a reasonable answer.</p><p>Q: Where do the greasers live in the outsiders? Tulsa, Oklahoma Oklahoma Explanation: The labled answer is "Tulsa, Oklahoma", but "Oklahoma" is also a correct answer.</p><p>Q: Where are unipolar neurons found in spinal cord? the granule region dorsal root ganglia Explanation: The labled answer is "the distal dorsal root", the GENREAD output "dorsal root ganglia" is the same. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Promptsource: An integrated development environment and repository for natural language prompts</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><surname>F?vry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autoregressive search engines: Generating substrings as document identifiers</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10628</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unitedqa: A hybrid approach for open domain question answering</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3080" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fool me twice: Entailment from wikipedia gamification</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>B?rschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging knowledge in multilingual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3237" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entities as experts: Sparse memory access with entity supervision</title>
		<author>
			<persName><surname>Thibault F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4937" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised dense information retrieval with contrastive learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathild</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relevance-guided supervision for openqa with colbert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="452" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Internetaugmented language models through few-shot prompting for open-domain question answering</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Grigorev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05115</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Standing on the shoulders of giant frozen language models</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoel</forename><surname>Zeldes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10019</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02336</idno>
		<title level="m">On the advance of making language models better reasoners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generated knowledge prompting for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3154" to="3169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generation-augmented retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4089" to="4100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ambigqa: Answering ambiguous open-domain questions</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5783" to="5797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Codex (available via openai api for free</title>
		<ptr target="https://openai.com/blog/openai-codex/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kilt: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Questions are all you need to train a dense passage retriever</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10658</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end training of multi-document reader and retriever for open-domain question answering</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25968" to="25981" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06991</idno>
		<title level="m">Transformer memory as a differentiable search index</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fever: a largescale dataset for fact extraction and verification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4961" to="4974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A technical question answering system with transfer learning</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchi</forename><surname>Mahindru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinem</forename><surname>Guven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="92" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Crossing variational autoencoders for answer retrieval</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5635" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A survey of knowledge-enhanced text generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Situatedqa: Incorporating extra-linguistic contexts into qa</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7371" to="7387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: A comprehensive survey on open-domain question answering</title>
		<author>
			<persName><forename type="first">Fengbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00774</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
