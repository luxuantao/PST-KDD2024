<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christine</forename><forename type="middle">C</forename><surname>Guillemot</surname></persName>
						</author>
						<author role="corresp">
							<persName><surname>Liu</surname></persName>
							<email>liu@cs.njit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<postCode>07102</postCode>
									<settlement>Newark</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<postCode>22030</postCode>
									<settlement>Fairfax</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1CB6C984A5DCA7EBECEECCF3F068D9E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gabor Feature Based Classification Using the Enhanced Fisher Linear Discriminant Model for Face Recognition Chengjun Liu, Member, IEEE, and Harry Wechsler, Fellow, IEEE Abstract-This paper introduces a novel Gabor-Fisher Classifier (GFC) for face recognition. The GFC method, which is robust to changes in illumination and facial expression, applies the Enhanced Fisher linear discriminant Model (EFM) to an augmented Gabor feature vector derived from the Gabor wavelet representation of face images. The novelty of this paper comes from 1) the derivation of an augmented Gabor feature vector, whose dimensionality is further reduced using the EFM by considering both data compression and recognition (generalization) performance; 2) the development of a Gabor-Fisher classifier for multi-class problems; and 3) extensive performance evaluation studies. In particular, we performed comparative studies of different similarity measures applied to various classifiers. We also performed comparative experimental studies of various face recognition schemes, including our novel GFC method, the Gabor wavelet method, the Eigenfaces method, the Fisherfaces method, the EFM method, the combination of Gabor and the Eigenfaces method, and the combination of Gabor and the Fisherfaces method. The feasibility of the new GFC method has been successfully tested on face recognition using 600 FERET frontal face images corresponding to 200 subjects, which were acquired under variable illumination and facial expressions. The novel GFC method achieves 100% accuracy on face recognition using only 62 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Eigenfaces, enhanced Fisher linear discriminant model (EFM), face recognition, Fisher linear discriminant (FLD),</head><p>Gabor-Fisher classifier (GFC), Gabor wavelets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A MONG the most challenging tasks for visual form ("shape") analysis and object recognition are understanding how people process and recognize each other's face, and the development of corresponding computational models for automated face recognition. Face recognition is largely motivated by the need for surveillance and security, telecommunication and digital libraries, human-computer intelligent interaction, and smart environments <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>A good face recognition methodology should consider representation as well as classification issues, and a good representation method should require minimum manual annotations. The Gabor wavelets, whose kernels are similar to the two-dimensional (2-D) receptive field profiles of the mammalian cortical simple cells, exhibit desirable characteristics of spatial locality and orientation selectivity. The biological relevance and computational properties of Gabor wavelets for image analysis have been described in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and <ref type="bibr" target="#b27">[27]</ref>. The Gabor wavelet representation facilitates recognition without correspondence (hence, no need for manual annotations) because it captures the local structure corresponding to spatial frequency (scale), spatial localization, and orientation selectivity <ref type="bibr" target="#b34">[34]</ref>. As a result, the Gabor wavelet representation of face images should be robust to variations due to illumination and facial expression changes <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b34">[34]</ref>.</p><p>This paper introduces a novel Gabor-Fisher Classifier (GFC) method for face recognition. The GFC method, which is robust to illumination and facial expression variability, applies the Enhanced Fisher linear discriminant Model (EFM) <ref type="bibr" target="#b22">[23]</ref> to an augmented Gabor feature vector derived from the Gabor wavelet representation of face images. To encompass all the features produced by the different Gabor kernels one concatenates the resulting Gabor wavelet features to derive an augmented Gabor feature vector. The dimensionality of the Gabor vector space is then reduced under the eigenvalue selectivity constraint of the EFM method to derive a low-dimensional feature representation with enhanced discrimination power. The feasibility of the new GFC method has been successfully tested on face recognition using a data set from the FERET database, which is a standard testbed for face recognition technologies <ref type="bibr" target="#b31">[31]</ref>. Specifically we used 600 FERET frontal face images corresponding to 200 subjects, which were acquired using variable illumination and facial expressions. The effectiveness of the GFC method is shown in terms of both absolute performance indices and comparative performance against some popular face recognition schemes such as the Gabor wavelet method <ref type="bibr" target="#b9">[10]</ref>, the Eigenfaces method <ref type="bibr" target="#b36">[36]</ref>, the Fisherfaces method <ref type="bibr" target="#b0">[1]</ref>, the EFM method <ref type="bibr" target="#b22">[23]</ref>, the combination of Gabor and the Eigenfaces method, and the combination of Gabor and the Fisherfaces method. In particular, the novel GFC method achieves 100% recognition accuracy using only 62 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>Face recognition depends heavily on the particular choice of features used by the classifier <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. One usually starts with a given set of features and then attempts to derive an optimal subset (under some criteria) of features leading to high classification performance with the expectation that similar performance can also 1057-7149/02$17.00 Â© 2002 IEEE be displayed on future trials using novel (unseen) test data. Principal component analysis (PCA) is a popular technique used to derive a starting set of features for both face representation and recognition. Kirby and Sirovich <ref type="bibr" target="#b18">[19]</ref> showed that any particular face can be (i) economically represented along the eigenpictures coordinate space, and (ii) approximately reconstructed using just a small collection of eigenpictures and their corresponding projections ('coefficients'). Applying PCA technique to face recognition, Turk and Pentland <ref type="bibr" target="#b36">[36]</ref> developed a well-known Eigenfaces method. The Eigenfaces method, however, does not consider the classification aspect, as it is based on the optimal representationcriterion(PCA)inthesenseofmean-squareerror.Toimprove the PCA standalone classification performance, one needs to combine further this optimalrepresentation criterion with some discrimination criterion.</p><p>One widely used discrimination criterion in the face recognition community is the Fisher linear discriminant (FLD, a.k.a. linear discriminant analysis, or LDA) <ref type="bibr" target="#b15">[16]</ref>, which defines a projection that makes the within-class scatter small and the between-class scatter large. As a result, FLD derives compact and well-separated clusters. FLD is behind several face recognition methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[35]</ref>. As the original image space is high dimensional, most of these methods apply PCA first for dimensionality reduction, as it is the case with the Fisherfaces method due to Belhumeur et al. <ref type="bibr" target="#b0">[1]</ref>. Subsequent FLD transformation is used then to build the most discriminating features (MDF) space for classification <ref type="bibr" target="#b35">[35]</ref>. The drawback of FLD is that it requires large training sample size for good generalization. For a face recognition problem, however, usually there are a large number of faces (classes), but only a few training examples per face. One possible remedy for this drawback is to artificially generate additional data and thus increase the sample size <ref type="bibr" target="#b11">[12]</ref>. Yet another remedy is to improve FLD's generalization performance by balancing the need for adequate signal representation and subsequent classification performance using sensitivity analysis on the spectral range of the within-class eigenvalues <ref type="bibr" target="#b22">[23]</ref>.</p><p>Gabor wavelets model quite well the receptive field profiles of cortical simple cells <ref type="bibr" target="#b14">[15]</ref>. The Gabor wavelet representation, therefore, captures salient visual properties such as spatial localization, orientation selectivity, spatial frequency characteristic. Lades et al. <ref type="bibr" target="#b20">[21]</ref> pioneered the use of Gabor wavelets for face recognition using the Dynamic Link Architecture (DLA) framework. The DLA starts by computing the Gabor jets, and then it performs a flexible template comparison between the resulting image decompositions using graph-matching. Wiskott et al. <ref type="bibr" target="#b38">[38]</ref> have expanded on DLA when they developed a Gabor wavelet based elastic bunch graph matching method to label and recognize human faces. Based on the 2-D Gabor wavelet representation and the labeled elastic graph matching, Lyons et al. <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> proposed an algorithm for two-class categorization of gender, race, and facial expression. The algorithm includes two steps: registration of a grid with the face using either labeled elastic graph matching <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b38">[38]</ref> or manual annotation of 34 points on every face image <ref type="bibr" target="#b26">[26]</ref>; and categorization based on the features extracted at grid points using linear discriminant analysis (LDA). Donato et al. <ref type="bibr" target="#b9">[10]</ref> have recently shown through experiments that the Gabor wavelet representation gives better performance than other techniques for classifying facial actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gabor Wavelets</head><p>Gabor wavelets were introduced to image analysis due to their biological relevance and computational properties <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b27">[27]</ref>. The Gabor wavelets, whose kernels are similar to the 2-D receptive field profiles of the mammalian cortical simple cells, exhibit desirable characteristics of spatial locality and orientation selectivity, and are optimally localized in the space and frequency domains.</p><p>The Gabor wavelets (kernels, filters) can be defined as follows <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b20">[21]</ref>:</p><p>(1) where and define the orientation and scale of the Gabor kernels, , denotes the norm operator, and the wave vector is defined as follows:</p><p>(</p><formula xml:id="formula_0">)<label>2</label></formula><p>where and .</p><p>is the maximum frequency, and is the spacing factor between kernels in the frequency domain <ref type="bibr" target="#b20">[21]</ref>.</p><p>The Gabor kernels in ( <ref type="formula">1</ref>) are all self-similar since they can be generated from one filter, the mother wavelet, by scaling and rotation via the wave vector . Each kernel is a product of a Gaussian envelope and a complex plane wave, while the first term in the square brackets in (1) determines the oscillatory part of the kernel and the second term compensates for the DC value. The effect of the DC term becomes negligible when the parameter , which determines the ratio of the Gaussian window width to wavelength, has sufficiently large values.</p><p>In most cases one would use Gabor wavelets of five different scales, , and eight orientations, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows the real part of the Gabor kernels at five scales and eight orientations and their magnitude, with the following parameters: , , and</p><p>. The kernels exhibit desirable characteristics of spatial frequency, spatial locality, and orientation selectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gabor Feature Representation</head><p>The Gabor wavelet representation of an image is the convolution of the image with a family of Gabor kernels as defined by <ref type="bibr" target="#b0">(1)</ref>. Let be the gray level distribution of an image, the convolution of image and a Gabor kernel is defined as follows: where and denote the Fourier and inverse Fourier transform, respectively. Fig. <ref type="figure" target="#fig_1">2</ref> shows the Gabor wavelet representation (the real part and the magnitude) of a sample image. These representation results display scale, locality, and orientation properties corresponding to those displayed by the Gabor wavelets in Fig. <ref type="figure" target="#fig_0">1</ref>. To encompass different spatial frequencies (scales), spatial localities, and orientation selectivities, we concatenate all these representation results and derive an augmented feature vector . Before the concatenation, we first downsample each by a factor to reduce the space dimension, and normalize it to zero mean and unit variance. We then construct a vector out of the by concatenating its rows (or columns). Now, let denote the normalized vector constructed from (downsampled by and normalized to zero mean and unit variance), the augmented Gabor feature vector is then defined as follows: <ref type="bibr" target="#b5">(6)</ref> where is the transpose operator. The augmented Gabor feature vector thus encompasses all the elements (downsampled and normalized) of the Gabor wavelet representation set, , as important discriminating information. Fig. <ref type="figure">3</ref> shows (in image form rather than in vector form) an example of the augmented Gabor feature vector, where the downsampling factor is 64, i.e., .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GABOR-FISHER CLASSIFIER</head><p>We describe in this section our novel Gabor-Fisher Classifier (GFC) method which applies the Enhanced Fisher linear discriminant Model (EFM) <ref type="bibr" target="#b22">[23]</ref> to the augmented Gabor feature vector derived in Section II.B. The dimensionality of the resulting vector space is reduced, using the eigenvalue selectivity constraint of the EFM method, in order to derive low-dimensional features with enhanced discrimination power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dimensionality Reduction and Discriminant Analysis</head><p>The augmented Gabor feature vector introduced in Section II-B resides in a space of very high dimensionality:</p><p>, where is the dimensionality of the vector space. Psychophysical findings indicate, however, that "perceptual tasks such as similarity judgment tend to be performed on a low-dimensional representation of the sensory data. Low dimensionality is especially important for learning, as the number of examples required for attaining a given level of performance grows exponentially with the dimensionality of the underlying representation space" <ref type="bibr" target="#b10">[11]</ref>. Low-dimensional representations are also important when one considers the intrinsic computational aspect. Principal component analysis, or PCA <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b8">[9]</ref>, whose primary goal is to project the high dimensional visual stimuli (face images) into a lower dimensional space, is the optimal method for dimensionality reduction in the sense of mean-square error.</p><p>PCA is a standard decorrelation technique and following its application one derives an orthogonal projection basis that directly leads to dimensionality reduction, and possibly to feature selection. Let define the covariance matrix of the augmented feature vector <ref type="bibr" target="#b6">(7)</ref> where</p><p>is the expectation operator. The PCA of a random vector factorizes its covariance matrix into the fol-lowing form:</p><formula xml:id="formula_1">with (<label>8</label></formula><formula xml:id="formula_2">)</formula><p>where is an orthogonal eigenvector matrix and a diagonal eigenvalue matrix with diagonal elements in decreasing order (</p><p>). An important property of PCA is its optimal signal reconstruction in the sense of minimum mean-square error when only a subset of principal components is used to represent the original signal. Following this property, an immediate application Fig. <ref type="figure">3</ref>. An example of the augmented Gabor feature vector (in image form rather than in vector form), where the downsampling factor is 64, i.e., = 64. In our experiments, the dimensionality of this vector space is as high as 10 240.</p><p>of PCA is dimensionality reduction <ref type="bibr" target="#b8">(9)</ref> where , and . The lower dimensional vector captures the most expressive features of the original data . However, one should be aware that the PCA driven coding schemes are optimal and useful only with respect to data compression and decorrelation of low (second) order statistics. PCA does not take into account the recognition (discrimination) aspect and one should thus not expect optimal performance for tasks such as face recognition when using such PCA-like encoding schemes. To address this obvious shortcoming, one has to reformulate the original problem as one where the search is still for low-dimensional patterns but is now also subject to seeking a high discrimination index, characteristic of separable low-dimensional patterns. One solution that has been proposed to solve this new problem is to use the Fisher linear discriminant (FLD) <ref type="bibr" target="#b15">[16]</ref> for the very purpose of achieving high separability between the different patterns in whose classification one is interested. Characteristic of this approach are recent schemes such as the most discriminating features (MDF) method <ref type="bibr" target="#b35">[35]</ref> and the Fisherfaces method <ref type="bibr" target="#b0">[1]</ref>.</p><p>FLD is a popular discriminant criterion that measures the between-class scatter normalized by the within-class scatter <ref type="bibr" target="#b16">[17]</ref>. Let and denote the classes and the number of images within each class, respectively. Let and be the means of the classes and the grand mean. The within-and between-class scatter matrices, and , are defined as follows: <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref> where is a priori probability, , , and denotes the number of classes.</p><p>FLD derives a projection matrix that maximizes the ratio <ref type="bibr" target="#b0">[1]</ref>. This ratio is maximized when consists of the eigenvectors of the matrix <ref type="bibr" target="#b35">[35]</ref> (</p><formula xml:id="formula_3">)<label>12</label></formula><p>where , are the eigenvector and eigenvalue matrices of , respectively. One drawback of FLD is that it requires large training sample size for good generalization. When such requirement is not met, FLD overfits to the training data and thus generalizes poorly to the novel testing data <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Enhanced Fisher Linear Discriminant Model</head><p>The Enhanced Fisher linear discriminant Model (EFM) improves the generalization capability of FLD by decomposing the FLD procedure into a simultaneous diagonalization of the two within-and between-class scatter matrices <ref type="bibr" target="#b22">[23]</ref>. The simultaneous diagonalization is stepwisely equivalent to two operations as pointed out by Fukunaga <ref type="bibr" target="#b16">[17]</ref>: whitening the withinclass scatter matrix and applying PCA on the between-class scatter matrix using the transformed data. The stepwise operation shows that during whitening the eigenvalues of the withinclass scatter matrix appear in the denominator. As the small (trailing) eigenvalues tend to capture noise <ref type="bibr" target="#b22">[23]</ref>, they cause the whitening step to fit for misleading variations and thus generalize poorly when exposed to new data. To achieve enhanced performance EFM preserves a proper balance between the need that the selected eigenvalues (corresponding to the principal components for the original image space) account for most of the spectral energy of the raw data, i.e., representational adequacy, and the that the eigenvalues of the class scatter matrix (in the reduced PCA space) are not too small, i.e., better generalization.</p><p>The choice of the range of principal components ( ) for dimensionality reduction [see <ref type="bibr" target="#b8">(9)</ref>] takes into account both the spectral energy and the magnitude requirements. The eigenvalue spectrum of the covariance matrix [see <ref type="bibr" target="#b7">(8)</ref>] provides a good indicator for meeting the energy criterion; one needs then to derive the eigenvalue spectrum of the within-class scatter matrix in the reduced PCA space to facilitate the choice of the range of principal components so that the magnitude requirement is met. Toward that end, one carries out the stepwise FLD process described earlier. In particular, the stepwise FLD procedure derives the eigenvalues and eigenvectors of as the result of the simultaneous diagonalization of and . First whiten the within-class scatter matrix: and (13) <ref type="bibr" target="#b13">(14)</ref> where , are the eigenvector and the diagonal eigenvalue matrices of , respectively. The eigenvalue spectrum of the within-class scatter matrix in the reduced PCA space can be derived by <ref type="bibr" target="#b12">(13)</ref>, and different spectra are obtained corresponding to different number of principal components utilized (see ( <ref type="formula">9</ref>) and ( <ref type="formula">10</ref>)). Now one has to simultaneously optimize the behavior of the trailing eigenvalues in the reduced PCA space (13) with the energy criteria for the original image space (8).</p><p>After the feature vector ( <ref type="formula">9</ref>) is derived, EFM first diagonalizes the within-class scatter matrix using ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>). Note that now and are the eigenvector and the eigenvalue matrices corresponding to the feature vector . EFM proceeds then to compute the between-class scatter matrix as follows: <ref type="bibr" target="#b14">(15)</ref> Diagonalize now the new between-class scatter matrix and <ref type="bibr" target="#b15">(16)</ref> where , are the eigenvector and the diagonal eigenvalue matrices of , respectively. The overall transformation matrix of EFM is now defined as follows:</p><p>(17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Similarity Measures and Classification Rule for Gabor Feature Based Classification</head><p>The GFC applies the EFM method on the (lower dimensional) augmented Gabor feature vector derived by <ref type="bibr" target="#b8">(9)</ref>. When an image is presented to the GFC classifier, the augmented Gabor feature vector of the image is first calculated as detailed in Section II-B, and the lower dimensional feature, , is derived using <ref type="bibr" target="#b8">(9)</ref>. The dimensionality of the lower dimensional feature space is determined by the EFM method, which derives further the overall transformation matrix, , as defined by <ref type="bibr" target="#b16">(17)</ref>. The new feature vector, , of the image is defined as follows:</p><p>(18) Let , , be the mean of the training samples for class after the EFM transformation. The GFC method applies, then, the nearest neighbor (to the mean) rule for classification using some similarity (distance) measure <ref type="bibr" target="#b18">(19)</ref> The image feature vector, , is classified as belonging to the class of the closest mean, , using the similarity measure . The similarity measures used in our experiments to evaluate the efficiency of different representation and recognition methods include distance measure, , distance measure, , Mahalanobis distance measure, , and cosine similarity measure, , which are defined as follows:</p><p>(20)</p><formula xml:id="formula_4">(21) (22) (<label>23</label></formula><formula xml:id="formula_5">)</formula><p>where is the covariance matrix, and denotes the norm operator. Note that the cosine similarity measure includes a minus sign in <ref type="bibr" target="#b22">(23)</ref>, because the nearest neighbor (to the mean) rule of ( <ref type="formula">19</ref>) applies minimum (distance) measure rather than maximum similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We assessed the feasibility and performance of our novel GFC on the face recognition task, using a data set from the FERET database, which is a standard testbed for face recognition technologies <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Specifically we used 600 FERET frontal face images corresponding to 200 subjects, which were acquired under variable illumination and facial expressions. Comparative performance is carried out against some popular face recognition schemes such as the Gabor wavelet method <ref type="bibr" target="#b9">[10]</ref>, the Eigenfaces method <ref type="bibr" target="#b36">[36]</ref>, the Fisherfaces method <ref type="bibr" target="#b0">[1]</ref>, the EFM method <ref type="bibr" target="#b22">[23]</ref>, the combination of Gabor and the Eigenfaces method, and the combination of Gabor and the Fisherfaces method.</p><p>The FERET database used for evaluating face recognition algorithms displays diversity across gender, ethnicity, and age. The image sets were acquired without any restrictions imposed on facial expression and with at least two frontal images shot at different times during the same photo session. The experiments involve 600 face images corresponding to 200 subjects such that each subject has three images of size 256 384 with 256 gray scale levels. First, the centers of the eyes of an image are manually detected, then rotation and scaling transformations align the centers of the eyes to predefined locations. Finally, the face image is cropped to the size of 128 128 to extract the facial region, which is further normalized to zero mean and unit variance. Fig. <ref type="figure" target="#fig_2">4</ref> shows some example images used in our experiments that are already cropped to the size of 128 128. Note that as the images were acquired during different photo sessions, they display different illumination characteristics and facial expressions. As two images are randomly chosen for training, while the remaining image (unseen during training) is used for testing (see Fig. <ref type="figure" target="#fig_2">4</ref>), the GFC has to cope with both illumination and facial expression variabilities.</p><p>For comparison purpose, we first implemented the Eigenfaces method <ref type="bibr" target="#b36">[36]</ref>, the Fisherfaces method <ref type="bibr" target="#b0">[1]</ref>, and the EFM method <ref type="bibr" target="#b22">[23]</ref> and tested their performance using the original face images as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The comparative face recognition performance of these three methods is shown in Fig. <ref type="figure">5</ref>, and one can see from the figure that the EFM method performs better than the Fisherfaces method followed by the Eigenfaces method. Both the EFM method and the Fisherfaces method apply the distance measure, while the Eigenfaces method applies the Mahalanobis distance measure. For the Eigenfaces method, the Mahalanobis distance measure performs better than the distance measure, followed in order by the distance measure and the cosine similarity measure as shown in Fig. <ref type="figure">6</ref>. The Mahalanobis distance measure performs better than the other similarity measures because the Mahalanobis distance measure counteracts the fact that and distance measures in the PCA space weight preferentially for low frequencies, and this is consistent with the results reported by Moghaddam and Pentland <ref type="bibr" target="#b28">[28]</ref> and Sung and Poggio <ref type="bibr" target="#b19">[20]</ref>. As the measure weights more the low frequencies than Fig. <ref type="figure">5</ref>. Comparative face recognition performance of the eigenfaces method, the Fisherfaces method, and the EFM method using the original images.</p><p>does, the distance measure should perform better than the distance measure, a conjecture validated by our experiments as shown in Fig. <ref type="figure">6</ref>. The cosine similarity measure does not compensate the low frequency preference, and it performs the worst among all the measures. Actually, the superiority of the cosine similarity measure to the others can be revealed only when the discriminating features (derived by the GFC method) rather than the expressive features (derived by the PCA) are used for classification <ref type="bibr" target="#b23">[24]</ref>.</p><p>The next series of experiments exploits the Gabor wavelet representation, , derived in Section II-B, using the , and cosine similarity measures, respectively. (The Mahalanobis metric is not used here because it involves transformed data and covariance matrix suitable for PCA-like schemes. The , and cosine metrics are here compared at different downsampling rates without further data transformations.) For the first set of Fig. <ref type="figure">6</ref>. Face recognition performance of the eigenfaces method using the original face images and the four different similarity measures: L1 (L distance measure), L2 (L distance measure), Md (Mahalanobis distance measure), and Cos (cosine similarity measure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I FACE RECOGNITION PERFORMANCE USING THE GABOR WAVELET REPRESENTATION WITH THE THREE DIFFERENT SIMILARITY MEASURES, RESPECTIVELY: (L DISTANCE MEASURE), L2 (L DISTANCE MEASURE), AND COS (COSINE SIMILARITY MEASURE). S</head><p>REPRESENTS THE GABOR REPRESENTATION DOWNSAMPLED BY A FACTOR OF 16 AND NORMALIZED TO UNIT LENGTH OF EACH O (z), AS SUGGESTED BY [10]. X , X , AND X</p><p>REPRESENT THE AUGMENTED GABOR FEATURE VECTOR experiments, we downsampled the Gabor wavelet representation set, , by a factor of 16 to reduce the dimensionality and normalized each to unit length, as suggested by Donato et al. <ref type="bibr" target="#b9">[10]</ref>. The face recognition performance using such a Gabor representation corresponding to different similarity measures is tabulated in Table <ref type="table">I</ref>, which shows that the best performance is achieved using the similarity measure. Comparing Table I with Fig. <ref type="figure">6</ref>, we found that 1) under the and cosine similarity measures, the Gabor features carry more discriminating information than the PCA features do, a finding consistent with that reported by Donato et al. <ref type="bibr" target="#b9">[10]</ref> on facial action classification and 2) the performance with the three similarity measures, , and cosine, varies less drastically than that shown in Fig. <ref type="figure">6</ref>. The second finding indicates Gabor representation is less likely affected by preferential low frequency weighting, which qualifies the Gabor representation as a discriminating representation method. We have also experimented on the augmented Gabor feature vector as defined by ( <ref type="formula">6</ref>) with three different downsampling factors, respectively: , 16, or 64. From the face recognition performance shown in Table <ref type="table">I</ref>, we found that 1) the augmented Gabor feature vector carries quite similar discriminating information to the one used by Donato et al. <ref type="bibr" target="#b9">[10]</ref> and 2) the performance differences among using the three different downsampling factors are not significant. As a result, we choose the downsampling factor of 64 for the next series of experiments, since it reduces to a larger extent the dimensionality of the vector space than the other two factors do. (We experimented with other downsampling factors as well. When the downsampling factors are 256 and 1024, the performance is marginally less effective; when the downsampling factor is 4096, the recognition rate drops drastically.)</p><p>Even though the performance shown in Table <ref type="table">I</ref> indicates that Gabor feature representation carries discriminating information, it is still not convenient to use such representation directly for classification, since the dimensionality of the augmented Gabor feature vector space is very high. To reduce the dimensionality of the vector space, we applied PCA on the augmented Gabor feature vector</p><p>, where the downsampling factor is set to be 64. Fig. <ref type="figure" target="#fig_4">7</ref> shows the face recognition performance of PCA using the augmented Gabor feature vector . Our results indicate that (i) the recognition performance improves by a large margin for all the similarity measures as compared with Fig. <ref type="figure">6</ref>; and (ii) Mahalanobis and distance measures perform better than the other two similarity measures, which shows again that PCA derives features that preferentially weight low frequencies. Our last series of experiments, performed using the novel Gabor-Fisher Classifier (GFC) method described in this paper, show that the GFC derives discriminating Gabor features with low dimensionality and enhanced discrimination power. Fig. <ref type="figure">8</ref> shows comparative face recognition performance of the combination of Gabor and the Eigenfaces method, and the combination of Gabor and the Fisherfaces method, and the GFC method, using the augmented Gabor feature vector downsampled by a factor of 64, i.e., . The GFC method performs better than both of the other two methods. In particular, GFC method achieves 100% correct recognition accuracy when using only 62 Fig. <ref type="figure">8</ref>. Comparative face recognition performance of the combination of Gabor and the eigenfaces method, the combination of Gabor and the Fisherfaces method, and the GFC method, using the augmented Gabor feature vector X downsampled by a factor of 64, i.e., = 64. features (note that the curves in Fig. <ref type="figure">8</ref> were drawn with an interval resolution of features, and it shows that 100% correct recognition rate happens when 65 features are used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We have introduced in this paper a novel Gabor-Fisher classification method for face recognition. The GFC method, which is robust to variations in illumination and facial expression, applies the EFM method to an augmented Gabor feature vector derived from the Gabor wavelet representation of face images. The Gabor transformed face images yield features that display scale, locality, and orientation selectivity. The feasibility of the new GFC method has been successfully tested on face recognition using a data set from the FERET database, which is a standard testbed for face recognition technologies. Specifically we used 600 FERET frontal face images corresponding to 200 subjects, which were acquired under variable illumination and facial expressions. The effectiveness of the GFC method is shown in terms of both absolute performance indices and comparative performance against some popular face recognition schemes such as the Gabor wavelet method, the Eigenfaces method, the Fisherfaces method, the EFM method, the combination of Gabor and the Eigenfaces method, and the combination of Gabor and the Fisherfaces method. In particular, the novel GFC method achieves 100% recognition accuracy using only 62 features.</p><p>The excellent performance shown by the GFC method is the direct result of coupling an augmented Gabor feature vector with the EFM method. The benefits resulting from using Gabor wavelets come from them being the result of evolutionary pressure on the mammalian visual system to develop an optimal sensory architecture tuned to an environment where people live and operate on a regularly basis <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. While the effectiveness of Gabor wavelets has been shown so far to match and capture only the statistics of natural scenes <ref type="bibr" target="#b13">[14]</ref>, it is also quite possible that the same Gabor wavelets are also tuned for face processing tasks <ref type="bibr" target="#b32">[32]</ref>, another task highly relevant for people. In particular, Field <ref type="bibr" target="#b13">[14]</ref> has shown for natural scenes that "they are approximately scale invariant with regards to both their power spectra and their phase spectra. Principally because of the phase spectra, self-similar wavelet-like codes are capable of producing a sparse but informative representation of these images." What we see and the range of images we are likely to see are limited. Therefore, the human visual system might be tuned for both natural scenery and human faces, and it can take advantage of "the degree of predictability or redundancy in our environment". Note that as the Gabor wavelets are scale invariant, the statistics of the image must remain constant as one magnifies any local region of the image. The two statistics Field refers to are 1) invariance in contrast across scale (reflected in the power spectrum) and 2) invariance in the local structure (reflected in the phase spectrum). Invariance in local structure means that there exists a number of structures which extend across different frequency bands. Therefore, for both consistency and robustness one can check if such structures are confirmed across several (bandwidth) channels, while moving from coarse to fine resolution, i.e., low to high frequency, even that some drifting may occur. The location of features at one scale can provide a guide for the search for features at other scales. In summary Gabor wavelets yield sparse codes. This does not mean dimensionality reduction, but rather something approaching a factorial code. The nonaccidental occurrence of coincidences in the self-similar wavelet code should facilitate enhanced associations and face recognition <ref type="bibr" target="#b14">[15]</ref>.</p><p>Our next goal is to further search for an optimal and sparse code resulting from the Gabor wavelet representation of face images, before forming the augmented Gabor feature vector and applying the GFC method for classification. The sparse code should represent the sparse structures as displayed by the features of the Gabor transformed face images in terms of spatial locality, scale and orientation selectivity, along the lines suggested by Olshausen and Field <ref type="bibr" target="#b29">[29]</ref> for natural image analysis. Another possibility is to search, using evolutionary pursuit (EP) method <ref type="bibr" target="#b21">[22]</ref>, for the sparse features directly with the twin goals of reducing the amount of data used for classification and simultaneously providing enhanced discriminatory power. The search for such features would be driven by the need to increase the generalization ability of the learning classification machine as a result of leveraging the trade-off between minimizing the empirical risk encountered during training and narrowing the confidence interval for reducing the guaranteed risk while testing on unseen data <ref type="bibr" target="#b37">[37]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Gabor Wavelets. (a) The real part of the Gabor kernels at five scales and eight orientations with the following parameters: = 2, k = =2, and f = p 2. (b) The magnitude of the Gabor kernels at five different scales. The kernels exhibit desirable characteristics of spatial frequency, spatial locality, and orientation selectivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Gabor wavelet representation (the real part and the magnitude) of a sample image. (a) The real part of the representation and (b) the magnitude of the representation.</figDesc><graphic coords="4,101.76,315.99,390.00,234.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example FERET images used in our experiments (cropped to the size of 128 2 128 to extract the facial region). Note that the images are acquired during different photo sessions, they display both different lighting conditions and facial expressions. Two images are randomly chosen from the three images available for each subject for training, while the remaining image (unseen during training) is used for testing. In particular, the above figure shows in the top two rows the examples of training images used in our experiments, and the bottom row examples of test images.</figDesc><graphic coords="7,108.42,62.25,374.00,183.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>X</head><label></label><figDesc>AS DEFINED BY (6) USING THREE DIFFERENT DOWNSAMPLING FACTORS, RESPECTIVELY: = 4, 16, OR 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Face recognition performance of PCA on the augmented Gabor feature vector X , = 64, using four different similarity measures: L1 (L distance measure), L2 (L distance measure), Md (Mahalanobis distance measure), and Cos (cosine similarity measure).</figDesc><graphic coords="8,307.50,62.28,241.44,189.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,100.26,62.24,390.00,239.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,78.96,62.25,433.00,270.33" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their critical and constructive comments and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Liu was supported in part by the New Jersey Institute of Technology under SBR Grant 421270. The associate editor coordinating the review of this manuscript and approving it for publication was Dr.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chengjun Liu (S'98-M'99) received the Ph.D. degree from George Mason University, Fairfax, VA, in 1999.</p><p>He is an Assistant Professor of computer science at the New Jersey Institute of Technology, Newark. His research interests are in computer vision, pattern recognition, image processing, evolutionary computation, and neural computation. His recent research has been concerned with the development of novel and robust methods for image/video retrieval and object detection, tracking and recognition based upon statistical and machine learning concepts. The class of new methods includes the probabilistic reasoning models (PRM), the enhanced Fisher models (EFM), the enhanced independent component analysis (EICA), the Gabor-Fisher Classifier (GFC), and the independent Gabor features (IGF) method. He has also pursued the development of novel evolutionary methods leading to the development of the evolutionary pursuit (EP) method for pattern recognition in general, and face recognition in particular. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Harry</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Facial recognition vendor test 2000</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="http://www.dodcounterdrug.com/facialrecogni-tion/FRVT2000/documents.htm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evidence for edge and bar detectors in human vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="431" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human and machine recognition of faces: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sirohey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="705" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-dimensional spectral analysis of cortical receptive field profiles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="847" to="856" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional cortical filters</title>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face and gesture recognition: Overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="675" to="676" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principal Component Neural Networks: Theory and Applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Diamantaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classifying facial actions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="974" to="989" />
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Representation and Recognition in Vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminant analysis for recognition of human face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1724" to="1733" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2379" to="2394" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scale-invariance and self-similar &apos;wavelet&apos; transforms: An analysis of natural scenes and mammalian visual systems</title>
	</analytic>
	<monogr>
		<title level="m">Wavelets, Fractals and Fourier Transforms: New Developments and New Applications</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Farge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hunt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vassilicos</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="151" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What is the goal of sensory coding</title>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The use of multiple measures in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophys</title>
		<imprint>
			<biblScope unit="page" from="1233" to="1258" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application of the Karhunen-Loeve procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Example-based learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distortion invariant object recognition in the dynamic link architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Konen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evolutionary pursuit and its application to face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="570" to="582" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust coding schemes for indexing and retrieval from large face databases</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="132" to="137" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Gabor feature classifier for face recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th</title>
		<meeting>8th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">Computer Vision</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">July 9-12, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classifying facial attributes using a 2-D Gabor wavelet representation and discriminant analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Automatic Face and Gestrure Recognition</title>
		<meeting>4th IEEE Int. Conf. Automatic Face and Gestrure Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mathematical description of the responses of simple cortical cells</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marcelja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1297" to="1300" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Looking at people: Sensing for ubiquitous and wearable computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The FERET database and evaluation procedure for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imag. Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An active vision architecture based on iconic representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="461" to="505" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic recognition and analysis of human faces and facial expression: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recognition without correspondence using multidimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="52" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using discriminant eigenfeatures for image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="831" to="836" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
