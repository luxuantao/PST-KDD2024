<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Insti-tute of Geodesy and Photogrammetry</orgName>
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<postCode>8093</postCode>
									<settlement>ZÃ¼rich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C33CC41544B31D273F5C5C2DEAE22743</idno>
					<idno type="DOI">10.1109/TGRS.2014.2321423</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T ODAY, the majority of humans live in cities, and it is antic- ipated that, by the year 2030, this will further increase to a level of 60% <ref type="bibr" target="#b0">[1]</ref>. Developing appropriate infrastructure capable of meeting the needs of a growing population calls for constant monitoring and map updating. The high amount of small (e.g., trees) or narrow objects (e.g., footpaths) can only be captured with very high resolution (VHR) remote sensing images. Huge amounts of data are acquired with either spaceborne or airborne sensors to serve as basis for cartographic data usually being drawn manually. Manual semantic annotation is extremely time consuming and costly, particularly if dealing with VHR images that have a high level of detail. Thus, map databases are often outdated. Moreover, in case of crisis situations evoked by, for example, earthquakes or inundations, rapid mapping is essential to organize instant response actions. In order to reduce the time and cost of mapping, automation is required-which is why one of the main goals of remote sensing research in the past decades has been semantic classification. Despite large efforts over the last 30 years, this task remains largely unsolved.</p><p>What makes semantic classification challenging particularly in urban areas is the presence of many object classes within a single VHR image, the often small extent and heterogeneous structure due to fine-grained texture details, and the strongly varying between-class dependences. This leads to a large within-class variability while keeping the between-class variability at a low level at the same time. Here, we view semantic classification in a supervised setting that assigns one class label to each pixel in the image. The different class labels correspond to the specific object categories needed for mapping which depend on the application at hand. In this paper, we group urban objects into four categories: buildings (of arbitrary size), streets (including bare ground), high vegetation, and low vegetation.</p><p>In general, a supervised classification consists of four main steps: 1) feature extraction from the raw input data; 2) classifier training on a subset of the input data with labeled ground truth; 3) validation on another subset with ground truth; and 4) classification of the data using the previously trained and validated classifier. Although a vast body of literature deals with the last two steps considering mostly machine learning methods like support vector machines (SVMs) <ref type="bibr" target="#b1">[2]</ref>, fuzzy approaches <ref type="bibr" target="#b2">[3]</ref>, random forests <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, or boosting <ref type="bibr" target="#b5">[6]</ref>, less systematic attention has been paid to the design and selection strategies of features. Usually, raw pixel intensities of all spectral bands are used, often enriched with features returned by the Normalized Differential Vegetation Index (NDVI) or some texture filter bank. Because it is, in general, not known in advance which combinations or mapping functions of features would help in separating object classes optimally, manual feature selection can be viewed as "educated guessing," although certain features are, of course, physically motivated (e.g., NDVI).</p><p>We propose to couple feature selection and training in such a way that we allow a boosting classifier to directly choose those features, which best discriminate the target classes, from an extensive set of feature candidates. Instead of learning a highly complex feature set like what is done in deep learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> or with texture priors in the framework of Markov random fields <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we aim at providing a quasi-exhaustive feature bank that is capable of adapting to scene specific properties in terms of radiometry, object classes, etc.</p><p>Since the seminal work of Viola and Jones <ref type="bibr" target="#b10">[11]</ref> (and similarly that of Simard et al. <ref type="bibr" target="#b11">[12]</ref>), integral images have become very popular to compute box filters to approximate smooth filters. For example, Bay et al. <ref type="bibr" target="#b12">[13]</ref> make use of this concept to approximate Hessians in order to speed up the computation of Speeded Up Robust Features (SURF) descriptors. We make use of integral images for averaging and approximating derivatives in order to efficiently extract this feature set despite its high dimensionality. Boosting has the advantage that the extraction of the whole feature set is only done once for a rather small training area, whereas for testing, only the most discriminative subset of features explicitly selected during training has to be extracted. More precisely, features that do contribute only very little are completely discarded as opposed to down-weighting like in case of SVMs for example. Computational speed, memory requirements, and classification accuracy further benefit from computing the majority of features randomly regarding patch sizes and location. These so-called Randomized Quasi-Exhaustive (RQE) features serve as input to boosting.</p><p>In order to evaluate the proposed feature/classifier setup, we perform an extensive set of experiments on five VHR urban scene images with three spectral bands. Cross-validation ensures that any bias caused by a specific training/testing setup is minimized. The goal of this study is threefold: 1) to compare the proposed RQE feature bank to various baseline features; 2) to evaluate the influence of three different color spaces; and 3) to test boosting decision stumps (linear) versus boosting decision trees, which, on the contrary to stumps, can represent nonlinear decision boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Although feature design and selection has recently led to significant progress in computer vision and machine learning (e.g., <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref>), the classification of VHR remote sensing data is usually still done based on standard features like raw pixel intensities. In <ref type="bibr" target="#b17">[18]</ref>, it is advocated to include intensities within a certain neighborhood around the pixel of interest for larger support. Other studies that use VHR aerial imagery to classify multiple object classes in urban scenes (e.g., <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>) additionally exploit height information obtained either from airborne-laser-scanning platforms or dense image matching.</p><p>One strategy for discriminating different object classes is to model texture via prior distributions of graphical models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Helmholz et al. <ref type="bibr" target="#b22">[23]</ref> apply the Markov random field of <ref type="bibr" target="#b8">[9]</ref> to characterize texture for the classification of aerial and satellite images.</p><p>A more common approach to texture description is the banks of texture filters (e.g., <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>). The image channels are convolved with filters (e.g., approximations of Gaussians, Laplacians, etc.), and their responses act as features during classification. A large variety of texture filter banks exists and has been applied to various tasks, for example, image segmentation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and image registration <ref type="bibr" target="#b27">[28]</ref>. Although being commonly used for pattern recognition problems in computer vision, filter banks have been less frequently adapted to remote sensing data. One of the few examples is the work in <ref type="bibr" target="#b28">[29]</ref> where the performance of Gabor filters on aerial images was evaluated.</p><p>Generally, banks of filters are rather straightforward, and they quickly lead to relatively promising results. However, performance varies depending on the type of texture <ref type="bibr" target="#b29">[30]</ref> because each filter bank is tuned for a specific application (i.e., a certain kind of texture). Thus, there is a need for a different approach that is capable of automatically adapting to various kinds of texture.</p><p>One concept that aims to resolve this problem is deep learning where (nonlinear) feature extractors are learned directly (e.g., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b30">[31]</ref>). This has been applied in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b32">[33]</ref> to patch-based road classification in aerial images. However, in a recent evaluation <ref type="bibr" target="#b33">[34]</ref> exploiting deep learning for feature extraction to classify VHR remote sensing data (with random forests), we found that deep learning is brittle to set up and, often, results are not improved compared to simple linear filter banks. In this paper, we thus resort to an alternative (linear) and somewhat more intuitive strategy by providing a quasiexhaustive set of candidate features to the classifier instead of learning these directly from raw pixels.</p><p>Alternatively, approaches exist that first compute a very large comprehensive set of features and, second, reduce the feature space dimensionality before training by partial least squares <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> or genetic algorithms <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> for example. In <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr">DollÃ¡r et al.</ref> propose to meaningfully organize the potentially huge comprehensive space of features in such a way that the most informative features can be picked by a boosting classifier without having to search the entire feature space brute force. Experiments on human face and pedestrian benchmark data indicate superior performance compared to manually chosen feature sets. As opposed to selecting a discriminative subset of features prior to training, DollÃ¡r et al. <ref type="bibr" target="#b39">[40]</ref> propose to let the classifier select discriminative features directly during training. Using integral images, they randomly generate a vast number of box filters and record their responses within a detection window. AdaBoost then selects the most discriminative features, thus largely improving the performance over standard baseline features. Note that the authors also report better performance compared to separate feature mining and learning as proposed earlier in <ref type="bibr" target="#b38">[39]</ref>. A similar approach, but using random forests instead of boosting, was proposed by FrÃ¶hlich et al. <ref type="bibr" target="#b40">[41]</ref> and applied to satellite images in <ref type="bibr" target="#b41">[42]</ref>. The authors randomly sample Haar-like features inside a window, which serve as input to an augmented random forest classifier. Contextual class-specific cues are learned iteratively based on previous classifier outputs. Another good example of joint optimization of classification and feature selection in the spectral dimension is shown in <ref type="bibr" target="#b42">[43]</ref>. The authors employ the generalized relevance learning vector quantization in order to distinguish between classes in the hyperspectral remotely sensed imagery.</p><p>A major advantage of doing feature selection and training jointly is that the classifier directly selects those features that it finds to be most discriminative, i.e., those that work best in combination with this particular classifier, data set, and object classes. Our approach is inspired by <ref type="bibr" target="#b39">[40]</ref> and follows similar lines of thought (in terms of the comprehensive feature candidates set) as <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>It is common practice in remote sensing to treat feature selection and classification as separate subsequent problems. In this paper, conforming to recent achievements in the field of machine learning, we pose these problems jointly. In contrast to the standard stepwise procedure, appropriate features for characterizing a particular scene are not being selected manually. This step is entirely left to the classifier that selects only those features from a large candidate set of RQE FEATURES, which minimize the misclassification error for a specific data set.</p><p>In the following sections, we describe the features which serve as baselines for our experiments. Thereafter, we turn to a detailed description of the proposed RQE features that serve as a candidate set for the boosting classifier, which is explained in the last part of this section. Emphasis is put on the differences between boosting stumps and boosting trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Features</head><p>Here, we describe the standard features used in remote sensing for aerial-and satellite-image classification. Usually, the user makes an "educated guess" about which specific features might be the best to classify a particular scene. While this procedure is straightforward, discriminative features not anticipated by the expert will not be computed, and class-specific patterns will possibly remain hidden. We propose to avoid ad hoc manual feature selection by letting the classifier choose features directly during training. In the following, we describe standard features that will act as baselines for experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Raw Pixel Intensities:</head><p>The most basic feature of a digital image is the intensity of a single pixel. Today's digital airborne and spaceborne optical sensors capture sun radiation in multiple spectral bands. Each band is recorded in a single channel, leading to several intensities per pixel, one for each channel. Standard VHR multispectral aerial sensors have four spectral channels (VHR multispectral satellite sensors usually have more): red, green, blue, and infrared. If we directly take the intensities of all available channels per pixel, the feature space dimension is equivalent to the number of channels (e.g., four in case of standard sensors). The advantage of these features is that they are intuitive and very fast to compute.</p><p>2) Raw Pixel Intensities Within 15 Ã 15 Neighborhood: The natural extension of taking single intensities over all channels per pixel is to include a certain neighborhood. State-of-the-art VHR data have a ground sampling distance (GSD) between 0.05 and 0.5 m. Although such high spatial resolution is of greatest value for urban remote sensing applications, it creates new challenges because a lot of small objects emerge that remain hidden in images of lower resolution. Single trees are composed of tens of pixels, and small superstructures on roofs (chimneys, dormers, etc.) become visible. Cars on the streets no longer consist of one or very few pixels but are clearly recognizable. As a consequence, the spectral variability within object classes considerably increases, whereas the interclass variability remains low. A common strategy to resolve potential ambiguities is discriminating classes based on their specific textural patterns. The most natural and straightforward approach for texture learning is to add intensities of neighboring pixels to such of the pixel of interest. More precisely, intensities of adjacent pixels within a square window are simply added to the feature vector. Usually, the window size varies between 3 Ã 3 and 21 Ã 21 pixels and is closely linked to image resolution and object size. We have tested a wide range of window sizes and found 15 Ã 15 to be sufficient for our data GSD, creating a 225-dimensional feature vector per channel. For input images with three channels, we thus obtain a 675-dimensional feature space. Note that, due to the strong overlap of neighboring windows, feature vectors of adjacent pixels are highly correlated. In turn, classification results can be expected to be a lot smoother than such based merely on single pixels.</p><p>3) 15 Ã 15 Pixel Neighborhood + NDVI: Along with the neighborhood of each single pixel, as described in the previous section, we add an NDVI channel. The NDVI is a standard feature created by the nonlinear combination of the red and near infrared channels and is widely used in optical remote sensing. It captures differences between vegetated and nonvegetated areas. In our experiments, we consider the NDVI as independent channel, i.e., all NDVI values in a 15 Ã 15 neighborhood of the pixel of interest are recorded. Adding the 225-dimensional NDVI feature vector to the already existing one of the three spectral bands thus results in a 900-dimensional feature space.</p><p>4) Texture Filters: Texture filter banks are another standard approach to capture object class patterns. They are a good example for "hand-crafted" features where an expert designs a relatively small bank of specifically engineered features to capture expected object class patterns. Images are convolved with a series of filters, and their responses are stored in a feature vector. A good representative for this kind of technique is the subset of the root filter set (RFS) used in <ref type="bibr" target="#b43">[44]</ref>, and we take it as baseline for our experiments.</p><p>All channels are convolved with the filters adopted from <ref type="bibr" target="#b43">[44]</ref>: four Laplacian-of-Gaussians (LoG), three Gaussians, and four first-order Gaussian derivatives. Four LoG and three Gaussian kernels are computed in different scales ({Ï, 2Ï, 4Ï, 8Ï} and {Ï, 2Ï, 4Ï}). As the first-order Gaussian derivatives are computed independently in two different scales {2Ï, 4Ï} and two directions (x and y), they produce four responses. In total, 11 features per channel are computed, generating a 33-dimensional feature space when using input images with three spectral bands. The Ï value has been empirically tested, and we found Ï = 0.7 to yield the best results. Note that, by convolving all channels with the described filters, a neighborhood of each pixel is implicitly considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RQE Features</head><p>Our aim is to avoid the data-specific design of features completely. Therefore, we generate a vast and redundant set of simple features, which can be computed efficiently via integral images <ref type="bibr" target="#b10">[11]</ref>, to allow the classifier to choose the most discriminative ones directly. This comprehensive RQE feature The difference is computed between the mean values ("-" subtracted from "+") of both image patches (red) and repeated with random position and size of patches.</p><p>bank enables one to handle different lighting conditions, object class properties, different scene structures, etc., thus mitigating scene-specific performance variations of standard filter banks <ref type="bibr" target="#b29">[30]</ref>. This approach can be viewed as an intuitive intermediate step between deep learning methods (e.g., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>) and standard feature banks (e.g., <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and <ref type="bibr" target="#b43">[44]</ref>).</p><p>A sliding window that is shifted pixelwise across the image acts as basic unit. All following operations take place inside this window and are exactly the same at each of its positions. Various window sizes were tested, and 15 Ã 15 performed best across all data sets.</p><p>The first subgroup of the RQE feature set consists of differences between pairs of patches. Instead of allowing only predefined regular patterns and patch shapes (as in <ref type="bibr" target="#b44">[45]</ref>), we generate a large pool of features by randomly choosing patch size and position (inspired by <ref type="bibr" target="#b39">[40]</ref>) within the 15 Ã 15 window. Each randomly chosen patch is being mirrored with respect to the central pixel of the window, and the difference between the mean intensities of both patches is calculated. Patches can take on any rectangular shape; sizes can vary from 1 Ã 1 to 15(14) Ã 14(15) (see Fig. <ref type="figure" target="#fig_1">1</ref>). Randomizing patch generation allows us to capture a wider range of textures while reducing runtime and memory, too. Note that this randomized patch sampling is done only once. The configuration then remains fixed for all positions of the sliding window, i.e., patch sampling is NOT repeated for each new window position.</p><p>Note that this definition of patch generation explicitly allows overlapping patches, both in the same iteration step and in consecutive steps. For example, consider a patch that has been randomly generated such that it overlaps with the central pixel of the 15 Ã 15 window. If we now mirror this patch at the central pixel, the second patch will overlap with the first one. In the following iteration, the newly generated initial patch may then overlap with either the first or the second patch of the previous iteration, etc. While this feature generation strategy results in highly correlated features, it is, in principle, capable of covering isotropic AND anisotropic texture patterns.</p><p>Designing symmetric filters accounts for the nadir perspective of remote sensing data. As opposed to terrestrial images, where a typical order of object classes naturally exists (e.g., sky appears in the top image part, a building facade in the middle, and cars at the bottom), the same scene acquired in nadir view does not have such a simple ordering. One could argue that still some typical organization of object classes exists (e.g., car on road and buildings close to streets), but we cannot assume any scene-specific structure in terms of "above," "below," "left," and "right." Recall that flight paths of aerial sensors vary and urban structures do often not follow any preferred direction (except cities with regular road grids). Thus, asymmetric filters would largely increase feature space dimensions, resulting in longer training times without significant information gain.</p><p>In addition to these differences based on randomly generated patch pairs, the second subgroup of the RQE feature set contains the following.  The first and the second subgroup contain only linear features.</p><p>To enable a fair comparison with the NDVI, which is a nonlinear combination of intensities across channels, we add a third nonlinear subgroup. This will also allow us to experimentally evaluate the general impact of nonlinear features, which do, of course, increase the feature dimensionality, on the classification performance. We include and extend the NDVI by computing it for all possible combinations of two channels. More precisely, for two different channels, we divide the difference of mean values of square patches by their sum. These square patches (ranging from 3 Ã 3 to 15 Ã 15) are centered on the central pixel of the 15 Ã 15 window, and both patches of a pair (i.e., one in the first channel and another one in the second channel of the channel combination) have the same size (42 features).</p><p>The second subgroup of the RQE feature set with only linear features consists of 2214 features. Adding the third subgroup, the nonlinear features, increases the feature space to 2256 dimensions. Recall that patches of the first subgroup are randomly sampled within the 15 Ã 15 window, and thus, feature space dimension depends on the number of samples.</p><p>This RQE feature set has inherited all the properties of <ref type="bibr" target="#b44">[45]</ref>, but it broadens them. True derivative filters are approximated reasonably, and the randomized patch-based subgroup naturally enables including a vast range of scales and texture frequencies. Patches are no longer restricted to a grid, and their shape is not imposed to be square. This new layout also takes advantage of color information explicitly by exploiting differences (and ratios) between spectral bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CLASSIFIER</head><p>The classifier that is chosen is a multiclass extension of discrete AdaBoost <ref type="bibr" target="#b45">[46]</ref>. Boosting has turned out to be significantly better (4%-10%) than linear SVMs and random forests in previous experiments for us. Kernelized SVMs have prohibitive memory and runtime requirements, so they have not been considered. AdaBoost combines multiple classifiers (so-called "weak learners") in an iterative fashion. If the weak classifiers (like decision trees) operate on a subset of features during testing, AdaBoost implicitly performs feature selection during training. In each iteration, the training data are reweighted so that samples that are hard for the current classifier obtain a larger weight. Then, the weak learner that reduces the weighted error rate the most is added to the classifier, and the next iteration starts. In image processing, weak learners for AdaBoost are usually decision trees, with decision stumps (trees with only one split and two leaves) being the most common choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AdaBoost</head><p>AdaBoost works by combining K weak classifiers h k into a strong classifier H. The final classifier (with the feature vector x) is a linear combination of the weak classifiers</p><formula xml:id="formula_0">H(x) = K k=1 Î± k h k (x).<label>(1)</label></formula><p>The classification result (if the sample is predicted in the positive or negative class) is the sign of H. The magnitude of H signifies the confidence of the classification result.</p><p>AdaBoost starts with equal weights (1/N ) for all N samples. In each iteration, AdaBoost tries to find the best weak learner to add to the strong classifier by looking at the weighted error rate . is the sum of all weights for which the weak learner h disagrees with the class label y i . The weak learner that maximizes | -0.5| is chosen for the next iteration and is assigned a weight</p><formula xml:id="formula_1">Î± Î± = 1 2 log 1 - . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The better the weak learner, the higher the weight that it gets in the final strong classifier. After the weak learner is added, the weights for each training sample are multiplied by a factor of exp(-Î±y i h(x i )), which is bigger than one if the weak learner disagrees with the ground-truth label (y i = h(x i )) and smaller than one if otherwise. This is the mechanism that ensures that "hard" samples are more likely to be classified correctly by the weak learner chosen in future iterations.</p><p>Before the next iterations, the weights are normalized to sum to one. Then, the weak learner that works best for the updated weights is added, and the whole procedure is repeated until no weak classifiers that operate better than chance are found or the desired number of weak classifier is reached.</p><p>As standard AdaBoost is a binary classifier, discriminating a "positive" (label y = +1) from a "negative" (label y = -1) class, and we are working with a multiclass scenario, we need to use a modification of it that allows more than two classes. In this paper, we use a variant of AdaBoost.MH <ref type="bibr" target="#b46">[47]</ref>, which is a multiclass extension to AdaBoost. <ref type="foot" target="#foot_1">2</ref> By using a one-versusall strategy (as known, for example, from multiclass SVMs), it is possible to reduce a C-class problem to a set of C binary problems. Instead of being scalar valued, H and h are now vector-valued functions with C components. The labels also become vectors. The label y i of a sample with class c has +1 in its cth component and -1 in the other components. Weak classifiers are of the form</p><formula xml:id="formula_3">h(x) = d â¢ Î·(x) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where d is also a vector with c components with +1 or -1 in each component and Î· is a binary classifier (as used in standard AdaBoost). The cth component of d is +1 or -1 if the correlation (under the influence of the sample weights) between the binary classifier Î· and the true class label is positive or negative, respectively. This construction means that each weak classifier tries to separate two sets of classes (the classes with +1 in d from those with -1), so we can expect that we need more weak classifiers than in the two-class case (by a factor of log 2 C) just by construction (since each weak learner is still effectively binary). The weights are also vectors whose components get updated independently of each other in the same fashion as in (2), so the weight increases for the components where h disagrees with the class label and decreases otherwise. There is also a per-class weighted error rate, and the "best" weak learner is the one that has the lowest weighted error rate averaged over all components. The predicted class in H(x) is the one whose component has the highest value. A possible confidence measure is the distance between this value and the second highest value.</p><p>If AdaBoost.MH were applied to a binary classification problem, the two components of each vector-valued variable would be equal in magnitude with different signs. The component of the "positive" class would be equal to a standard AdaBoost classifier, and the component of the "negative" class would be its inverse. As mentioned, weak learners used for AdaBoost are usually decision trees, often decision stumps. In general, decision trees are able to handle multiclass problems; however, the trees used in AdaBoost and AdaBoost.MH produce a binary decisioneither -1 or +1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decision Trees</head><p>We will describe stumps and trees with standard AdaBoost terminology for notational simplicity, as for AdaBoost.MH averages over vector components are necessary in multiple places. A decision tree consists of inner nodes (splits) and end nodes (leaves). A decision is reached by starting at the root and applying the rules in the inner nodes until a leaf is reached.</p><p>is the decision function for nodes. Nodes defer to their child nodes if they are inner nodes. Which child node is chosen is determined by the splitting rule. The splitting rules are, in our case, simple thresholds on single feature dimensions. The decision of a leaf is +1 if the sum of weights of positive samples Î¼ + that ended up in this leaf during training is bigger than the sum of weights of negative samples Î¼ -and -1 if otherwise.</p><formula xml:id="formula_5">h(x) = root (x) (4) inner (x) = left (x), if x i &lt; Î¸ right (x), otherwise<label>(5)</label></formula><formula xml:id="formula_6">leaf (x) = +1, if Î¼ + &gt; Î¼ - -1, otherwise<label>(6)</label></formula><p>Each inner node has its own parameter set of i (the feature index) and Î¸ (the threshold). An example tree is shown in Fig. <ref type="figure" target="#fig_5">3</ref>.</p><p>During training, trees are grown top-down. The tree learner seeks to find a combination of a feature index i and a threshold Î¸ that splits the set at a node in a way that decreases the weighted error rate the most. With Î¼ + and Î¼ -being the sum of weights of positive and negative samples in a leaf node, the weighted error in that node is given by</p><formula xml:id="formula_7">p = min(Î¼ + , Î¼ -). (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>This is because the label that the node predicts corresponds to the label with the bigger sum of weights, so the smaller sum corresponds to the weighted error.</p><p>A beneficial split has a +1 label on one side and a -1 label on the other side, as a split with the same label for both child nodes would not decrease the error rate. We call the sums for the weights in the left and right child nodes of a potential split Î¼ l,â¢ and Î¼ l,â¢ , respectively. This leads to the error after splitting as s = min(Î¼ l,+ + Î¼ r,-, Î¼ l,-+ Î¼ r,+ )</p><p>where the first part corresponds to a label of -1 for the left side and 1 for the right side and the second part is for the other case. The gain by splitting is</p><formula xml:id="formula_10">Î³ = p -s . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>To train the tree, we start with a root node that contains each training sample (the number of leaves is 1 at the start). At each iteration, we examine for each leaf node the possible gain that can be achieved by splitting. We split the node with the highest gain and repeat the procedure until we get the desired number of leaves. In the case of stumps, we only perform one split, meaning that we only have two leaf nodes. For decision trees, we perform three splits, leading to four leaf nodes. Since the decisions along the path perform an AND-combination of the split rules, this enables the four-leaf decision trees to capture correlations between features, something that decision stumps cannot capture (because the final classifier is a linear combination of simple thresholds over single features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We perform extensive experiments on five different data sets to do the following: 1) to compare the different feature sets; 2) to evaluate the impact of different color spaces in this context; and 3) to assess whether we gain any performance if boosting trees instead of stumps.</p><p>We select four standard categories that incorporate many subcategories: buildings, high vegetation (trees, high bushes, etc.), low vegetation (grass, low bushes, etc.), and streets (including cars). The ground truth for all data sets used in this evaluation was labeled manually.</p><p>In order to minimize the bias of specific train/test splits, we perform fivefold cross-validation for each data set as recommended in machine learning and pattern recognition literature (e.g., <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b49">[50]</ref>). All five data sets are divided into five vertical strips of equal size. Subsequently, the classifier is trained on features extracted from four different strips and tested on the fifth. This procedure is repeated five times, each time with another training/testing configuration. Finally, quality numbers for all five runs are averaged.</p><p>To speed up feature extraction and training, we do not extract features for all pixels in the training strips but select pixels randomly across the four training strips. Note that all pixels within a 15 Ã 15 window (i.e., the size of the sliding window used for feature extraction) are highly correlated anyway. Therefore, we do not expect to loose any significant information if using a subset. <ref type="foot" target="#foot_2">3</ref>For training the boosting classifier, we use single stumps or decision trees as weak learners. Three parameters of the classifier had to be chosen manually: the number of boosting iterations, the number of extracted random features per channel, and the number of leaves in the decision trees. We tested numerous choices of the latter two and found 500 extracted random features per channel and four leaves per decision tree to be good compromises between classification accuracy and computation time. The maximum number of boosting rounds was set to 500 iterations, but it should be noted that, in practice, most training runs converge much earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>We perform the evaluation of the proposed method on four aerial images and one satellite image (see Fig. <ref type="figure" target="#fig_6">4</ref>). Test images GRAZ1 and GRAZ2 have dimensions 800 Ã 800 and 1000 Ã 1000 pixels, respectively. Both are subsets of a large RGB image block acquired with a Microsoft Vexcel Ultracam D camera of the city of Graz (Austria) at a GSD of 25 cm. These images show a typical European city center with narrow streets, dense building formations, and vegetation. Due to the absence of a near infrared band, the pseudo-NDVI was computed for the tests that would use NDVI, replacing near infrared with the green channel.</p><p>Both images VAIH1 and VAIH2 are 1000 Ã 1000 pixels wide. They are a subset of a true-orthophoto mosaic (with GSD = 20 cm) acquired over the town Vaihingen an der Enz (Germany) using an Intergraph DMC sensor with red, green, and near infrared channels. Typical suburban areas of a European city are mapped with a majority of detached singlefamily houses and lots of vegetation. This data set is publicly available as part of a benchmark data set for urban object classification and 3-D building reconstruction <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p><p>Our satellite image (WV2) is a 1000 Ã 1000 pixel subset of a WorldView-2 stereo scene over the city of Zurich (Switzerland), showing the suburban parts of the city with mainly high buildings (blocks of flats). The image was pansharpened and thus has a GSD of 50 cm. For the orthophoto generation, we choose three out of eight spectral channels: red, green, and near infrared. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Color Spaces</head><p>In order to evaluate whether a specific choice of color space does have an impact on classification results, we compare the performance of the classifier with features extracted in three different color spaces: 1) standard RGB<ref type="foot" target="#foot_3">4</ref> ; 2) data set-dependent principal component analysis (PCA)-derived channels which can be viewed as a relative color transform because it adapts to each training image and decorrelates channels; and 3) YUV, a linear transform of RGB that separates intensity from color information (also known as YCbCr).</p><p>The YUV color space is primarily used in video applications. This model defines a color space as consisting of one luminance (Y) and two chrominance (UV) components. The two latter ones are the differences between the blue (U) and red (V) components of the original image and a reference value <ref type="bibr" target="#b52">[53]</ref>.</p><p>The PCA is a method commonly used in remote sensing for satellite image destriping, data decorrelation, or reduction of the data dimensionality <ref type="bibr" target="#b53">[54]</ref>. Here, as our goal is none of the above, we take advantage of its ability to perform the orthogonal transformation of the image's original color space into a linearly uncorrelated one. The first channel of the new color space will have the largest possible variance, and each subsequential channel has the highest variance possible under the constraint that it is again orthogonal to the preceding channels. It has to be emphasized that, because of the data-driven nature of this method, the parameters of the transformation are dependent on the input data and potentially adapt better to lighting and scene. An example for the GRAZ1 image shown in Fig. <ref type="figure" target="#fig_6">4</ref> is given in Fig. <ref type="figure" target="#fig_7">5</ref>. It can be seen that the first PCA channel mainly contains the intensity information of the image and the second one contains its degree of redness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussion</head><p>We show direct per-pixel overall classification accuracy results of the boosting classifier achieved with either decision stumps or decision trees. Results after cross-validation per color space are shown in Table <ref type="table" target="#tab_0">I</ref>. Per-class user's and producer's accuracies are presented in Table <ref type="table" target="#tab_1">II</ref>. The CPU times used for feature extraction, training decision stumps, training decision trees, and testing the classifier are shown in Table <ref type="table" target="#tab_2">III</ref>, respectively. One cutout of each of the five original images (see Fig. <ref type="figure" target="#fig_6">4</ref>) is shown in Fig. <ref type="figure">6</ref> together with the corresponding manually labeled ground truth. For these five cutouts, we display classification results (boosting decision trees) for RGB in Fig. <ref type="figure" target="#fig_9">7</ref> and for PCA in Fig. <ref type="figure" target="#fig_10">8</ref>. Note that we did neither any preprocessing of the input images (e.g., segmentation into superpixels and noise filtering) nor any postprocessing after classification (e.g., morphological cleaning and smoothing prior) to avoid biasing the results. To test for the statistical significance of the results, we employ the Wilcoxon signedrank test <ref type="bibr" target="#b54">[55]</ref>. It was chosen over the paired Student's t-test because it does not make the assumption that the differences in classifier performance are normally distributed. We test five data sets with five cross-validation runs each, resulting in 25 data points for the comparison of two different classifiers. When comparing families of classifiers (i.e., stumps against trees), more data points are available. We use the 99% confidence interval (p &lt; 0.01) to decide if two classifiers are significantly different. All significance tests are for boosting trees with 500 iterations unless otherwise specified. Boosting deci-sion trees instead of stumps yields significantly (p â 10 -70 ) better results (see Table <ref type="table" target="#tab_0">I</ref>). We plot the overall classification accuracy versus boosting iterations for all color spaces and separate stumps from trees (see Fig. <ref type="figure" target="#fig_11">9</ref>). For all three color spaces, it becomes apparent that boosting four-leaf trees outperforms stumps in terms of the final achievable accuracy after 500 iterations (see zoomed windows inside the graphs). It seems that trees somewhat better capture discriminative patterns in features per object class, i.e., feature structures and dependences can be better expressed with trees. One possible explanation is that trees with more than one level are capable of modeling more complex class boundaries in feature space directly. Another interesting observation underpining this clue is that boosting decision trees reduces the accuracy gap between the different input feature sets. In general, simpler features (e.g., 15 Ã 15 pixel neighborhood) gain more accuracy by using decision trees. On the downside, boosting decision trees takes longer.</p><p>The choice of color space does only have a small impact on classification performance. Whether or not the differences are significant depends on the choice of feature. Classifiers using a 15 Ã 15 neighborhood in YUV and PCA obtain significantly (p â¤ 0.002) better results over that using RGB, but when NDVI is added to RGB, the difference becomes insignificant (cf. Fig. <ref type="figure" target="#fig_11">9</ref>). When using our RQE features, using PCA results in a slight (0.5%-0.7%) but significant (p â¤ 0.002) improvement over both RGB and YUV. There is no significant difference between RGB and YUV when used with the RQE features. These effects can be seen, for example, if we visually compare the classification results of the subscene of GRAZ2 for RGB (see Fig. <ref type="figure" target="#fig_9">7</ref>) and PCA (see Fig. <ref type="figure" target="#fig_10">8</ref>). Boosting with RQE or 15 Ã 15 features better separates building roofs and street, leading to generally smoother results.</p><p>The proposed RQE feature bank either outperforms the baselines or performs identically. In Fig. <ref type="figure" target="#fig_12">10</ref>, we accumulate ranks one to seven for RGB <ref type="foot" target="#foot_4">5</ref> and one to five for PCA and YUV, i.e., we count how often a feature set ranks first, second, third, etc., based on the results in Table <ref type="table" target="#tab_0">I</ref>. Note that  we only provide rankings for tree boosting per color space, where the performance gap between different feature sets is smaller than for stump boosting. These cumulative diagrams underline that the proposed RQE features most often rank first or second. However, the real performance gap in terms of percent points is small. Often, differences between the top three methods are below one percent point (e.g., see Table <ref type="table" target="#tab_0">I</ref>).</p><p>In terms of statistical significance, including the nonlinear RQE features does not result in a significant improvement.</p><p>For the RGB color space, we observe that RAW â RAW + NDVI &lt; WINN &lt; 15 Ã 15 &lt; 15 Ã 15 + NDVI â RQE linear â RQE nonlinear. In the other color spaces, we get similar results (without the NDVI features). When using PCA, the 5% improvement that WINN features provide over using raw pixels is interestingly actually not statistically significant (p = 0.15). While WINN is slightly better on some crossvalidation folds, RAW is over 5% better than WINN on a few other folds, leading to this result. Also, when using PCA or YUV, 15 Ã 15 features are not significantly worse than RQE features. In most cases, boosting with RQE features achieves a given accuracy sooner than other methods (see red and orange curves in graphs in Fig. <ref type="figure" target="#fig_11">9</ref>). It turns out that this faster convergence of boosting with the RQE feature bank is mainly due to the averaging patches like that displayed in Fig. <ref type="figure" target="#fig_1">1</ref>. A closer look at the features being chosen by the boosting classifier (see Table <ref type="table" target="#tab_3">IV</ref>) reveals that, in the later iterations, the classifier focuses on smaller details and chooses features composed of single-pixel values. This suggests that coarser features help the boosting classifier in capturing lower scale structures first before extracting fine-grained details. Particularly in conjunction with boosting, which ranks features with respect to their ability to distinguish between the object classes, this strategy is beneficial.</p><p>The subset of the RFS as used in <ref type="bibr" target="#b43">[44]</ref> (WINN), which is a good representative of standard texture filter banks, does perform better than single raw pixel values (compare blue curves with brown/green curves in Fig. <ref type="figure" target="#fig_11">9</ref>) but worse than all other features. Interestingly, these filters perform inferior not only compared to the RQE filter bank, which can be expected be-cause RQE represents a much wider range of textures, but also compared to raw pixel intensities within a 15 Ã 15 neighborhood. It seems that banks of few specifically predesigned filters decrease performance by artificially constraining the possible solution space of the learner.</p><p>The full power of the learning method becomes apparent if we consider the performance of raw pixel intensities within a 15 Ã 15 neighborhood (displayed in purple in Fig. <ref type="figure" target="#fig_11">9</ref>). Although RQE features outperform this baseline (if averaged across all data sets), 15 Ã 15 results are very close (within two percent points). Rankings for tree boosting (see Fig. <ref type="figure" target="#fig_12">10</ref>), which count all data sets separately, show that raw pixel intensities within a 15 Ã 15 neighborhood rank identically in YUV color space as the linear RQE feature set and very close to the nonlinear RQE feature set. If one wants to avoid the high memory and runtime requirements during the training with RQE, a possibility is to simply use raw pixel intensities within a certain neighborhood (in conjunction with the NDVI, if available, see black curve in Fig. <ref type="figure" target="#fig_11">9</ref>, RGB color space). In general, features collected within To put the results in our study into a broader context, we compare to the related work on VHR aerial image classification of urban scenes. For example, Kluckner et al. <ref type="bibr" target="#b18">[19]</ref> attempt to classify the aerial VHR data of Graz, Dallas, and San Francisco into five urban land-cover classes quite similar to the ones that we use (buildings, green areas, water bodies, trees, and street layer). The data consists of standard RGB imagery and the corresponding normalized digital surface model. By combining the covariance descriptors in <ref type="bibr" target="#b55">[56]</ref> with a random forest classifier and contextual information modeled by a conditional random field, they achieve overall classification accuracies up to 79% (without height information) and 93% (with height information), respectively. Their follow-up work <ref type="bibr" target="#b19">[20]</ref>, which focuses on improved covariance descriptors, achieves similar accuracies on the same data sets: up to 78% (without height information) and 86% (with height information). In a similar setting, Guo et al. <ref type="bibr" target="#b20">[21]</ref> classify four urban land-cover classes nearly identical to ours (buildings, vegetation, artificial ground, and natural ground) based on VHR aerial images and full-waveform LiDAR data, also using random forests. Overall, accuracies range from 82% (using only spectral features) to 96% (spectral and LiDAR features). In that study, the class frequencies of the test data are rather unbalanced, for example, they report only 40% classification accuracy for vegetation, which, however, comprises only 4% of the test data. Rottensteiner et al. <ref type="bibr" target="#b21">[22]</ref> combine LiDAR data with optical features (NDVI) using the Dempster-Shafer theory to detect buildings in urban areas. They achieve 90%-95% accuracy.  Here, we obtain accuracies of 79%-83% without using height information, which is on par with or slightly above the mentioned state-of-the-art works. It should be pointed out that the numbers are not really comparable because of large differences between the data sets (e.g., Vaihingen is more difficult than Graz due to its narrow winding roads and small houses), evaluation procedures (e.g., not all authors use crossvalidation), class definitions, and amount of training data used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND OUTLOOK</head><p>We have presented a simple yet powerful strategy for the semantic segmentation of VHR remote sensing images. Providing a comprehensive feature bank where the learner directly picks optimal features (implemented efficiently via integral images) leads to very good classification results. A detailed experimental comparison of the proposed RQE feature bank with several baselines using VHR remote sensing images of complex urban scenes underpins this claim. The RQE feature bank in combination with boosting is capable of covering a large range of texture frequencies. Note that no postprocessing or smoothness prior was introduced, which would probably further improve results.</p><p>It turns out that data-specific feature engineering seems unnecessary once we directly let the classifier choose the most discriminative features for the data and object classes at hand.</p><p>In future work, we plan to add more prior knowledge about the scene structure into the classifier. For example, one could insert this method as unary potential into a conditional random field <ref type="bibr" target="#b56">[57]</ref>. Another interesting possibility is to learn object and scene structures via a bag-of-visual-words approach like that done for the categorization of scenes in <ref type="bibr" target="#b57">[58]</ref>.</p><p>A general bottleneck of boosting with very large sets of features is the significant computational cost at training time (runtime and memory). In future work, we will thus investigate how we can speed up training and reduce memory requirements. For example, Appel et al. <ref type="bibr" target="#b58">[59]</ref> recently proposed to accelerate training by pruning irrelevant features early, bounding the split errors early, and only evaluating features to completion that have a chance of becoming the best feature. Another possibility is to parallelize the training of the weak classifiers. The most time-consuming part is finding the best feature to split on in the decision tree nodes; thus, evaluating candidate splits in parallel will result in large speed gains (1-2 orders of magnitude, depending on the number of available processors). That step is trivial to parallelize since the computations are independent.</p><p>We believe that, in many remote sensing applications, large general purpose feature sets in combination with discriminative learning can completely relieve the user of choosing features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Features,</head><label></label><figDesc>Color Spaces, and Boosting: New Insights on Semantic Classification of Remote Sensing Images Piotr Tokarczyk, Jan Dirk Wegner, Stefan Walk, and Konrad Schindler, Senior Member, IEEE Abstract-A major yet largely unsolved problem in the semantic classification of very high resolution remote sensing images is the design and selection of appropriate features. At a ground sampling distance below half a meter, fine-grained texture details of objects emerge and lead to a large intraclass variability while generally keeping the between-class variability at a low level. Usually, the user makes an educated guess on what features seem to appropriately capture characteristic object class patterns. Here, we propose to avoid manual feature selection and let a boosting classifier choose optimal features from a vast Randomized Quasi-Exhaustive (RQE) set of feature candidates directly during training. This RQE feature set consists of a multitude of very simple features that are computed efficiently via integral images inside a sliding window. This simple but comprehensive feature candidate set enables the boosting classifier to assemble the most discriminative textures at different scale levels to classify a small number of broad urban land-cover classes. We do an extensive evaluation on several data sets and compare performance against multiple feature extraction baselines in different color spaces. In addition, we verify experimentally if we gain any classification accuracy if moving from boosting stumps to trees. Cross-validation minimizes the possible bias caused by specific training/testing setups. It turns out that boosting in combination with the proposed RQE feature set outperforms all baseline features while still remaining computationally efficient. Particularly boosting trees (instead of stumps) captures class patterns so well that results suggest to completely leave feature selection to the classifier. Index Terms-Classification, feature extraction, land cover, pattern recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Randomized patches: Examples of rectangular patches of random size. The grid within dashed lines symbolizes the image, and solid black lines depict a sliding window of size 15 Ã 15 centered on the pixel of interest (dark blue).The difference is computed between the mean values ("-" subtracted from "+") of both image patches (red) and repeated with random position and size of patches.</figDesc><graphic coords="4,45.73,70.54,233.88,83.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left image: Differences between means inside square patches with different sizes [(dark gray) 3 Ã 3 to 15 Ã 15 (light gray)] centered on the pixel of interest. Right image: Differences between means inside square patches (red) of equal size over different image channels [within the sliding 15 Ã 15 window (solid black line)].</figDesc><graphic coords="4,308.74,69.98,234.12,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 )</head><label>1</label><figDesc>All pixel intensities within the 15 Ã 15 window in three different scales: 1) raw intensities per pixel (675 features); 2) the same window after 3 Ã 3 mean filtering (675 features); and 3) after 5 Ã 5 mean filtering (675 features). We also add mean values for 3 Ã 3 and 5 Ã 5 up to 15 Ã 15 patches, but only for the central pixel of the 15 Ã 15 window (21 features). Note that this is different from filtering the whole window (as in case of 3 Ã 3 and 5 Ã 5 meanfiltering), which would largely increase the feature space dimensionality without significant performance gain. 2) Differences between mean values of square patches of the same size centered on the central pixel of the 15 Ã 15 window between different spectral channels [see Fig. 2 (right)]. Square patch sizes range from 3 Ã 3 to 15 Ã 15 (42 features). 3) Differences between mean values of square patches of different sizes (ranging from 3 Ã 3 to 15 Ã 15), but only within the same spectral channel (126 features) [see Fig. 2 (left)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example decision tree with three leaves.</figDesc><graphic coords="6,96.73,70.18,132.12,176.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Original images used for the experiments.</figDesc><graphic coords="7,96.44,70.14,405.96,79.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. PCA example with (from left to right) the first, second, and third PCA components of the GRAZ1 image shown left in Fig. 4.</figDesc><graphic coords="7,307.95,197.04,246.12,74.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 )</head><label>4</label><figDesc>Fig.6. Cutouts of the test images (cf. Fig.4) with manually labeled ground truth in the bottom row.</figDesc><graphic coords="10,118.73,418.62,350.28,191.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. RGB with boosting decision trees: Results for the different feature settings of a subscene per original test image. Streets are displayed black, buildings red, high vegetation green, and low vegetation yellow.</figDesc><graphic coords="11,96.44,70.46,405.96,423.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. PCA with boosting decision trees: results for the different feature settings of a subscene per original test image. Streets are displayed black, buildings are red, high vegetation is green, and low vegetation is yellow. a certain neighborhood around the pixel of interest always outperform single pixel intensities.To put the results in our study into a broader context, we compare to the related work on VHR aerial image classification of urban scenes. For example, Kluckner et al.<ref type="bibr" target="#b18">[19]</ref> attempt to classify the aerial VHR data of Graz, Dallas, and San Francisco into five urban land-cover classes quite similar to the ones that we use (buildings, green areas, water bodies, trees, and street layer). The data consists of standard RGB imagery and the corresponding normalized digital surface model. By combining the covariance descriptors in<ref type="bibr" target="#b55">[56]</ref> with a random forest classifier and contextual information modeled by a conditional random field, they achieve overall classification accuracies up to 79% (without height information) and 93% (with height information), respectively. Their follow-up work<ref type="bibr" target="#b19">[20]</ref>, which focuses on</figDesc><graphic coords="12,108.73,70.54,370.44,473.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Overall classification accuracy plotted versus (logarithmically scaled) boosting iterations for (upper row) stumps and (lower row) trees with features extracted from RGB, PCA, and YUV channels.</figDesc><graphic coords="13,47.44,69.82,504.12,449.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Cumulative ranking of the feature sets per color space (for trees). Darker colors indicate higher scores at the corresponding rank (1-7 for RGB and 1-5 for PCA and YUV), and exact numbers are written inside cells.</figDesc><graphic coords="13,75.95,567.39,446.28,99.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,132.74,101.78,323.04,306.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,100.24,110.36,387.48,401.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DECISION</head><label>I</label><figDesc>STUMPS (DS)/DECISION TREES (DT): OVERALL CLASSIFICATION ACCURACIES (IN %) AFTER CROSS-VALIDATION AND 500 BOOSTING ITERATIONS. BEST SCORES ARE BOLD AND UNDERLINED, SECOND BEST SCORES ARE BOLD, AND THIRD BEST SCORES ARE UNDERLINED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DECISION</head><label>II</label><figDesc>TREES: USER'S (UA) AND PRODUCER'S (PA) ACCURACIES (IN %) AFTER CROSS-VALIDATION AND 500 BOOSTING ITERATIONS. BUI STANDS FOR BUILDINGS, STR STANDS FOR STREETS, GRA STANDS FOR GRASS, AND TRE STANDS FOR TREES. BEST SCORES ARE BOLD AND UNDERLINED, SECOND BEST SCORES ARE BOLD, AND THIRD BEST SCORES ARE UNDERLINED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III FEATURE</head><label>III</label><figDesc>EXTRACTION, TRAINING DECISION STUMPS, TRAINING DECISION TREES, TESTING: CPU TIMES (IN MINUTES) NEEDED FOR EXTRACTING AND RECORDING DIFFERENT FEATURES. THE EVALUATION HAS BEEN PERFORMED ON THE AMD OPTERON CPUS AND 32/64 GB OF RAM Fig. 6. Cutouts of the test images (cf.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV VAIH2</head><label>IV</label><figDesc>, RGB, RQE NONLINEAR: A LIST OF THE FEATURE TYPES THAT HAVE BEEN CHOSEN DURING TRAINING THE BOOSTING CLASSIFIER (DECISION STUMP SETTING) AFTER 10, 100, AND 500 ITERATIONS. WE SHOW THE FREQUENCY OF A PARTICULAR GROUP BEING CHOSEN AND A SUM OF THE WEIGHTS GIVEN TO A SPECIFIC FEATURE GROUP BY THE CLASSIFIER</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The numbers of features in brackets are always given under the assumption of three channels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the experiments, the software in<ref type="bibr" target="#b47">[48]</ref> was used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that one-fifth of all training pixels correspond to 200k pixels in case of 1000 Ã 1000 images and to 128k pixels for 800 Ã 800 images, which was found to deliver sufficiently stable features for training.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We use the label "RGB" also for the data sets where the image channels are red, green, and near infrared. The available channels for each data set are described in the previous section.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Recall that we tested two more input feature sets for the RGB color space (compared to PCA and YUV) because we added the NDVI to raw pixel intensities and to such of a 15 Ã 15 neighborhood.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In December 2011, he won an Eidgenoessische Technische Hochschule (ETH) postdoctoral fellowship grant, and since April 2012, he has been with the photogrammetry and remote sensing group at the Institute of Geodesy and Photogrammetry, ETH ZÃ¼rich, ZÃ¼rich, Switzerland. In 2014, he was awarded the "Wissenschaftspreis der Deutschen GeodÃ¤tischen Kommission" (science award of the German Geodetic Commission), which is awarded every two years to a highly qualified young scientist with a distinct international profile. His main interests are in the interdisciplinary field of photogrammetry, remote sensing, computer vision, and machine learning, with focus on probabilistic pattern recognition and </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">World Population Prospects 1950-2050. The 2012 Revision. Key Findings and Advance Tables</title>
		<ptr target="http://esa.un.org/wpp/Documentation/pdf/WPP2012_%20KEY%20FINDINGS.pdf" />
		<imprint>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>United Nations Population Division</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fusion of support vector machines for classification of multisensor data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Waske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3858" to="3866" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel technique for subpixel image classification based on support vector machine</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2983" to="2999" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random forest classifier for remote sensing classification</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="222" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests for land cover classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Gislason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="300" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple classifiers applied to multisource remote sensing data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Briem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2291" to="2299" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture modeling by multiple pairwise pixel interactions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1110" to="1114" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its application to texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1660" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boxlets: A fast convolution algorithm for signal processing and neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Inf. Process. Syst</title>
		<meeting>Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="571" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speeded-Up Robust Features (SURF)</title>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selection of relevant features and examples in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="245" to="271" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="384" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, context</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Limits and Challenges of Optical Very-High-Spatial-Resolution Satellite Remote Sensing for Urban Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dell'acqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stasolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lisini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Wiley</publisher>
			<biblScope unit="page" from="35" to="48" />
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic classification in aerial imagery by integrating appearance and height information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kluckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2009</title>
		<title level="s">ser. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Zha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>-I. Taniguchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Maybank</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5995</biblScope>
			<biblScope unit="page" from="477" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic classification by covariance descriptors within a randomized forest</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kluckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th ICCV Workshops</title>
		<meeting>IEEE 12th ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relevance of airborne lidar and multispectral image data for urban scene classification using random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boukir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using the Dempster-Shafer method for the fusion of LIDAR data and multi-spectral images for building detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kubik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="300" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-automatic quality control of topographic data sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Helmholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="959" to="972" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constructing models for content-based image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Texture segmentation by multiscale aggregation of filter responses and shape elements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, texture cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image registration using the Walsh transform</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lazaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2343" to="2357" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gabor wavelets for texture edge extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Foerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Commiss. III Symp</title>
		<meeting>ISPRS Commiss. III Symp</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluation of texture energies for classification of facade images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drauschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Archives Photogramm</title>
		<meeting>ISPRS Archives Photogramm</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Inf</title>
		<meeting>Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An evaluation of feature learning methods for high resolution image classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Ann</title>
		<meeting>ISPRS Ann</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="389" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature sets and dimensionality reduction for visual object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf., 2010</title>
		<meeting>Brit. Mach. Vis. Conf., 2010</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature selection by genetic algorithms in object-based classification of IKONOS imagery for forest mapping in Flanders, Belgium</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Coillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Wulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="476" to="487" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Endmember extraction using a combination of orthogonal projection and genetic algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mobasheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Zoej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaepman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="165" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature mining for image classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic segmentation with millions of features: Integrating multiple cues in a combined random forest approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>FrÃ¶hlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="218" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Land cover classification of satellite images using contextual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>FrÃ¶hlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Ann</title>
		<meeting>ISPRS Ann</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relevance-based feature extraction for hyperspectral images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mendenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Merenyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="658" to="672" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">A</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond handcrafted features in remote sensing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Ann. Photogramm</title>
		<meeting>ISPRS Ann. Photogramm</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MULTIBOOST: A multi-purpose boosting package</title>
		<author>
			<persName><forename type="first">D</forename><surname>Benbouzid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-D</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>KÃ©gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The DGPF-test on digital airborne camera evaluation overview and test design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm.-Fernerkundung-Geoinf</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The ISPRS benchmark on urban object classification and 3D building reconstruction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS Ann</title>
		<meeting>ISPRS Ann</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>rd ed. Upper Saddle River</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Remote Sensing and Image Interpretation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lillesand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr. Bull</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945-12">Dec. 1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning on lie groups for invariant detection and tracking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An overview and comparison of smooth labeling methods for land-cover classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4534" to="4545" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for aerial scene classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheriyadat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="439" to="451" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Quickly boosting decision trees-Pruning underachieving features early</title>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
