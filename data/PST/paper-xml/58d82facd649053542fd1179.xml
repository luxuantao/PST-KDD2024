<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video pornography detection through deep learning techniques and motion information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-12-08">December 8, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sandra</forename><surname>Avila</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computing Engineering</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Moreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Moraes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vanessa</forename><surname>Testoni</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung Research Institute Brazil</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computing Engineering</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siome</forename><surname>Goldenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
							<email>anderson.rocha@ic.unicamp.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video pornography detection through deep learning techniques and motion information</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-12-08">December 8, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">BC251620D5A0F67133C6C13DB7678DD1</idno>
					<idno type="DOI">10.1016/j.neucom.2016.12.017</idno>
					<note type="submission">Received date: 16 July 2016 Revised date: 28 October 2016 Accepted date: 6 December 2016 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pornography classification</term>
					<term>Deep learning and motion information</term>
					<term>Optical flow</term>
					<term>MPEG motion vectors</term>
					<term>Sensitive video classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent literature has explored automated pornographic detection -a bold move to replace humans in the tedious task of moderating online content. Unfortunately, on scenes with high skin exposure, such as people sunbathing and wrestling, the state of the art can have many false alarms. This paper is based on the premise that incorporating motion information in the models can alleviate the problem of mapping skin exposure to pornographic content, and advances the bar on automated pornography detection with the use of motion information and deep learning architectures. Deep Learning, especially in the form of Convolutional Neural Networks, have striking results on computer vision, but their potential for pornography detection is yet to be fully explored through the use of motion information. We propose novel ways for combining static (picture) and dynamic (motion) information using optical flow and MPEG motion vectors. We show that both methods provide equivalent accuracies, but that MPEG motion vectors allow a more efficient implementation. The best proposed method yields a classification accuracy of 97.9% -an error reduction of 64.4% when compared to the state of the art -on a dataset of 800 challenging test cases. Finally, we present and discuss results on a larger, and more challenging, dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Filtering sensitive media (pornographic, violent, gory, etc.) has growing importance, due to the booming consumption of online media by people of all ages; and among sensitive media types, pornography is often the most unwelcome. A range of applications has increased societal interest on the problem, e.g., detecting inappropriate behavior via surveillance cameras; or curtailing the exchange of sexually-charged instant messages, also known as "sexting", by minors. In addition, law enforcers may use pornography filters as a first sieve when looking for child pornography in the forensic examination of computers, or internet content. The main application, however, remains preventing uploading or accessing undesired content for certain demographics (e.g., minors), or environments (e.g., schools, workplace).</p><p>The precise definition of pornography is, of course, subjective, but here we will consider "any sexually explicit material with the aim of sexual arousal or fantasy" <ref type="bibr" target="#b0">[1]</ref>.</p><p>A natural approach to pornography detection consists in first trying to detect nudity <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and then defining appropriate thresholds to further filter the content. Such solutions commonly use human skin features, such as color and texture, and human geometry <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Those methods normally use that information for modeling which pixel values and spatial distribution characterize a nude person. Although the motivation for such methods is intuitive, it reveals ultimately na√Øve. People may show a lot of skin in activities that have nothing to do with sex (e.g., sunbathing, swimming, running, wrestling), leading to a lot of false positives. Skin exposure in itself, is not a reliable proxy for pornography detection. Conversely, some sexual practices involve very little exposed skin, leading to unacceptable false negatives. In addition, the reliance on many adhoc thresholds hinders the generalization of those techniques when facing diversity of ethnicities and skin colors.</p><p>Departing from the low-level skin-based methods, in more recent years, several authors have explored other types of solutions for adult content filtering, specially the ones inspired by the bag of words model from text classification, <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Those methods insert an intermediary description stage between the low-level features extracted from the images, and the classification component. Such methods normally involve choosing some low-level feature representation (e.g., gradient-like information), and creating a representative codebook. The involved steps are referred to as generating the codebook, coding the features and pooling the codewords count. In the end, a classifier learns, through examples, which representations belong to the pornography class. Clearly, such methods are more robust than the skin-based ones, but still suffer from some ambiguous cases. Choosing the codewords, the size of the codebook and which of the many coding and pooling strategies to use are also crucial steps for the good performance of the solutions.</p><p>Although thus far relatively underestimated for this problem, the motion information available in videos would likely help to disambiguate the most difficult cases in pornography classification. Unfortunately, only a few works have exploited spatio-temporal features or motion information in this problem until now <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. In these cases, the spatio-temporal feature evaluated was Space-Time Interest Points (STIP) <ref type="bibr" target="#b16">[17]</ref>, Dense Trajectories <ref type="bibr" target="#b17">[18]</ref>, Temporal Robust Features (TRoF) <ref type="bibr" target="#b15">[16]</ref>, and the motion information coming from a statistical analysis of the MPEG Motion Vectors. Particularly, in <ref type="bibr" target="#b15">[16]</ref>, the experiments confirmed that the incorporation of spatio-temporal information leads to more effective video-pornography classifiers. In that work, the authors showed that a custom-tailored method to capture motion outperforms the mentioned dense trajectories. In this work, in turn, we show that data-driven features are even more powerful, especially when extracted both from spatial and temporal data.</p><p>Given the difficulty of developing appropriate thresholds for skin-based detectors and also the several available choices when coding low-and mid-level features, in addition to the lack of proper motion-based features, and the recent success of Deep Learning solutions on similar tasks, we set forth the task of designing and developing deep learning techniques, to automatically grasp static and motion-based deep representations, straight from the data, that could leverage pornography classification.</p><p>Amongst the many machine learning techniques available, Deep Neural Networks, more specifically Convolutional Neural Networks (CNN), are showing groundbreaking results for image and video classification tasks <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Of particular attention, some authors have been studying how to adapt CNNs for human action recognition in videos, whereby the spatio-temporal information can be explored to improve the extracted features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Different architectures are possible, each one combining the spatial and temporal information in different ways, leading to better or worse features for the classification task. Some authors sought to extract the motion information implicitly by feeding a sequence of frames to the CNNs <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, while others opted for explicitly feeding this information to the network through a previously computed Optical Flow Displacement Fields image representation <ref type="bibr" target="#b19">[20]</ref>.</p><p>In spite of the success of deep learning techniques in the computer vision arena, their literature on pornography detection is very scarce. Pioneering the trend for pornography detection, Moustafa <ref type="bibr" target="#b21">[22]</ref> has explored majority voting classification on a sample of frames classified with off-the-shelf CNNs. However, the authors did not explore the most appropriate network configurations, parameters nor any spatio-temporal or motion information in their solution.</p><p>When targeting at sensitive media filtering, some interesting challenges appear for deep learning-based solutions: how to define an appropriate architecture; the possibility of reusing already trained architectures for related image categorization problems, thus avoiding the need for huge amounts of training data; and how to incorporate time/motion information, which complements the spatial/static information.</p><p>In this work, we design and develop deep learning-based approaches to automatically extracting discriminative spatio-temporal characteristics for filtering pornographic content in videos. As far as we know, this is the first time convolutional neural networks -along with motion information -is applied for pornography detection in videos. Although in this work we focus on the pornography modality, the methodology we discuss herein is versatile and its extension to other types of sensitive content is straightforward. The contributions of this paper are three-fold: i) A novel method for classifying pornographic videos, using convolutional neural networks along with static and motion information;</p><p>ii) A new technique for exploring the motion information contained in the MPEG motion vectors <ref type="bibr" target="#b25">[26]</ref>;</p><p>iii) A study of different forms of combining the static and motion information extracted from questioned videos.</p><p>We organized the remaining of this paper into five sections. In Sec. 2, we discuss existing approaches for dealing with the pornography detection problem, while in Sec. 3, we present a short summary of the necessary concepts to understanding this work. We then move on to Sec. 4, in which we introduce the methods we propose for classifying pornography in videos, incorporating static and motion information. In Sec. 5, we present the experimental setup, along with the experiments and validation of the proposed methods and existing counterparts in the literature. Finally, in Sec. 6, we conclude the work and point out to some possible future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review on Pornography Detection</head><p>Short et al. <ref type="bibr" target="#b0">[1]</ref> wrote a review of 46 articles that approached, to some extent, internet pornography. In their work, the authors highlighted the importance of explicitly defining the term pornography in each work, since it has direct influence on the results and issues that can be encountered further on. The definition is also relevant for comparisons among different works. As an example, some works consider the presence of genitals as being enough for classifying the content as pornography, whereas other authors argue that explicit sexual acts are necessary. It is proposed that a well-formalized definition should contain the type of pornography and the reason that it is apparently expecting to motivate the viewers. The definition we adopted in this work, proposed by Short et al. <ref type="bibr" target="#b0">[1]</ref>, denotes pornography as "any sexually explicit material with the aim of sexual arousal or fantasy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skin-based Techniques</head><p>Nudity detection using skin information has been extensively explored in the literature <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b26">27]</ref>. Fleck et al. <ref type="bibr" target="#b1">[2]</ref> proposed a two-step content-based retrieval strategy for returning images with naked people. First, the method filters the images that have large areas of skin regions. To identify skin pixels, it is used thresholds on the intensity, hue and saturation value of each pixel. Then, these areas are grouped and analyzed geometrically, validating if they could represent human limbs. On one hand, the authors point out that the first phase is vulnerable to scale and saturation, and returns false positives from scenes with many people, or from materials with colors that are similar to human skin. On the other, the geometrical analysis suffers from missing limbs because of occlusion, close-ups or even by failure of the skin detector, among other reasons. These aspects lead to low precision and recall measures, when in comparison to newer methods we shall see later.</p><p>Following a similar path, Jones and Rehg <ref type="bibr" target="#b5">[6]</ref> focused exclusively on the color information from the pixels, building skin-based statistical models. A histogram of 256 bins for each channel is computed from the skin images, and another for the non-skin. These histograms model the probability of the color belonging to a skin region. With a standard likelihood ratio approach, an RGB value can be labeled skin if above a certain threshold. A feature vector is then created comprising features that include the number of pixels detected as skin and the average confidence of the detected skin. A C4.5 decision tree classifier is used for the decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bag-of-Visual-Words techniques</head><p>The next milestone for the pornography detection problem was reached with the Bag-of-Visual-Words (BoVW) models. Deselaers et al. <ref type="bibr" target="#b9">[10]</ref>, aware that this type of solution had showed good results in many image classification problems, built a classifier for adult images using visual codebooks. Patches around interest points, with scaling and dimensionality reduction via Principal Component Analysis (PCA), were used as features. Codewords were selected through Gaussian mixture models, generating the codebook. The authors employed a hard-assignment coding policy followed by sum pooling. Other types of coding and pooling were proposed later on. The decision making considered Support Vector Machines (SVM) and Log-linear classifiers. The reported results showed that their method clearly outperforms the previous methods, mainly based on color features. In addition, combining this method with skin-based ones does not bring anything new to the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Classifying Videos</head><p>When it comes to video, the basic approach considers extracting the frames and applying an image-based description and classification approach. However, these methods disregard valuable information that videos provide, the concept of motion. Although not directly in the field of pornography detection, the importance of temporal information for action recognition has been assessed for many years now. Dollar et al. <ref type="bibr" target="#b27">[28]</ref> proposed a corner detector algorithm similar to the Harris detector <ref type="bibr" target="#b28">[29]</ref>, that seeks for "corners" in time. The detected "motion" corners are then described with cuboids around them. With the help of a codebook from the cuboids, the histograms of features from the short scenes demonstrated greater discrimination than the spatial-based descriptors. Some recent works keep the trend of exploring motion information for action recognition as Laptev et al. <ref type="bibr" target="#b29">[30]</ref>, Wang et al. <ref type="bibr" target="#b17">[18]</ref>, and Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref>.</p><p>Turning our attention to pornography detection, Jansohn et al. <ref type="bibr" target="#b10">[11]</ref> were one of the first authors to explore the time information while detecting pornography. They used a statistical analysis of MPEG-4 motion vectors, with a bag of visual words similar to the one proposed by Deselaers et al. <ref type="bibr" target="#b9">[10]</ref>. Different ways of combining the motion vector information in overlapping windows of time were experimented. A description of the video was generated by pooling these windows, generating a motion histogram. The decision making in the end considers an SVM classifier. The classifier using the time information alone gave effective results and was improved upon when combined with a BoVW-based (spatial characterization) approach.</p><p>Avila et al. <ref type="bibr" target="#b13">[14]</ref> proposed an extension of the bag-of-visual-words approach for the pornographic video detection task. The improved design involved new pooling and coding formalisms for the local descriptors. Instead of simply summing up the activations in the pooling step, as in Deselaers et al. <ref type="bibr" target="#b9">[10]</ref>, an estimation on the distribution of the descriptors distance to the codewords is used. For the new coding, a semi-soft scheme was used, on which different softness parameters, based on the variation of each cluster, are applied. The decisionmaking process considers an SVM classifier. Different datasets were used to validate the extension including a pornographic benchmark available online.</p><p>Another supplementary information provided by video, that can be used for pornography detection, comes from audio. Rea et al. <ref type="bibr" target="#b30">[31]</ref> proposed an audio feature extraction approach for this problem, that consists of analyzing the periodicity from the sound. The inspiration comes from the fact that this type of content usually has repetitive sounds. To capture and measure the periodicity, an autocorrelation of the energy filter is applied to the audio signal, and the area between the local maxima's and minima's curves is computed. If the area is above a certain threshold, that configures a repetitive sound, suggesting it comes from pornography. In an evaluation with diverse audio samples, not from pornographic content, a false alarm rate of 2% was reported. However, as the authors point out, this approach alone is not robust to other periodic sounds, such as in a tennis match, hence visual features should also be present to remove ambiguities.</p><p>Although the audio information might also be useful in the intricate task of pornography detection, in this work, we do not take audio into consideration. As a matter of fact, we opted to solely focus on visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Convolutional Neural Networks</head><p>Although Deep Learning has been responsible for most of the current breakthroughs in image classification tasks, few explorations have been made for these techniques within the context of pornography detection in video. Moustafa <ref type="bibr" target="#b21">[22]</ref> performed a superficial adaptation of well-known CNN architectures for image classification, to the pornographic video classification task. He used AlexNet <ref type="bibr" target="#b18">[19]</ref> and GoogLeNet <ref type="bibr" target="#b20">[21]</ref> architectures directly on selected frames, for classifying them in porn or non-porn, integrating the final result for a video through a majority voting process. The author used the weights learned from ImageNet dataset, fine-tuning only the last layer, which corresponds to the classifier. Within this approach, no motion information was explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Third-party Solutions</head><p>The nudity and pornography detection problem has not been tackled only in the academia. There exists some software solutions, mostly commercial, aiming at solving this problem. Some focus on blocking websites that contain this type of content (e.g., CyberPatrol, CYBERsitter, NetNanny, K9 Web Protection, Profil Parental Filter) while others scan the hard drive in search of pornography (e.g., SurfRecon, Porn Detection Stick, PornSeer Pro). There is even a Brazilian software, called NuDetective, developed by the Brazilian Federal Police, that focus on detecting child pornography. These solutions are mainly based on skin detection approaches, and none explored the space-time nature of videos for aiding the detection of pornography. Therefore, such solutions normally fall short when compared to the current state of the art for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Summary Table</head><p>Table <ref type="table" target="#tab_0">1</ref> presents an overview of the related works on the pornography detection problem and its sub-problems, skin and nudity detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Concepts</head><p>In this section, we cover the main concepts necessary for understanding this manuscript. In Sec. 3.1, we explain some Convolutional Neural Networks (CNNs) extensions to deal with motion/temporal information while, in Sec. 3.2, we explain the motion information sources of interest in this work, Optical Flow and MPEG Motion Vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion/Temporal Networks</head><p>The first CNNs architectures were designed targeting at image classification. Although they can be used for video classification in a frame-wise approach, the temporal information will be almost completely discarded. As previously discussed, this type of information should not only increase the performance, but it could also be indispensable for removing the ambiguity on the pornography classification problem. However, only a few authors have addressed video classification with Convolutional Networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> thus far, offering us a whole new venue for possible original contributions.</p><p>Karpathy et al. <ref type="bibr" target="#b24">[25]</ref> explored a variety of architectures to implicitly capture temporal information amongst a sequence of frames. These architectures received sequential frames, or frames temporally close to each other, as input. The reported results exhibited small performance variance between the fusion approaches and also in comparison with the single-frame network. These initial results indicate that Convolutional Networks have some troubles to implicitly capture the motion information from sequential frames. Following a different strategy, Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref> proposed a Two-Stream convolutional neural network, that uses optical flow to supply complementary information to the classification. Inspired upon the biological aspect of human vision, they designed an architecture related to the two-stream hypothesis, in which the visual cortex separately recognizes objects and motion <ref type="bibr" target="#b40">[41]</ref>. This is accomplished by having an architecture with two pathways, one for the frames and another for the motion information. The pathways are later combined by score averaging. For the motion information, the authors used stacked optical flow displacement fields. This stacking comprises the image representations of the vertical and horizontal components, from the displacement vector field, of an arbitrary number of consecutive frames. This new representation strategy led to important improvement upon previous state-of-the-art deep neural nets on action recognition datasets such as the UCF-101 <ref type="bibr" target="#b41">[42]</ref> and HMDB-51 <ref type="bibr" target="#b42">[43]</ref>. The aforementioned networks were designed for action recognitions tasks (e.g., golfing, sitting, running, etc.), which at first may seem very different than pornographic detection on videos. But, although pornography is more complex and subjective than such actions, it can be reduced to a collection of smaller actions that characterize it. Therefore, through learning, convolutional networks can also identify the static and temporal visual patterns, that will lead to discriminative features for pornography detection in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Information</head><p>As stated earlier, the motion information contained in a sequence of images is invaluable for tasks of video classification. We now review two motion information sources of particular interest: Optical Flow and MPEG Motion Vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Optical Flow</head><p>Optical flow comes from the computer vision problem of estimating the visual motion between two images <ref type="bibr" target="#b43">[44]</ref>. Although this is an old problem in Computer Vision, there are somewhat recent works improving optical flow computing <ref type="bibr" target="#b44">[45]</ref>.</p><p>The final output for the different optical flow computation methods is a displacement field, which contains, in each pixel, its relative movement from the source image to the reference one. Each position in this field has a displacement vector indicating the estimation of which direction the respective pixel has moved to and the intensity (gradient) of this movement. Fig. <ref type="figure" target="#fig_0">1</ref> depicts an example of the output. Altogether, these vectors provide us with a relevant proxy for the motion of the objects in the scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">MPEG Motion Vectors</head><p>Another source of motion information frequently used for video classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> is the motion vectors contained in the MPEG coding of the analyzed media. Differently from the optical flow, this motion estimation was not originally created to allow an understanding of the movement in the scene; but rather, it was designed for aiding the compression of the video <ref type="bibr" target="#b25">[26]</ref>.</p><p>During video compressing, there is a method named Motion estimation and compensation <ref type="bibr" target="#b25">[26]</ref>, which in one of its steps, maps the pixel movements between the current frame being compressed onto a reference frame. This information is what is called Motion Vectors. Motion estimation in video compression is performed in a block-based fashion, where pixels are grouped in macroblocks in order to reduce computational complexity.</p><p>Motion vectors are then computed per macroblock and contain the following information: the position (x,y) of the macroblock of pixels in the current frame; its position (x',y' ) in the reference frame; and the size of the macroblock (M √ó N ). Fig. <ref type="figure" target="#fig_1">2</ref> shows an example. This mapping only occurs during compression, hence, when the video is decompressed for analysis, this information is readily available. The gathering of the motion vectors, from a frame being reconstructed, gives us useful information about the motion that has occurred at that time.</p><p>Although we reference here to the motion vectors from the MPEG codec, and this codec is one of the most used today, this source of information is commonly present in other codecs as well, such as Google's VP9 <ref type="bibr" target="#b45">[46]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we present the details of our proposed method for exploring and developing deep learning techniques, jointly with motion information, targeting at video pornography detection. The approaches we designed, were mainly inspired upon the seminal work of Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref>, in which the motion information is explicitly provided to the convolutional neural network, and each type of information (static and motion) is independently processed by the network. Notwithstanding, we explore the motion information differently and incorporate novel sources of motion information in our work. Moreover, we also propose new ways for combining static and motion for a more effective decision making.</p><p>In Sec. 4.1, we present the details for the static stream of information while in Sec. 4.2, we explain the motion stream and also the two motion sources explored in this work: optical flows and MPEG motion vectors. Thereafter, in Sec. 4.3, we describe the distinct ways we have explored for fusing the static and motion information in this work. Finally, in Sec. 4.4, we detail the CNN architecture and training process we adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Static Information</head><p>In the static pipeline we propose, which is represented in Fig. <ref type="figure" target="#fig_2">3</ref>, we start with a chosen sampling of the video frames and extract their features with a convolutional network. These features are average pooled to form a single description of the whole video (there are some alternatives to the pooling, e.g., voting, and other types of pooling, such as max and sum, but throughout some prior experiments and our own experience, we opted for a standard average pooling procedure). Finally, we feed a classifier with the video description for the final classification. One can see this is the simplest possible approach for tackling a video: divide it into frames, pool the different features and train a classifier. In addition, each frame is preprocessed, being resized, maintaining the aspect ratio and having its smaller dimension as the network input dimension (224√ó224 pixels). Then a center cropping is performed, resulting in an image with the necessary shape for the convolutional network architecture we adopt.</p><p>For the static CNN, we explored two paths for solving the problem. The first one considers a network model trained with natural images obtained with the ImageNet dataset <ref type="bibr" target="#b46">[47]</ref> whilst the second model is custom-tailored (i.e., finetuned and properly adapted) to our problem, starting with the weights obtained with the ImageNet samples during a pre-training step rather than using random weights for initializing the network. Our experience shows that initializing weights with a related (although not directly) problem is more effective than random weights for this particular problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Motion Information</head><p>The most important challenge we want to tackle is how to add time information (motion) to the pipeline, since it was demonstrated that it enhances classification power <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> in other problems. Initially, we analyze the motion information independently from the static information. The pipeline (Fig. <ref type="figure" target="#fig_3">4</ref>) for this type of information is somewhat similar to the static pipeline, with differences in the input and output of the convolutional network. In our methodology, we evaluate two sources for the motion information: optical flow displacement fields <ref type="bibr" target="#b43">[44]</ref> and MPEG motion vectors <ref type="bibr" target="#b25">[26]</ref>. The motion sources follow the pipeline independently, therefore there is a specific motion CNN model and classifier for each. Each source requires an unique form for extracting the motion information, whose details we shall present later on.</p><p>It is important to highlight that the motion information does not come readyto-use in a CNN and require, upfront, a proper representation. We represent the motion information, extracted with optical flows or MPEG motion vectors, by two motion maps, one for the horizontal (dx) component of the motion and another for the vertical (dy), containing in each (x,y) position, a measure of motion in that respective direction. When transforming these motion maps to images, we linearly rescale them to the [0, 255] interval and store them as gray-scale images, one image for each component of the motion. Fig. <ref type="figure" target="#fig_5">5</ref> depicts examples of the generated image representations.</p><p>After the feature extraction, the descriptions of the components (dx and dy) from the same motion are concatenated to form a single feature vector. The rest of the pipeline is then similar to the static one: the combined descriptions are pooled and fed to a classifier for final decision making.</p><p>At first sight, this pipeline is similar to Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref> temporal stream. However, here we have opted for each motion information and each component to be separately processed by the convolutional neural network, in contrast to Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref>, who stacked both components of the motion information from a temporal neighborhood (e.g., displacement fields from an arbitrary number of consecutive frames) before feeding it to the network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Optical Flow</head><p>Our first explored source of motion information is the optical flow displacement fields technique. Each position of interest provides us with the gradient's magnitude and the direction of the motion. For a more direct representation, we decompose this information in its horizontal (dx) and vertical (dy) components, generating two motion maps with the magnitude values for each component separately. Fig. <ref type="figure" target="#fig_5">5</ref>(b) depicts an example of the optical flow representation, calculated from the generated motion maps (see Sec. 4.2).</p><p>We compute the optical flow displacement fields using Brox et al.'s method <ref type="bibr" target="#b44">[45]</ref>, whose GPU implementation is readily available at OpenCV 2.4.10 toolbox. The frames, and their pairs, were preprocessed before extraction of the optical flows, just as the raw frames: resizing preserving the aspect ratio, then center cropping with the input dimensions of the chosen CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">MPEG Motion Vectors</head><p>Another explored source of the motion information is the motion vector data encoded within the MPEG codec. In each vector for a particular frame, it is encoded the position from a given macroblock of pixels in the current frame and its position at the reference frame. In this work, we propose a novel representation for the motion information contained in these vectors. We measure how much the block from each motion vector has moved by computing the distance, in pixel coordinates, from the reference position to the current position in each direction, horizontal and vertical, separately. Furthermore, these distances are analogous to the magnitude of the movement at the region contained in that macroblock, and generate two motion maps, one for each direction, similarly to the optical flow motion extraction.</p><p>Motion vectors are extracted using FFMPEG 2.7 API. They are extracted from the original videos and no resizing is performed. Therefore, differently from optical flow, for the motion vectors, we apply the resizing operation later on, directly on the generated image representations. Fig. <ref type="figure" target="#fig_5">5(c</ref>) illustrates an example of the generated image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fusion</head><p>The static and motion information can lead to more effective results if their collected evidence (video telltales) are complementary in some sense. Therefore in this section, we explore different forms of combining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Early Fusion</head><p>In the early fusion method, the static and the motion information are combined at the very beginning of the pipeline, being processed together by a special convolutional network. This way, the features benefit from both the static and the motion information. Fig. <ref type="figure" target="#fig_6">6</ref> depicts a representation of the pipeline. The three color channels of the frame, along with its respective motion representations, dx and dy, are stacked together for input in the convolutional network, giving rise to a 5-channel input. It is also straightforward to generate an image containing the raw frame information in gray scale on one of its channels and the motion information on the other two channels, one for the horizontal component and another for the vertical. The advantage of having a 3-channel input is the ability to custom-tailor the network weights starting from pre-trained 3-channel network weights instead of starting the weights randomly from scratch. In this work, we have explored both options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Mid-level Fusion</head><p>Differently from the early fusion strategy, in the mid-level fusion, we concatenate the features extracted from each type of information (static or motionbased), and from each independent CNN, into a single feature vector before feeding a classifier. Fig. <ref type="figure" target="#fig_7">7</ref> shows a representation of the mid-level fusion pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Late Fusion 440</head><p>In this fusion scheme, each information is processed by a separate decisionmaking approach (e.g., SVM classifier), generating independent classification scores that can then be combined later on on a single score for the final classification. Fig. <ref type="figure" target="#fig_8">8</ref> depicts a representation of this pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Architecture Specifications</head><p>The convolutional neural network architecture we adopt for the experiments was proposed in <ref type="bibr" target="#b20">[21]</ref>, and is referred to as GoogLeNet. This architecture was employed for all types of data: Static (raw frames), Motion (optical flow and motion vectors) and Static-Motion (early fusion).</p><p>For feature extraction, we pick the output from the last layer -fullyconnected (FC) -before the final classification. Indeed, for other CNN architectures, it is common to utilize as features not only the output from the last FC layer, but also from other layers before it (earlier layers). However this only makes sense if those lower layers are also FC, which are able to capture the patterns from the low-level features of the convolutional, or pooling, layers bellow it, and output mid-level features. With GoogLeNet, between the convolutional layers and the final classification layer, there is only one FC layer. Although GoogLeNet architecture contains other FC layers, they are associated with auxiliary classifiers in branches at the middle of the network, being located too early in the network. Thus, these layers are still too close to the raw data, containing too low-level information that may mislead a high-level classifier later on such as SVM. Moreover, as described by Szegedy et al. <ref type="bibr" target="#b20">[21]</ref>, these auxiliary classifiers are only used during the learning strategy, by adding a low weighted amount to the gradient loss, but they are not used at inference time as they often do not contribute to the final decision-making process. Given these observations and previous studies in the literature on this matter, for our experiments, we have opted to use as features only the output from the last FC layer. The output from this layer has a dimensionality of 1,024-d.</p><p>The motivation for exploring this CNN model as a feature extractor, comes from the fact that GoogLeNet was the winner of ImageNet 2014 Challenge <ref type="bibr" target="#b46">[47]</ref>, achieving a striking 6.67 top-5 error rate in the object classification competition.</p><p>The ImageNet training dataset comprises about 1.2 million images, containing 1,000 classes with a wide range of subjects, from plants and animals to persons, sports, and objects. Thus it is expected that GoogLeNet architecture has the capability of learning to extract highly discriminative visual features from input images, although not initially fine-tuned to the problem of interest in this work. It is also expected that a model pre-trained with ImageNet 2014 dataset should hold an advanced state of optimization for image feature extraction, which may be useful for application on pornography detection, by itself, or by generating a custom-tailored model with weights fine-tuned to our problem.</p><p>To adapt the weights to our particular 2-class detection problem, taking as input the static and motion data of interest, we extend upon the initial architecture to map the last layer with 1,000 filter outcomes (which is the number of classes in the ImageNet classification problem) to two (porn vs. non-porn). In addition, all the network weights, except within Early Fusion, are fine-tuned to the problem of interest herein via backpropagation, initializing the weights with the values learned on the ImageNet 1.2 million images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>We now discuss the experimental setup, including: the dataset; evaluation metrics; training specifications; and details on the existing methods in the literature as well as third-party solutions (Sec. 5.1). Next, we present the experiments and obtained results, comparing the proposed method with the existing ones in the literature, and third-party solutions (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>In the next subsections, we present the experimental setup designed for the evaluation of the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Datasets</head><p>We adopted two datasets in our experiments: Pornography-800 <ref type="bibr" target="#b13">[14]</ref> and Pornography-2k <ref type="bibr" target="#b15">[16]</ref>. As a matter of fact, Pornography-2k is an extension of Pornography-800. Therefore, we opted to report all the experiments with the proposed methods on the Pornography-2k (more complete and challenging), along with the methods we choose as baselines. Finally, we evaluate our best proposed approaches on Pornography-800, for direct comparisons with existing work in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pornography-800 dataset</head><p>This dataset<ref type="foot" target="#foot_0">1</ref> was originally proposed in <ref type="bibr" target="#b47">[48]</ref> and is distributed upon acceptance of a user agreement. It comprises approximately 80 hours, spanning 800 videos, 400 pornographic and 400 non-pornographic.</p><p>The videos with pornography content were acquired from websites specialized on that type of content, searching for samples within a wide range of genres and with actors from distinct ethnicities (e.g, Asians, Blacks, Caucasian).</p><p>With respect to non-pornographic content, general-public purpose video networks were considered<ref type="foot" target="#foot_1">2</ref> for acquiring the videos. The dataset contains two levels of difficulty, easy and difficult. The former comprises videos randomly selected from various websites, while the latter considers videos gathered through textual queries containing words such as "wrestling", "sumo", "swimming", "beach", etc. (i.e., words associated to skin exposure).</p><p>The official evaluation protocol for this dataset is the 5-fold cross-validation (640 videos for training and 160 for testing on each fold).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pornography-2k dataset</head><p>The Pornography-2k dataset <ref type="bibr" target="#b15">[16]</ref> is an extended version of the Pornography-800 dataset <ref type="bibr" target="#b47">[48]</ref>. The new dataset comprises nearly 140 hours of 1,000 pornographic and 1,000 non-pornographic videos, varying from six seconds to 33 minutes long.</p><p>The non-pornographic videos were acquired similarly to Pornography-800, targeting at both easy and difficult samples. Concerning the pornographic material, differently than Pornography-800, it is not restricted to pornographyspecialized websites. Instead, it was also explored general-public purpose video networks, in which it was surprisingly easy to find pornographic content. As a result, the new Pornography-2k dataset is very assorted, including both professional and amateur content. Moreover, it depicts several genres of pornography, from cartoon to live action, with diverse behavior and ethnicity. Fig. <ref type="figure" target="#fig_9">9</ref> depicts some example frames from the Pornography-2k dataset. This dataset is available publicly <ref type="bibr" target="#b15">[16]</ref>.  <ref type="bibr" target="#b13">[14]</ref>, with added samples. Note that the dataset is very challenging, with a variety of pornography styles (e.g., hentai vs. live-action) and difficult non-pornographic cases with a lot of skin exposure.</p><p>For the validation protocol, as suggested by Moreira et al. <ref type="bibr" target="#b15">[16]</ref>, we apply a 5√ó2-fold cross-validation protocol <ref type="bibr" target="#b48">[49]</ref>, which consists of randomly splitting the dataset five times into two folds, balanced by class. In each round of experiments, training and testing sets are switched and, consequently, 10 analyses are conducted for each considered method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Evaluation Metrics</head><p>As evaluation metrics, we adopt the default evaluation metrics of the Pornography-2k dataset: the video classification accuracy (ACC) and the F 2 measure (F 2 ), both averaged in all experimental folds.</p><p>ACC is simply the percentage of correctly classified videos. F 2 , in turn, is the weighted harmonic mean of precision and recall, which gives twice the weight to recall (Œ≤ = 2) than precision. In the case of pornography filtering, the F 2 measure is crucial because false negative results are harmful, allowing one to be exposed to pornographic content. It is thus less prejudicial to wrongly deny the access to non-pornographic material, than to wrongly disclose pornographic content. F Œ≤ measure is defined as:</p><formula xml:id="formula_0">F Œ≤ = (1 + Œ≤ 2 ) √ó precision √ó recall Œ≤ 2 √ó precision + recall , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Œ≤ is a parameter denoting the importance of recall compared to precision. The mean over the 5√ó2 folds from the evaluation metrics, ACC and F 2 , may be insufficient to certify that a specific method is better than another, due to some large variations in the population of measures that may be hidden during averaging. To overcome this, we employ a non-parametric Wilcoxon signedrank test <ref type="bibr" target="#b49">[50]</ref>, which is a paired difference test that allows us to quantify how different two populations are; in this case, the populations are sampled from each fold measure from each method, without assuming a normal distribution of the population. Therefore, we can confirm more confidently whether or not that two methods are statistically different from one another. Two methods are considered statistically different if their Wilcoxon's returned p-value is lower than 0.05 (95% confidence test).</p><p>For the Pornography-800 dataset, we only report the mean video classification accuracy, following the default evaluation metric of the dataset. Since we do not have the result by fold from the related works, we could not employ Wilcoxon signed-rank test <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Proposed Method's Setup</head><p>The focus of the proposed methodology is to classify whole videos as porn or non-porn. Videos are a collection of frames, but using all of them for video classification would demand a great computational effort. However, this experiment can be turned more manageable by using a sampling of the frames, and still maintain consistent effectiveness for comparison between the distinct methods proposed. With this established requirements, we opted for using a frame sampling rate of one frame per second (1fps). For the motion information, this frame sampling dictates from which frames this type of information will be extracted.</p><p>The same sampling was used for both the training and test phases. During training, the frames (or motion image representations) were utilized separately, for learning the CNN models, and pooled by video after feature extraction, for learning the classification model. During testing, they pass directly to feature extraction by the trained CNN model.</p><p>All CNNs were fine-tuned starting with the weights from ImageNet, except Static-Motion CNN for the early fusion color variation. Static-Motion CNN for color variation is trained from scratch, because its input contains 5 channels, and therefore we could not start with the the same filter configuration and weights from ImageNet, which contains 3-channel kernels.</p><p>The training of the CNN model was performed with the Caffe framework <ref type="bibr" target="#b50">[51]</ref>. We picked the polynomial learning rate decay policy, because the GoogLeNet ImageNet model we considered, from Caffe, was trained much faster using this policy. For each type or source of information (static and/or motion-based), we picked a suitable value for the base learning rate, weight decay, polynomial power and the number of epochs to run. In the following, we overview these hyperparameters.</p><p>In a nutshell, learning rate is the value that determines how much the network will learn at each iteration. As a matter of fact, the learning rate can be seen as a weight that controls the rate at which the values of the filter banks will be updated during backpropagation. The learning rate is initialized to the value of this hyperparameter, also known as base learning rate, then it will decay according to a chosen policy, in our case, by the polynomial learning rate decay policy.</p><p>The weight decay hyperparameter controls what will be the load of the regularization technique, sometimes referred to as 1 / 2 regularization, while computing the cost function during backpropagation. Regularization techniques are employed for preventing overfitting, and weight decay achieves this by penalizing filters with large weights.</p><p>Polynomial power is the value used for determining what will be the decay of the learning rate, given the polynomial learning rate decay policy. At each iteration, a new learning rate is computed using the following formula:</p><formula xml:id="formula_2">learning rate = base learning rate √ó (1 - current iteration max iteration ) polynomial power .</formula><p>An epoch is the number of times of all train samples are used once to update the weights. In fact the number of epochs is converted to number of iterations, according to the number of iterations necessary for an epoch.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the exact values for these hyperparameters. It is important to mention that such hyperparameters have no associated units. Also, a dropout layer, with 40% ratio of dropped outputs, was maintained from the original GoogLeNet architecture. For training a CNN, another sub-split of the dataset is necessary. For that, each training fold from the 5√ó2-fold cross-validation was re-partitioned into actual training and validation, with a proportion of 85%/15% videos in each part.</p><p>For this problem, we did not consider data augmentation techniques while training the network model, as we could gather enough training samples due to the high quantity of frames contained in the training videos, and therefore properly optimize the fine-tuning procedure of the method. We perform the final classification with a linear Support Vector Machine (SVM) classifier using LIBSVM <ref type="bibr" target="#b51">[52]</ref> (version 3.18). We apply grid search to find the best regularization C SVM parameter during training, C ‚àà {2 c : c ‚àà [-5, -3, . . . , 15]}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Comparison with Spatio-temporal Video Descriptors</head><p>For a better interpretation of the performance of the proposed methods, it is necessary to compare them with the current state-of-the-art spatio-temporal video descriptor: Dense Trajectories <ref type="bibr" target="#b17">[18]</ref>. This method relies on a dense sampling of descriptors, not only spatially, at feature points in the starting frame, but also temporally, by tracking the feature points in the subsequent frames. We extract the dense trajectories from the video files using the source-code provided by Wang et al <ref type="bibr" target="#b17">[18]</ref>, with default values.</p><p>More recently, Moreira et al. <ref type="bibr" target="#b15">[16]</ref> proposed the Temporal Robust Features (TroF), a fast spatio-temporal interest point detector and descriptor, which is directly inspired by the still-image Speeded-Up Robust Features (SURF) <ref type="bibr" target="#b52">[53]</ref>. TRoF relies on three major extensions of the original method to use the video space-time. We extract the TRoF descriptors, with default values.</p><p>As mid-level representation, for Dense Trajectories and TRoF descriptors, we extract Fisher Vectors <ref type="bibr" target="#b53">[54]</ref>, the state-of-the-art model of bags of visual words <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. To obtain the visual codebook, the Gaussian mixture model parameters are trained over 10 million descriptions, randomly sampled (half of the descriptions sampled from positive videos, and half from negative ones in the training set), using an expectation maximization algorithm. We followed the default configuration of 256 Gaussians.</p><p>As with the feature vector from the CNNs, the mid-level descriptions generated with the Fisher Vectors can also be temporally pooled to form a single feature vector for the whole video. Finally, this information is fed to an SVM for label prediction. We perform the classification with SVM classifiers using the LI-BLINEAR library <ref type="bibr" target="#b56">[57]</ref> (version 1.94). We apply grid search to find the best regularization C SVM parameter during training, C ‚àà {2 c : c ‚àà [-5, -3, . . . , 15]}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5.">Comparison with Third-party Solutions</head><p>We also compare the proposed methods with some third-party solutions readily accessible. We selected the most recent ones, that rely exclusively on visual data: MediaDetective <ref type="bibr" target="#b57">[58]</ref>, Snitch Plus <ref type="bibr" target="#b58">[59]</ref>, PornSeer Pro [60], and NuDetective <ref type="bibr" target="#b59">[61]</ref>.</p><p>For MediaDetective and Snitch Plus, the video are rated according to their potential (i.e., probability) for pornography. In those cases, we tag a video as pornographic if such probability is equal to or greater than 50%. NuDetective and PornSeer Pro, on the other hand, assigns binary labels to the video: positive (i.e., the video is pornographic) or negative (i.e., the video is non-pornographic).</p><p>Moreover, MediaDetective and Snitch Plus have four predefined execution modes, which differ mostly on the rigorousness of the skin detector. In the experiments, we opted for the most rigorous execution mode. For NuDetective and PornSeer Pro, we employed their default settings.</p><p>As these solutions do not demand a training phase, they are executed directly at the dataset, without the need for training for each fold. Even so, the reported metrics are the average over all 5 √ó 2 folds, for fair comparison with the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6.">Comparison with Existing Methods on the Pornography-800 Dataset</head><p>After evaluating the proposed methods with the Pornography-2k dataset, we turn our attention to evaluating the best methods with the Pornography-800 dataset. We do this for comparison with previous work that adopted this dataset: Avila et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b47">48]</ref>, Valle et al. <ref type="bibr" target="#b14">[15]</ref>, Moreira et al. <ref type="bibr" target="#b15">[16]</ref>, Moustafa <ref type="bibr" target="#b21">[22]</ref>, Caetano et al. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b60">62]</ref>, and Souza et al. <ref type="bibr" target="#b61">[63]</ref>.</p><p>Avila et al. <ref type="bibr" target="#b47">[48]</ref>, which is the work that introduces the Pornography-800 dataset, employed a HueSIFT descriptor at a regular grid of interest points for obtaining the low-level features; k-means, for construction of the codebook, with BOSSA -their proposed extension to BoVW -for the mid-level; and a non-linear SVM for the final classification. In turn, Valle et al. <ref type="bibr" target="#b14">[15]</ref> evaluated the spatio-temporal descriptor STIP <ref type="bibr" target="#b16">[17]</ref> with a standard BoVW, with random sampling for construction of the codebook, and a linear SVM for the decision making. Following a similar path, Souza et al. <ref type="bibr" target="#b61">[63]</ref> used a traditional BoVW, with random sampling, and a linear SVM, but with ColorSTIP for low-level description.</p><p>Continuing their previous work <ref type="bibr" target="#b47">[48]</ref>, Avila et al. <ref type="bibr" target="#b13">[14]</ref> proposed an extension upon BOSSA, named BossaNova, maintaining the use of HueSIFT, k-means, and a non-linear SVM for decision making. Aiming at more efficient descriptors, Caetano et al. <ref type="bibr" target="#b60">[62]</ref> experimented with binary descriptors, of which BinBoost had the best performance, replacing the HueSIFT in the pipeline from Avila et al. <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b38">[39]</ref>, Caetano et al. improved the classification performance by proposing an extension to the BossaNova approach, named BossaNova Video Descriptor, with binary descriptors. Moreira et al. <ref type="bibr" target="#b15">[16]</ref> introduced the spatiotemporal detector and descriptor TRoF and aggregated local information into Fisher Vectors <ref type="bibr" target="#b53">[54]</ref>.</p><p>Differently from previous approaches, Moustafa <ref type="bibr" target="#b21">[22]</ref> did not use a BoVWbased method, instead, the author relied upon a CNN for the low-and midlevel representations and also for classification. The classification was given by a majority voting among the video frames. Their best results were obtained with a max fusion of scores from different CNN models, pre-trained with the ImageNet dataset and with fine-tuning of the last layer of the network using the Pornography-800 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>In this section, we present and discuss the obtained results from the outlined experiments. First, we assess the approaches we have proposed. Afterwards, we compare our best proposed approach to methods from the literature and third-party solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Proposed Approaches</head><p>In Table <ref type="table">3</ref>, we show the obtained video classification accuracy and F 2 measure for each approach we have proposed, considering the static and motion information as well as the fusion of different methods. In these experiments, we adopted the Pornography-2k dataset (c.f., Sec. 5.1.1 for details).</p><p>Table <ref type="table">3</ref>: Video classification accuracy and the F 2 measure, averaged over the 5 √ó 2 experimental folds, for the proposed approaches on the Pornography-2k dataset. The methods are subdivided in static, motion and fusion modalities. Fusion is performed with the fine-tuned model for static information, and with both motion sources, optical flow (OF) and MPEG motion vectors (MV), except for the early fusion, which, due to its inferior performance with OF, is not employed with MV. In the static stream, we show that the model relying on the GoogLeNet architecture trained with ImageNet data yields an impressive performance of 94.6% ACC and 95.1% F 2 . These results are further improved upon by finetuning the network weights with the pornographic data, thus specializing the network to the problem of interest, reaching 96.0% ACC and 96.1% F 2 , a 1.5 percentage point improvement in ACC (26% error reduction) and 1 percentage point in F 2 .</p><p>When considering the motion information, optical flow (OF) by itself yielded a performance close to the static model. Meanwhile, the MPEG motion vectors (MV) led to a lower performance, of 91.0% ACC and 92.0% F 2 . This difference in performance between these two sources of motion information may be explained by the fact that the MV represents the motion of a macroblock of pixels, which is a much lesser fine-grained description form than OF, which takes into account the motion information for each pixel.</p><p>Despite the lower performance of the motion information alone, when we combine it with the static information from the fine-tuned network (pornography-specialized network), by mid-level fusion and late fusion, we improve the ACC and F 2 results. Both early fusion variations, Gray and Color, yielded a lower performance than using the fine-tuned static information by itself. Perhaps it is better to specialize the network to a single type of information, leaving the fusion to a higher level. Another reason might be related to the architecture considered in this work, GoogLeNet. It may not be appropriate for processing five channels or combining static and motion right at the lowest level (e.g., raw data) of the network, demanding some customization such as increasing the number of filters or processing each information independently at the first layers.</p><p>We believe that the better performance from the gray variation over color, comes from the fact that we could fine-tune its model using the ImageNet model and that the 3-channel input data is more appropriate for the GoogLeNet architecture. However, we expect that if these issues were overcome (e.g., by training an appropriate architecture with a large collection of samples), the full potential from using all color channels could be reached, outperforming the gray-only variation of this fusion, and perhaps the other fusion approaches, mid-level and late.</p><p>Given the low performance of early fusion, and its costly requirements for training, we have opted for not fusing MPEG motion vectors this way.</p><p>Mid-level fusion and late fusion, on the other hand, apparently could better combine static and motion information, surpassing the performance of the finetuned network alone. Surprisingly, this happened even while combining with MV, showing that, although it had a worse performance when used alone, its complementarity to the static information is still advantageous. In addition, another advantage of using the MVs is that they are readily available during decoding of the video. Still, even that by a small margin, late fusion with OF obtained the best combination of results for ACC and F 2 measures.</p><p>In fact, our architecture was able to properly learn effective features from the motion data, as our results with middle-and late-fusion approaches showed, which take into account the information provided by the Static Raw Frames and Optical Flows simultaneously. However, it is possible that using an innate motion-based network could equally produce good results; however such network could be more complex (with more weights) than the one we have extended upon. Moreover, unfortunately, there is no motion CNN model readily available, as far as we know, that has an input in accordance with the pipeline we proposed here, with a single motion image representation per time. Current available motion CNNs often require stacked motions and thus, are also not amenable to fast implementation and deployment in mobile devices, which is our ultimate goal in this research.</p><p>In spite of all of this, we have performed some experiments with other traditional CNNs, AlexNet <ref type="bibr" target="#b18">[19]</ref> and VGG <ref type="bibr" target="#b62">[64]</ref> networks, also taking, as features, the output from their last FC layers before classification. Table <ref type="table" target="#tab_3">4</ref> shows the results from the experiments, which consisted in evaluating how each architecture would perform for the static information, with the ImageNet model and with the fine-tuned model from the ImageNet weights, and also for the motion information (optical flows) after fine-tuning, again from the ImageNet weights.</p><p>For all considered architectures, if we use the ImageNet weights for feature extraction (Static -ImageNet), the performance for the static information is almost equivalent, with not more than 0.3 perceptual points of difference between the best accuracy and the worst. After fine-tuning the models, initializing the networks with the ImageNet weights, there is a considerable improvement in performance for VGG and GoogLeNet networks, while maintaining equivalency between one another, over the results with no fine-tuning and over AlexNet.</p><p>When dealing with the motion information (Motion -Fine-tuned), VGG showed slightly better results when in comparison to GoogLeNet and AlexNet. The fact that VGG had equivalent performance to GoogLeNet for static information, but superior for motion, supports the suspicion that motion information does indeed have a particular structure, even after being represented as images, that some CNN architectures are better to capture. Therefore, an architecture specialized only in motion information could improve even more the results.</p><p>Finally, it is important to highlight that the choice for one architecture should not be based only on the classification numbers. In our case, as our ultimate goal is to implement our solution in a mobile device with more limited resources than a traditional server, the overall configuration and size of a network also matters. In this regard, for instance, GoogLeNet is superior than VGG as the former has a learned model with only 40MBs against a learned model of 533MBs of VGG, an order of magnitude of difference and a gamechanger when we aim at a mobile implementation. In addition to the obvious impact when testing a new video, this difference also plays an important role during training as it has a huge influence on the batch size we can use for training, and consequently, the speed of training. For GoogLeNet we used a batch size of 96, and could have even used a bigger one. For VGG, it was 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Comparison with Existing Methods using the Pornography-2k Dataset</head><p>For a better evaluation of the proposed approaches that obtained the best results in each modality (c.f., Sec. 5.2.1), we compare them with the current state-of-the-art spatio-temporal video description and third-party solutions. Table <ref type="table" target="#tab_4">5</ref> shows the respective video classification accuracy and F 2 measure of the considered methods. Note that the best proposed methods outperform most of the existing solutions.</p><p>The third-party solutions, which heavily depend on skin detection and do not take advantage of the space-time information, have shown a poor performance.</p><p>PornSeer Pro [60] obtained the best ACC and F 2 measures among them, with 79.1% and 75.6% respectively, far below the performance using the solutions in the literature and our proposed approaches. The proposed methods also outperform the Dense Trajectories method <ref type="bibr" target="#b17">[18]</ref>. For instance, the spatio-temporal approach, Late Fusion (OF), outperforms Dense Trajectories by a margin of 0.5 percentage point in ACC (14.3% error reduction) and over 1.0 in F 2 measure. Also, we can assert that motion feature plays an important role in pornography video detection when comparing the motion-based approaches (Dense Trajectories and proposed approaches) with the third-party solutions. The motionbased approaches remarkably outperform the third-party solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Comparison with Existing Methods using the Pornography-800 Dataset</head><p>In Table <ref type="table">6</ref>, we compare our best proposed approaches with the reported results from other methods in the literature using the Pornography-800 dataset.</p><p>The proposed approaches significantly outperform the existing BoVW-based methods <ref type="bibr">[14-16, 48, 62, 63]</ref>, by 3-11 percentage points. The proposed methods also outperform, by almost four percentage points, the results reported in Moustafa <ref type="bibr" target="#b21">[22]</ref>, which also use Deep Learning. In this case, the error reduction was over 64%. Even though we could not apply Wilcoxon's test, given the large perceptual difference in accuracy between the related works and our best approaches, with smaller standard deviation in some cases, we believe that the Table <ref type="table">6</ref>: Results on the Pornography-800 dataset considering the best approaches we have proposed in each modality (Static -Fine-tuned; Motion -Optical Flow; Mid-level and Late Fusion with Optical Flow) and existing methods in the literature. We report the average performance and standard deviations using the dataset's 5-fold evaluation protocol. NA stands for a non-reported information in the original work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution ACC (%)</head><p>BoVW-based Avila et al. <ref type="bibr" target="#b47">[48]</ref> 87.1 ¬± 2.0 Valle et al. <ref type="bibr" target="#b14">[15]</ref> 91.9 ¬± NA Souza et al. <ref type="bibr" target="#b61">[63]</ref> 91.0 ¬± NA Avila et al. <ref type="bibr" target="#b13">[14]</ref> 89.5 ¬± 1.0 Caetano et al. <ref type="bibr" target="#b60">[62]</ref> 90.9 ¬± 1.0 Caetano et al. <ref type="bibr" target="#b38">[39]</ref> 92.4 ¬± 2.0 Moreira et al. <ref type="bibr" target="#b15">[16]</ref> 95.0 ¬± 1.3 CNN Moustafa <ref type="bibr" target="#b21">[22]</ref> 94.1 ¬± 2.0 Proposed Approaches Static -Fine-tuned 97.0 ¬± 2.0 Motion -Optical Flow 95.8 ¬± 2.0 Mid-level Fusion (OF) 97.9 ¬± 0.7 Late Fusion (OF) 97.9 ¬± 1.5 results would probably be statistically significant.</p><p>Although <ref type="bibr">Moustafa [22]</ref> employs the same architecture we use in this work, GoogLeNet, there are critical differences, thus leading to the important difference in performance, we report herein: first of all, he only fine-tuned the network last layer, while in our work we fine-tuned all layers, creating a network model specialized to the problem of interest; second, the network output in that work, for each frame, was used in a majority voting scheme for classifying the video, while, in turn, we have opted for using the network as a feature extractor, pooling the frame descriptions, then feeding them to an classifier for the video classification; finally, that work only considered static information, meanwhile our methods rely upon static and motion information, as well as on effective methods for combining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>The evaluation of our techniques, shows that the association of Deep Learning with the combined use of static and motion information, considerably improves pornography detection. Not only over current scientific state of the art <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b60">62]</ref>, but also over off-the-shelf software solutions <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr">[60]</ref><ref type="bibr" target="#b59">[61]</ref>. Our solution also proves to be superior to general-purpose action recognition features <ref type="bibr" target="#b17">[18]</ref>, when applied to pornography detection.</p><p>The Deep Learning solution using only static information is already competitive with state-of-the-art action recognition features, Dense Trajectories <ref type="bibr" target="#b17">[18]</ref>,</p><p>reaching an error rate of 4%, which is low for such a subjective problem as pornography. For further reducing the error rate, we believe the focus should be on the motion information: by adjusting the CNN, adapting the architecture, boosting the model with more training samples, or improving static-dynamic information fusion.</p><p>Besides improving whole-video classification, we are interested in applying our techniques to the harder task of locating in time the pornographic content within the video. To reach that goal, we are currently annotating the Pornography-2k video dataset at frame level. The main motivation for that harder task is filtering pornography in real time, an important goal for video streaming, camera-surveillance systems, or surveillance of video chats for certain publics.</p><p>Finally, in addition to adapting our current methods for the localization problem (e.g., <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b64">66]</ref>), another aspect worth exploring is to integrate them to the so called Long Short Term Memory (LSTM) networks. LSTMs are a model of Recurrent Neural Network (RNN) that captures the sequential information of the input data, a highly desirable feature for classification of videos. The LSTM architecture could be used to process the CNN extracted features, using the proposed methods in this work, from a fixed number of frames, improving the real-time classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sequential raw frames (left and middle) and the respective Optical-Flow Displacement Field (right) computed from them. The regions with more movement in the raw frames (e.g., macaw's body and head) are also the ones with the greatest displacement vectors in the field. The original images are under Creative Commons Licence.</figDesc><graphic coords="10,134.48,485.39,109.99,109.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a macroblock and its respective Motion Vector between the current frame (left) and the reference frame (right). The original images are under Creative Commons Licence</figDesc><graphic coords="11,219.70,413.22,171.85,118.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pipeline for the static information flow. It comprises the feature extraction from a sampling of the frames, which are average pooled for feeding a decision-making classifier in the end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pipeline for the motion information flow. It comprises the extraction of the motion information from the video; generation of an image representation to this extracted information; the feature extraction with the selected motion CNN (each motion source has its own CNN model); concatenation of the horizontal (dx) and vertical (dy) descriptions; average pooling of the descriptions; and a classifier (e.g., Support Vector Machines) for the final classification.</figDesc><graphic coords="13,150.96,220.79,309.34,101.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sequential raw frames (a) and motion image representations from optical flows (b) and MPEG motion vectors (c). The horizontal (dx) component is on top, and the vertical (dy) one is on the bottom. The regions with more movement in raw frames (e.g., macaw's head and body) appear highlighted (dark or light) in the motion representations, while regions without movement correspond to the neutral middle gray. The original images are under Creative Commons Licence.</figDesc><graphic coords="14,141.54,263.94,103.11,103.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pipeline for the early fusion strategy. The static and motion information are combined before feature extraction, through a custom-tailored CNN trained for extracting features with both the static and motion information.</figDesc><graphic coords="15,133.77,439.56,343.72,70.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Pipeline for the mid-level fusion. The fusion of static and motion information takes place after feature extraction, and before the decision making, by concatenating the feature vectors into a single representation vector.</figDesc><graphic coords="16,185.33,197.22,240.59,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Pipeline for the late-fusion scheme. The information is combined at the end, after each classifier (e.g., SVM) produces a prediction score, by averaging the scores for the final classification.</figDesc><graphic coords="16,150.96,466.32,309.34,156.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Representative sample frames from Pornography-2k dataset videos. Image adapted from Avila et al.<ref type="bibr" target="#b13">[14]</ref>, with added samples. Note that the dataset is very challenging, with a variety of pornography styles (e.g., hentai vs. live-action) and difficult non-pornographic cases with a lot of skin exposure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of approaches on skin, nudity or pornography detection. The "Type" column comprises four categories: Nude Image (NI), Nude Video (NV), Porn Image (PI) and Porn Video (PV).</figDesc><table><row><cell>Authors</cell><cell>Type</cell><cell>Method</cell><cell>Classifier</cell></row><row><cell>Fleck et al. [2]</cell><cell>NI</cell><cell>Skin detection; geometrical analysis</cell><cell>Threshold</cell></row><row><cell>Lopes et al. [32]</cell><cell>NI</cell><cell>BoVW model; PCA on SIFT and Hue-SIFT descriptors</cell><cell>Linear SVM</cell></row><row><cell>Lopes et al. [33]</cell><cell>NV</cell><cell>BoVW model; PCA on SIFT and Hue-SIFT descriptors; voting scheme</cell><cell>Linear SVM</cell></row><row><cell>Jones and Rehg [6]</cell><cell>PI</cell><cell>Skin color histogram; color probabilities</cell><cell cols="2">C4.5 decision tree</cell></row><row><cell>Rowley et al. [7]</cell><cell>PI</cell><cell>Skin color histogram; skin texture his-togram; face detection</cell><cell>RBF SVM</cell></row><row><cell>Zheng et al. [5]</cell><cell>PI</cell><cell>Skin color detection; skin region detec-tion; shape descriptors</cell><cell cols="2">AdaBoost C4.5 decision tree with</cell></row><row><cell>Zuo et al. [34]</cell><cell>PI</cell><cell>Patch-based skin color detection; hu-man body part detection</cell><cell cols="2">Random forest</cell></row><row><cell>Deselaers et al. [10]</cell><cell>PI</cell><cell>BoVW model; PCA on SIFT descrip-tors; GMM model</cell><cell cols="2">SVM; histogram in-tersection kernel</cell></row><row><cell>Ulges and Stahl [12]</cell><cell>PI</cell><cell>BoVW model; DCT in YUV color space</cell><cell cols="2">SVM; œá 2 kernel</cell></row><row><cell>Steel [13]</cell><cell>PI</cell><cell>Mask-SIFT; skin percentage</cell><cell cols="2">Cascade classifier of three stages</cell></row><row><cell></cell><cell></cell><cell>Bayesian method with a grouping</cell><cell>Artificial</cell><cell>neural</cell></row><row><cell>Zaidan et al. [35]</cell><cell>PI</cell><cell>histogram; segmentation with back-</cell><cell>network</cell></row><row><cell></cell><cell></cell><cell>propagation neural network</cell><cell></cell></row><row><cell>Zhuo et al. [36]</cell><cell>PI</cell><cell>ORB descriptors; BoVW model</cell><cell>SVM</cell></row><row><cell>Nian et al. [37]</cell><cell>PI</cell><cell>CNN architecture CaffeNet</cell><cell cols="2">CNN softmax</cell></row><row><cell>Jansohn et al. [11]</cell><cell>PV</cell><cell>BoVW model; DCT in YUV color space; motion histograms</cell><cell cols="2">SVM; late fusion</cell></row><row><cell cols="4">Avila et al. [14]  Linear SVM</cell></row><row><cell>Rea et al. [31]</cell><cell>PV</cell><cell>Skin color estimation; MPEG motion information; periodic patterns detection</cell><cell cols="2">Threshold over pe-riodicity measure</cell></row><row><cell></cell><cell></cell><cell>BoVW model; DCT in YUV color</cell><cell cols="2">SVM; RBF and œá 2</cell></row><row><cell>Ulges et al. [40]</cell><cell>PV</cell><cell>space; MFCC audio features; motion</cell><cell cols="2">kernels; late fusion</cell></row><row><cell></cell><cell></cell><cell>histograms; skin detection</cell><cell></cell></row><row><cell>Moustafa [22]  *</cell><cell>PV</cell><cell>CNNs on raw frames</cell><cell cols="2">Majority voting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* PV BoVW-based model: BossaNova; Hue-SIFT descriptors RBF SVM Caetano et al. [38] * PV BoVW-based model: BossaNova; binary descriptors RBF SVM Caetano et al. [39] * PV BoVW-based model: BossaNova; binary descriptors; multiple aggregation functions RBF SVM Valle et al. [15] * PV BoVW model; STIP descriptors Linear SVM Moreira et al. [16] * PV BoVW-based model: TRoF descriptors * The reported results from these works are used for comparison with our proposed approaches</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Learning hyperparameters used for training the architecture used in this work.</figDesc><table><row><cell></cell><cell cols="4">Learning Rate Weight Decay Power Max Epochs</cell></row><row><cell>Raw Frames</cell><cell>0.000009</cell><cell>0.005</cell><cell>0.5</cell><cell>200</cell></row><row><cell>Optical Flow</cell><cell>0.00006</cell><cell>0.001</cell><cell>0.9</cell><cell>200</cell></row><row><cell>MPEG Motion Vectors</cell><cell>0.0002</cell><cell>0.001</cell><cell>0.9</cell><cell>100</cell></row><row><cell>Early Fusion (Gray)</cell><cell>0.0002</cell><cell>0.001</cell><cell>0.9</cell><cell>75</cell></row><row><cell>Early Fusion (Color)</cell><cell>0.001</cell><cell>0.005</cell><cell>0.5</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Video classification accuracy and the F 2 measure, over the first fold from the 5 √ó 2 experimental folds, for the chosen architectures on the Pornography-2k dataset. The chosen architectures are GoogLeNet, AlexNet and VGG, they are evaluated within three different setups: 1) Static data with ImageNet weights; 2) Static data with model fine-tuned from the ImageNet weights; and 3) Motion data (Optical Flows), also with model fine-tuned from the ImageNet weights.</figDesc><table><row><cell></cell><cell cols="2">Static -ImageNet</cell><cell cols="2">Static -Fine-tuned</cell><cell cols="2">Motion -Fine-tuned</cell></row><row><cell cols="6">CNN Architecture ACC (%) F 2 (%) ACC (%) F 2 (%) ACC (%)</cell><cell>F 2 (%)</cell></row><row><cell>GoogLeNet [21]</cell><cell>94.7</cell><cell>95.4</cell><cell>95.9</cell><cell>95.5</cell><cell>94.5</cell><cell>93.1</cell></row><row><cell>AlexNet [19]</cell><cell>94.9</cell><cell>94.6</cell><cell>95.0</cell><cell>94.4</cell><cell>93.4</cell><cell>93.7</cell></row><row><cell>VGG [64]</cell><cell>94.6</cell><cell>95.2</cell><cell>95.9</cell><cell>95.3</cell><cell>95.7</cell><cell>96.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on the Pornography-2k dataset for the third-party solutions, the current state-of-the-art spatio-temporal video description, and the best approaches we have proposed in each modality (Static -Fine-tuned; Motion -Optical Flow; Late Fusion with Optical Flow). We report the average performance on 5 √ó 2 folds. ACC: accuracy -F2: F2 measure - * Dense Trajectories and Late Fusion (OF) are statistically different (p-values: ACC ‚âà 0.028; F2 ‚âà 0.001) - ‚Ä† Dense Trajectories and Static -Fine-tuned are not statistically different in ACC, but are in F2 (p-values: ACC ‚âà 0.239; F2 ‚âà 0.037) -TRoF and Late Fusion (OF) are statistically different (p-values: ACC ‚âà 0.014; F2 ‚âà 0.009) - ‚Ä° TRoF and Static -Fine-tuned are not statistically different in ACC, but are in F2 (p-values: ACC ‚âà 0.202; F2 ‚âà 0.037) -All standard deviations are smaller than 0.1.</figDesc><table><row><cell></cell><cell>Solution</cell><cell cols="2">ACC (%) F2 (%)</cell></row><row><cell></cell><cell>Snitch Plus [59]</cell><cell>66.6</cell><cell>46.4</cell></row><row><cell>Third-party</cell><cell>MediaDetective [58] NuDetective [61]</cell><cell>71.9 72.6</cell><cell>66.5 62.9</cell></row><row><cell></cell><cell>PornSeer Pro [60]</cell><cell>79.1</cell><cell>75.6</cell></row><row><cell>BoVW-based</cell><cell>Dense Trajectories [18]  *  ‚Ä† TRoF [16]  ‚Ä°</cell><cell>95.8 95.6</cell><cell>95.6 95.3</cell></row><row><cell>Proposed Approaches</cell><cell>Static -Fine-tuned Motion -Optical Flow Late Fusion (OF)</cell><cell>96.0 94.4 96.4</cell><cell>96.1 95.3 96.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://sites.google.com/site/pornographydatabase/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>YouTube (www.youtube.com), Vimeo (vimeo.com) and Vine (vine.co)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Part of the results presented in this paper were obtained through the project "Sensitive Media Analysis", sponsored by Samsung Eletr√¥nica da Amaz√¥nia Ltda., in the framework of law No. 8,248/91. We also thank the financial support from CNPq, FAPESP (Grant #2015/19222-9) through the D√©j√†Vu project, and CAPES, through the DeepEyes project. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wetterneck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A review of internet pornography use research: Methodology and content from the past 10 years</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding naked people</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1065</biblScope>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying nude pictures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic detection of human nudes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Blocking Adult Images Based on Statistical Skin Detection, Electronic Letters on Computer Vision and Image Analysis (ELCVIA</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jedynak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale image-based adult-content filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications (VIS-APP)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="290" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical system for objectionable video detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="677" to="684" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skin detection in pornographic videos using threshold technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bouirouga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">El</forename><surname>Fkihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jilbab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aboutajdine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical and Applied Information Technology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words models for adult image classification and filtering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pimenidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting pornographic video content by combining image features with motion information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jansohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic detection of child pornography using color visual words</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Mask-SIFT cascading classifier for pornography detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Steel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Internet Security (WorldCIS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="139" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pooling in image representation: The visual codeword point of view</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU)</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier Computer Vision and Image Understanding</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contentbased filtering for video sharing social networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luz</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Symposium on Information and Computer System Security</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="625" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pornography classification: The hidden clues in video space-time</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Testoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elsevier Forensic Science International (FSI)</title>
		<imprint>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Space-Time Interest Points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applying deep learning to classify pornographic images and videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Pacific-Rim Symposium on Image and Video Technology (PSIVT)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Richardson</surname></persName>
		</author>
		<title level="m">H.264 and MPEG-4 Video Compression: Video Coding for Next-generation Multimedia</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Body plans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Behavior Recognition via Sparse Spatio-Temporal Features, in: IEEE Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Combined</forename><surname>Corner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edge</forename><surname>Detector</surname></persName>
		</author>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal periodicity analysis for illicit content detection in videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Visual Media Production (CVMP)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="106" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nude detection in video using bag-of-visual-features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A bag-offeatures approach based on Hue-SIFT descriptor for nude detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1552" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patch-Based Skin Color Detection and Its Application to Pornography Image Filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web (WWW)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1227" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the multi-agent learning neural and Bayesian methods in skin detector and pornography classifier: An automated anti-pornography system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Abdul</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="397" to="418" />
			<date type="published" when="2014">2014</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ORB feature based web pornographic image recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="511" to="517" />
			<date type="published" when="2016">2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pornographic Image Detection Utilizing Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2016">2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representing local binary descriptors with bossanova for visual recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guimar√£es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium On Applied Computing (SAC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A mid-level video representation based on binary descriptors: A case study for pornography detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J F</forename><surname>Guimar√£es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2016">2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pornography detection in video benefits (a lot) from a multi-modal approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Audio and Multimedia Methods for Large-Scale Video Analysis</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elsevier Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Dataset of 101 Human Actions Classes From Videos in the Wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
	</analytic>
	<monogr>
		<title level="j">UCF</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics Technical Symposium East</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="319" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<title level="m">High Accuracy Optical Flow Estimation Based on a Theory for Warping</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Grange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<ptr target="http://www.webmproject.org/vp9/#draft-vp9-bitstream-and-decoding-process-specification" />
		<title level="m">VP9 Bitstream &amp; Decoding Process Specification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extended bow formalism for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
		<author>
			<persName><surname>Bossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2909" to="2912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Individual Comparisons by Ranking Methods</title>
		<imprint>
			<date type="published" when="1945">1945</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>software</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<ptr target="http://mediadetective.com/" />
		<title level="m">Media Detective</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<ptr target="http://www.hyperdynesoftware.com/" />
		<title level="m">Snitch Plus</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Nudetective: A forensic tool to help combat child pornography through automatic nudity detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Polastro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eleuterio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Database and Expert Systems Applications (DEXA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="349" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pornography detection using bossanova video descriptor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guimar√£es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1681" to="1685" />
		</imprint>
		<respStmt>
			<orgName>EU-SIPCO</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An evaluation on color invariant based local spatiotemporal features for action recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>C√°mara-Ch√°vez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara√∫jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale 1055 Image Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Complex event detection using semantic saliency and nearly-isotonic SVM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1348" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Searching persuasively: Joint event 1060 detection and evidence recounting with limited supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
