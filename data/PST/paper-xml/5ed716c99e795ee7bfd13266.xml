<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNDERSTANDING BIG DATA ANALYTIC WORKLOADS ON MODERN PROCESSORS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-04-20">20 Apr 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute Of Computing Technology Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNDERSTANDING BIG DATA ANALYTIC WORKLOADS ON MODERN PROCESSORS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-04-20">20 Apr 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1504.04974v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big data analytics applications play a significant role in data centers, and hence it has become increasingly important to understand their behaviors in order to further improve the performance of data center computer systems, in which characterizing representative workloads is a key practical problem. In this paper, after investigating three most important application domains in terms of page views and daily visitors, we chose 11 representative data analytics workloads and characterized their micro-architectural behaviors by using hardware performance counters, so as to understand the impacts and implications of data analytics workloads on the systems equipped with modern superscalar out-of-order processors. Our study reveals that big data analytics applications themselves share many inherent characteristics, which place them in a different class from traditional workloads and scale-out services. To further understand the characteristics of big data analytics workloads we performed a correlation analysis of CPI (cycles per instruction) with other microarchitecture level characteristics and an investigation of the big data software stack impacts on application behaviors. Our correlation analysis showed that even though big data analytics workloads own notable pipeline front end stalls, the main factors affecting the CPI performance are long latency data accesses rather than the front end stalls. Our software stack investigation found that the typical big data software stack significantly contributes to the front end stalls and incurs bigger working set. Finally we gave several recommendations for architects, programmers and big data system designers with the knowledge acquired from this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>the context of digitalized information explosion, more and more businesses are analyzing massive amount of data -so-called big data -with the goal of converting big data to "big value" by means of modern data center systems. Typically, data center workloads can be classified into two categories: services and data analytics workloads as mentioned in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b12">[13]</ref>. For the data analytics workloads always process a large mount of data in data centers, we call them big data analytics workloads. Typical big data analytics workloads include business intelligence, machine learning, bio-informatics, and ad hoc analysis <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The business potential of the big data analytics applications is a driving force behind the design of innovative data center systems including both hardware and software <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22]</ref>. For example, the recommendation system is a typical example with huge financial implications, aiming at recommending the right products to the right buyers by mining user behaviors and other logs. Given that big data analytics is a very important application area, there is a need to identify the representative data analytics algorithms or applications in big data fields and understand their performance characteristics with the purpose of improving the big data analytics systems' performance <ref type="bibr" target="#b23">[24]</ref>. In order to achieve this purpose, the following two questions should be answered. <ref type="bibr" target="#b0">1)</ref>. What are the potential bottlenecks and optimization points with higher priority in current systems. 2). What programmers should pay attention to when they develop applications with modern software stacks in order to gain more efficient big data analytics applications. This paper seeks to address the above questions by characterizing representative big data analytics applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Big Data analytics Workloads</head><p>In order to identify representative big data analytics applications in data centers, we single out three important application domains in Internet services: search engine, social networks, and electronic commerce (listed in Figure <ref type="figure" target="#fig_0">1</ref>) according to widely acceptable metrics -the number of page views and daily visitors. And then, we choose eleven representative big data analytics workloads (especially intersection workloads) among the three application domains. Considering our community may feel interest in using those workloads to evaluate the benefits of new system designs and implementations, we release those workloads and the corresponding data sets into an open-source big data benchmark suite-BigDataBench <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref>, which is publicly available from <ref type="bibr" target="#b2">[3]</ref>. Based on selected representative big data analytics applications, we embark on a study to understand big data analytics workloads' behaviors on modern processors. We first characterize big data analytics workloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type="bibr" target="#b16">[17]</ref>. We adopt larger input data sets varying from 147 to 187 GB that are stored in both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type="bibr" target="#b16">[17]</ref>) in the memory system. And for each workload, we collect the performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type="bibr" target="#b16">[17]</ref>).</p><p>We find that big data analytics applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU2006), HPC (HPCC), traditional service (SPECweb2005 and TPC-W), chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type="bibr" target="#b16">[17]</ref>) workloads. Meanwhile the service workloads in data center (scale-out service workloads) share many similarities in terms of micro-architecture characteristics with that of traditional service workloads, so in the rest of this paper, we just use the service workloads to describe them. Furthermore, we perform a correlation analysis between cycles per instruction (CPI) performance and micro-architecture characteristics to find the potential optimization methods for big data analytics workloads. At last we analyze the impacts of a typical big data software stack on critical metrics that have proved by correlation analysis for big data analytics applications on modern processors as a case study and show the aspects that programmers should pay much attention to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper Contributions and Outlines</head><p>This paper has the following major contributions:</p><p>1) The characterization of big data analytics workloads and comparison with traditional workloads. Base on the characterization we find that:</p><p>• The big data analytics workloads have higher instruction level parallelism (i.e. IPC) than that of the services workloads while lower than those of computation-intensive HPCC workloads, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>• Corroborating previous work <ref type="bibr" target="#b16">[17]</ref>, both the big data analytics workloads and service workloads suffer from notable pipeline front end stalls.</p><p>• The significant differences between the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type="bibr" target="#b16">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall breakdown: the big data analytics workloads suffer more stalls in the out-of-order part of the pipeline (about 57% on average), while the service workloads suffer more stalls before instructions entering the outof-order part of pipeline (about 75% on average).</p><p>• Big data analytics workloads have lower L2 cache miss ratios (about 11 L2 cache misses per thousand instructions on average) than those of the service workloads (about 66 L2 cache misses per thousand instructions on average) while higher than those of the HPCC workloads. Meanwhile, for the big data analytics and service workloads, on the average 85.5% and 95.5% of L2 cache misses are hit in L3 cache (last level cache), respectively. For the service workloads, our observations corroborate the previous work <ref type="bibr" target="#b16">[17]</ref>: the L2 cache is ineffective.</p><p>• For the big data analytics workloads, the misprediction ratios are lower than those of most of the service workloads, which implies that the branch predictor of modern processor is good. Further more, a simpler branch predictor may be preferred so as to save power and die area for big data analytics workloads.</p><p>2) The correlation analysis of the CPI performance and other micro-architecture metrics. Our results reveal that:</p><p>• Although big data analytics workloads own a notable processor pipeline front end stall in our workload characterization study (Section 4), the front end stall does not have a strong correlation with CPI performance. This implies that the front end stall is not the factor that affects CPI performance most for data analytics workloads from the perspective of micro-architecture.</p><p>• For big data analytics workloads, the TLB and private united cache (L2 cache in our architecture) performances have strong correlations with CPI performance. So the TLB and private united cache need to be optimized with the highest priority in order to achieve better performance. Further more, considering our findings in workload characterization study (Section 4), the L2 cache miss ratio is acceptable for big data analytics workloads and the last level cache can satisfy most of cache misses from previous level caches. So reducing the capacity of last level cache properly may benefit the performance, since a smaller last level cache can shorten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. Moreover, for modern processors dedicate approximately half of the die area to caches, a smaller last level cache can also improve the energy-efficiency of processor and save the die size.</p><p>3) The investigation of modern big data software stack's impacts on big data analytics application behaviors from the perspective of micro-architecture. We find that the big data software stack has impacts on the following aspects:</p><p>• The big data software stack makes contribution to front end stalls by increasing application's binary size and further increases the pressure on instruction fetch unit.</p><p>• The big data software stack incurs larger working set and prolongs the memory access latency, especially for load operations.</p><p>• Most of the big data software stack functions are implemented on user-mode and do not invoke many system calls. The large amounts of user-mode instructions reduce the kernel-mode instruction ratio of the whole application.</p><p>The remainder of the paper is organized as follows. Section 2 lists the related work. Section 3 states our experiment methodology. Section 4 presents the micro-architectural characteristics of the data analysis workloads in comparison with other benchmark suites. Section 5 analyzes the correlation of each of the measured characteristics with CPI. Section 6 investigates a typical big data analytics software stack's impacts on application behaviors. Section 7 draws conclusions of the full paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been much work proposed to evaluate data mining algorithms or evaluate clusters using data analytics workloads in different aspects, such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> and etc. Narayanan et al. <ref type="bibr" target="#b32">[33]</ref> characterize traditional data analytics workloads on single node other then workloads running at data center scale. Huang et al. <ref type="bibr" target="#b18">[19]</ref> characterize the MapReduce framework in system level performance. They evaluate the Hadoop framework and do not focus on the micro-architecture's characteristics. Awasthi et al. <ref type="bibr" target="#b11">[12]</ref> also perform a system level characterization of data center applications. Anwar et al. <ref type="bibr" target="#b10">[11]</ref> conduct a quantitative study of representative Hadoop applications on five hardware configurations with the purpose of evaluating the different clusters' performance. The state-of-the-art work of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type="bibr" target="#b16">[17]</ref>. However, CloudSuite paper is biased towards online service workloads: among six benchmarks, there are four scale-out service workloads, (including Data Serving, Media Streaming, Web Search, Web Serving), and only one big data analytics workload-Naive Bayes. One application can not cover all the characteristics big data analytics applications own. Our work shows that the data analytics workloads are significantly diverse in terms of micro-architectural level characteristics (Section 4) on modern processors. Previous work also finds that big data analytics applications show varying performance, energy behavior and preferable system configuration parameters <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>. In a word, only one application is not enough to represent various categories of big data analytics workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>This section firstly describes the experimental environments on which we conduct our study, and then explains our experiment methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Workloads Selection</head><p>In order to find representative big data analytics workloads we firstly decide and rank the main application domains according to widely acceptable metrics-the number of  pageviews and daily visitors, and then single out the main applications from the most important application domains. We investigate the top sites listed in Alexa <ref type="bibr" target="#b0">[1]</ref>, of which the rank of sites is calculated using a combination of average daily visitors and page views. We classified the top 20 sites into 5 categories including search engine, social network, electronic commerce, media streaming and others. Figure <ref type="figure" target="#fig_0">1</ref> shows the categories and their respective share. To keep concise and simple, we focus on the top three application domains: search engine, social networks and electronic commerce.</p><p>We choose the most popular applications in those three application domains. Table <ref type="table" target="#tab_1">2</ref> shows application scenarios of each workload, which is characterized in this paper, indicating most of our chosen workloads are intersections among three domains. Considering our community may feel interest in using those those workloads to evaluate the benefits of new system designs and implementations, we release those workloads and corresponding data sets into an open-source big data benchmark suite-BigDataBench <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>, which is an open-source big data benchmark suite modeling diversified typical and important big data application domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hardware Configurations</head><p>We use a 5-node Hadoop cluster (one master and four slaves) to run all big data analytics workloads. The nodes in our Hadoop cluster are connected through 1 Gb ethernet network. Each node has two Intel Xeon E5645 (Westmere) processors and 32 GB memory. A Xeon E5645 processor includes six physical out-of-order cores with speculative pipelines. Each core has private L1 and L2 caches, and all cores share the L3 cache. Table <ref type="table" target="#tab_2">3</ref> lists the important hardware configurations of the processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Big Data Analytics Applications Setups</head><p>All the big data analytics applications are implemented on the Hadoop <ref type="bibr" target="#b41">[42]</ref> system, which is an open source MapReduce implementation. The version of Hadoop and JDK is 1.0.2 and 1.6.0, respectively. For data warehouse workloads, we use Hive of the 0.6 version. Each node runs Linux CentOS 5.5 with the 2.6.34 Linux kernel. Each slave node is configured with 24 map task slots and 12 reduce task slots. For each map and reduce task, we assigned 1 GB Java heap in order to achieve better performance. Table <ref type="table" target="#tab_0">1</ref> presents the size of input data set and the instructions retired of each big data analytics workload. The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type="bibr" target="#b16">[17]</ref>, our approach are more pragmatic. We adopt a larger data input that are stored in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type="bibr" target="#b16">[17]</ref>) in memory. The number of instructions retired of the big data analytics workloads ranges from thousand of billions to tens of thousands of billions, which indicates that those applications are not trivial ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Compared Benchmarks Setups</head><p>In addition to big data analytics workloads, we deployed several benchmark suites, including SPEC CPU2006, HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type="bibr" target="#b16">[17]</ref>, and compared them with big data analytics workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Traditional benchmarks setups</head><p>SPEC CPU2006: we run the official applications with the first reference input, reporting results averaged into two groups, integer benchmarks (SPECINT) and floating point benchmarks (SPECFP). The gcc which we used to compile the SPEC CPU is version 4.1.2.</p><p>HPCC: we deploy HPCC -a representative HPC benchmark suite. The HPCC version is 1.4. It has seven benchmarks<ref type="foot" target="#foot_0">1</ref> , including HPL, STREAM, PTRANS, RandomAccess, DGEMM, FFT, and COMM. We run each benchmark respectively.</p><p>SPECweb 2005: we run the bank application as the Web server on one node with 24 GB data set. We use distributed clients to generate the workloads, and the number of the total simultaneous sessions is 3000.</p><p>PARSEC: we deploy PARSEC 2.0 Release. We run all benchmarks with native input data sets and use gcc with version 4.1.2 to compile them.</p><p>TPC-W: we deploy a Java TPC-W Implementation Distribution from University of Wisconsin-Madison <ref type="bibr" target="#b6">[7]</ref> with MySQL version 5.1.73 and JDK version 1.6.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">CloudSuite Setups</head><p>CloudSuite 1.0 has six benchmarks, including one big data analytics workload-Naive Bayes. We also choose Naive Bayes as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type="bibr" target="#b16">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the other five benchmarks following the introduction on the CloudSuite web site <ref type="bibr" target="#b4">[5]</ref>.</p><p>Data Serving: we benchmark Cassandra 0.7.3 database with 30 million records. The request is generated by a YCSB <ref type="bibr" target="#b14">[15]</ref> client with a 50:50 ratio of read to update.</p><p>Media Streaming: we use Darwin streaming server 6.0.6. We set 20 Java processes and issue 20 client threads by using the Faban driver <ref type="bibr" target="#b5">[6]</ref> with GetMediumLow 70 and GetshortHi 30.</p><p>Software Testing: we use the cloud9 execution engine, and run the printf.bc coreutils binary file.</p><p>Web Search: we benchmark a distributed Nutch 1.1 index server. The index and data segment size is 17, and 35 GB, respectively.</p><p>Web Serving: we characterize a front end of Olio server. We simulate 500 concurrent users to send requests with 30 seconds ramp-up time and 300 seconds steady state time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experimental Methodology</head><p>Modern superscalar Out-of-Order (OoO) processors prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16]</ref>. The retirement centric analysis is also difficult to account how the CPU cycles are used because the pipelines will continue executing instructions even though the instruction retirement is blocked <ref type="bibr" target="#b28">[29]</ref>. So in this paper, we focus on counting cycles stalled due to resource conflict, e.g. the reorder buffer full stall, which prevents new instructions from entering the pipelines.</p><p>We get the micro-architectural data by using hardware performance counters to measure the architectural events. In order to monitor micro-architectural events, a Xeon processor provides several performance event select MSRs (Model Specific Registers), which specify hardware events to be counted, and performance monitoring counter MSRs, which store results of performance monitoring events. We use Perf-a profiling tool for Linux 2.6+ based systems <ref type="bibr" target="#b7">[8]</ref>, to manipulate those MSRs by specifying the event numbers and corresponding unit masks. We collect about 20 events whose number and corresponding unit masks can be found in the Intel Software Developer's Manual <ref type="bibr" target="#b19">[20]</ref>. In addition, we access the proc file system to collect OS-level performance data, such as the number of disk writes.</p><p>We perform a ramp-up period for each application, and then start collecting the performance data. Different from the experiment methodology of CloudSuite, which only performs 180-second measurement, the performance data we collected cover the whole lifetime of each application, including map, shuffle, and reduce stages. We collect the data of all the four working nodes and report the mean value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Characterization Results</head><p>We provide a detailed analysis of the inefficiencies of running big data analytics workloads on modern OoO (Out of Order) processors in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instructions Execution</head><p>Instructions per cycle (in short IPC) is used to measure instruction level parallelism, indicating how many instructions can execute simultaneously. Our processors have 6 cores, and each core can commit up to 4 instructions on each cycle in theory. However, for different workloads, IPC can be limited by pipeline stalls and data or instructions dependencies.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows IPC of each workload. The CloudSuite has six benchmarks, among which we report the Naive Bayes on the leftmost side, separated from the other five workloads (in the middle side), since Naive Bayes is also included into our eleven workloads.</p><p>The main workloads of CloudSuite (four among six) are service workloads: Media Streaming, Data Services, Web Services, and Web Search. From Figure <ref type="figure" target="#fig_1">2</ref>  that the service workloads, including four of CloudSuite, TPC-W and SPECweb, have the low IPCs (all less than 0.6) in comparison with the other workloads, including our chosen big data analytics workloads, PARSEC, SPECFP, SPECINT, and most of HPCC workloads.</p><p>Most of big data analytics workloads have middle IPC values, greater than those of the service workloads. The IPCs of the eleven big data analytics workloads ranges from 0.52 to 0.95 with an average value of 0.78. The avg bar in Figure <ref type="figure" target="#fig_1">2</ref> means the average IPC of the eleven big data analysis workloads. Naive Bayes has the lowest IPC value among the eleven big data analysis workloads. The IPCs of the HPCC workloads have a large discrepancy among each workload since they are all micro-benchmark designed for measuring different aspects of systems. For example, HPCC-HPL and HPCC-DGEMM are computation-intensive, and hence have a higher IPC (close to 1.2). While HPCC-STREAM is designed to stream access memory, it has poor temporal locality, causing long-latency memory accesses, and hence it has lower IPCs (less than 0.5).</p><p>Figure <ref type="figure">3</ref> illustrates the retired instructions breakdown of each workload. We also notice that the service workloads (four of CloudSuite, TPC-W SPECWeb) execute a large percentage of kernel-mode instructions (greater than 40%), while most big data analytics workloads execute a small percentage of kernel-mode instructions. The service workloads have higher percentages of kernel-mode instructions because serving a large amount of requests will result in a large number of network and disk activities.</p><p>Among the big data analytics workloads, only Sort has a high proportion (about 24%) of kernel-mode instructions whereas on average the big data analytics workloads only have about 4% instructions executed in kernel-mode. This is caused by the two unique characteristics of Sort. The first one is that different from most of the big data analytics workloads, We can find that Sort has the highest disk writes frequency. We also observed that network communication activities of Sort are also more frequent than those of the other big data analytics workloads.</p><p>Among the HPCC workloads, RandomAccess has a large percentage of kernel-mode instructions (about 31%). RandomAccess measures the rate of integer random updates of (remote) memory. An update is a read-modify-write operation on a table of 64-bit words, and it involves a large amount of copy user generic string system calls. The other factors contributing a large percentage of kernel-mode instructions need further investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations:</head><p>Big data analytics workloads have higher IPCs than those of services workloads, which are characterized by CloudSuite, traditional web server workload (SPECweb2005) and traditional transactional web workload (TPC-W), while lower than those of computationintensive workloads, e.g., HPCC-HPL, HPCC-DGEMM, PARSEC. Meanwhile we also observe that the most of big data analytics workloads involve less kernel-mode instructions than that of the service workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pipeline Behaviors</head><p>Modern processor implements dynamic execution using out of order and speculative engine. The whole processor architecture can be divided into two parts: including an in-order front end, which fetches, decodes, and issues instructions, and an out-of-order back end, which executes instructions and write data back to register files. A stall can happen in any part of the pipeline. In this paper we focus on the major pipeline stalls (not exhausted), including front end (instruction fetch), register allocation table (in short RAT), load-store buffers, reservation station (in short RS), and re-order buffer (in short ROB). For modern X86 architecture, front end will fetch instructions from L1 Instruction cache and then decode the CISC instructions into RISC-like instructions, which Intel calls micro-operations. RAT will change the registers used by the program into internal registers available. Loadstore buffers are also known as memory order buffers, holding in-flight memory microoperations (load and store), and they ensure that writes to memory take place in the right order. RS queues micro-operations until all source operands are ready. ROB tracks all micro-operations in-flight and make the out-of-order executed instructions retire in order.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> presents those major stalls in pipelines for each workload including instruction fetch stalls, RAT stalls, load buffer full stalls, store buffer full stalls, RS full stalls, and ROB full stalls. We can get the blocked cycles of those kind of stalls mentioned above by using hardware performance counters. Different kinds of pipeline stalls may occur simultaneously, that is to say, the stall cycles may overlap. For example, when the back end is stalled due to RS full, the front end can also be stalled due to L1 instruction cache misses. So in Figure <ref type="figure" target="#fig_3">5</ref>, we report the normalized values of the stalled cycles. We calculate the normalized value by using the following way: we sum up all the blocked cycles for all kinds of stalls as the total blocked cycles. Then we divide each kind of stall's blocked cycles by the total blocked cycles as their percentage in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>Different from HPCC, PARSEC and SPEC CPU2006 workloads, the big data analytics workloads and service workloads suffer from notable instruction fetch stalls, which are mainly caused by L1 instruction cache miss, ITLB (Instruction Translation Lookaside  Buffer) miss or ITLB fault, reported in front-end performance data in Section 4.3. The notable instruction fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type="bibr" target="#b16">[17]</ref>. The front end inefficiency may caused by high-level languages, third-party libraries, deep software stacks used by the big data analytics and service workloads. The most possible reason is that the complicated software stack and middle-ware increase the binary size of the whole application even though some of them only implement a simple algorithm. We also find that there are notable differences in terms of stalls breakdown between the big data analytics workloads and the service workloads (including four service workloads of CloudSuite, SPECWeb and TPC-W). The latter workloads own a large percentage of RAT stalls, which may be caused by partial register stalls or register read port conflicts. While the big data analytics workloads suffer from more RS stalls and ROB stalls, which are caused by limited RS and ROB entries. RAT and instruction fetch stalls occur before instruction entering the out-of-order part of the pipeline while the RS and ROB stalls occur at the out of order part of the pipeline. The service workloads (including Media Streaming, Data Severing, Web Severing, Web Search, SPECweb and TPC-W) have 63% RAT stalls and 12% instruction fetch stalls on average, whereas the big data analytics workloads have about 37% RS full stalls and 20% ROB full stalls on average. So we can find that the big data analytics workloads suffer more stalls in the out-of-order part of the pipeline, while the service workloads suffer more stalls in the in-order part of the pipeline. Further investigation is necessary to understand the root cause behind the differences between two kinds of workloads.</p><p>For the HPCC workloads are composed of micro benchmarks and kernel programs, different programs focus on a specific aspect of the system. So their stall data vary dramatically from each other in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>Implications: Corroborating previous work <ref type="bibr" target="#b16">[17]</ref>, both the big data analytics workloads and the service workloads suffer from notable front-end stalls (i.e. instruction fetch stalls). The instruction fetch stall means that the front end has to wait for fetching instructions, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type="bibr" target="#b16">[17]</ref>, and large binary size complicated by high-level language, third-party libraries and deep software stacks. And we verify that the software stack makes contribute to the front end stall for big data analytics workloads in Section 5.</p><p>However, we note the significant differences between the big data analytics workloads and the service workloads in terms of stall breakdown: the big data analytics workloads suffer more stalls in the out-of-order part of the pipeline, while the service workloads suffer more stalls before instructions entering the out-of-order part. This observation can give us some implications about how to alleviate the bottlenecks in pipeline, although one well known consequence is that right after of alleviating the bottleneck, the next bottleneck emerges <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Front-end Behaviors</head><p>The instruction-fetch stall will prevent core from making forward progress due to lack of instructions. Instruction cache and instruction Translation Look-aside Buffer (TLB) are two fundamental components, which must be accessed when fetching instructions from memory. Instruction cache is the place where the fetch unit directly get instructions. TLB stores page table entries (PTE), which are used to translate virtual addresses to physical addresses. Each time a virtual memory access, the processor searches the TLB for the virtual page number of the page that is being accessed. If a TLB entry is found with a matching virtual page number, a TLB hit occurs and the processor can use the retrieved physical address to access memory. Otherwise there is a TLB miss, the processor has to look up the page table, which called a page walk. The page walk is an expensive operation.</p><p>With a three-level page table, three memory accesses would be required. In other words, it would result in four physical memory accesses. Figure <ref type="figure" target="#fig_4">6</ref> and Figure <ref type="figure" target="#fig_5">7</ref> present the L1 instruction cache misses and the instruction TLB misses, which trigger page walks, per thousand instructions, respectively. On average, the big data analytics workloads generate about 23 L1 instruction cache misses per thousand instructions. They own higher L1 instruction cache misses than those of SPECINT, SPECFP, and all the HPCC workloads. Most of the big data analytics applications have less L1 instruction cache misses than those of the service workloads including Media Streaming, Data Severing, Web Serving, TPC-W and SPECweb. Media streaming has a larger instruction footprint and suffers from severe L1 instruction cache misses, whose L1 instruction cache misses are about three times more than the average of that of the big data analytics workloads. Higher L1 instruction cache misses result in higher instruction fetch stalls as shown in Figure <ref type="figure" target="#fig_3">5</ref>, indicating less efficiency of the front-end. For most of the others benchmarks, the L1 instruction cache misses are really very rare, especially the HPCC workloads, whose instruction footprint is relatively small.</p><p>Consistent with the performance trend of L1 instruction cache misses, the big data analytics workloads' instruction TLB misses are more frequently than those of SPECINT, SPECFP, PARSEC, and all HPCC workloads. Some service workloads (Media Streaming and Data Serving workloads) have more instruction TLB misses than those of the big data analytics workloads. Page walks will cause a long latency instruction fetch stall, waiting for correct physical addresses so as to fetch instructions, and hence result in inefficiency of front end. Among the big data analytics workloads, Naive Bayes is an exception with the fewest L1 instruction cache misses and instruction TLB misses, so it is not enough to represent the spectrum of all big data analytics workloads.</p><p>Implications:</p><p>Improving the L1 instruction cache and instruction TLB performance can improve the performance of data center workloads, especially the service workloads. The third-party libraries and software stacks used by data center workloads may enlarge the binary size of applications and further aggravate the inefficiency of instruction cache and TLB. So when writing the program (with the support of third-party libraries and software stack), the engineers should pay more attention to the code size and the potential burden to pipeline front end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unified Cache and Data TLB Behaviors</head><p>The manufacturers of processors introduce a deep memory hierarchy to reduce the performance impacts of memory wall. Nearly all of the modern processors own three-level caches. A miss penalty of last-level cache can reach up to several hundred cycles in modern processors.</p><p>Figure <ref type="figure">8</ref> shows the L2 cache MPKI (misses per thousand instructions). Figure <ref type="figure">9</ref> reports the ratio of L3 cache hits over L2 cache misses. This ratio can be calculated by using Equation <ref type="formula" target="#formula_0">1</ref>. Please note that we do not analyze the L1 data cache statistics for the miss penalty can be hidden by the out-of-order cores <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ratio =</head><p>L2 cache misses − L3 cache misses L2 cache misses</p><p>For most of the big data analytics workloads, they have lower L2 cache misses (about 11 L2 cache MPKI on average) than those of the service workloads (about 66 L2 cache MPKI on average) while higher than those of the HPCC workloads. The L2 cache statistic indicates the big data analytics workloads own better locality than the service workloads. The HPCC workloads have different localities as the official web site mentioned, which can explain the different cache behaviors among the HPCC workloads.</p><p>From Figure <ref type="figure">9</ref>, we can find that for both the big data analytics workloads and service workloads, the average ratio of L2 cache misses that are hit in L3 cache (85.5% for the big data analytics workloads and 95.5% for the service workloads) is higher than that of the PARSEC and HPCC workloads. We can conclude that for most of the big data analytics and service workloads, modern processor's LLC is large enough to cache most of data missed from L2 cache.</p><p>Figure <ref type="figure" target="#fig_6">10</ref> shows the data TLB misses per thousand instructions. For most of the big data analytics workloads with the exception of Naive Bayes, the data TLB misses are less than most of the service workloads and SPEC CPU2006 workloads (SPECINT and SPECFP), but higher than most of the HPCC workloads with the exception of HPCC-RandomAcess and HPCC-PTRANS. That means the data locality of most of the big data analytics workloads is much better than that of the service workloads.</p><p>Implications:  For the big data analytics workloads, L2 cache is acceptably effective when compared with service workloads. They have lower L2 cache MPKI than that of the service workloads, while higher than that of the HPCC workloads. Meanwhile, for the big data analytics and service workloads, most of L2 cache misses are hit in L3 cache, indicating L3 cache is pretty effective. Modern processors dedicate approximately half of the die area for caches, and hence optimizing the LLC capacity properly may not only reduce the memory access latency but also improve the energy-efficiency of processor and save the die area. For the service workloads, our observation corroborate the previous work <ref type="bibr" target="#b16">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id="formula_1">0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Branch Prediction</head><p>The branch instruction prediction accuracy is one of the most important factor that directly affects the performance. Modern out-of-order processors introduce a functional unit (e.g. Branch Target Buffer) to predict the next branch to avoid pipeline stalls due to branches. If the predict is correct, the pipeline will continue. However, if a branch instruction is mispredicted, the pipeline must flush the wrong instructions and fetch the correct ones, which will cause at least a dozen of cycles' penalty. So branch prediction is not a trivial issue in the pipeline.</p><p>Figure <ref type="figure" target="#fig_7">11</ref> presents the branch miss prediction ratios of each workload. We find that most of the big data analytics workloads own a lower branch misprediction ratio in comparison with that of the service workloads and SPEC CPU workloads. The HPCC workloads own very low misprediction ratios because the branch logic codes of the seven micro benchmarks are simple and the branch behaviors have great regularity. The low misprediction ratios of the big data analytics workloads indicates that most of the branch instructions  in the big data analytics workloads have simple patterns. The simple patterns are conducive to BTB (Branch Target Buffer) to predict whether the next branch needs to jump or not. For the big data analytics workloads, simple algorithms chosen for big data always beat better sophisticated algorithms <ref type="bibr" target="#b33">[34]</ref>, which may be the possible reason for their low misprediction ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications:</head><p>Modern processors invest heavily in silicon real estate and algorithms for the branch prediction unit in order to minimize the frequency and the impact of wrong branch prediction. For the big data analytics workload, the misprediction ratio is lower than most of the compared workloads, even for the CPU benchmark -SPECINT, which implies that the branch predictor of modern processor is good enough for the big data analytics workloads. A simpler branch predictor may be preferred so as to save power and die area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Correlation Analysis</head><p>Correlation analysis can measures the relationship between two items and show the statistical relationships involving dependence <ref type="bibr" target="#b36">[37]</ref>. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. The Pearson's correlation coefficient is the most popular method for correlation analysis. It is a measure of the linear correlation between two variables. It is defined as the covariance of the two variables divided by the product of their standard deviations, which is represented by Equation <ref type="formula" target="#formula_2">2</ref>.</p><formula xml:id="formula_2">ρ(X, Y ) = corr(X, Y ) = cov(X, Y ) σ X σ Y = E[(X − µ X )(Y − µ Y )] σ X σ Y<label>(2)</label></formula><p>The Pearson's correlation coefficient ranges from -1 to 1. The absolute value of the correlation coefficient shows the dependency. The bigger the absolute value, the stronger the correlation between the two variables. The positive number means positive correlation, and vice versa. A value of 1 implies that a linear equation describes the relationship between X and Y perfectly. A value of 0 implies that there is no linear correlation between the variables. CPI (Cycles Per Instruction) refers to the number of processor cycles an instruction consumed in the pipeline. The more cycles a processor takes to complete an instruction, the poorer the performance of the application in the pipeline. So CPI is the metric used to evaluate the application's performance on the pipeline from the perspective of microarchitecture. In order to decide which factors affect the CPI performance, we compute the correlation coefficients of the above micro-architecture level characteristics in Section 4 with CPI. We use CPI to perform correlation analysis, because most of the metrics shown in Section 4 will potentially increase the CPI value, so we can see the positive correlations between CPI and other metrics. We analyze workloads' instruction level parallelism with IPC, which is the multiplicative inverse of CPI, in Section 4.1, and use CPI to represent processor performance in this section.</p><p>Tables <ref type="table" target="#tab_9">4 and 5</ref> show the correlation coefficients of each of the above characteristics with CPI. In those Tables, we only present the first five metrics that own the highest correlation coefficients with CPI. The metrics and the corresponding correlation coefficients are shown in those table in a decreasing order. From Tables <ref type="table" target="#tab_9">4 and 5</ref>, we can make the following observations.</p><p>In contrast to traditional workloads and service workloads, most of big data analytics workloads' CPI performance is sensitive to load buffer performance, i.e. the load buffer full stall has a strong positive correlation with CPI. For the service workloads, the metrics that affect the workloads' CPI are more diverse. Some are very sensitive to instruction fetch stall, such as Media Streaming. Some are sensitive to branch instruction execution situation including branch misprediction ratio and branch instruction ratio. Different from all of other workloads, the chip multiprocessors (PARSEC) and high performance (HPCC) workloads are more sensitive to L3 cache performance. We also can find that nearly for all workloads, the L2 cache performance and kernel-mode instruction have positive correlations with CPI. Most of the correlation coefficients between L2 cache miss ratio (or kernel-mode instructions) and CPI are no less than 0.6, which indicates strong positive correlations. Both instruction and data TLB performance also have impacts on CPI performance for nearly all workloads we investigated, because of the large miss penalty the TLB miss owns<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications:</head><p>Although big data analytics workloads own notable instruction fetch stalls, the instruction fetch stall is not the factor that affects the CPI performance most. We can find that the instruction fetch stall does not appear in Table <ref type="table" target="#tab_8">4</ref> with high correlation coefficient value for most of big data analytics workloads. The only exceptions are Naive Bayes and Sort, however the correlation coefficients are small, only 0.198 and 0.424 respectively, which indicates very weak correlations. The instruction fetch stall play a very critical role in program performance by preventing the pipeline from making forward; however it is not the optimization point with highest priority for big data analytics applications. There are many potential optimization points in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type="bibr" target="#b16">[17]</ref>. According to our correlation analysis in this section, architects should focus on improving TLB performance and the private unified cache (L2 cache for our processor) performance with the highest priority for big data analytics workloads. Just as a page walk, which is caused by a TLB miss, is a very expensive operation. Optimizations should focus on reducing the miss penalty either by enlarging the TLB capacity to hold more entries or by accelerating the speed that refills the TLB. For the private unified cache (L2 cache), the big data analytics workloads have pretty good performance from the perspective of cache miss ratio. So the miss penalty of private unified cache should be the optimization point for big data analytics workloads. Considering that the last level cache can hold most of the misses from previous cache levels as mentioned in Section 4.4, reducing the capacity of last level cache appropriately may be a good choice, just as we suggested in Section 4.4. A smaller last level cache can not only reduce the L2 cache miss penalty but also improve the energy efficiency and save die area. However for chip multiprocessors (PARSEC) and high performance (HPCC) workloads, reducing the last level cache capacity may not be a good choice since their performance is very sensitive to L3 cache miss ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Software Stack Impact</head><p>Software stacks are being proposed to facilitate the development of big data analytics applications. Those software stacks, such as Spark <ref type="bibr" target="#b42">[43]</ref> and Hadoop <ref type="bibr" target="#b41">[42]</ref>, have attracted a large number of users and companies in a short period of time <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. On one hand, the big data analytics software stack facilitates the programmer to write a big data analytics application without considering the messy details of data partitioning, task distribution, load balancing, failure handling and other warehouse-scale system details <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref>. On the other hand, the big data analytics software stack may affect the application behaviors for the software stack increase the call hierarchy of big data analytics applications. Since all the big data analytics workloads we characterized in this paper are based on Hadoop software stack, we would like to investigate the Hadoop's impacts on modern processors as a case study and show what programmers and architects can learn from those impacts.</p><p>Hadoop has three different operation modes: standalone (local) mode, pseudo-distributed mode and fully distributed mode <ref type="bibr" target="#b27">[28]</ref>. In the standalone mode, the Hadoop will run completely on the local machine. It does not use HDFS, nor will it launch any of the Hadoop   daemons. The pseudo-distributed mode is running Hadoop in a "cluster of one" with all daemons running on a single machine. And the fully distributed mode provides a production environment, which can manage a large number of nodes. All the big data analytics workloads are running in the fully distributed mode in previous sections. We can take a glimpse at the big data software stack's impacts by comparing the application behaviors between standalone mode and (pseudo or fully) distributed mode. Actually the standalone mode does not eliminate the impact of the software stack completely, but it eliminates the HDFS and daemon processes' impacts and further alleviates the software stack's impacts largely. The standalone mode really provide us the chance that executes the same user application code but with less call hierarchy.</p><p>In this section, we chose the pseudo-distributed mode as the compared running mode, which invoke the full software stack, for the pseudo-distributed mode eliminates the network factor brought by fully distributed mode. Table <ref type="table" target="#tab_10">6</ref> shows the call hierarchy for those two modes.</p><p>We choose eight applications to investigate the impact of the typical big data analytics software stack, i.e. Hadoop, because the other three applications use third party libraries heavily <ref type="foot" target="#foot_2">3</ref> , which may make it difficult to analyze the Hadoop software stack's impact. For all the eight applications are running on a single node, we must drive them with smaller data sets to avoid overload. We use about 10 GB data set for each workload and use the same data set to drive applications running in different operation mode in order to eliminate the input data set factor. We run the same application on different modes and collect the micro-architecture level metrics.</p><p>In the rest parts of this section, we mainly focus on investigating the software stack's impacts on the following aspects. 1) The instruction fetch unit performance. For the instruction fetch stall is a notable feature that differentiate big data analytics workloads from most of traditional workloads. We want to verify the impacts that the software stack has on instruction fetch unit. 2) The private unified cache, TLB and load buffer performance. Not only for they are the critical units along the data path, but also for they are the metrics that have strong correlations with CPI performance as elaborating in Section 5. We want to know how the software stack affect those units' performance. 3) Kernel-mode instruction ratio. For it is also one of the metrics that has a strong correlation with CPI performance. In addition, we want to investigate on which level do software stack introduced instructions executed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Instruction Fetch</head><p>Figure <ref type="figure" target="#fig_8">12</ref> shows the instruction fetch stall per cycle. It presents the normalized pseudodistributed mode values using the standalone mode data as the baseline. We can find that the software stack really has impacts on the instruction fetch unit. For all the workloads the pseudo-distributed mode has more instruction fetch stalls than that of standalone mode. The ratio ranges from 1.17 to 3.77 and on average the pseudo-distributed workloads' instruction fetch stalls are 2.05 times of those of their standalone counterparts. This implies that the software stack puts more pressure on the instruction fetch unit. This phenomenon most probably caused by the increased application binary size. With the participation of full software stack, a large number of instructions are needed to implement the strategies and mechanisms provided by software stack such as fault tolerant, which increases big data application's binary size and aggravates the inefficiency of instruction fetch unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Path performance</head><p>Load buffer is another very important unit along data path, and it also has great impact on CPI performance for big data analytics workloads as discussed in Section 5. Each load micro-operation requires a load buffer entry and will access the data TLB and data cache. The load buffer keeps track of in-flight loads in the out of order processor. The load buffer full stall event records the cycles of stall due to lack of load buffer entry. We calculate the ratio of load buffer full stall cycles to the total cycles the application used. We also give the normalized value in Figure <ref type="figure" target="#fig_9">13</ref> by using the standalone mode workloads as baseline. From Figure <ref type="figure" target="#fig_9">13</ref>, we can find that most of the applications running at pseudo-distributed mode have more load buffer full stalls than their standalone counterpart. The only exceptions are WordCount and PageRank, which seem not sensitive to software stack from the perspective of load buffer full stall. The notable one is Sort, which has about 72.5 times more load buffer full stalls when the full software stack involved.</p><p>Long latency memory accesses increase the load buffer's pressure since lots of load operations are in-flight and the new load operations can not be issued for lacking of load buffer entries, where a pipeline load buffer full stall occurs. This phenomenon implies that the participation of full software stack increases memory access latency for most of big data analytics applications <ref type="foot" target="#foot_3">4</ref> .</p><p>In Section 5 we find that the private united cache and TLB miss ratio have strong positive correlations with CPI. Also the private united cache (L2 cache) and TLB are key units along the data path. Figure <ref type="figure" target="#fig_10">14</ref> and Figure <ref type="figure" target="#fig_12">15</ref> show the L2 cache MPKI (misses per thousand instructions) and data TLB MPKI. Here we also show the normalized value using standalone mode as baseline just as previous subsection does.</p><p>We can find from Figure <ref type="figure" target="#fig_10">14</ref> that for different applications the software stack has different impacts on L2 cache and data TLB behaviors. For some applications, the pseudodistributed mode has more L2 cache misses. Such as K-means and Pagerank have 1.64 and 1.53 times as many L2 cache misses running on pseudo-distributed mode as running on standalone mode respectively. Other applications have less cache misses after invoking the full software stack. For example, psesudo-distributed mode Sort only has 20% L2 cache misses of its standalone counterpart. For data TLB performance, we can find from Figure <ref type="figure" target="#fig_12">15</ref> that nearly all of the big data analytics applications have more data TLB misses per thousand instructions when they run under the pseudo-distributed mode. The notable one is K-means, which has 1.6 times more data TLB misses running at pseudo-distributed mode than running at standalone mode. The only two exception are Sort, which has less data TLB misses when using full software stack (only 0.41 time of that of its standalone counterpart), and Fuzzy K-means, which seams insensitive to software stack from the perspective of data TLB behavior. So for most of big data analytics applications, the participation of full software stack increases the burden of the data TLB.</p><p>The L2 cache miss and data TLB miss can trigger long latency memory access. And we can find that some applications have less L2 cache and less data TLB MPKI whereas own more load buffer full stalls, such as Sort. Those phenomena are not inconsistent. The event of L2 cache miss or TLB miss records how many times the miss happened. The load buffer full event records the total cycles stalled due to lack of load buffer entry. We calculate L2 cache miss ratio and TLB miss ratio by normalizing those misses with the total number of instructions retired. The participation of full software stack increases the total number of instructions executed by processor, which also enlarge the denominator of L2 cache MPKI and data TLB MPKI. So the software stack instructions amortize the L2 cache MPKI and data TLB MPKI. However the software stack does not reduce the memory access latency. On the contrary, the full software stack incurs larger working set <ref type="foot" target="#foot_4">5</ref> and prolong the memory  access latency for most of the big data analytics applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Kernel-mode Instruction Ratio</head><p>Figure <ref type="figure" target="#fig_13">16</ref> illustrates the retired kernel-mode instruction ratio. We can find that applications running at pseudo-distributed mode have less kernel-mode instructions than their standalone counterpart for most of data analytics workloads. The exceptions are PageRank and IBCF, which have slightly increased kernel-mode instructions, no more than 2%. This implies that the software stack do not invoke a lot of system calls. Most of the functions are implements in the application level. So most of the instructions introduced by full software stack are executed at user-mode (i.e. executed on ring 1 to ring 3). The kernel-mode instruction's ratio is diluted. The notable one is Sort, which triggers a lot of system calls with CPI performance.   as explained in Section 4.1. After invoke the full software stack, a large amount user-mode instructions dilute the kernel-mode instruction ratio and make its kernel-mode instruction ratio be reduced from 0.7 to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Observations and Implications</head><p>From above comparative experiments, we find that the software stack has the impacts on application behaviors from the perspective of micro-architecture. 1) The software stack increases application binary size and aggravates the front end inefficiency for big data analytics workloads. 2) Even though software stack can amortize cache miss ratio and TLB miss ratio for some applications, it does not decrease the memory access latency. On the contrary, the full software stack incurs larger working set and increases the memory access latency especially for load operations. 3) Most of the software stack's functions are implemented on application level and do not invoke lots of system calls. So most of the instructions are executed at user-mode and reduce the whole applications's kernel-mode instruction ratio.</p><p>In order to optimize big data analytics applications developed with the typical big data software stack, i.e. Hadoop in this paper, the potential burden introduce by third-party libraries and software stacks should be noticed, such as the data operations that may incur long memory access latency. And the OS functions' performance should not be pay much attention for the software stack does not invoke lots of kernel-mode instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, after investigating most important application domains in terms of page views and daily visitors, we chosen eleven representative big data analytics workloads and characterized their micro-architectural characteristics on the systems equipped with modern superscalar out-of-order processors by using hardware performance counters.</p><p>Our study on the workload characterizations reveals that the big data analytics applications share many inherent characteristics, which place them in a different class from desktop, HPC, chip multiprocessors, traditional service and scale-out service workloads. Meanwhile, we also observe that the scale-out service workloads (four among six benchmarks in CloudSuite) share many similarities in terms of micro-architectural characteristics with that of the traditional server workloads characterized by SPECweb 2005 and TPC-W.</p><p>Our correlation analysis shows that even though big data analytics workloads suffer from notable front end stalls, the factor that affects CPI performance most is not the front end stall, but the long latency data accesses. So the long latency data accesses should be reduced with the highest priority for big data analytics applications.</p><p>Our investigation finds that the typical big data analytics software stack, i.e. Hadoop, does have impacts on application behaviors, especial on instruction fetch unit and load operations. So for programmers who writer big data analytics applications with the big data software stack, should pay much attention to the burden brought by the software stack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top sites in the web [1].</figDesc><graphic url="image-2.png" coords="9,205.46,389.15,250.37,104.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Instructions per cycle for each workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Disk Writes per Second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Pipeline Stall Break Down of Each Workload</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: L1 Instruction Cache misses per thousand instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Instruction TLB misses per thousand instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: DTLB Misses per Thousand Instructions Retired.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Branch Miss-prediction ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Instruction Fetch Stall.</figDesc><graphic url="image-3.png" coords="28,169.45,123.97,262.87,99.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Load buffer full stall.</figDesc><graphic url="image-17.png" coords="30,164.94,338.23,269.89,91.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: L2 cache MPKI.</figDesc><graphic url="image-15.png" coords="30,162.13,337.15,275.51,96.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Data TLB MPKI.</figDesc><graphic url="image-30.png" coords="31,221.88,342.63,246.23,82.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Kernel-mode instruction ratio.</figDesc><graphic url="image-28.png" coords="31,219.23,341.61,251.53,87.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Representative big data analytics workloads</figDesc><table><row><cell>No.</cell><cell>Workload</cell><cell>Input Data Size</cell><cell>#Retired Instructions (Billions)</cell><cell>Source</cell></row><row><cell>1</cell><cell>Sort</cell><cell>150 GB documents</cell><cell>4578</cell><cell>Hadoop example</cell></row><row><cell>2</cell><cell>WordCount</cell><cell>154 GB documents</cell><cell>3533</cell><cell>Hadoop example</cell></row><row><cell>3</cell><cell>Grep</cell><cell>154 GB documents</cell><cell>1499</cell><cell>Hadoop example</cell></row><row><cell>4</cell><cell>Naive Bayes</cell><cell>147 GB text</cell><cell>68131</cell><cell>Mahout[2]</cell></row><row><cell>5</cell><cell>SVM</cell><cell>148 GB html file</cell><cell>2051</cell><cell>our implementation</cell></row><row><cell>6</cell><cell>K-means</cell><cell>150 GB vector</cell><cell>3227</cell><cell>Mahout</cell></row><row><cell>7</cell><cell>Fuzzy K-means</cell><cell>150 GB vector</cell><cell>15470</cell><cell>Mahout</cell></row><row><cell>8</cell><cell>IBCF</cell><cell>147 GB ratings data</cell><cell>32340</cell><cell>Mahout</cell></row><row><cell>9</cell><cell>HMM</cell><cell>147 GB html file</cell><cell>1841</cell><cell>our implementation</cell></row><row><cell>10</cell><cell>PageRank</cell><cell>187 GB web page</cell><cell>18470</cell><cell>Mahout</cell></row><row><cell>11</cell><cell>Hive-bench</cell><cell>156 GB DBtable</cell><cell>3659</cell><cell>Hivebench</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Scenarios of big data analytics applications.</figDesc><table><row><cell>Name</cell><cell>Domain</cell><cell></cell><cell>Scenarios</cell></row><row><cell></cell><cell cols="2">search engine</cell><cell>Log analysis</cell></row><row><cell>Grep</cell><cell cols="2">social network</cell><cell>Web information extraction</cell></row><row><cell></cell><cell cols="2">electronic commerce</cell><cell>Fuzzy search</cell></row><row><cell>Bayes</cell><cell cols="2">social network</cell><cell>Spam recognition</cell></row><row><cell></cell><cell cols="2">electronic commerce</cell><cell>Web page classification</cell></row><row><cell></cell><cell cols="2">social network</cell><cell>Image Processing</cell></row><row><cell>SVM</cell><cell cols="2">electronic commerce</cell><cell>Data Mining</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Text Categorization</cell></row><row><cell>PageRank</cell><cell cols="2">search engine</cell><cell>Compute the page rank</cell></row><row><cell>Fuzzy</cell><cell cols="2">search engine</cell><cell>Image processing</cell></row><row><cell>K-means,</cell><cell cols="2">social network</cell><cell>High-resolution landform</cell></row><row><cell>K-means</cell><cell cols="2">electronic commerce</cell><cell>classification</cell></row><row><cell></cell><cell cols="2">social network</cell><cell>Speech recognition</cell></row><row><cell>HMM</cell><cell cols="2">search engine</cell><cell>Word Segmentation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Handwriting recognition</cell></row><row><cell></cell><cell cols="2">search engine</cell><cell>Word frequency count</cell></row><row><cell>WordCount</cell><cell cols="2">social network</cell><cell>Calculating the TF-IDF value</cell></row><row><cell></cell><cell cols="2">electronic commerce</cell><cell>Obtaining the user operations count</cell></row><row><cell>Sort</cell><cell cols="2">electronic commerce</cell><cell>Document sorting</cell></row><row><cell></cell><cell cols="2">search engine</cell><cell>Pages sorting</cell></row><row><cell></cell><cell cols="2">social network</cell></row><row><cell></cell><cell>Search Engine</cell><cell></cell><cell>Social Network</cell></row><row><cell></cell><cell cols="2">Electronic Commerce</cell><cell>Media Streaming</cell></row><row><cell></cell><cell>Others</cell><cell></cell></row><row><cell></cell><cell>5%</cell><cell>15%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40%</cell></row><row><cell></cell><cell>15%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>25%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Details of Hardware Configurations.</figDesc><table><row><cell>CPU Type</cell><cell>Intel R Xeon E5645</cell></row><row><cell># Cores</cell><cell>6 cores@2.4G</cell></row><row><cell># threads</cell><cell>12 threads</cell></row><row><cell>#Sockets</cell><cell>2</cell></row><row><cell>ITLB</cell><cell>4-way set associative, 64 entries</cell></row><row><cell>DTLB</cell><cell>4-way set associative, 64 entries</cell></row><row><cell>L2 TLB</cell><cell>4-way associative, 512 entries</cell></row><row><cell>L1 DCache</cell><cell>32KB, 8-way associative, 64 byte/line</cell></row><row><cell>L1 ICache</cell><cell>32KB, 4-way associative, 64 byte/line</cell></row><row><cell>L2 Cache</cell><cell>256 KB, 8-way associative, 64 byte/line</cell></row><row><cell>L3 Cache</cell><cell>12 MB, 16-way associative, 64 byte/line</cell></row><row><cell>Memory</cell><cell>32 GB , DDR3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we can observe</figDesc><table><row><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Instruction per cycle (IPC)</cell><cell>0.2 0.4 0.6 0.8 1 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Naive Bayes</cell><cell>SVM</cell><cell>Grep</cell><cell>WordCount</cell><cell>K-means</cell><cell>Fuzzy K-means</cell><cell>PageRank</cell><cell>Sort</cell><cell>Hive-bench</cell><cell>IBCF</cell><cell>HMM</cell><cell>avg</cell><cell>Software Testing</cell><cell>Media Streaming</cell><cell>Data Serving</cell><cell>Web Search</cell><cell>Web Serving</cell><cell>SPECWeb</cell><cell>TPC-W</cell><cell>SPECFP</cell><cell>SPECINT</cell><cell>PARSEC</cell><cell>HPCC-COMM</cell><cell>HPCC-DGEMM</cell><cell>HPCC-FFT</cell><cell>HPCC-HPL</cell><cell>HPCC-PTRANS</cell><cell>HPCC-RandomAccess</cell><cell>HPCC-STREAM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>The ratio of L3 Cache satisfed L2 Cache Miss Figure</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20 40 60 80 100 L2 Cache misses per k-Instruction</cell><cell>Data TLB misses per K-Instruction</cell><cell cols="3">0 0.5 1 1.5 2</cell><cell></cell><cell cols="2">Naive Bayes</cell><cell cols="2">SVM</cell><cell></cell><cell>Grep</cell><cell></cell><cell>WordCount</cell><cell>K-means</cell><cell>Fuzzy K-means</cell><cell>PageRank</cell><cell>Sort</cell><cell>Hive-bench</cell><cell>IBCF</cell><cell cols="2">HMM</cell><cell cols="2">avg</cell><cell cols="2">Software Testing</cell><cell cols="2">Media Streaming</cell><cell cols="2">Data Serving</cell><cell cols="2">Web Search</cell><cell>Web Serving</cell><cell>SPECWeb</cell><cell>TPC-W</cell><cell>SPECFP</cell><cell>SPECINT</cell><cell>PARSEC</cell><cell>HPCC-COMM</cell><cell>HPCC-DGEMM</cell><cell>HPCC-FFT</cell><cell>HPCC-HPL</cell><cell>HPCC-PTRANS</cell><cell>HPCC-RandomAccess</cell><cell>HPCC-STREAM</cell></row><row><cell></cell><cell cols="2">Naive Bayes</cell><cell cols="2">SVM</cell><cell cols="2">Grep</cell><cell cols="2">WordCount</cell><cell cols="2">K-means</cell><cell cols="2">Fuzzy K-means</cell><cell>PageRank</cell><cell>Sort</cell><cell>Hive-bench</cell><cell>IBCF</cell><cell>HMM</cell><cell>avg</cell><cell></cell><cell>Software Testing</cell><cell cols="2">Media Streaming</cell><cell>Data Serving</cell><cell>Web Search</cell><cell cols="2">Web Serving</cell><cell cols="2">SPECWeb</cell><cell cols="2">TPC-W</cell><cell cols="2">SPECFP</cell><cell>SPECINT</cell><cell>PARSEC</cell><cell>HPCC-COMM</cell><cell>HPCC-DGEMM</cell><cell>HPCC-FFT</cell><cell>HPCC-HPL</cell><cell>HPCC-PTRANS</cell><cell>HPCC-RandomAccess</cell><cell>HPCC-STREAM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="28">Figure 8: L2 cache misses per thousand instructions.</cell></row><row><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Naive Bayes</cell><cell cols="2">SVM</cell><cell cols="2">Grep</cell><cell cols="2">WordCount</cell><cell cols="2">K-means</cell><cell cols="2">Fuzzy K-means</cell><cell cols="2">PageRank</cell><cell>Sort</cell><cell>Hive-bench</cell><cell>IBCF</cell><cell>HMM</cell><cell>avg</cell><cell></cell><cell>Software Testing</cell><cell></cell><cell>Media Streaming</cell><cell>Data Serving</cell><cell>Web Search</cell><cell></cell><cell cols="2">Web Serving</cell><cell cols="2">SPECWeb</cell><cell cols="2">TPC-W</cell><cell>SPECFP</cell><cell>SPECINT</cell><cell>PARSEC</cell><cell>HPCC-COMM</cell><cell>HPCC-DGEMM</cell><cell>HPCC-FFT</cell><cell>HPCC-HPL</cell><cell>HPCC-PTRANS</cell><cell>HPCC-RandomAccess</cell><cell>HPCC-STREAM</cell></row></table><note>9:The ratio of L3 cache satisfying L2 cache misses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Big data analytics and service workloads correlation coefficients</figDesc><table><row><cell>Workload</cell><cell>Correlation Coefficients</cell><cell></cell><cell>Workload</cell><cell>Correlation Coefficients</cell><cell></cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.631</cell><cell></cell><cell>L2 cache miss</cell><cell>0.949</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.419</cell><cell></cell><cell>Instruction TLB miss</cell><cell>0.930</cell></row><row><cell>Naive Bayes</cell><cell>Data TLB miss</cell><cell>0.334</cell><cell>SVM</cell><cell>Data TLB miss</cell><cell>0.901</cell></row><row><cell></cell><cell>L1 Instruction cache miss</cell><cell>0.237</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.890</cell></row><row><cell></cell><cell>Instruction fetch stall</cell><cell>0.198</cell><cell></cell><cell>Load buffer full stall</cell><cell>0.875</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.967</cell><cell></cell><cell>Reservation station full stall</cell><cell>0.763</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.944</cell><cell></cell><cell>L2 cache miss</cell><cell>0.626</cell></row><row><cell>Grep</cell><cell>Data TLB miss</cell><cell>0.893</cell><cell>WordCount</cell><cell>Instruction TLB miss</cell><cell>0.618</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.834</cell><cell></cell><cell>Data TLB miss</cell><cell>0.571</cell></row><row><cell></cell><cell>Load buffer full stall</cell><cell>0.728</cell><cell></cell><cell>Branch misprediction</cell><cell>0.501</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.954</cell><cell></cell><cell>L2 cache miss</cell><cell>0.918</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.914</cell><cell></cell><cell>Instruction TLB miss</cell><cell>0.905</cell></row><row><cell>K-means</cell><cell>Load buffer full stall</cell><cell>0.822</cell><cell>Fuzzy K-means</cell><cell>Data TLB miss</cell><cell>0.895</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.821</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.837</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.816</cell><cell></cell><cell>Load buffer full stall</cell><cell>0.748</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.872</cell><cell></cell><cell>Load buffer full stall</cell><cell>0.592</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.743</cell><cell></cell><cell>Data TLB miss</cell><cell>0.493</cell></row><row><cell>PageRank</cell><cell>L2 cache miss</cell><cell>0.679</cell><cell>Sort</cell><cell>L2 cache miss</cell><cell>0.485</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.586</cell><cell></cell><cell>Instruction fetch stall</cell><cell>0.424</cell></row><row><cell></cell><cell>L1 instruction cache miss</cell><cell>0.405</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.423</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.901</cell><cell></cell><cell>L2 cache miss</cell><cell>0.809</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.856</cell><cell></cell><cell>Data TLB miss</cell><cell>0.793</cell></row><row><cell>Hive Bench</cell><cell>Reservation Station stall</cell><cell>0.815</cell><cell>IBCF</cell><cell>Kernel-mode instruction</cell><cell>0.607</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.766</cell><cell></cell><cell>L3 cache miss</cell><cell>0.478</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.555</cell><cell></cell><cell>Branch misprediction</cell><cell>0.478</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.894</cell><cell></cell><cell>Data TLB miss</cell><cell>0.972</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.874</cell><cell></cell><cell>L2 cache miss</cell><cell>0.939</cell></row><row><cell>HMM</cell><cell>Instruction TLB miss</cell><cell>0.783</cell><cell>TPC-W</cell><cell>Instruction TLB miss</cell><cell>0.855</cell></row><row><cell></cell><cell>Branch misprediction</cell><cell>0.612</cell><cell></cell><cell>Branch instruction retired</cell><cell>0.577</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.609</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.557</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.983</cell><cell></cell><cell>Branch instruction retired</cell><cell>0.954</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.978</cell><cell></cell><cell>L1 instruction miss</cell><cell>0.902</cell></row><row><cell>Software Testing</cell><cell>L3 cache miss</cell><cell>0.977</cell><cell>Media Streaming</cell><cell>Instruction fetch stall</cell><cell>0.872</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.886</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.821</cell></row><row><cell></cell><cell>Instruction fetch stall</cell><cell>0.877</cell><cell></cell><cell>ReOrder Buffer stall</cell><cell>0.808</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.971</cell><cell></cell><cell>Data TLB miss</cell><cell>0.995</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.953</cell><cell></cell><cell>L2 cache miss</cell><cell>0.994</cell></row><row><cell>Data Serving</cell><cell>Instruction TLB miss</cell><cell>0.948</cell><cell>Web Search</cell><cell>Instruction TLB miss</cell><cell>0.988</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.925</cell><cell></cell><cell>Kernel-mode instruction</cell><cell>0.963</cell></row><row><cell></cell><cell>Branch misprediction</cell><cell>0.925</cell><cell></cell><cell>Load buffer full stall</cell><cell>0.912</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.981</cell><cell></cell><cell>Instruction TLB miss</cell><cell>0.920</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.947</cell><cell></cell><cell>Data TLB miss</cell><cell>0.845</cell></row><row><cell>Web Serving</cell><cell>Instruction TLB miss</cell><cell>0.911</cell><cell>SPECWeb</cell><cell>Kernel-mode instruction</cell><cell>0.786</cell></row><row><cell></cell><cell>Reservation Station full stall</cell><cell>0.566</cell><cell></cell><cell>Store buffer full stall</cell><cell>0.641</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.545</cell><cell></cell><cell>L1 instruction cache miss</cell><cell>0.592</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Traditional workloads's correlation coefficients</figDesc><table><row><cell>Workload</cell><cell>Correlation Coefficients</cell><cell></cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.899</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.513</cell></row><row><cell>PARSEC</cell><cell>Data TLB miss</cell><cell>0.375</cell></row><row><cell></cell><cell>L1 instruction miss</cell><cell>0.295</cell></row><row><cell></cell><cell>Instruction fetch stall</cell><cell>0.877</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.999</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.988</cell></row><row><cell>HPCC-COMM</cell><cell>Kernel-mode instruction</cell><cell>0.980</cell></row><row><cell></cell><cell>Branch misprediction</cell><cell>0.495</cell></row><row><cell></cell><cell>L1 instruction miss</cell><cell>0.400</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.988</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.751</cell></row><row><cell>HPCC-DGEMM</cell><cell>Data TLB miss</cell><cell>0.662</cell></row><row><cell></cell><cell>Branch misprediction</cell><cell>0.611</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.353</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.997</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.961</cell></row><row><cell>HPCC-FFT</cell><cell>Instruction TLB miss</cell><cell>0.828</cell></row><row><cell></cell><cell>L1 instruction miss</cell><cell>0.459</cell></row><row><cell></cell><cell>kernel-mode instruction</cell><cell>0.435</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.859</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.834</cell></row><row><cell>HPCC-HPL</cell><cell>Kernel-mode instruction</cell><cell></cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.589</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.435</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.871</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.812</cell></row><row><cell>HPCC-PTRANS</cell><cell>Instruction TLB miss</cell><cell>0.809</cell></row><row><cell></cell><cell>Reservation buffer full store</cell><cell>0.734</cell></row><row><cell></cell><cell>ReOder Buffer full stall</cell><cell>0.645</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.999</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.999</cell></row><row><cell>HPCC-Random Access</cell><cell>Data TLB miss</cell><cell>0.999</cell></row><row><cell></cell><cell>L1 instruction miss</cell><cell>0.911</cell></row><row><cell></cell><cell>Instruction fetch stall</cell><cell>0.890</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.995</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.978</cell></row><row><cell>HPCC-Stream</cell><cell>L1 instruction cache miss</cell><cell>0.873</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.674</cell></row><row><cell></cell><cell>Application instruction retired</cell><cell>0.398</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.767</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.699</cell></row><row><cell>SPEC INT</cell><cell>Instruction TLB miss</cell><cell>0.493</cell></row><row><cell></cell><cell>Kernel-mode instruction</cell><cell>0.454</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.389</cell></row><row><cell></cell><cell>Data TLB miss</cell><cell>0.719</cell></row><row><cell></cell><cell>Instruction TLB miss</cell><cell>0.582</cell></row><row><cell>SPEC CFP</cell><cell>Kernel-mode instruction</cell><cell>0.549</cell></row><row><cell></cell><cell>L2 cache miss</cell><cell>0.452</cell></row><row><cell></cell><cell>L3 cache miss</cell><cell>0.308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Hadoop call hierarchy among different modes."Y" means the corresponding mode will invoke the item. "N" means the corresponding mode will not invoke the item</figDesc><table><row><cell></cell><cell>Standalone</cell><cell>Pseudo-distributed</cell></row><row><cell>Hadoop daemons</cell><cell>N</cell><cell>Y</cell></row><row><cell>HDFS</cell><cell>N</cell><cell>Y</cell></row><row><cell>MapReduce API</cell><cell>Y</cell><cell>Y</cell></row><row><cell>JVM</cell><cell>Y</cell><cell>Y</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">HPL solves linear equations. STREAM is a simple synthetic benchmark, streaming access memory. Ran-domAccess updates (remote) memory randomly. DGEMM performs matrix multiplications. FFT performs discrete fourier transform. COMM is a set of tests to measure latency and bandwidth of the interconnection system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Our Xeon processor have a tow-level TLB. The first level has separate instruction and data TLBs. The second level is shared. We measure a TLB miss at both level, which means a page walk happened.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">HMM invokes ICTCLASS<ref type="bibr" target="#b3">[4]</ref>; SVM invokes LIBSVM<ref type="bibr" target="#b13">[14]</ref> and Hive-bench invokes Hive.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We also observe a similar phenomenon for store buffer. The applications run at pseudo-distributed mode have more store buffer full stalls than their standalone counterpart. However, the ratio is not as big as load buffer full stalls. The maximum ratio is about 1.6. For the store buffer full stall does not have a strong correlation with performance, we do not discuss it here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We also examine the L3 cache statistic. We find that all of the big data analytics applications have more L3 cache MPKI when they are running at pseudo-distribute mode. The ratio ranges from 1.4 to 2.7. This phenomenon also indicates that the application running at pseudo-distribute mode owns larger working set. We do not show the data both for the space limitation and the L3 cache miss does not have a strong correlation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Alexa: The top 500 sites on the web</title>
		<ptr target="http://www.alexa.com/topsites/global;0" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Apache mahout</title>
		<ptr target="http://mahout.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BigDataBench homepage</title>
		<ptr target="http://prof.ict.ac.cn/BigDataBench/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Chinese word segmentation library</title>
		<ptr target="http://ictclas.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cloudsuite homepage</title>
		<ptr target="http://parsa.epfl.ch/cloudsuite/cloudsuite.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Faban harness and benchmark framework</title>
		<ptr target="http://java.net/project/faban" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Java tpc-w implementation distribution</title>
		<ptr target="http://pharm.ece.wisc.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Performance counters for linux</title>
		<ptr target="https://perf.wiki.kernel.org/index.php/Main_Page" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Power by hadoop</title>
		<ptr target="http://wiki.apache.org/hadoop/PoweredBy" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Powered by spark</title>
		<ptr target="https://cwiki.apache.org/confluence/display/SPARK/\Powered+By+Spark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the use of microservers in supporting hadoop applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing (CLUSTER)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Systemlevel characterization of datacenter applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shayesteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th ACM/SPEC International Conference on Performance Engineering</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The datacenter as a computer: An introduction to the design of warehouse-scale machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="108" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing</title>
				<meeting>the 1st ACM symposium on Cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A performance counter architecture for computing accurate cpi components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bigdatabench: a big data benchmark suite from web search engines</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Architectures and Systems for Big Data (ASBD 2013) in conjunction with The 40th International Symposium on Computer Architecture(ISCA)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The hibench benchmark suite: Characterization of the mapreduce-based data analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Engineering Workshops (ICDEW), 2010 IEEE 26th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Software Developers Manual</title>
		<author>
			<persName><forename type="first">R</forename><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards energy awareness in hadoop</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rafique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network-Aware Data Management (NDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
	<note>Fourth International Workshop on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterizing data analysis workloads in data centers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization (IISWC), 2013 IEEE International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterizing and subsetting big data workloads</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization (IISWC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>IEEE International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The implications of diverse applications and scalable data sets in benchmarking big data systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Specifying Big Data Benchmarks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="44" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A first-order superscalar processor model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture, 2004. Proceedings. 31st Annual International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Performance characterization of a quad Pentium pro SMP using OLTP workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ϕsched: A heterogeneity-aware hadoop workflow scheduler</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 22nd Modeling, Analysis, and Simulation On Computer and Telecommunication Systems</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<title level="m">Hadoop in action</title>
				<imprint>
			<publisher>Manning Publications Co</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cycle accounting analysis on Intel Core 2 processors</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mronline: Mapreduce online performance tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international symposium on High-performance parallel and distributed computing</title>
				<meeting>the 23rd international symposium on High-performance parallel and distributed computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale-out processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual International Symposium on Computer Architecture</title>
				<meeting>the 39th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="500" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bdgs: A scalable big data generator suite in big data benchmarking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advancing Big Data Benchmarks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="138" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minebench: A benchmark suite for data mining workloads</title>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ozisikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zambreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Memik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="182" to="188" />
		</imprint>
	</monogr>
	<note>in Workload Characterization</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">More data usually beats better algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<ptr target="http://anand.typepad.com/datawocky/2008/03/more-data-usual.html" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Datawocky Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hadoop&apos;s adolescence: an analysis of hadoop usage in scientific workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="853" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Precise, scalable, and online request tracing for multitier services of black boxes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1159" to="1167" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Parallel and Distributed Systems</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Francis galton&apos;s account of the invention of correlation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Stigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="73" to="79" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The architecture of the nehalem processor and nehalem-ep smp platforms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Thomadakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011-3, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data warehousing and analytics infrastructure at facebook</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 international conference on Management of data</title>
				<meeting>the 2010 international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BigDataBench: A big data benchmark suite from internet services</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">In cloud, can scientific communities benefit from the economies of scale?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="303" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Parallel and Distributed Systems</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hadoop: The Definitive Guide</title>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on Networked Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High volume throughput computing: Identifying and characterizing throughput oriented workloads in data centers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel and Distributed Processing Symposium Workshops &amp; PhD Forum (IPDPSW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1712" to="1721" />
		</imprint>
	</monogr>
	<note>IEEE 26th International</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cost-aware cooperative resource provisioning for heterogeneous workloads in data centers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint/>
	</monogr>
	<note>Computers</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bigop: generating comprehensive big data workloads as a benchmarking framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database Systems for Advanced Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
