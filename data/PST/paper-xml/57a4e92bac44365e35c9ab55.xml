<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey Of Techniques for Architecting DRAM Caches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
						</author>
						<title level="a" type="main">A Survey Of Techniques for Architecting DRAM Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPDS.2015.2461155</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS 4 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS 8 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2015.2461155, IEEE Transactions on Parallel and Distributed Systems</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Review</term>
					<term>classification</term>
					<term>last level cache</term>
					<term>die-stacking</term>
					<term>3D</term>
					<term>stacked DRAM</term>
					<term>bandwidth wall</term>
					<term>extreme-scale system</term>
					<term>architectural techniques</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent trends of increasing core-count and memory/bandwidth-wall have led to major overhauls in chip architecture. In face of increasing cache capacity demands, researchers have now explored DRAM, which was conventionally considered synonymous to main memory, for designing large last level caches. Efficient integration of DRAM caches in mainstream computing systems, however, also presents several challenges and several recent techniques have been proposed to address them. In this paper, we present a survey of techniques for architecting DRAM caches. Also, by classifying these techniques across several dimensions, we underscore their similarities and differences. We believe that this paper will be very helpful to researchers for gaining insights into the potential, tradeoffs and challenges of DRAM caches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the quest of ongoing process technology scaling meets the formidable challenges of power-and bandwidth-wall, architecture of modern processors is already witnessing dramatic shifts. As the cache capacity demands of modern applications escalate due to increasing number of on-chip cores, the conventional wisdom of using SRAM for designing caches has been subjected to critical scrutiny. The low density coupled with high leakage power consumption of SRAM make use of large SRAM caches infeasible and hence, the size of largest SRAM cache in modern processors ranges in merely few tens of megabytes (e.g. 37.5MB SRAM LLC in Ivytown processor <ref type="bibr" target="#b0">[1]</ref>). By comparison, modern scale-out workloads (e.g. business analytics, web search etc.) demand hundreds of megabytes of last level cache <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Large caches are also required for avoiding bandwidth wall and if such caches are designed using SRAM, they may occupy 90% of the chip-area in future process technology generations <ref type="bibr" target="#b3">[4]</ref>. This severely limits the number of cores that can be placed on-chip and thus restricts multicore scaling <ref type="bibr" target="#b4">[5]</ref>. Further, given the high leakage power of SRAM, large SRAM caches may necessitate expensive cooling solutions (e.g. liquid cooling), which increase the cost and complexity of chip significantly. These factors and trends have motivated the researchers to explore the alternatives of SRAM for designing caches.</p><p>? S. Mittal is with the Future Technologies Group, Oak Ridge National Laboratory, Oak Ridge, TN 37830. E-mail: mittals@ornl.gov. ? J. S. Vetter is with the Future Technologies Group, Oak Ridge National Laboratory, Oak Ridge, TN 37830, and with the Georgia Institute of Technology, Atlanta, GA 30332. E-mail: vetter@ornl.gov.</p><p>Although DRAM provides nearly 8? <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> density advantage compared to SRAM, its relatively long latency had conventionally restricted its use to designing main memories only. However, recent advancements in die-stacking have led to significant improvement in these properties <ref type="bibr" target="#b5">[6]</ref>. By virtue of improving integration density and eliminating wires, stacked DRAM can provide between 8? <ref type="bibr" target="#b7">[8]</ref> to 15? <ref type="bibr" target="#b8">[9]</ref> more bandwidth than off-chip DRAM while incurring only half <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or one-third <ref type="bibr" target="#b11">[12]</ref> the latency.</p><p>These features have motivated the development and release of high performance stacked DRAM from several leading vendors <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and as such, stacked DRAM is poised to be integrated in the memory hierarchy of both CPUs and GPUs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b17">19]</ref>. In fact, Intel's 14nm Knights Landing processor is said to use a 16GB stacked DRAM last level cache with 500GB/s bandwidth <ref type="bibr" target="#b8">[9]</ref>. Although nonvolatile memories (NVMs) such as STTRAM (spin transfer torque RAM) and ReRAM (resistive RAM) have also been considered for designing last level caches (LLCs) <ref type="bibr" target="#b18">[20]</ref>, they have yet to reach the level of fabrication maturity and commercial viability that may justify their adoption in mainstream computing systems. Clearly, at least for the foreseeable future, DRAM will be a promising technology for mitigating power/bandwidth barriers by enabling design of large caches.</p><p>Fully leveraging the potential of DRAM caches, however, requires addressing several challenges and making careful choice of design parameters such as block size, tag store architecture, 3D partitioning strategy, thermal management issues etc. In fact, due to the marked difference between SRAM and DRAM, blindly porting even well-known SRAM cache optimizations (e.g. minimizing miss-rate, using high associativity and better replacement policies) to DRAM cache may degrade its performance <ref type="bibr" target="#b7">[8]</ref>. Clearly, novel techniques are required to fully realize the potential of DRAM caches.</p><p>In this paper, we survey techniques proposed for architecting DRAM caches. We classify these techniques based on key parameters to highlight their similarities and differences. We focus on architecture and systemlevel techniques and not on device-level techniques. Since the evaluation approach and workloads used in different works vary, we only focus on their key research insights and typically do not discuss their quantitative results. We hope that by presenting a unified view of frontiers of DRAM cache research, this paper will provide clear directions for future research and development in this area. In this paper, DRAM cache generally refers to stacked DRAM cache, since the higher latency and lower bandwidth of off-chip (commodity) DRAM make them unsuitable for cache design.</p><p>The rest of the paper is organized as follows. Section 2 provides a brief background on DRAM caches and tradeoffs involved in their design. Section 3 first provides an overview and classification of DRAM cache management techniques. It then reviews several techniques for managing DRAM caches. Finally, Section 4 concludes this paper with a discussion of future challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We now discuss the motivation behind use of DRAM cache, along with the challenges associated in their use. We refer the reader to prior work for more details on DRAM architecture <ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref> and die-stacking technology <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Promises of DRAM Caches</head><p>While DRAM has been traditionally considered synonymous to main memory, several recent trends present compelling reasons to consider its use for designing caches.</p><p>Increasing memory pressure and high density of DRAM: With increasing number of cores, the pressure on memory hierarchy is increasing. The poor pincount scalability leads to memory bandwidth wall, which needs to be offset using effective approaches such as large-sized caches. As mentioned before, low density and high leakage energy of SRAM present a major challenge in its use for designing large caches. Although architectural techniques can partially offset the energy challenges of SRAM <ref type="bibr" target="#b24">[26]</ref>, they cannot bridge the magnitude order gap compared to the energy efficiency and capacity requirements placed on extreme-scale memory systems.</p><p>Looking at device-level characteristic, we find that SRAM has 6T (6 transistor) structure with a cell size of 120-200F 2 (F shows the feature size), whereas, DRAM has 1T1C (one transistor and one capacitor) structure with a cell size of 6-10F 2 <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b21">23]</ref>. Clearly, DRAM provides much higher density than SRAM. Hence, DRAM technology can allow integration of giga-byte size caches and thus, presents as a promising memory technology to overcome memory and bandwidth walls. The density advantage provided by DRAM can be leveraged to place more cores on the chip which paves the way for multicore scaling to continue <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. These factors have motivated the researchers to use DRAM even as an L4 cache <ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref> and as a cache for NVM main memory <ref type="bibr" target="#b30">[32]</ref>.</p><p>Benefits of die-stacking: As mentioned before, diestacking provides significant latency and bandwidth benefits compared to conventional off-chip memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b31">33]</ref>. In terms of energy per bit, the TSV (through silicon via)-based interface provides an order of magnitude improvement over low-power DRAM interfaces (LPDDR2) and two orders of magnitude improvement over DDR3 interface <ref type="bibr" target="#b1">[2]</ref>. By sharing the peripheral circuitry over the stack, die-stacked DRAM can also improve cost efficiency. Further, while stacking multiple processor chips may create thermal issues, stacking memory on top of processor cores results in only a small temperature increase (less than 10 ? for 8 additional layers of memory <ref type="bibr" target="#b23">[25]</ref>).</p><p>The bandwidth provided by off-chip main memory is limited due to pin-count limitations. Die-stacking circumvents the pin limitations and thus provides low latency interconnects and very high bandwidth. At the same time, the capacity provided by stacked DRAM is significantly smaller than the off-chip DRAM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">34]</ref>, for example, in Intel's Knights Landing system, the capacity of stacked DRAM and off-chip DRAM are 16GB and 384GB, respectively <ref type="bibr" target="#b8">[9]</ref>. Also, the bandwidth to stacked DRAM is restricted by the parallelism in the stacked DRAM itself and not by the interface. These facts make the stacked DRAM a better fit as a cache rather than as main memory.</p><p>Maintaining software transparency: In comparison to using DRAM as a main memory, the benefit of using DRAM as a cache is that the software (both application and OS) need not be modified to reap the benefits of this cache. Thus, legacy software can be benefited and dependence on OS vendors is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factors and tradeoffs in designing DRAM caches</head><p>Despite their promises, DRAM caches also present several challenges that need to be addressed for ensuring their efficient integration in product systems.</p><p>Performance challenges: For applications with poor locality, addition of DRAM cache may not reduce the miss-rate sufficiently. In fact, since their latency is only 50% lower than that of main memory, DRAM caches may significantly increase the latency of misses. There is also a tradeoff between capacity (or miss-rate) and latency and hence, some techniques aim to optimize latency <ref type="bibr" target="#b7">[8]</ref>, while others optimize miss-rate <ref type="bibr" target="#b27">[29]</ref> at the expense of other parameter. DRAM also requires refresh which consume significant amount of energy and reduces device availability <ref type="bibr" target="#b22">[24]</ref>. Due to these tradeoffs, increasing the size of DRAM beyond a limit may not be feasible.</p><p>Architecture and metadata management: Due to their large capacity, DRAM caches also require a large amount of metadata, such as tags, dirty, valid bits etc. (we henceforth use the words 'tag' and 'metadata' synonymously) and tag lookups lie on the critical path of all requests. This makes the choice of cache line granularity and storage location of tags (e.g. in SRAM or DRAM) key design issues for DRAM caches, which are also interrelated. At two extremes are fine-grained (also called block-based, having 64B granularity) and coarse-grained (also called page-based with granularity of few KBs) designs which lead to vastly different tradeoffs. Assuming 6B tag overhead per cache line, a 1GB DRAM cache will require 96MB and 3MB of tag storage for fine-grained (64B line) and coarse-grained (2KB line) designs, respectively.</p><p>It is clear that the fine-grained organizations incur prohibitively high tag storage overhead, which may be much greater than the size of last level SRAM cache itself. This forces the designers to store tags in DRAM itself, which leads to increased hit latency. Serial tag-data access and use of set-associative caches further increase this latency and hence, parallel tagdata access and use of direct-mapped caches have been proposed (see Table <ref type="table" target="#tab_0">1</ref>), although these design choices have their own limitations <ref type="bibr" target="#b24">[26]</ref>. Access to DRAM for updating replacement information reduces DRAM availability and hence, some works use random replacement policy instead of LRU (least recently used) policy since the former does not require update <ref type="bibr" target="#b7">[8]</ref>. Several techniques use intelligent predictors to bypass access to DRAM cache for requests which are predicted to miss and their efficacy depends on the accuracy of prediction and proper management of dirty data <ref type="bibr" target="#b33">[35]</ref>. Fine-grained organizations are also less effective in exploiting spatial locality and hence, incur higher miss rates, which incurs the penalty of off-chip access. The advantage of fine-grained organization is that they use off-chip bandwidth and cache capacity efficiently.</p><p>Coarse-grained organizations incur much smaller tag overhead which allows the tag to be stored in SRAM, thus leading to fast lookups. They rely on exploiting spatial locality and hence, they may see higher hit ratio compared to fine-grained caches, although their miss-rate reduction may not be sufficient to reduce the bandwidth wastage. The limitation of coarse-grained approach is that it leads to bandwidth wastage and queue contention leading to backpressure delays in the cache hierarchy (writing back only dirty subblocks can partially reduce the memory traffic <ref type="bibr" target="#b20">[22]</ref>). Large cache lines lead to under-utilization of cache capacity and also lead to false-sharing in multi-threaded applications <ref type="bibr" target="#b27">[29]</ref>. Also, the cacheline granularity of coarse-grained designs is limited by DRAM page size, since use of larger granularity will necessitate mapping the data across more than one physical rows which leads to timing and energy inefficiencies. Further, with rising DRAM cache capacity, even coarse-grained organizations incur increasing amount of tag overhead which makes storing their tag in SRAM infeasible.</p><p>Fabrication and reliability challenges: Due to unique design of DRAM, the cache controller in DRAM caches need to also perform device-level tasks such as managing refresh, issuing row activation and precharge commands, fulfilling DRAM device timing constraints, etc. These may lead to significant complexity and may require redesign of the cache control logic <ref type="bibr" target="#b2">[3]</ref>. Further, compared to conventional 2D off-chip memories, die-stacked memories demand much higher reliability, since a fault in die-stacked memory may not be easily serviceable and hence, may require discarding the entire package including the functioning processor layer <ref type="bibr" target="#b34">[36]</ref>. Also, designing memory stacks with very large number of layers presents challenges of power delivery and cooling, yield, testability etc. <ref type="bibr" target="#b23">[25]</ref>.</p><p>Challenges in hybrid caches: Some researchers have proposed SRAM/DRAM hybrid caches (refer Table <ref type="table" target="#tab_0">1</ref>). Since the latency of SRAM is less than 10ns <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b35">37]</ref>, while that of DRAM is 60-80ns <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b35">37]</ref>, the large difference between the latency of SRAM and DRAM may make the hit latency different for different accesses which makes the scheduling of dependent instructions difficult. For this reason, the data migration schemes as used in SRAM/STTRAM or SRAM/ReRAM hybrid caches <ref type="bibr" target="#b36">[38]</ref> may not be feasible in SRAM/DRAM hybrid caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DRAM CACHE MANAGEMENT TECH-NIQUES</head><p>Table <ref type="table" target="#tab_0">1</ref> provides an overview and classification of DRAM cache management techniques discussed in this survey. This table classifies the research techniques based on their optimization objective, their key feature and the cache architecture used by them. This classification is expected to be useful for CAD-designers, researchers and technical marketingprofessionals.</p><p>We now briefly discuss several DRAM cache management techniques. For sake of convenience, we roughly organize them in several categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparative evaluation of die-stacked DRAM</head><p>In this subsection, we discuss research works that explore the architecture of DRAM caches and/or compare them with SRAM caches.  <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45]</ref> Area saving for integrating more cores <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b60">62]</ref> Refresh overhead management <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b59">61]</ref> Dynamically optimizing bandwidth usage <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b56">58]</ref> Thermal management <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b58">60</ref>  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b53">55]</ref> Adaptive granularity <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b30">32</ref>] Key feature of technique Steering accesses to DRAM cache or main memory <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b33">35]</ref> Stacked DRAM as cache or OS-managed main memory <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b46">48]</ref> Cache reconfiguration <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b52">54</ref>] Cache mapping scheme <ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref> Use of additional structures/approaches Hit/miss or location predictor or tag cache <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58]</ref> Predictor for selecting useful/hot blocks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b51">53]</ref> Prefetching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b50">52</ref>] Dynamic voltage/frequency scaling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b58">60]</ref> Black et al. <ref type="bibr" target="#b5">[6]</ref> evaluate different SRAM and DRAM 2D/3D cache designs from performance and thermal perspectives. Their baseline is a dual-core processor with 4MB SRAM cache. For the second design, they stack an 8MB SRAM cache directly above the processor die, assuming that the area of 8MB stacked L2 is nearly the same as that of the baseline die. Assuming the DRAM density to be 8? that of SRAM density, in the third design, 4MB SRAM L2 is replaced with a 32MB stacked DRAM L2 and the area saved on processor die is used for storing tags for DRAM cache. In the fourth design, 64MB DRAM cache is stacked on top and its tags are stored in 4MB L2 on processor die. They show that increased cache capacity enabled by die-stacked DRAM cache helps in improving the performance and energy efficiency and reducing the off-chip bandwidth. They also note that compared to baseline, none of the stacked cache designs impact the thermal profile significantly, although stacking SRAM results in higher thermal increase due to the higher power density of SRAM compared to DRAM.</p><p>Sun et al. <ref type="bibr" target="#b39">[41]</ref> present a design for multicore systems where 3D DRAM is used to implement a private L2 cache for each core and main memory shared by all the cores. To architect 3D DRAM without requiring stringent constraints on TSV fabrication, they propose a coarse-grained 3D partitioning strategy. Instead of splitting individual memory sub-array across several layers, they propose retaining individual memory sub-array is a single layer. They show that as the number of DRAM dies and the L2 cache capacity increase, the 3D DRAM L2 cache may have comparable or even smaller latency than the 2D SRAM L2 cache. To reduce the latency of DRAM further, they propose reducing the size of each individual DRAM subarray and using low threshold-voltage transistors in peripheral circuits and H-tree buffers. They show that compared to the baseline using 3D DRAM as main memory, their approach of using 3D DRAM for both cache and main memory improves the performance significantly.</p><p>Xu et al. <ref type="bibr" target="#b6">[7]</ref> study different NoC (network-on-chip) designs with SRAM and DRAM LLCs to evaluate the effect of DRAM cache in terms of access latency, cache size, area and power consumption. Compared to 2D SRAM LLC, both 2D DRAM LLC and 3D DRAM LLC reduce miss-rate which also reduces the total amount of router/link activities. However, compared to 2D DRAM LLC, 3D DRAM LLC shows lower latency due to shorter wire length but higher power consumption due to increased number of routers and links and the increased complexity of routers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Techniques for managing metadata</head><p>Zhao et al. <ref type="bibr" target="#b37">[39]</ref> evaluate latency/area/bandwidth tradeoffs of multiple DRAM cache designs, viz. 1) using tags-in-DRAM, 2) using tags-in-SRAM, 3) sectored cache <ref type="bibr" target="#b61">[63]</ref> with tags in SRAM 4) using 'partial tags' in SRAM, 5) a combination of sectored cache and partial tags. For tags-in-SRAM approach, the tags are stored in part of last-level SRAM cache, which reduces its capacity for data storage. A sectored DRAM cache uses large granularity block, which is composed of multiple sub-blocks and the block address. Each subblock consists of small cache block and its state. A sectored DRAM cache reduces tag overhead by using large granularity block and also reduces cache traffic by only bringing the sub-block and not the entire sector on each miss event. To offset its high missrate, prefetching is also used. The partial-tag scheme maintains few least significant bits of tags (i.e. partial tag) in SRAM and full tag directory in DRAM. By consulting partial tags, prediction of hit/miss can be made to avoid DRAM access on a predicted miss. They show that combining sectored cache with partial tags provides the highest performance improvement while also reducing the tag overhead.</p><p>Jiang et al. <ref type="bibr" target="#b11">[12]</ref> present filter-based techniques to avoid bandwidth-wastage in page-based DRAM caches. Their first technique uses a filter cache that profiles pages and selects the hot (i.e. most frequently accessed) pages which are allocated in DRAM cache. By not allocating pages with low spatial locality in DRAM cache, their technique avoids bandwidth wastage. The limitation of this technique is that when a victim page is evicted from filter cache, its access history is lost and on a later access, this page is considered as new and hence, many pages get evicted before reaching the threshold for being considered as hot. To avoid this, the second technique stores the counter of the page evicted from filter cache into memory and restores it later when the page is accessed. Thus, the second technique improves the accuracy of hotpage identification which allows for reducing the size of filter cache. Their third technique adapts to the intra-and inter-application variation by dynamically activating or deactivating the filter cache based on whether an application or its phase does or does not (respectively) saturate the memory bandwidth. When the filter cache is deactivated, all pages are allocated in the DRAM cache.</p><p>Loh et al. <ref type="bibr" target="#b27">[29]</ref> propose a technique to enable use of conventional block size (64B) in DRAM cache. They store tags in DRAM and schedule tag and data accesses as a compound access which ensures that data access always results in a row buffer hit. For this, a 2KB DRAM row which can store up to thirtytwo 64B data blocks is reorganized such that it stores twenty-nine 64B data blocks along with 6B tag for each of those data blocks (18B are left unused). Thus, at the cost of DRAM capacity, their technique provides scalable tag management approach by storing the tag in DRAM. On a DRAM cache miss, the overhead of DRAM access is incurred and to avoid this, they maintain a structure called MissMap which tracks the contents of DRAM cache. When MissMap determines a cache miss, access to the cache is skipped. The limitation of their approach is the large size of MissMap due to which storing it in last level SRAM cache is unattractive. Note that a few other works also use MissMap for DRAM cache management <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref>.</p><p>Sim et al. <ref type="bibr" target="#b33">[35]</ref> note that the key idea used in MissMap design of maintaining precise information about DRAM cache contents is overly conservative. To avoid the overhead of MissMap, they use lowcost hit/miss predictor which helps in speculating whether a request can be served by the DRAM cache or main memory. They also use a load-balancing scheme which redirects memory requests to either the die-stacked DRAM cache or off-chip main memory based on the instantaneous queuing delays at the two memories. This helps in improving overall system bandwidth by utilizing the otherwise idle off-chip bandwidth when the DRAM cache is servicing a burst of cache hits. The limitation of these techniques is that in presence of dirty data, they may lead to incorrect execution since they may access the stale value in main memory. To address this, they use a hybrid write policy that operates the majority of DRAM cache in write-through mode and enables write-back only for a few hot pages. This ensures that the DRAM cache is mostly clean and the requests to only the dirty pages need to go to DRAM cache. This obviates the need of waiting on predictor verification and allows more opportunities to redirect requests to off-chip memory. Their technique is useful when latency and storage overheads of MissMap are unacceptable and workloads are write-unintensive but benefit from larger system bandwidth achieved by their load-balancing approach.</p><p>To avoid the overhead of MissMap, El-Nacouzi et al. <ref type="bibr" target="#b29">[31]</ref> propose a predictor for estimating hit/miss in a DRAM cache. They note that due to spatial locality, first access to a page will be followed by access to other blocks in the same page. Also, at any time, only few pages will experience misses and these misses are likely to happen soon after the first miss. Hence, tracking a small number of pages and their blocks is likely to provide reasonable prediction accuracy. Based on this, their technique uses two filters viz. coarse and fine-grain. The coarse-grain filter tracks a superset of pages that have cached blocks in DRAM cache and the fine-grain filter tracks all blocks in selected pages. Their technique first checks coarse-grain filter and if the page is predicted to be cached, fine-grain filter is checked to see if the block is predicted to be cached. If any of the filters predicts a miss, access to DRAM cache is bypassed.</p><p>Qureshi et al. <ref type="bibr" target="#b7">[8]</ref> note that since DRAM caches are slower than conventional caches, optimizations that degrade the already high hit latency (e.g. use of MissMap <ref type="bibr" target="#b27">[29]</ref> and tag-data serialization) may reduce the performance even if they provide a small improvement in hit rate. Hence, DRAM cache should be optimized first for latency and then for hit rate. They propose using a direct-mapped DRAM cache instead of set-associative cache, which also improves the row-buffer hit to further reduce the access latency. Furthermore, to avoid tag serialization latency, both data and tag are retrieved together on each access. They also use a memory access predictor to estimate whether the data is present in DRAM cache. If so, the DRAM cache is accessed before accessing main memory to save bandwidth. If the data are unlikely to be present in DRAM cache, both the DRAM cache and main memory are accessed in parallel to reduce the latency by avoiding cache miss penalty.</p><p>To bring together the best of both block-based and page-based cache designs, Jevdjic et al. <ref type="bibr" target="#b51">[53]</ref> present a DRAM cache design which allocates data at the granularity of pages, but fetches only those blocks within a page that will be touched during the page's residency in the cache. Their technique also identifies pages that do not show spatial or temporal locality and does not allocate such pages in the cache. For this, their technique uses a predictor which identifies the footprint of the pages based on the observation that there is a high correlation between data and the code that accesses those data. Based on this, the first instruction that accesses a page provides hint about the data that the page contains and by monitoring and remembering which blocks the code further accesses, a prediction can be made about which blocks will be demanded when another page is accessed by the same piece of code. They show that their technique achieves high hit ratio, small lookup latency and low tag array overhead while also eliminating the off-chip traffic overhead of page-based designs.</p><p>Jevdjic et al. <ref type="bibr" target="#b49">[51]</ref> aim to bring the best of two previous approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">53]</ref> together. They use page-based cache allocation to achieve high hit rate and low tag overhead, while estimating and fetching only the useful blocks within each page to minimize bandwidth wastage. They use set-associative cache along with a way-predictor which avoids fetching all the ways in parallel. This avoids the need of direct-mapped cache which induces conflict misses. Also, they maintain single tag per page to simplify footprint tracking and the tag read is overlapped with data read to avoid tag serialization latency.</p><p>Hameed et al. <ref type="bibr" target="#b44">[46]</ref> propose a DRAM cache architecture where each DRAM row comprises four cache sets, consisting of 1 tag block and 7 data blocks (i.e. 7-way set associativity). The DRAM cache set number within a row is determined by the two least significant bits of the memory block address. By virtue of looking up smaller number of tags than the miss-rate optimized design and having higher row-buffer hit rate, their technique achieves lower DRAM hit latency. Also, compared to the direct-mapped DRAM cache, it provides lower miss-rate by using a 7-way associative cache. Since non-uniform distribution of accesses to different cache sets may increase conflict misses, they propose a set-mapping policy which assigns the DRAM row number to each core in a round robin manner. This leads to uniform distribution of accesses to different sets of DRAM cache which reduces the miss-rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using tag cache</head><p>Huang et al. <ref type="bibr" target="#b50">[52]</ref> note that although the tags-in-SRAM approach incurs higher area overhead than the tagsin-DRAM approach, its performance is superior. To bring the best of both together, their technique maintains the tags in DRAM, but also caches a small fraction of tags in a dedicated SRAM cache, called aggressive tag-cache (ATCache). ATCache leverages temporal locality by caching the tags of recently accessed DRAM sets only and spatial locality by prefetching the tags of adjacent DRAM cache sets only when such locality amongst the sets has been confirmed. They show that due to the small access latency of ATCache, their technique achieves much better performance than tags-in-DRAM approach. ATCache incurs only a fraction of area overhead of all the tags-in-SRAM approach but has disadvantage of duplicating (due to caching) some tags in SRAM which are also in DRAM cache and this overhead grows with increasing size of DRAM cache. Hence, their technique and similar techniques (e.g. <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b30">32]</ref>) which cache the tags on-chip, are useful when resultant waste of on-chip space and higher complexity of tag management are acceptable for achieving performance comparable to that of having all the tags-in-SRAM.</p><p>Hameed et al. <ref type="bibr" target="#b25">[27]</ref> present a technique to address the high tag latency of large DRAM caches. For a system with large L3 SRAM cache and L4 DRAM cache, they propose small SRAM structures named SRAM tag-cache and DRAM tag-cache, which hold the tags of the sets that were recently accessed in L3 and L4, respectively. These tag-caches quickly identify hit/miss for large caches and thus reduce the tag lookup latency. For applications with limited spatial locality, the hit-rate in tag-cache remains small. To avoid this, their technique identifies the number of useless insertions into tag-caches based on the behavior in recent past and restricts such insertions for increasing their hit rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Use of adaptive data granularity</head><p>Meza et al. <ref type="bibr" target="#b30">[32]</ref> store the metadata in DRAM in the same row as their data to avoid row buffer miss on DRAM cache hits. To further reduce the metadata lookup latency, they cache the metadata for the most recently accessed DRAM rows in a small onchip buffer (cache). Due to data access locality, the metadata for hot data are likely to be cached on-chip and hence, they can be accessed with the same latency as the SRAM tag store. Compared to a "full" SRAM tag store, the storage overhead of their "hot" SRAM tag buffer is three orders of magnitude smaller. To achieve a balance between locality and bandwidth consumption, they also propose dynamically adjusting the migration granularity. By testing with different granularities, e.g. 128B, 256B, 512B, 1KB, 2KB, 4KB and no migration, their technique decides the right migration granularity for a thread. This improves the DRAM cache utilization and reduces bandwidth contention at the cost of additional complexity of adaptive migration granularity.</p><p>Gulur et al. <ref type="bibr" target="#b19">[21]</ref> present a technique which organizes the data in bi-modal fashion, such that the blocks with high spatial locality are organized as large blocks and those with limited spatial locality as small blocks. By dynamically selecting the right granularity of storage for different blocks at runtime, their technique uses the DRAM cache capacity effectively and also reduces the off-chip memory bandwidth consumption. Each set can hold X big (512B) and Y small (64B) blocks. Denoting the state of the set as (X+Y), a 2KB set can have states (4, 0), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b15">16)</ref>. Thus, the set-associativity (=X+Y) can vary dynamically on perset basis, but the number of sets is fixed. By tracking the utilization of 64B sub-blocks in sampled sets, their technique decides whether the sampled way should be classified as a big or a small block. Unlike the scheme of Loh et al. <ref type="bibr" target="#b27">[29]</ref> where the metadata are interleaved with data on the same DRAM rows, they store the metadata for data banks belonging to one channel onto a bank of another channel which enables packing more metadata entries per page increasing the rowbuffer hit rate. At the same time, it allows concurrent access of metadata and data due to high internal bandwidth of stacked DRAM. To further reduce the cache hit latency, their technique uses a small SRAM based way locator which caches the way IDs of the most recent accesses to DRAM cache sets. On a hit to way locator, access to DRAM for metadata is avoided. They show that their technique improves average access latency over both tags-in-DRAM and tags-in-SRAM approaches. The limitation of their technique comes from the additional complexity of organizing the data in bi-modal manner.</p><p>A comparison of bandwidth-saving approaches: It is noteworthy that Jiang et al. <ref type="bibr" target="#b11">[12]</ref> optimize bandwidth by using adaptive page allocation in DRAM, which is different from Meza et al. <ref type="bibr" target="#b30">[32]</ref> and Gulur et al. <ref type="bibr" target="#b19">[21]</ref> who use adaptive fetch granularity to optimize bandwidth. Sim et al. <ref type="bibr" target="#b33">[35]</ref> and Gulur et al. <ref type="bibr" target="#b20">[22]</ref> (refer Section 3.13) optimize bandwidth by utilizing otherwise idle off-chip bandwidth also, in addition to bandwidth of die-stacked DRAM cache. Qureshi et al. <ref type="bibr" target="#b7">[8]</ref> optimize bandwidth by reading tag and data together (i.e. avoiding tag-data serialization) and using a direct-mapped cache that avoids transferring multiple tags as in a set-associative cache. Jevdjic et al. <ref type="bibr" target="#b51">[53]</ref> use page-based cache design but still save bandwidth by fetching selected blocks from the page that will be accessed while the page resides in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Exploring two-level memory</head><p>Chou et al. <ref type="bibr" target="#b9">[10]</ref> propose an approach to use the stacked DRAM both as part of main memory to increase its capacity and as cache to capture datalocality and perform data management at finegranularity. This approach is especially useful for large-sized stacked memories, such as 4GB stacked memory and 12GB main memory. Their technique stores recently accessed data lines in stacked DRAM and swaps the victim lines to main memory. Since this dynamically changes the physical location of a line, they also use a line location table (LLT) which tracks the physical location of all data lines. The LLT acts similar to a tag directory for conventional caches, except that LLT also identifies the location of line in main memory, in case it is not found in stacked DRAM. LLT is co-located with data in stacked DRAM to avoid SRAM storage overhead. For the lines not stored in stacked DRAM, LLT lookup and memory access become serialized and to avoid this, they use a line location predictor. If this predictor estimates the line to be in main memory, it is accessed from main memory in parallel with the LLT access. They show that their technique provides larger improvement than that offered by using stacked DRAM either as a cache or as a part of main memory.</p><p>Meswani et al. <ref type="bibr" target="#b46">[48]</ref> note that given the limited capability provided by die-stacked DRAM, the offchip memory is still required to satisfy the memory requirement of large-scale applications. This leads to a two-level memory (TLM) design which demands codesign of memory architecture and software applications. To manage such TLM, they study hardwarebased, OS-based and programmer-driven approaches and compare their relative merits. The hardware approach uses stacked DRAM as last level cache. The OS-based approach uses this as main memory and the off-chip memory is used as the conventional swapdevice. In programmer-driven approach, the information about existence of multiple memories is exposed to the programmer who can decide about allocating or pinning different data objects on different memories. Their study using exascale proxy applications shows that the hardware cache achieves high hit rate and consumes small bandwidth. The programmermanaged TLM also consumes small bandwidth but has a low hit rate while the OS-based approach wastes bandwidth due to use of page-granularity allocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Addressing issues related to multicore processors</head><p>Hameed et al. <ref type="bibr" target="#b43">[45]</ref> note that application-unaware bank-mapping policies in large DRAM caches can lead to unfair slowdown of some applications. Towards this, their technique tracks the run-time miss rate information of concurrently executing applications to detect thrashing applications. Also, the cache banks are organized into two regions namely "private region" and "shared region". Since thrashing applications may evict useful cache lines of other applications, their technique aims to confine the memory segments from thrashing applications into a single cache bank. This reduces the inter-core cache contention and provides performance isolation between thrashing and non-thrashing applications. Further, to improve the utilization of cache, the shared region is allocated to non-thrashing applications which provides cooperative cache sharing.</p><p>Loh <ref type="bibr" target="#b40">[42]</ref> propose a cache organization scheme for DRAM cache to improve its performance. Each set is organized as multi-level logical queues, where a single cache line constitutes the queue entry. All cache lines are initially inserted into the first-level queue. After Q insertions to a queue of size Q (&lt; W , where W is the set-associativity), the original line leaves this queue. If it has not received any hit during its residency in this queue, it is evicted, otherwise, it is inserted into next (second) level queue (and so on). The last queue uses LRU replacement policy. Using this organization, a line which sees no reuse is evicted after Q insertions only, while in conventional LRU replacement policy, it will be evicted after W insertions. This improves the efficiency of cache by ensuring quick eviction of dead lines. The lines seeing burst-accesses satisfy those requests while residing in first-level queue and after the burst has ended, these lines are evicted from second-level queue. Thus, the key idea behind using multi-level queues is that they act as filters for cache access patterns with limited reuse. In multicore processors, each core has its own dedicated first-level queue and the second-level queue can be shared. By virtue of isolating traffic from different cores into different first-level queues, their organization also provides performance isolation to those cores, since the cores with streaming behavior cannot replace lines of other cores. The increased complexity in replacement logic and design of queues are limitations of this technique.</p><p>The difference between the technique of Hameed et al. <ref type="bibr" target="#b43">[45]</ref> and Loh <ref type="bibr" target="#b40">[42]</ref> is that for providing intercore isolation, the former uses a bank mapping policy that explicitly limits the cache quota of thrashing applications, while the latter uses a cache replacement policy that indirectly accelerates eviction of lines from thrashing applications or those with little reuse. Thus, the former technique uses 'strict allocation', while the latter uses 'soft allocation'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">DRAM caches for enabling multicore scaling</head><p>Rogers et al. <ref type="bibr" target="#b3">[4]</ref> study how multicore scaling may be restricted due to bandwidth-wall problem and to what extent can the bandwidth conservation techniques mitigate this challenge. They evaluate several techniques such as use of smaller sized cores to enable larger sized caches, 3D stacked caches, cache compression and link compression and DRAM cache. Under proportional scaling, the number of cores on the chip should reach 16 (= 2 4 ) in four technology generations, however, bandwidth wall limits it to only 11 cores (assuming SRAM L2 caches). They show that on assuming the DRAM density to be 4X and 8X compared to that SRAM, the number of supportable cores reaches 16 and 18, respectively. Although their model makes some simplifying assumptions (e.g. ignoring refresh overhead etc.), their results confirm that use of DRAM caches is an effective approach to filter extra memory traffic generated by large number of cores for enabling multicore scaling. They also show that by combining use of DRAM cache with other bandwidth conservation approaches (mentioned above), the bandwidth-wall issue can be delayed by several technology generations, thus allowing the multicore scaling to continue.</p><p>Hardavellas et al. <ref type="bibr" target="#b4">[5]</ref> note that due to power-wall and off-chip bandwidth-wall, an increase in corecount does not translate into corresponding performance improvements. They show that power-wall can be partially overcome by using customized energyefficient heterogeneous multicores and for overcoming the bandwidth-wall, large 3D-stacked DRAM caches can be used. The headroom provided by mitigation of bandwidth-wall allows for adding more cores on the chip for performance scaling, although in such case, the increased energy consumption of network subsystem becomes the new bottleneck. In other words, the benefits of using DRAM cache are bounded.</p><p>Pan et al. <ref type="bibr" target="#b60">[62]</ref> study the use of die-stacked DRAM cache in VLIW (very large instruction word) processors. They note that die-stacking enables the latency of a 3D DRAM cache to approach that of a 2D SRAM cache. Using this, they propose replacing the 2D SRAM cache by 3D DRAM cache and allocating the saved area to additional clusters for improving parallelism and performance. They show that a four cluster system with 3D DRAM L2 cache and 3D DRAM main memory occupies similar logic die area as a two cluster system with 2D SRAM L2 cache and 3D DRAM main memory and the former also provides better performance.</p><p>Thus, while Rogers et al. <ref type="bibr" target="#b3">[4]</ref>, Hardavellas et al. <ref type="bibr" target="#b4">[5]</ref> and Pan et al. <ref type="bibr" target="#b60">[62]</ref> use different models or evaluation platforms, they all provide the same conclusion that DRAM caches can be effective in allowing multicore scaling to continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Refresh management techniques</head><p>Jaksic et al. <ref type="bibr" target="#b57">[59]</ref> present a DRAM-based L1 and L2 coherent cache design. To reduce the refresh overhead, the refresh operation for a block are decided based on its coherence state. For example, for MESI (modified, exclusive, shared, invalid) coherence protocol, they show that an invalid line need not be refreshed, a line in shared or exclusive state transitions to invalid if not refreshed but need not be written back and a line in modified state needs to be invalidated and written back if it is not refreshed. Based on the combination of the line states that can be refreshed or not, they propose multiple refresh policies ranging from one where no line is refreshed to one where modified, shared and exclusive lines are refreshed. By choosing a suitable refresh policy, a tradeoff between performance loss and energy saving can be achieved. They also show that compared to a design with SRAMbased caches, the design with DRAM-based cache provides significant energy saving with only small performance loss.</p><p>Ghosh et al. <ref type="bibr" target="#b59">[61]</ref> present a technique to reduce the refresh requirement of a 3D DRAM cache. They note that from the perspective of data retention, an access to a memory row performs an operation equivalent to a regular refresh and hence, the conventional refresh mechanisms which periodically refresh all memory rows for retaining data lead to wastage of energy. Based on this, their technique maintains counters for each row in the memory module and avoids refreshing a memory row if it has recently seen a read/write access.</p><p>The difference between the technique of Jaksic et al. <ref type="bibr" target="#b57">[59]</ref> and Ghosh et al. <ref type="bibr" target="#b59">[61]</ref> is that former uses properties of cache coherence to skip refresh operations, while the latter uses equivalence of access operation to refresh operation from refresh perspective, to skip refresh operation to a row that has just seen an access</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Thermal management techniques</head><p>Milojevic et al. <ref type="bibr" target="#b1">[2]</ref> study a server-on-chip architecture targeted for the datacenter market. The bottom layer of the chip has many-core compute engine and the top layer has wide I/O DRAMs (similar to <ref type="bibr" target="#b12">[13]</ref>) which are used as LLC shared by all cores. To optimize the total chip throughput, the bottom layer allocates minimal amount of area to second-level cache, which is just enough to capture the instruction and data working sets of the data-intensive commercial workloads. A larger fraction of area is dedicated to many cores. Their evaluation using CPU-centric workloads (e.g. SPECInt and Dhrystone) shows that temperature in the server-on-chip (logic+DRAM) lies in the range of 175-200 ? C which exceeds the reliable operating bounds. By comparison, cloud-workloads consume less power in the processing cores due to their memory-bound nature and hence, an evaluation using cloud workloads shows reduced power density at hotspots and temperatures that are within operating bounds of stacked DRAM. Thus, their study shows that a server-on-chip system is feasible even with a low-cost cooling option which provides opportunity for cost and energy savings in datacenters.</p><p>Yun et al. <ref type="bibr" target="#b58">[60]</ref> note that the high integration density of 3D chips may lead to high operating temperature, which increases leakage power consumption and error-rate. Towards this, they present a DVFS scheme for 3D-stacked DRAM cache where voltage/frequency of each cache bank or each group of cache banks can be adjusted, based on the error-rate (due to retention and sensing failure), cache access rate and temperature-induced power consumption. Their technique monitors the cache access rate and temperature of cache zones in each time-interval at runtime. For applications with low frequency of accesses, low voltage and frequency are used to reduce power consumption with small performance loss and vice versa. Since the error-rate increases with decreasing supply voltage, the minimum value of voltage is also determined by the error-rate constraint.</p><p>Thus, Milojevic et al. <ref type="bibr" target="#b1">[2]</ref> study and compare temperature profile for CPU-centric and cloud workloads, while Yun et al. <ref type="bibr" target="#b58">[60]</ref> present a DVFS-based technique for thermal management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Ensuring resilience to soft-errors and process-variation</head><p>Given the high reliability requirement of die-stacked DRAMs, Sim et al. <ref type="bibr" target="#b34">[36]</ref> propose a technique which provides protection at both fine-grain (e.g. singlebit faults) and coarse-grain (e.g. for row, bank and channel-faults) levels. They note that in a die-stacked DRAM LLC, adding an extra chip in the stack for storing ECC information may not be practical since the overhead of ECC becomes very large and hence, they utilize only non-ECC stacked DRAM chips. Instead of using SECDED (single error correction, double error detection) ECC, they use SEC and CRC (cyclic redundancy check). CRCs can detect multi-bit errors, regardless of whether these errors are clustered and thus, they greatly improve the error-detection capability and reduce the silent data corruption (SDC) rates <ref type="bibr" target="#b62">[64]</ref>. They also note that on an error in clean data in cache, merely detecting the error is sufficient since the correct copy can be retrieved form the main memory. By comparison, for the dirty data, both error correction and detection are required, since the modified copy in the cache is the only valid copy <ref type="bibr" target="#b62">[64]</ref>. Based on this, their technique relies on main memory to correct errors in clean blocks in DRAM cache and uses duplication of dirty blocks in other banks to provide error-correction for them. To avoid the capacity overhead of duplication, they propose duplicating only critical applications or memory regions and writing dirty blocks from the DRAM cache to main memory. Zhao et al. <ref type="bibr" target="#b48">[50]</ref> note that process variation can lead to non-uniform access times in different subbanks in a DRAM LLC. They propose techniques which work by migrating data from slow subbanks to fast subbanks to reduce the average access time. One technique always migrates data to fastest subbank. Since this may cause contention in fastest subbank, a second technique divides the subbanks in a rank into several tiers where each tier has several subbanks of similar speed. The data are migrated from a slow tier to the fastest tier where the data are uniformly distributed to its subbanks. Since DRAM cache provides large capacity, the working set of several applications fits in the fast subbanks which makes their technique effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11">Cache reconfiguration techniques</head><p>Chang et al. <ref type="bibr" target="#b52">[54]</ref> note that die-stacked DRAM caches provide large capacity, however, several applications and application phases may not benefit from this capacity due to their small working set size. Hence, the unused cache portions consume power due to refresh overhead and leakage in the peripheral circuitry. They present a cache reconfiguration technique for such caches. Conventional reconfiguration approaches (e.g. <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b35">37]</ref>) remap the data stored in sets of both powered-on and powered-down banks, which leads to large overhead. Their technique works using consistent hashing algorithm <ref type="bibr" target="#b52">[54]</ref> whereby a machine failure (or analogously, powering-down a DRAM bank) results in remapping only the data mapped to that machine (or DRAM bank). Further, the remapping is done in a load-balanced manner which does not turn the powered-on banks into hotspots leading to increased cache access latencies, and the data already mapped to the powered-on banks remain where they were. While previous techniques e.g. <ref type="bibr" target="#b35">[37]</ref> allow reconfiguring the cache to power-of-two setcounts only, the technique of Chang et al. <ref type="bibr" target="#b52">[54]</ref> does not impose such restriction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.12">SRAM/DRAM Hybrid Caches</head><p>Madan et al. <ref type="bibr" target="#b47">[49]</ref> present a 3D hybrid SRAM/DRAM cache design that aims to bring together the lowlatency advantage of SRAM and high density advantage of DRAM. They assume a stacked processor with three layers, where bottom layer contains 16 cores, the middle layer contains 16 SRAM banks and the top layer contains 16 DRAM banks. Both SRAM and DRAM together constitute the L2 cache. Based on offline analysis, they first identify pages private to each core and those shared by multiple cores. Afterwards, using OS page coloring, they place private pages in the SRAM bank directly above the core and shared pages in one of the central SRAM banks. Since the applications running on different cores present different L2 cache demand, spilling the pages of an application in adjacent banks can help in adjusting the cache quota of each core, however, this also increases the access latency and pressure on the inter-bank network. To avoid this, their technique provisions spilling the additional pages into the third dimension, namely the DRAM bank directly above the SRAM cache bank. Since the density of DRAM is 8? compared to that of SRAM, activation of a DRAM bank increases the cache capacity from 1MB to 9MB. Further, by attempting to serve most of the requests from SRAM bank, their technique keeps the average cache access latency small. As for cache reconfiguration approach, Madan et al. use selectiveway approach while Chang et al. <ref type="bibr" target="#b52">[54]</ref> use selective-set approach <ref type="bibr" target="#b24">[26]</ref>.</p><p>Inoue et al. <ref type="bibr" target="#b41">[43]</ref> present an SRAM/DRAM hybrid cache architecture. They assume a processor designed with small SRAM L2 cache in same layer as the core layer (note the difference with the design of Madan et al. <ref type="bibr" target="#b47">[49]</ref> where SRAM cache is in a separate layer) and a large die-stacked DRAM L2 cache. Using profiling, the cache requirement of an application is estimated and based on it, the cache is configured to work in one of the following two modes before the application execution. When the cache requirement is small, the DRAM cache is power-gated and only the SRAM cache is used, thus, access latency is kept small. For applications with large cache requirement, both SRAM and DRAM are kept active and the SRAM is used to store the tags for data stored in DRAM cache. In this mode, the capacity becomes higher, at the cost of higher access latency of DRAM. Thus, at a time, either SRAM or DRAM, but not both, store the data, which makes their technique different from that of Madan et al. <ref type="bibr" target="#b47">[49]</ref>, where both SRAM and DRAM may simultaneously store the data.</p><p>Hameed et al. <ref type="bibr" target="#b42">[44]</ref> propose a hybrid SRAM-DRAM LLC (L3) which avoids data duplication compared to a hierarchy using L3 SRAM and L4 DRAM caches. They note that in a shared LLC, unnecessary fill requests from thrashing applications can delay the critical read/write accesses from non-thrashing applications, which creates inter-core DRAM interference. To address this, they propose a policy to dynamically decide whether a line brought from main memory is placed in SRAM and DRAM part of LLC or only in SRAM part. Their technique detects whether an application is thrashing based on the reuse distance and for such applications, the line is placed in DRAM cache with low probability. This avoids unnecessary insertions into DRAM. Unlike Inoue et al. <ref type="bibr" target="#b41">[43]</ref>, they do not power-gate the DRAM for saving energy, but instead focus on reducing inter-core interference in DRAM for improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.13">Performance model for DRAM caches</head><p>Gulur et al. <ref type="bibr" target="#b20">[22]</ref> present an analytical performance model of the DRAM Cache for both tags-in-SRAM and tags-in-DRAM organizations. Their model accounts for key parameters such as DRAM Cache and off-chip memory timing values, cache block size, hit rate of tag cache/predictor and application characteristics. Their model estimates the average access latency and request arrival rate at DRAM cache. Based on their model, they show that for the tags-in-DRAM design to outperform the tags-in-SRAM design, the hit rate of auxiliary tag cache/predictor needs to be very high. They also note that when DRAM cache hit rate is high, the large traffic at DRAM cache leads to contention and queuing delays. For such cases, some cache traffic can be diverted to idle main memory to achieve load-balancing. Using their model, they also present a load-balancing scheme which minimizes the average latency by identifying the optimal fraction of accesses to divert to main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.14">Comparison with NVM caches</head><p>Dong et al. <ref type="bibr" target="#b54">[56]</ref> compare STT-RAM cache with SRAM and DRAM caches in terms of area, performance, and energy. NVMs such as STT-RAM consume nearzero leakage power, however, their write latency and energy are significantly higher than that of SRAM and even DRAM <ref type="bibr" target="#b18">[20]</ref>. The density of STT-RAM is close to DRAM and is 4X compared to SRAM. Using architectural simulations they show that due to the requirement of refresh operations, DRAM consumes significantly more power than STT-RAM, while their performance values are nearly similar.</p><p>While byte-addressable NVMs may show superior energy efficiency compared to DRAM, it is also noteworthy that in terms of commercial viability and maturity, DRAM is superior to these NVMs. Further, NVMs also present reliability issue <ref type="bibr" target="#b62">[64]</ref>, for example, the resistance of a PCM cell increases over time, due to which an MLC (multi-level cell) PCM can start representing different value than the one originally stored. Due to this, soft-error rate in a 4-level PCM may be 10 6 times higher than that in DRAM <ref type="bibr" target="#b63">[65]</ref>.</p><p>Furthermore, the write endurance of NVMs is orders of magnitude smaller than that of DRAM. For example, while the write endurance of DRAM is 10 16 , this value for PCM and ReRAM is 10 8 and 10 11 , respectively <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b64">66]</ref>. For STT-RAM, this value is expected to be 10 15 , although best results so far show only 4 ? 10 12 <ref type="bibr" target="#b18">[20]</ref>. This not only limits the lifetime of NVM devices, but also presents a crucial security vulnerability from both malicious attackers and greedy-users who may make the system fail by write-attacks just before the warranty period to get a new system. Unlike NVMs, DRAM does not face such write-endurance issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FUTURE CHALLENGES AND CONCLUSION</head><p>While DRAM caches may provide a short-term solution to power issues, even they fall far short when measured against the goal of Exascale computing which seeks 10 18 operations/second computation capability within a power budget of 20MW <ref type="bibr" target="#b35">[37]</ref>. To accomplish this goal, solutions spanning across the whole software-stack are required. For example, at device-level, novel array organization can reduce refresh requirement and DRAM access energy <ref type="bibr" target="#b65">[67]</ref>. At architecture level, multiple techniques such as data compression, rank subsetting, access scheduling etc. <ref type="bibr" target="#b22">[24]</ref> can be synergistically integrated to bring the best of them together. Similarly, compiler and OS techniques can be used to perform data placement in a manner that improves hit rate by co-locating frequently used data on the same page <ref type="bibr" target="#b28">[30]</ref>.</p><p>With feature size scaling, the effect of process variation (which affects DRAM cell retention period) and susceptibility of DRAM cells to soft-errors also increase <ref type="bibr" target="#b62">[64]</ref>. While initial studies on DRAM cache have mainly focused on architectural design and performance/energy issues, moving forward, a thorough evaluation of other issues such as resilience, thermal management etc. will be definitely required to ensure reliable operation of these DRAM caches.</p><p>Architectural studies performed using simulators or real-systems are crucial for evaluating design innovations proposed for DRAM caches. Due to their large size, DRAM caches require large evaluation window for performing representative studies. However, due to the slow speed of existing simulators, along with the requirement of storing huge workload traces, full design-space exploration of large caches may not be feasible. Further, due to the emerging nature of stacked DRAM caches, their real-world prototypes may not be widely available or may not be economically viable. Clearly, improvements in manufacturing and commercial viability of stacked DRAM, along with development of extremely-fast simulation infrastructures will be very helpful in boosting further research on DRAM caches.</p><p>Memory system plays a crucial role in determining performance of high-end computing systems and given the limitations of other memory technologies (e.g. SRAM and NVMs), DRAM promises to be the most suitable candidate for designing multi-gigabyte caches. In this paper, we reviewed several techniques for managing DRAM caches. To underscore their similarities and differences, we classified them based on several key parameters. We also briefly discussed the challenges that lie ahead in this area. It is hoped that this paper will be useful for chip-designers, computerarchitects and system-researchers and will motivate further research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">A classification of research works</cell></row><row><cell>Classification</cell><cell>References</cell></row><row><cell cols="2">Study/optimization objective</cell></row><row><cell>Performance</cell><cell>[3, 4, 6-8, 10-12, 21, 27-32, 35,</cell></row><row><cell></cell><cell>39-58]</cell></row><row><cell>Energy Saving</cell><cell>[2, 6, 7, 32, 43, 47, 49, 53, 54, 56,</cell></row><row><cell></cell><cell>57, 59-61]</cell></row><row><cell>Providing inter-core isola-</cell><cell></cell></row><row><cell>tion</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ivytown: A 22nm 15core enterprise Xeon? processor family</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muljono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Varada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="102" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thermal characterization of cloud workloads on a power-efficient server-on-chip</title>
		<author>
			<persName><forename type="first">D</forename><surname>Milojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panteli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prodromou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Meswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ignatowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling the bandwidth wall: challenges in and avenues for CMP scaling</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="371" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Power scaling: the ultimate obstacle to 1K-core chips</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<idno>NWU-EECS-10-05</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Die stacking (3D) microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pantuso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="469" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring DRAM last level cache for 3D network-on-chip architecture</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liljeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tenhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Materials Research</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="4009" to="4018" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fundamental latency tradeoff in architecting DRAM caches: Outperforming impractical SRAM-tags with a simple and practical design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="235" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Intel Xeon Phi Knights Landing Processors to Feature Onboard Stacked DRAM Supercharged Hybrid Memory Cube (HMC) upto 16GB</title>
		<ptr target="http://wccftech.com/intel-xeon-phi-knights-landing-processors-stacked-dram-hmc-16gb/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CAMEO: A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cooperatively managing dynamic writeback and insertion policies in a lastlevel DRAM cache</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="187" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CHOP: Adaptive filter-based DRAM caching for CMP server platforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 1.2V 12.8GB/s 2Gb mobile Wide-I/O DRAM with 4X128 I/Os using TSV-based stacking</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-R</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE International Solid-State Circuits Conference Digest of Technical Papers</title>
		<imprint>
			<biblScope unit="page" from="496" to="498" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">8Gb 3D DDR3 DRAM using through-silicon-via technology</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Solid-State Circuits Conference-Digest of Technical Papers</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="130" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Octopus 8-Port DRAM for Die-Stack Applications</title>
		<author>
			<persName><forename type="first">Tezzaron</forename><surname>Semiconductor</surname></persName>
		</author>
		<ptr target="www.tachyonsemi.com/memory/datasheets/TSC10080x01.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid memory cube new DRAM architecture increases density and performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeddeloh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on VLSI Technology (VLSIT)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A 1.2 V 8 Gb 8-Channel 128 GB/s High-Bandwidth Memory (HBM) Stacked DRAM With Effective I/O Test Circuits</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="203" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="http://devblogs.nvidia.com/parallelforall/nvlink-pascal-stacked-memory-feeding-appetite-big-data/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Survey Of Architectural Approaches for Managing Embedded DRAM and Non-volatile On-chip Caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manikantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindarajan</surname></persName>
		</author>
		<title level="m">Bi-Modal DRAM Cache: Improving Hit Rate, Hit Latency and Bandwidth</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="38" to="50" />
		</imprint>
	</monogr>
	<note>International Symposium on Microarchitecture</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Comprehensive Analytical Performance Model of DRAM Caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Performance Engineering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CACTI-3DD: Architecture-level modeling for 3D die-stacked DRAM main memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Design, Automation and Test in Europe</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Survey of Architectural Techniques For DRAM Power Management</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Systems Architecture</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D-stacked memory architectures for multi-core processors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of architectural techniques for improving cache power efficiency</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Elsevier Sustainable Computing: Informatics and Systems</title>
		<imprint>
			<date type="published" when="2014-03">March 2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reducing latency in an SRAM/DRAM cache hierarchy via a novel Tag-Cache architecture</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple but effective heterogeneous main memory with on-chip memory controller support</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficiently enabling conventional block sizes for very large die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Challenges in heterogeneous die-stacked and off-chip memory systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd Workshop on SoCs, Heterogeneity, and Workloads</title>
		<meeting>of 3rd Workshop on SoCs, Heterogeneity, and Workloads</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dual grain hit-miss detector for large die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Nacouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Atta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papadopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zebchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation and Test in Europe</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enabling efficient and scalable hybrid memories using finegranularity DRAM cache management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="64" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DESTINY: A Tool for Modeling Emerging 3D NVM and eDRAM caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation and Test in Europe</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1543" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BATMAN: Maximizing Bandwidth Utilization of Hybrid Memory Systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<idno>TR-CARET-2015-01</idno>
	</analytic>
	<monogr>
		<title level="j">Georgia Tech</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A mostly-clean DRAM cache for effective hit speculation and self-balancing dispatch</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Resilient die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EnCache: A Dynamic Profiling Based Reconfiguration Technique for Improving Cache Energy Efficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Circuits, Systems, and Computers</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AYUSH: A Technique for Extending Lifetime of SRAM-NVM Hybrid Caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring DRAM cache architectures for CMP server platforms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Parallel &amp; Distributed Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Design of 3D DRAM and Its Application in 3D Integrated Multi-Core Computing Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anigundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Design and Test of Computers</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Extending the effectiveness of 3D-stacked DRAM caches with an adaptive multi-queue policy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="201" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D implemented SRAM/DRAM hybrid cache architecture for high-performance and low power consumption</title>
		<author>
			<persName><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Midwest Symposium on Circuits and Systems (MWSCAS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive cache management for a combined SRAM and DRAM cache hierarchy for multi-cores</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reducing inter-core cache contention with an adaptive bank mapping policy in DRAM cache</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simultaneously optimizing dram cache hit latency and miss rate via novel set mapping policies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compilers, Architectures IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTING SYSTEMS 13 and Synthesis for Embedded Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Design of controller for L2 cache mapped in Tezzaron stacked DRAM</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Tshibangu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Franzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International 3D Systems Integration Conference (3DIC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Toward efficient programmer-managed two-level memory hierarchies in exascale computers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Meswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ignatowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hardware-Software Co-Design for High Performance Computing (Co-HPC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimizing communication and capacity in a 3D stacked reconfigurable cache hierarchy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Udipi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="262" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variation-tolerant non-uniform 3D cache management in die stacked multicore processor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM international Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ATCache: reducing DRAM cache latency via a small SRAM tag cache</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Parallel architectures and compilation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Die-stacked DRAM caches for servers: hit ratio, latency, or bandwidth? have it all with footprint cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="404" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Enabling Efficient Dynamic Resizing of Large DRAM Caches via A Hardware Consistent Hashing Mechanism</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Tech. Rep. 2013-001</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient memory management of a hierarchical and a hybrid main memory for MN-MATE platform</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Programming Models and Applications for Multicores and Manycores</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Circuit and microarchitecture evaluation of 3D stacking magnetic RAM (MRAM) as a universal memory replacement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hybrid cache architecture replacing SRAM cache with future memory technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Kyung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2481" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DRAM-based coherent caches and how to take advantage of the coherence protocol to reduce the refresh energy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jaksic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation and Test in Europe Conference and Exhibition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temperatureaware energy minimization of 3D-stacked L2 DRAM cache through DVFS</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Kyung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International SoC Design Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="475" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Smart refresh: An enhanced memory controller design for reducing energy in conventional and 3D Die-Stacked DRAMs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="134" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving VLIW processor performance using three-dimensional (3d) DRAM stacking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Application-specific Systems, Architectures and Processors</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Design and evaluation of a subblock cache coherence protocol for bus-based multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<idno>94-05-02</idno>
	</analytic>
	<monogr>
		<title level="j">Univ. of Washington</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Survey of Techniques for Modeling and Improving Reliability of Computing Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tri-level-cell phase change memory: Toward an efficient and reliable memory system</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of power management techniques for phase change memory</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Aided Engineering and Technology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Exploring DRAM organizations for energy-efficient and resilient exascale memories</title>
		<author>
			<persName><forename type="first">B</forename><surname>Giridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blaauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Sparsh Mittal received the B.Tech. degree in electronics and communications engineering from IIT, Roorkee, India and the Ph.D. degree in computer engineering from Iowa State University, USA. He is currently working as a Post-Doctoral Research Associate at ORNL. His research interests include cache and main memory systems, resilience and energy efficiency, non-volatile memory and GPU architectures</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">At ORNL, he is a Distinguished R&amp;D Staff Member, and the founding group leader of the Future Technologies Group. At GT, he is a Joint Professor in the Computational Science and Engineering School, the Project Director for the NSF Track 2D Experimental Computing Facility for large scale heterogeneous computing using graphics processors, and the Director of the NVIDIA CUDA Center of Excellence. His research interests include massively multithreaded processors</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>ORNL and Georgia Institute of Technology (GT</orgName>
		</respStmt>
	</monogr>
	<note>memory architecture for extreme-scale systems and heterogeneous multicore processors</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
