<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate and Fast Performance Modeling of Processors with Decoupled Front-end</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuya</forename><surname>Degawa</surname></persName>
							<email>degawa@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Toru</forename><surname>Koizumi</surname></persName>
							<email>koizumi@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomoki</forename><surname>Nakamura</surname></persName>
							<email>tomokin@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryota</forename><surname>Shioya</surname></persName>
							<email>shioya@ci.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junichiro</forename><surname>Kadomoto</surname></persName>
							<email>kadomoto@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hidetsugu</forename><surname>Irie</surname></persName>
							<email>irie@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuichi</forename><surname>Sakai</surname></persName>
							<email>sakai@mtl.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate and Fast Performance Modeling of Processors with Decoupled Front-end</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICCD53106.2021.00025</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>instruction fetch, modeling techniques</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Various techniques, such as cache replacement algorithms and prefetching, have been studied to prevent instruction cache misses from becoming a bottleneck in the processor frontend. In such studies, the goal of the design has been to reduce the number of instruction cache misses. However, owing to the increasing complexity of modern processors, the correlation between reducing instruction cache misses and reducing the number of executed cycles has become smaller than in previous cases. In this paper, we propose a new guideline for improving the performance of modern processors. In addition, we propose a method for estimating the approximate performance of a design two orders of magnitude faster than a full simulation each time the designers modify their design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Various techniques, such as cache replacement algorithms and prefetching, have been studied to prevent instruction cache misses from becoming a bottleneck in the processor frontend. In such studies, reducing the number of instruction cache misses has often been focused on evaluations. Although the metrics used by the studies vary, such as the number of cache misses per 1000 instructions <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, as well as cache hit rate <ref type="bibr" target="#b3">[4]</ref>, cache miss rate <ref type="bibr" target="#b4">[5]</ref>, cache miss reduction rate <ref type="bibr" target="#b5">[6]</ref>, and coverage <ref type="bibr" target="#b6">[7]</ref>, they all focus on whether the number of cache misses is reduced.</p><p>However, owing to the increasing complexity of modern processors, the correlation between reducing the number of instruction cache misses and reducing the number of executed cycles has increasingly decreased. Indeed, in classical processors, an instruction cache miss can immediately lead to a stall, which can be a performance-critical issue. By contrast, this is not always the case with modern processors. In the front-end of a modern processor, an instruction cache access is decoupled from branch prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. In such processors, subsequent instructions continue to be fetched after an instruction cache miss, and subsequent instruction cache misses are handled in overlap with the previous instruction cache miss until the next branch misprediction. Therefore, in modern processors, a reduction in the number of instruction cache misses does not always lead to a reduction in the number of executed cycles.</p><p>Hence, designers of instruction caches are required to verify the performance by simulating the entire processor pipeline, not just the behavior of the instruction cache. Although simulations of the entire processor pipeline indicate the performance of the designed processor, they provide the designer no guidance for increasing the performance. Hence, the designers repeat the simulation and explore a design space based on their heuristics. In addition, simulating the entire processor pipeline wastes time by simulating components unrelated to the instruction cache.</p><p>In this paper, instead of reducing the number of instruction cache misses, we propose a new guideline for improving the performance of modern processors. We also propose a method for estimating the processor performance without simulating the entire processor pipeline when the instruction cache design is modified. Using this method, once the designers have obtained the baseline performance by simulating the entire processor pipeline, they can estimate the processor performance without repeating simulations of the entire processor pipeline.</p><p>Our contributions are as follows.</p><p>• We show that reducing the number of instruction cache misses is insufficient for designing instruction caches in modern processors. • We propose a new guideline for achieving a performance improvement instead of reducing the number of instruction cache misses. • Based on the guideline, we propose a method for estimating the performance after modifying the instruction cache design. Using our proposed method, once designers obtain the baseline performance by simulating the entire processor pipeline, they can estimate the change in performance by simulating only the behavior of the instruction cache. In our preliminary evaluation, the simulation time for the instruction cache alone was ∼ 3s on average, and that for the entire processor pipeline was ∼650s on average. • Our proposed method estimates the performance when applying an instruction prefetcher with an average error of 1.1% and a maximum error of 4.1%, whereas the estimate using the number of instruction cache misses achieved an average error of 9.0% and a maximum error of 43.1%.  </p><formula xml:id="formula_0">F D X F D X M M M F D X F D M M M F Stall Stall F D X F D X M M M F D X F D M M M F P P P P P D X X Latency is hidden. cycle X D X (b) (a)</formula><p>Fig. <ref type="figure">2</ref>. Pipeline diagrams of (a) a classical processor and (b) a modern processor. P, F, D, X, and M represent instruction fetching according to the addresses in the FTQ (prefetching), demand fetching, instruction decode, execution, and processing of an instruction cache miss, respectively. cache misses immediately stall the pipeline. Fig. <ref type="figure" target="#fig_0">1a</ref> shows its structure and behavior. In such processors, each instruction cache miss delays the instruction fetching, as shown in Fig. <ref type="figure">2a</ref>.</p><p>By contrast, modern processors have a front-end where instruction cache accesses are decoupled from a branch prediction <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Fig. <ref type="figure" target="#fig_0">1b</ref> shows its structure and behavior. The branch predictor predicts instruction addresses, which are then inserted into a queue called fetch target queue (FTQ). According to the addresses in the FTQ, instruction fetching from the instruction cache proceeds sequentially.</p><p>In this decoupled front-end, an instruction cache miss stalls neither branch prediction nor instruction cache accesses, whereas it stalls both in classical processors. Even if an instruction cache miss occurs, the branch predictor continues to predict until the FTQ is full. In addition, addresses following the missed address are sequentially retrieved from the FTQ, and instruction cache accesses continue using those addresses.</p><p>If a miss occurs in the subsequent cache access, the latency of the first miss hides that of the subsequent miss, as shown in Fig. <ref type="figure">2b</ref>, thus mitigating the increase in processing time. Considering that modern processors have a decoupled front-end, researchers at Arm have argued that the study of instruction prefetching should be based on a decoupled front-end <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DESIGN GUIDELINE FOR IMPROVED PERFORMANCE A. Gap between Reduction in Instruction Cache Misses and Reduction in Executed Cycles</head><p>Instruction cache designers have focused on reducing instruction cache misses as a guideline for improving the performance. However, we found that the guideline for reducing the number of instruction cache misses does not necessarily  achieve a performance improvement in modern processors, where instruction cache accesses are decoupled from branch prediction. Fig. <ref type="figure" target="#fig_2">3a</ref> shows the relationship between a reduction in instruction cache misses and a reduction in executed cycles when adding return-address-stack directed instruction prefetching (RDIP) <ref type="bibr" target="#b0">[1]</ref> in a decoupled front-end. The simulation environment and workload are described in Section V-A. Some points are distributed around the horizontal axis in the figure, indicating that reducing the number of instruction cache misses does not reduce the number of executed cycles for some workloads. Hence, the strategy of reducing the number of instruction cache misses is insufficient to achieve a performance improvement in modern processors with a decoupled front-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metric Showing a High Correlation with a Reduction in Executed Cycles 1) Hit and Miss Regions:</head><p>To clarify the relationship between instruction cache misses and performance in processors with a decoupled front-end, we focused on the relationship between branch mispredictions and instruction cache misses. Herein, "branch misprediction" includes a branch target buffer (BTB) miss. As mentioned above, once an instruction cache miss occurs in a decoupled front-end, the latency of the subsequent instruction cache misses is hidden. This hiding of the latency is effective until the next branch misprediction. When a branch misprediction is detected during the decoding or execution stage, the pipeline, including the FTQ, is flushed. Then fetching instructions on the correct path starts in the next cycle. Because the FTQ is empty at this time, the interval between the P-stage and F-stage, which has widened after the first cache miss shown in Fig. <ref type="figure">2b</ref>, becomes zero. Thus, the latency of the next cache miss increases the number of executed cycles.</p><p>Based on this observation, we consider one branch misprediction to the next branch misprediction as a region. We call a region not containing instruction cache misses, as shown in Fig. <ref type="figure" target="#fig_5">4a</ref>, a hit region, and a region containing one or more instruction cache misses, as shown in Figs. <ref type="figure" target="#fig_5">4b and  4c</ref>, a miss region. As Figs. <ref type="figure" target="#fig_5">4a and 4b</ref> show, if there is one instruction cache miss in a region, the length of the processing time increases based on the latency of lower cache access in comparison to when no instruction cache misses occur. As   shown in Figs. <ref type="figure" target="#fig_5">4b and 4c</ref>, if there are two or more instruction cache misses in a region, the processing time is the same as one instruction cache miss.</p><p>2) Reductions in Miss Regions and Executed Cycles: Based on the above insights, we found that to improve the performance of processors with a decoupled front-end, it is crucial to reduce the number of miss regions rather than aiming to reduce the number of instruction cache misses. To demonstrate this, we depict the relationship between a reduction in miss regions and a reduction in executed cycles when adding RDIP <ref type="bibr" target="#b0">[1]</ref> in a decoupled front-end, as shown in Fig. <ref type="figure" target="#fig_2">3b</ref>. The simulation environment and workload are described in Section V-A. Unlike in Fig. <ref type="figure" target="#fig_2">3a</ref>, the points in Fig. <ref type="figure" target="#fig_2">3b</ref> do not concentrate on the horizontal axis. This result means that the number of executed cycles can certainly be reduced by reducing the number of miss regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERFORMANCE ESTIMATION</head><p>We propose a method for estimating the reduction in executed cycles from a reduction in miss regions. This method allows designers to estimate the performance of a processor using a simple simulator that reproduces only the instruction cache. In the following discussion, we introduce some symbols. Let MR and HR be the set of miss regions and hit regions of the baseline, respectively. In addition, let MR and HR be the set of miss regions and hit regions after modifying the instruction cache design, respectively. The number of regions in a set is expressed using vertical bars, such as in |MR|.</p><p>To estimate the reduction in executed cycles from a reduction in miss regions, we define a penalty for a region. This is the maximum number of cycles required to fetch instructions in the region. Let P i denote the baseline penalty for region i, and P i denote the penalty after modifying the instruction cache design. Suppose a workload is instructioncache-intensive, where modifying the instruction cache design improves the performance. In this case, (a reduction in executed cycles)</p><formula xml:id="formula_1">i P i − i P i (1)</formula><p>under the condition that the number of branch mispredictions does not change before or after modifying the instruction cache design. Some instruction prefetch techniques reduce the number of branch mispredictions <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. When applying such techniques, we need to add a penalty for branch mispredictions to <ref type="bibr" target="#b0">(1)</ref>. This study considers the case in which the number of branch mispredictions does not change before or after modifying the instruction cache design. In Section V-B, we demonstrate that (1) holds for the workload we used for the evaluation.</p><p>Transforming the right-hand side of (1), we obtain the following:</p><formula xml:id="formula_2">i P i − i P i = i∈MR P i |MR| |MR| + i∈HR P i |HR| |HR| − i∈MR P i MR MR + i∈HR P i HR HR .<label>(2)</label></formula><p>We assume that the mean penalties for miss and hit regions are almost unchanged before and after modifying the instruction cache design, respectively, that is,</p><formula xml:id="formula_3">i∈MR P i |MR| i∈MR P i MR , i∈HR P i |HR| i∈HR P i HR .<label>(3)</label></formula><p>In Section V-B, we describe in detail whether (3) holds. Because this study considers the case in which the number of branch mispredictions does not change before or after modifying the instruction cache design, the total number of regions does not change before or after the modification, that is,</p><formula xml:id="formula_4">|MR| + |HR| = MR + HR . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>From ( <ref type="formula">1</ref>), ( <ref type="formula" target="#formula_2">2</ref>), (3), and ( <ref type="formula" target="#formula_4">4</ref>), (a reduction in executed cycles)</p><formula xml:id="formula_6">i∈MR P i |MR| − i∈HR P i |HR| |MR| − MR ,<label>(5)</label></formula><p>where i∈MR Pi |MR| , i∈HR Pi |HR| , and |MR| can be computed simultaneously with the baseline performance by simulating the entire processor pipeline. Therefore, |MR | is the only additional value needed to estimate a reduction in executed cycles achieved by modifying the instruction cache design.  2) Using a simple simulator that replicates only the behavior of the instruction cache and receives a sequence of cache accesses and branch mispredictions, |MR | is obtained. 3) A reduction in executed cycles is estimated according to <ref type="bibr" target="#b4">(5)</ref>. In this flow, we need to simulate the entire processor pipeline only once. In our preliminary evaluation, the simulation time for the instruction cache alone was ∼3s on average, whereas that for the entire processor pipeline described in Section V-A was ∼650s on average. Hence, the above flow significantly reduces the time required to explore the design space compared to repeating the simulations of the entire processor pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Methodology</head><p>We used ChampSim <ref type="bibr" target="#b15">[16]</ref> to simulate a processor with the configuration listed in Table <ref type="table">I</ref>. We determined the value of the parameters based on Intel Sunny Cove <ref type="bibr" target="#b16">[17]</ref>. We made the size of the FTQ larger than the default size of ChampSim, which is 12, to simulate a front-end where instruction cache access is decoupled from branch prediction. To simulate realistic branch target prediction, we implemented the BTB and the return address stack in ChampSim.</p><p>As the workloads, we used 50 traces distributed in the 1st Instruction Prefetching Championship <ref type="bibr" target="#b17">[18]</ref>. We used the first 50M instructions of these traces for warming up and the following 50M instructions for the evaluation. Some of the traces contain fewer than 100M instructions, and for those, we warmed up with the first 50M instructions and then ran simulations for the evaluation until reaching the end of the traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>We added RDIP <ref type="bibr" target="#b0">[1]</ref> to the processor as an example of a modification in the instruction cache design. We checked that Fig. <ref type="figure">5</ref>. Scatter plot comparing the difference between the sum of the penalties before and after applying RDIP ( i P i − i P i ) and a reduction in executed cycles achieved by RDIP.   (1) and (3) held, i.e., that the workloads were instruction cache intensive and that the mean penalty for miss/hit regions barely changed before and after applying RDIP. Fig. <ref type="figure">5</ref> shows the difference between the sum of the penalties before and after applying RDIP ( i P i − i P i ) on the horizontal axis and a reduction in the number of executed cycles by applying RDIP on the vertical axis. Each point represents a workload, which mostly lie on a straight line with a slope of 1 passing through the origin. This result means that these workloads satisfy (1); namely, they are instruction-cacheintensive.</p><p>Fig. <ref type="figure" target="#fig_9">6a</ref> shows the mean penalty for miss regions before applying RDIP ( i∈MR Pi |MR| ) on the horizontal axis and the mean penalty after applying RDIP ( i∈MR P i |MR | ) on the vertical axis. Similarly, Fig. <ref type="figure" target="#fig_9">6b</ref> shows the mean penalties for hit regions ( i∈R Pi |HR| and i∈HR P i |HR | ). Each point represents a workload, most of which lie on a straight line with a slope of 1 passing through the origin, satisfying (3). Fig. <ref type="figure">7</ref> shows the absolute value of the error of the estimated cycle per instruction (CPI). The proposal represents the CPI when applying RDIP, as calculated using our proposed method, in which a reduction in executed cycles is estimated by <ref type="bibr" target="#b4">(5)</ref>. Although, as mentioned above, |MR | in (5) can be obtained by simulating only the behavior of the instruction cache, we calculated it using ChampSim, the reason for which being that the purpose of this evaluation is not to verify that simulating only the behavior of the instruction cache reduces the design time, but to verify the validity of <ref type="bibr" target="#b4">(5)</ref>. We also estimated a reduction in the executed cycles from a</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Front-end behavior of (a) classical and (b) modern processors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) A reduction in cache misses.(b) A reduction in miss regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Relationship between each metric and a reduction in executed cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A region containing no instruction cache miss. This is a hit region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>A region containing an instruction cache miss. This is a miss region. A region containing two instruction cache misses. This is a miss region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Concept of regions. P, F, D, X, and M represent instruction fetching according to the addresses in the FTQ (prefetching), demand fetching, instruction decode, execution, and processing of an instruction cache miss, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>90</head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:16:17 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>applying RDIP (cycles) Before applying RDIP (cycles) (a) Miss regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>After applying RDIP (cycles)Before applying RDIP (cycles) (b) Hit regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Scatter plot comparing the mean penalties before and after applying RDIP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:16:17 UTC from IEEE Xplore. Restrictions apply.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">2021 IEEE 39th International Conference on Computer Design (ICCD)</cell></row><row><cell>Branch predictor</cell><cell>addr.</cell><cell>L1I cache</cell><cell>insn.</cell><cell>Decode</cell><cell></cell><cell></cell></row><row><cell cols="4">Every instruction cache miss stall this part.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch predictor</cell><cell>addr.</cell><cell>FTQ</cell><cell>addr.</cell><cell>L1I cache</cell><cell>insn.</cell><cell>Decode</cell></row><row><cell>2021 IEEE 39th International Conference on Computer Design (ICCD) | 978-1-6654-3219-1/21/$31.00 ©2021 IEEE | DOI: 10.1109/ICCD53106.2021.00025</cell><cell cols="4">Instruction fetching continues even if an instruction cache miss occurs.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>II. FRONT-END OF MODERN PROCESSORS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>In classical processors, because branch prediction and in-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>struction cache accesses operate in synchronization, instruction</cell></row><row><cell cols="5">978-1-6654-3219-1/21/$31.00 ©2021 IEEE</cell><cell></cell><cell>88</cell></row><row><cell cols="3">DOI 10.1109/ICCD53106.2021.00025</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>These values are typical (minimum) load-to-use latencies.b Realized through the instruction-granularity 144-entry FTQ.</figDesc><table><row><cell></cell><cell>TABLE I</cell></row><row><cell></cell><cell>PROCESSOR CONFIGURATION</cell></row><row><cell>Module</cell><cell>Parameter</cell></row><row><cell cols="2">Branch predictor GEHL perceptron, 64 KiB (232-bit history)</cell></row><row><cell>Target predictor</cell><cell>128-entry L1 BTB, 32-entry RAS, 1 cycle</cell></row><row><cell></cell><cell>4096-entry L2 BTB, 2 additional cycles</cell></row><row><cell>FTQ</cell><cell>instruction granularity, 144 entries</cell></row><row><cell>Front-end width</cell><cell>fetch width: 6, decode width: 6</cell></row><row><cell>L1I cache</cell><cell>32 KiB, 8-way, 4 cycles a , FDP b</cell></row><row><cell>L1D cache</cell><cell>48 KiB, 12-way, 4 cycles a , Next-line</cell></row><row><cell>L2 cache</cell><cell>512 KiB, 8-way, 15 cycles a , SPP</cell></row><row><cell>L3 cache</cell><cell>2 MiB, 16-way, 45 cycles a</cell></row><row><cell>Main memory</cell><cell>116 cycles a</cell></row><row><cell cols="2">a Based on the above discussion, the following performance</cell></row><row><cell cols="2">estimation flow is feasible.</cell></row><row><cell cols="2">1) By simulating the entire processor pipeline, i∈MR Pi |MR| , i∈HR Pi |HR| , |MR|, and the baseline performance are ob-tained.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:16:17 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors are grateful to Reoma Matsuo, for his study, which inspired us to conduct this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:16:17 UTC from IEEE Xplore. Restrictions apply.  reduction in L1I cache misses multiplied by 11 cycles, which is the difference between the L2 cache load-to-use latency (15 cycles) and the L1I cache load-to-use latency (4 cycles). We then calculated CPI when applying RDIP, which is depicted as # of cache misses × 11 cycles in Fig. <ref type="figure">7</ref>. The proposed method estimated CPI more accurately than the estimate using the reduction in L1I cache misses.</p><p>To show that our proposed method is also effective for stateof-the-art prefetchers, we conducted the same evaluation as RDIP on FNL+MMA <ref type="bibr" target="#b5">[6]</ref> and D-JOLT <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table">II</ref> presents the results of this study. Although the error rate of our proposed method was larger than that when applying RDIP, the performance estimate using our proposed method is more accurate than the performance estimate using a reduction in L1I cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Matsuo et al. used the notion that after an instruction cache miss occurs, the latency of the instruction cache miss can be hidden until the next branch misprediction in their method of dynamically changing the length of the instruction supply pipeline <ref type="bibr" target="#b18">[19]</ref>.</p><p>There are several performance-modeling techniques with a dependence graph <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Although they help designers determine which components should be improved, our proposed method helps designers estimate the performance obtained by making specific modifications to the components, such as changing the cache replacement algorithm or adding prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We introduced the idea of a miss region as an appropriate metric for designing an instruction cache. We also proposed a method for estimating the performance when modifying the instruction cache design based on the number of miss regions. Our proposed method represents the performance of a processor applying an existing instruction prefetcher with an average error of 1.1% and a maximum error of 4.1%.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">RDIP: Return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring predictive replacement policies for instruction cache and branch target buffer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirbagher Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="519" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D-JOLT: Distant jolt prefetcher</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The entangling instruction prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The FNL+MMA instruction cache prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Samsung M3 processor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Zuraski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The AMD &quot;Zen 2&quot; processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subramony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouvier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Arm neoverse N1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Elastic instruction fetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcilvaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Clancy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="478" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rebasing instruction prefetching: An industry perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="150" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Divide and conquer frontend bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ChampSim</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel&apos;s Sunny Cove sits on an icy lake</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The 1st instruction prefetching championship</title>
		<ptr target="https://research.ece.ncsu.edu/ipc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the instruction fetch throughput with dynamically configuring the fetch pipeline</title>
		<author>
			<persName><forename type="first">R</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shioya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="173" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Using interaction costs for microarchitectural bottleneck analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Newburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="228" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rpstacks: Fast and accurate processor design space exploration using representative stall-event stacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="255" to="267" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
