<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECURSIVE TRUST-REGION METHODS FOR MULTISCALE NONLINEAR OPTIMIZATION *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-04-16">April 16, 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Serge</forename><surname>Gratton</surname></persName>
							<email>gratton@cerfacs.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CNES and CERFACS</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Annick</forename><surname>Sartenaer</surname></persName>
							<email>annick.sartenaer@fundp.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Namur</orgName>
								<address>
									<postCode>B-5000</postCode>
									<settlement>Namur</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Philippe</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
							<email>philippe.toint@fundp.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Namur</orgName>
								<address>
									<postCode>B-5000</postCode>
									<settlement>Namur</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECURSIVE TRUST-REGION METHODS FOR MULTISCALE NONLINEAR OPTIMIZATION *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-04-16">April 16, 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">F43F9BD8796185459EDD0A60BBA35F52</idno>
					<idno type="DOI">10.1137/050623012</idno>
					<note type="submission">Received by the editors January 20, 2005; accepted for publication (in revised form) September 17, 2007;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>nonlinear optimization</term>
					<term>multiscale problems</term>
					<term>simplified models</term>
					<term>recursive algorithms</term>
					<term>convergence theory AMS subject classifications. 90C30</term>
					<term>65K05</term>
					<term>90C26</term>
					<term>90C06</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A class of trust-region methods is presented for solving unconstrained nonlinear and possibly nonconvex discretized optimization problems, like those arising in systems governed by partial differential equations. The algorithms in this class make use of the discretization level as a means of speeding up the computation of the step. This use is recursive, leading to true multilevel/multiscale optimization methods reminiscent of multigrid methods in linear algebra and the solution of partial differential equations. A simple algorithm of the class is then described and its numerical performance is shown to be numerically promising. This observation then motivates a proof of global convergence to first-order stationary points on the fine grid that is valid for all algorithms in the class.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>Large-scale finite-dimensional optimization problems often arise from the discretization of infinite-dimensional problems, a primary example being optimal control problems defined in terms of either ordinary or partial differential equations <ref type="bibr" target="#b7">[8]</ref>. While the direct solution of such problems for a discretization level is often possible using existing packages for large-scale numerical optimization, this technique typically makes very little use of the fact that the underlying infinitedimensional problem may be described at several discretization levels; the approach thus rapidly becomes cumbersome. Motivated by this observation, we explore here a class of algorithms which makes explicit use of this fact.</p><p>Using the different levels of discretization for an infinite-dimensional problem is not a new idea. A simple first approach is to use coarser grids in order to compute approximate solutions which can then be used as starting points for the optimization problem on a finer grid (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">22]</ref>, for instance). Other efficient techniques are inspired from the multigrid paradigm in the solution of partial differential equations and associated systems of linear algebraic equations (see, for example, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b44">43]</ref> for descriptions and references).</p><p>The purpose of our paper is threefold. We first introduce a new extension of the full approximation scheme (FAS) (see, for instance, Chapter 3 of <ref type="bibr" target="#b11">[12]</ref> or <ref type="bibr" target="#b26">[25]</ref>), an existing multigrid-type method, to a class of trust-region based optimization algorithms. We then indicate that this class contains numerically efficient members, thereby motivating further analysis. We finally provide a global convergence proof for all members of the class, which gives a robustness guarantee typical in optimization but, to the authors' knowledge, uncommon in multigrid approaches. Significantly, this guarantee holds even for nonconvex (nonelliptic) problems.</p><p>The work presented here was in particular motivated by the paper by Gelman and Mandel <ref type="bibr" target="#b15">[16]</ref>, the "generalized truncated Newton algorithm" presented in Fisher <ref type="bibr" target="#b14">[15]</ref>, a talk by Moré <ref type="bibr" target="#b29">[28]</ref>, and the contributions by Nash and coauthors <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b31">30]</ref>. These latter three papers present the description of MG/OPT, a linesearch-based recursive algorithm, an outline of its convergence properties, and impressive numerical results. The generalized truncated Newton algorithm and MG/OPT are very similar and, like many linesearch methods, naturally suited to convex problems, although their generalization to the nonconvex case is possible. An older contribution for convex problems is the damped nonlinear multilevel method by Hackbusch and Reusken <ref type="bibr" target="#b25">[24]</ref>, where convergence is analyzed for a variant of the FAS under the condition that a Lipschitz constant for the problem Hessian is explicitly known or can be numerically estimated. In the same spirit, the very recent contribution by Yavneh and Dardyk <ref type="bibr" target="#b46">[45]</ref> considers a linesearch to improve the radius of local convergence of a nonlinear equations solver. Further motivation to consider the more general nonconvex problem is also provided by the computational success of the low/high-fidelity model management techniques of Alexandrov, Lewis, and coauthors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and a paper by Borzi and Kunisch <ref type="bibr" target="#b8">[9]</ref> on multigrid globalization.</p><p>The class of algorithms discussed in this note can be viewed as an alternative where one uses the trust-region technology whose efficiency and reliability in the solution of nonconvex problems is well known (we refer the reader to <ref type="bibr" target="#b12">[13]</ref> for more complete coverage of this subject). Our developments are organized as follows. We first describe our class of multiscale trust-region algorithms in section 2 and show in section 3 that it can be specialized to a multigrid method that performs well on examples. This observation then motivates the proof of global convergence to firstorder critical points presented in section 4. The main results of this section are Theorem 4.10, which establishes a level-independent complexity bound for general trust-region algorithms, and Theorem 4.13, which is the desired convergence property. Some conclusions and perspectives are presented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Recursive multiscale trust-region algorithms.</head><p>We start by considering the solution of the unconstrained optimization problem min x∈ n f (x), <ref type="bibr">(2.1)</ref> where f is a twice-continuously differentiable objective function which maps n into and is bounded below. The trust-region methods which we investigate are iterative: given an initial point x 0 , they produce a sequence {x k } of iterates (hopefully) converging to a first-order critical point for the problem, i.e., to a point where g(x) def = ∇f (x) = 0. At each iterate x k , trust-region methods build a model m k (x) of f (x) around x k . This model is then assumed to be adequate in a "trust region," defined as a sphere of radius Δ k &gt; 0 centered at x k , and a step s k is then computed such that the trial point x k + s k sufficiently reduces this model in the region. The objective function is computed at x k + s k and the trial point is accepted as the next iterate if the ratio of achieved to predicted reduction is larger than a small positive constant. The value of the radius is finally updated to ensure that it is decreased when the trial point cannot be accepted as the next iterate and is increased or unchanged otherwise. In many practical trust-region algorithms, the model m k is quadratic, and obtaining sufficient decrease then amounts to (approximately) solving min</p><formula xml:id="formula_0">s ≤Δ k m k (x k + s) = min s ≤Δ k f (x k ) + g k , s + 1 2 s, H k s (2.</formula><p>2) Downloaded 01/02/13 to 138. <ref type="bibr">26.31.3</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php for s, where g k def = ∇f (x k ), H k is a symmetric n × n approximation of ∇ 2 f (x k ), •, • is the Euclidean inner product, and • is the Euclidean norm.</p><p>Such methods are efficient and reliable and provably converge to first-order critical points whenever the sequence { H k } is uniformly bounded. Besides computing the value f (x k + s k ), their work per iteration is dominated by the numerical solution of the subproblem (2.2), which crucially depends on the dimension n of the problem. When (2.1) results from the discretization of some infinite-dimensional problem on a relatively fine grid, the solution cost is therefore often significant.</p><p>In what follows, we investigate what can be done to reduce this cost by exploiting the knowledge of alternative simplified expressions of the objective function, when available. More specifically, we assume that we know a collection of functions {f i } r i=0 such that each f i is a twice-continuously differentiable function from ni to (with n i ≥ n i-1 ), the connection with our original problem being that n r = n and f r (x) = f (x) for all x ∈ n . We will also assume that, for each i = 1, . . . , r, f i is "more costly" to minimize than f i-1 . This may be because f i has more variables than f i-1 (as would typically be the case if the f i represent increasingly finer discretizations of the same infinite-dimensional objective), or because the structure (in terms of partial separability, sparsity, or eigenstructure) of f i is more complex than that of f i-1 , or for any other reason. To fix terminology, we will refer to a particular i as a level.</p><p>Of course, for f i-1 to be useful at all in minimizing f i , there should be some relation between the variables of these two functions. We henceforth assume that, for each i = 1, . . . , r, there exist a full-rank linear operator R i from ni into ni-<ref type="foot" target="#foot_0">1</ref> (the restriction) and another full-rank operator P i from ni-1 into ni (the prolongation) such that</p><formula xml:id="formula_1">σ i P i = R T i (2.3)</formula><p>for some known constant σ i &gt; 0. In the context of multigrid algorithms, P i and R i are interpreted as restriction and prolongation between a fine and a coarse grid (see <ref type="bibr" target="#b11">[12]</ref>, for instance). This assumption is also used in Nash <ref type="bibr" target="#b31">[30]</ref>.</p><p>The main idea is then to use f r-1 to construct an alternative model h r-1 for f r = f in the neighborhood of the current iterate that is cheaper than the quadratic model at level r, and to use this alternative model, whenever suitable, to define the step in the trust-region algorithm. If more than two levels are available (r &gt; 1), this can be done recursively, the approximation process stopping at level 0, where the quadratic model is always used. In what follows, we use a simple notation where quantities of interest have a double subscript i, k. The first, i (0 ≤ i ≤ r), is the level index (meaning, in particular, if applied to a vector, that this vector belongs to ni ), and the second, k, is the index of the current iteration within level i and is reset to 0 each time level i is entered. 1  Consider now some iteration k at level i (with current iterate x i,k ) and suppose that one decides to use the lower level model h i-1 based on f i-1 to compute a step. The first task is to restrict x i,k to create the starting iterate</p><formula xml:id="formula_2">x i-1,0 at level i -1, that is, x i-1,0 = R i x i,k .</formula><p>We then define the lower level model by</p><formula xml:id="formula_3">h i-1 (x i-1,0 + s i-1 ) def = f i-1 (x i-1,0 + s i-1 ) + v i-1 , s i-1 , (2.4) where v i-1 = R i g i,k -∇f i-1 (x i-1,0 ) with g i,k def = ∇h i (x i,k ). By convention, we set v r = 0 such that, for all s r , h r (x r,0 + s r ) = f r (x r,0 + s r ) = f (x r,0 + s r ) and g r,k = ∇h r (x r,k ) = ∇f (x r,k ).</formula><p>The function h i therefore corresponds to a modification of f i by a linear term that enforces the relation</p><formula xml:id="formula_4">g i-1,0 = ∇h i-1 (x i-1,0 ) = R i g i,k . (2.5)</formula><p>The first-order modification (2.4) is not unusual in multigrid applications in the context of the FAS and is also used by Fisher <ref type="bibr" target="#b14">[15]</ref> and Nash <ref type="bibr" target="#b31">[30]</ref>. It crucially ensures that the first-order behaviors of h i and h i-1 are coherent in a neighborhood of x i,k and x i-1,0 , respectively: indeed, one verifies that, if s i and s i-1 satisfy s i = P i s i-1 , then</p><formula xml:id="formula_5">g i,k , s i = g i,k , P i s i-1 = 1 σ i R i g i,k , s i-1 = 1 σ i g i-1,0 , s i-1 , (2.6)</formula><p>where we have also used (2.3) and <ref type="bibr">(2.5)</ref>. This coherence was independently imposed in <ref type="bibr" target="#b27">[26]</ref> and, in a slightly different context, in <ref type="bibr" target="#b1">[2]</ref> and other papers on first-order model management.</p><p>Our task, when entering level i = 0, . . . , r, is then to (locally) minimize h i starting from x i,0 . At iteration k of this minimization, we first choose, at iterate x i,k , either the model h i-1 (x i-1,0 + s i-1 ) (given by (2.4)) or</p><formula xml:id="formula_6">m i,k (x i,k + s i ) = h i (x i,k ) + g i,k , s i + 1 2 s i , H i,k s i , (2.7)</formula><p>where the latter is the usual truncated Taylor series in which H i,k is a symmetric n i ×n i approximation to the second derivatives of h i (which are also the second derivatives of f i ) at x i,k . Once the model is chosen (we will return to the conditions of this choice below), we then compute a step s i,k that generates a decrease on this model within a trust region {s i | s i i ≤ Δ i,k } for some trust-region radius Δ i,k &gt; 0. The norm • i in this last expression is level-dependent and defined, for some symmetric positive-definite matrix M i , by</p><formula xml:id="formula_7">s i i def = s i , M i s i def = s i Mi . (2.8)</formula><p>If the model (2.7) is chosen,<ref type="foot" target="#foot_1">2</ref> this is nothing but a usual ellipsoidal trust-region subproblem solution yielding a step s i,k . The decrease of the model m i,k is then understood in its usual meaning for trust-region methods, which is to say that s i,k is such that</p><formula xml:id="formula_8">m i,k (x i,k ) -m i,k (x i,k + s i,k ) ≥ κ red g i,k min g i,k 1 + H i,k , Δ i,k (2.9)</formula><p>for some constant κ red ∈ (0, 1). This condition is known as the "sufficient decrease" or "Cauchy point" condition. Chapter 7 of <ref type="bibr" target="#b12">[13]</ref> reviews several techniques that enforce it, including the exact minimization of m i,k within the trust region or an approximate minimization using (possibly preconditioned) Krylov space methods. On the other S. GRATTON, A. SARTENAER, AND P. L. TOINT hand, if the model h i-1 is chosen, minimization of this latter model (hopefully) produces a new point x i-1, * such that h i-1 (x i-1, * ) &lt; h i-1 (x i-1,0 ) and a corresponding step x i-1, *x i-1,0 which must then be brought back to level i by the prolongation P i . Since</p><formula xml:id="formula_9">s i i = s i Mi = P i s i-1 Mi = s i-1 P T i MiPi def = s i-1 Mi-1 = s i-1 i-1 (2.10)</formula><p>(which is well defined since P i is full-rank), the trust-region constraint at level i -1 then becomes</p><formula xml:id="formula_10">x i-1, * -x i-1,0 i-1 ≤ Δ i,k . (2.11)</formula><p>The lower level subproblem consists in (possibly approximately) solving</p><formula xml:id="formula_11">min si-1 i-1≤Δi,k h i-1 (x i-1,0 + s i-1 ). (2.12)</formula><p>The relation (2.10) also implies that, for i = 0 . . . , r -1,</p><formula xml:id="formula_12">M i = Q T i Q i , where Q i = P r . . . P i+2 P i+1 , (2.13)</formula><p>while we define M r = I for consistency. Preconditioning can also be accommodated by choosing M r more elaborately.</p><p>Is the cheaper model h i-1 always useful? Obviously not, as it may happen, for instance, that g i,k lies in the nullspace of R i and thus that R i g i,k is zero while g i,k is not. In this case, the current iterate appears to be first-order critical for h i-1 in ni-1 while it is not for h i in ni . Using the model h i-1 is hence potentially useful only if g i-1,0 = R i g i,k is large enough compared to g i,k . We therefore restrict the use of the model h i-1 to iterations where</p><formula xml:id="formula_13">R i g i,k ≥ κ g g i,k and R i g i,k &gt; g i-1</formula><p>(2. <ref type="bibr" target="#b13">14)</ref> for some constant κ g ∈ (0, min[1, min i R i ]) and where g i-1 ∈ (0, 1) is a measure of the first-order criticality for h i-1 that is judged sufficient at level i -1. Note that, given g i,k and R i , this condition is easy to check before even attempting to compute a step at level i -1.</p><p>We are now in a position to describe our recursive multiscale trust-region (RMTR) algorithm more formally as Algorithm 2.1.</p><p>In this description, we use the constants η 1 , η 2 , γ 1 , and γ 2 satisfying the conditions 0 &lt; η 1 ≤ η 2 &lt; 1 and 0 &lt; γ 1 ≤ γ 2 &lt; 1. It is assumed that the prolongations/restrictions P i and R i are known, as are the description of the levels i = 0, . . . , r. An initial trust-region radius for each level Δ s i &gt; 0 is also defined, as well as leveldependent gradient norm tolerances g i ∈ (0, 1) and trust-region tolerances Δ i ∈ (0, 1) for i = 0, . . . , r. The algorithm's initial data consists of the level index i (0 ≤ i ≤ r), a starting point x i,0 , the gradient g i,0 at this point, the radius Δ i+1 of the level-(i + 1) trust region, and the tolerances g i and Δ i . The original task of minimizing f (x) = f r (x r ) = h r (x r ) (up to the gradient norm tolerance g r &lt; ∇f r (x r,0 ) ) is achieved by calling RMTR(r, x r,0 , ∇f r (x r,0 ), Δ r+1,0 , g r , Δ r , Δ s r ) for some starting point x r,0 and initial trust-region radius Δ s r , and where we define Δ r+1,0 = ∞. For coherence of notation, we thus view this call as being made with an infinite radius from some (virtual) iteration 0 at level r + 1. The motivation for (2.17) in Step 6 of the algorithm and the termination test </p><formula xml:id="formula_14">x i,k+1 -x i,0 i &gt; (1 -Δ i )Δ i+1 in</formula><formula xml:id="formula_15">(i -1, R i x i,k , R i g i,k , Δ i,k , g i-1 , Δ i-1 , Δ s i-1</formula><p>), yielding an approximate solution x i-1, * of (2.12). Then define </p><formula xml:id="formula_16">s i,k = P i (x i-1, * -R i x i,k ), set δ i,k = h i-1 (R i x i,k ) -h i-1 (x i-1, * ),</formula><formula xml:id="formula_17">i,k i ≤ Δ i,k . Set δ i,k = m i,k (x i,k )-m i,k (x i,k +s i,k ).</formula><p>Step 4: Acceptance of the trial point. Compute h i (x i,k + s i,k ) and define</p><formula xml:id="formula_18">ρ i,k = (h i (x i,k ) -h i (x i,k + s i,k ))/δ i,k . (2.15) If ρ i,k ≥ η 1 , then define x i,k+1 = x i,k +s i,k ; otherwise define x i,k+1 = x i,k . Step 5: Termination. Compute g i,k+1 . If g i,k+1 ∞ ≤ g i or x i,k+1 -x i,0 i &gt; (1 -Δ i )Δ i+1</formula><p>, then return with the approximate solution x i, * = x i,k+1 .</p><p>Step 6: Trust-region radius update. Set</p><formula xml:id="formula_19">Δ + i,k ∈ ⎧ ⎪ ⎨ ⎪ ⎩ [Δ i,k , +∞) i f ρ i,k ≥ η 2 , [γ 2 Δ i,k , Δ i,k ] if ρ i,k ∈ [η 1 , η 2 ), [γ 1 Δ i,k , γ 2 Δ i,k ] if ρ i,k &lt; η 1 ,<label>(2.16)</label></formula><p>and  to guarantee that iterates at a lower level in a recursion remain in the trust region defined at the calling level, as verified below in Lemma 4.1.</p><formula xml:id="formula_20">Δ i,k+1 = min Δ + i,k , Δ i+1 -x i,k+1 -x i,0 i . (2.</formula><p>Iteration k at level i, associated with the computation of the step s i,k , will be referred to as iteration (i, k). It will be called a Taylor iteration</p><formula xml:id="formula_21">if Step 3 is used (that is, if Taylor's model m i,k (x i,k + s i ) is chosen at Step 1). If</formula><p>Step 2 is used instead, iteration (i, k) will then be called a recursive iteration. We emphasize that we expect the most efficient algorithms in our class to make use of a combination of both iteration types, which means, in particular, that recursive iterations should not be automatic if (2.14) holds. As is usual for trust-region methods, iteration (i, k) is said to be successful</p><formula xml:id="formula_22">if ρ i,k ≥ η 1 , that is, if the trial point x i,k + s i,k is accepted as the next iterate x i,k+1 . It is said to be very successful if ρ i,k ≥ η 2 , implying that Δ + i,k ≥ Δ i,k .</formula><p>In the case where r = 0, that is, if there is only one level in the problem, the algorithm reduces to the well-known usual trust-region method (see p. 116 of <ref type="bibr" target="#b12">[13]</ref>) and enjoys all the desirable properties of this method. If r &gt; 0, the recursive nature Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php of Algorithm RMTR is clear from Step 2. It is, in that sense, reminiscent of multigrid methods for linear systems <ref type="bibr" target="#b24">[23]</ref> and is close in spirit to the MG/OPT method <ref type="bibr" target="#b31">[30]</ref>. However, this latter method differs from ours in two main respects: Algorithm RMTR is of trust-region type, and its global convergence properties considered in this paper do not rely on performing Taylor iterations before or after a recursive one. Algorithm RMTR can also be viewed as an extension of the low-/high-fidelity model management method of <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>. The main differences are that our framework explicitly uses prolongation and restriction operators between possibly different variable spaces, allows more than two nested levels of fidelity, and, maybe less importantly, does not require coherence of low-fidelity model values with the high-fidelity objective function (zeroth-order model management). On the other hand, Algorithm RMTR does not fit in the framework of <ref type="bibr" target="#b15">[16]</ref> because this latter formalism considers only "memoryless" iterations and therefore does not cover adaptive algorithmic features such as the trust-region radius. Moreover, the convergence results analyzed in this reference require nonlocal properties on the involved functions and the limit points are proved only to belong to a set containing the problem's critical points and the iteration fixed points. Finally, the proposal by Borzi and Kunisch <ref type="bibr" target="#b8">[9]</ref> differs from ours in that it emphasizes convergence to minimizers on the coarsest grid but does not directly consider globalization on finer ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A practical algorithm and some numerical motivation. Clearly, our algorithmic description so far leaves a number of practical choices unspecified and is best viewed at this stage as a theoretical shell which potentially contains both efficient and inefficient algorithms. Can efficient algorithms be found in this shell? It is the purpose of this section to show that this is indeed the case. Instead of considering the RMTR class in its full generality, we will therefore focus on a simple implementation of our framework, and then show that the resulting method is, in our view, numerically promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm definition.</head><p>Smoothing and Taylor iterations. The most important of the open algorithmic questions is how one enforces sufficient decrease at Taylor iterations. A first answer is provided by existing algorithms for large-scale optimization, such as truncated conjugate-gradient (TCG) <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref> or generalized Lanczos trust-region (GLTR) <ref type="bibr" target="#b20">[19]</ref> methods, in which the problem of minimizing (2.7) is solved in successive embedded Krylov subspaces (see also section 7.5 in <ref type="bibr" target="#b12">[13]</ref>). This method is known to ensure (2.9). While it can be viewed as a Ritz procedure where solutions of subproblems of increasing sizes approach the desired high-dimensional one, the definition of these embedded subspaces does not exploit the explicit knowledge of discretization grids. We are thus interested in alternatives that exploit this knowledge.</p><p>If the model (2.7) is strictly convex and the trust-region radius Δ k sufficiently large, minimizing (2.7) amounts to an (approximate) solution of the classical Newton equations H i,k s i = -g i,k . If the problem additionally results from discretizing a convex operator on successively finer grids, then multigrid solvers constitute a most interesting alternative. Our intention is not to review this vast class of numerical algorithms here (we refer the reader to <ref type="bibr" target="#b11">[12]</ref> for an excellent introduction to the field), but we briefly outline their main characteristics. Multigrid algorithms are based on three complementary observations. The first is that some algorithms, called smoothers, are very efficient at selectively reducing the high-frequency components of the error on a grid, that is (in most cases), components whose "wavelength" is comparable to the grid's mesh-size. The second is that a low-frequency error component on a fine grid Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php appears more oscillatory on a coarse grid and may thus be viewed as a high-frequency component on this grid. The third is that computations on coarse grids are typically much cheaper than on finer grids. These observations may be exploited by a two-grid procedure, as follows. A few iterations of a smoother are first applied on the fine grid, reducing the error's high frequencies. The residual is then projected on the coarse grid where the low frequencies are more oscillatory and thus efficiently and cheaply reduced by the smoother applied on the coarse grid. The remaining error on the coarse grid is then prolongated back to the fine grid, which reintroduces a small amount of high-frequency error. A few more steps of the fine-grid smoother are finally applied to eliminate it. The multigrid algorithm is obtained by recursively replacing the error smoothing on the coarse grid by another two-grid procedure. Multigrid methods for positive-definite systems of equations typically result in remarkably efficient linearly convergent processes. Our intention here is to exploit the same features in minimizing (2.7), although it is expected only to reduce to a positive-definite system of linear equations asymptotically, when a minimizer of the problem is approached.</p><p>At the coarsest level, where further recursion is impossible, the cost of exactly minimizing (2.7) within the trust region remains small, because of the low dimensionality of the subproblem. Our strategy is thus to solve it using the method by Moré and Sorensen <ref type="bibr" target="#b30">[29]</ref> (see also section 7.3 in <ref type="bibr" target="#b12">[13]</ref>), whose very acceptable cost is then dominated by that of a small number of small-scale Cholesky factorizations. At finer levels, we have the choice of using the TCG or GLTR algorithms mentioned above, or an adaptation of the multigrid smoothing techniques that guarantees sufficient descent inside the trust region and also handles the possible nonconvexity of the model. The remainder of this section is devoted to describing this last option.</p><p>A very well-known multigrid smoother for systems of equations is the Gauss-Seidel method, in which every individual equation of the Newton system is solved in succession. <ref type="foot" target="#foot_2">3</ref> This procedure can be extended to optimization without major difficulty as follows: instead of successively solving equations, we may perform cyclic successive one-dimensional minimizations along the coordinate axes of the model (2.7), provided the curvature of this model along each axis is positive. Thus, if j is an index such that the jth diagonal entry of H i,k is strictly positive, the updates</p><formula xml:id="formula_23">α j = -[g] j /[H i,k ] jj , [s] j ← [s] j + α j , and g ← g + α j H i,k e i,j</formula><p>are performed for the minimization along the jth axis (starting each cycle from s such that ∇m i,k (x i,k + s) = g), where we denote by [v] j the jth component of the vector v and by [M ] ij the (i, j)th entry of the matrix M , and where e i,j is the jth vector of the canonical basis of ni . This is nothing but the well-known (and widely ill-considered) sequential coordinate minimization (see, for instance, [33, section 14.6]), which we abbreviate as SCM. In order to enforce convergence on nonconvex problems to firstorder points, we still have to ensure sufficient model decrease (2.9) while keeping the step in the trust region. This can be achieved in various ways, but we choose here to start the SCM cycle by initiating the cycle with the axis corresponding to the largest component of the gradient g i,k in absolute value. Indeed, if this component is the th one and if d = -sign([g i,k ] )e i, , then minimization of the model m i,k along d within the trust region is guaranteed to yield a Cauchy step α d such that the inequality</p><formula xml:id="formula_24">m i,k (x i,k ) -m i,k (x i,k + α d ) ≥ 1 2 |[g i,k ] | min |[g i,k ] | 1 + |[H i,k ] | , Δ i,k (3.1) holds. But |[g i,k ] | = max j | [g i,k ] j | ≥ 1 √ n g i,k , and |[H i,k ] | ≤ H i,k ,</formula><p>and (2.9) then follows from these inequalities and (3.1) since the remaining SCM operations only reduce the value of the model m i,k further. If, after completing one SCM cycle, one then notices that the overall step s lies outside of the trust region, we then apply a variant of the dogleg strategy (see <ref type="bibr" target="#b36">[35]</ref>, or [13, section 7.5.3]) to the step, by minimizing m i,k along the segment [α d , s] restricted to the trust region. The final step is then given by α d + α s (sα d ), where α s is the multiple of sα d where the minimizer is achieved.</p><p>Our description of the smoothing method is complete if we finally specify what is done when negative curvature is encountered along one of the coordinate axes, the jth one, say, during the SCM cycles. In this case, the model minimizer along e i,j lies on the boundary of the trust region, and it is very easy to compute the associated model reduction. The largest of these reductions is remembered (along with the corresponding step) if negative curvature is met along more than one axis. It is then compared to the reduction obtained by minimizing along the axes with positive curvature, and the step is finally chosen as that giving the maximum reduction.</p><p>The V-cycles. One of the flexible features of our RMTR framework is that the minimization at lower levels (i = 1, . . . , r -1) can be stopped after the first successful iteration without affecting convergence properties (as will become clear in section 4). This therefore opens the possibility of considering fixed form recursion patterns and free form ones. A free form pattern is obtained when Algorithm RMTR is run without using the premature termination option, in which case minimization is carried out at each level until the gradient becomes small enough or the relevant trust-region boundary is approached sufficiently (see Step 5 of Algorithm RMTR). The actual recursion pattern is then uniquely determined by the progress of minimization at each level and may be difficult to forecast. By contrast, the fixed form recursion patterns are obtained by specifying a maximum number of successful iterations at each level, a technique directly inspired from the definitions of V-and W-cycles in multigrid algorithms (see <ref type="bibr">[12, p. 40]</ref>, for instance).</p><p>In this section, we consider only V-cycle iterations, where minimization at lower levels (above the coarsest) consists in, at most, one successful smoothing iteration followed by either a successful TCG Taylor iteration (if (2.14) fails) or a recursive iteration (if (2.14) holds), itself followed by a second successful smoothing iteration. The lower iteration is however terminated if the boundary of the upper-level trust region is met, which typically occurs only far from a solution, or if the gradient becomes sufficiently small.</p><p>Second-order and Galerkin models. The definition of the gradient correction v i-1 in (2.4) is engineered to ensure (2.6), which is to say that h i and h i-1 coincide at first order (up to the constant σ i ) in the range of the prolongation operator. But coherence of the models can also be achieved at second order: if we choose</p><formula xml:id="formula_25">h i-1 (x i-1,0 + s i-1 ) = f i-1 (x i-1,0 + s i-1 ) + v i-1 , s i-1 + 1 2 s i-1 , W i-1 s i-1 , (3.2)</formula><p>where</p><formula xml:id="formula_26">W i-1 = R i H i,k P i -∇ 2 f i-1 (x i-1,0</formula><p>), then we also have that</p><formula xml:id="formula_27">P i s i-1 , H i,k P i s i-1, = 1 σ i s i-1 , ∇ 2 h i-1 (x i-1,0 )s i-1</formula><p>, Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php as desired. An even more radical strategy is to choose f i-1 (x i-1,0 + s i-1 ) = 0 for all s i-1 in (3.2), which amounts to choosing the lower-level objective function as the "restricted" version of the quadratic model at the upper level, also known as the "Galerkin approximation." This technique is known to improve performance for difficult cases involving an underlying infinite-dimensional problem with discontinuous coefficients (see, in particular, the recent analysis in <ref type="bibr" target="#b46">[45]</ref>). This is also the option considered in this section. In the case where this model is strictly convex and the trustregion radius is large enough, an iteration of the algorithm reduces to the solution of a positive-definite linear system; multigrid algorithms for solving this system, such as the multigrid V-Cycle scheme of <ref type="bibr">[12, p. 44]</ref>, can then be viewed as instances of Algorithm RMTR.</p><p>Computing the starting point at successively finer levels. It is clear that, if the multilevel recursion idea has any power within an iteration from the finest level down and back, it must also be advantageous to use the lower-level problems for computing the starting point x r,0 . In our motivating application, we have chosen to compute x r,0 by successively minimizing at levels 0 up to r -1 starting from the lowest one, where an initial starting point is assumed to be supplied by the user. (Note that, in general, the starting point can be supplied at any discretization level and transferred to other levels by using the prolongations or restrictions.) At level i &lt; r, the accuracy on the gradient infinity norm that is required for termination is given by</p><formula xml:id="formula_28">g i = min(0.01, g i+1 /ν ψ i ), (3.3)</formula><p>where ψ is the dimension of the underlying continuous problem, ν i is the discretization mesh-size along one of these dimensions, and g r is the user-supplied gradient accuracy requirement for the topmost level. Once computed at level i, the solution is prolongated to level i + 1 using cubic interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two test examples.</head><p>A simple quadratic example. We consider here the two-dimensional model problem for multigrid solvers in the unit square domain S 2 ,</p><formula xml:id="formula_29">-Δu(x, y) = f in S 2 , u (x, y) = 0 on ∂S 2 ,</formula><p>where f is such that the analytical solution to this problem is</p><formula xml:id="formula_30">u(x, y) = sin[2πx(1 -x)] sin[2πy(1 -y)].</formula><p>This problem is discretized using a five-point finite-difference scheme, giving a linear system A i x = b i at level i, where A i is a symmetric positive-definite matrix. Algorithm RMTR is used on the variational minimization problem min</p><formula xml:id="formula_31">x∈ nr 1 2 x T A r x -x T b r ,</formula><p>which is equivalent to the linear system A r x = b r . The starting point for the values of u not on the boundary is chosen as a random perturbation (of amplitude 10 -5 ) of the vector of all ones. This example illustrates that RMTR exhibits performance similar to traditional linear multigrid solvers on a model problem.</p><p>A nonconvex example. We introduce the nonlinear least-squares problem min u,γ where the unknown functions u(x, y) and γ(x, y) are defined on the unit square S 2 and the function u 0 (x, y) is defined on S 2 by u 0 (x, y) = sin(6πx) sin(2πy). This problem is again discretized using five-point finite differences, but the square in the last term makes the Hessian denser than for the pure Laplacian. The starting values for u and γ are random perturbations (of amplitude 100) of u 0 and zero, respectively. The nonconvexity of the resulting discretized problem on the fine grid has been assessed by a direct eigenvalue computation on the Hessian of the problem.</p><formula xml:id="formula_32">1 1000 S2 γ(x, y) 2 + S2 [u(x, y) -u 0 (x, y)] 2 + S2 [Δu(x, y) -γ(x, y)u(x, y)]</formula><p>Prolongations and restrictions. In both examples, we have defined the prolongation to be the linear interpolation operator and the restriction to be its transpose normalized to ensure that R i = 1. These operators are never assembled but are applied locally for improved efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Numerical results</head><p>. The algorithm described above has been coded in MATLAB (Release 7.0.0) and the experiments below were run on a Dell Precision M70 laptop computer with 2MBytes of RAM. The test problems are solved with g r = 0.5×10 -9 . Smoothing iterations use a single SCM cycle, and we choose η 1 = 0.01, η 2 = 0.95, γ 1 = 0.05, γ 2 = 0.25, κ g = 0.5, and Δ i = 0.001 for all i. The choice of Δ s r , the initial trust-region radius at level r, is slightly more difficult (see, for instance, <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b37">36]</ref> for suggested strategies), but here we choose to use Δ s r = 1. The gradient thresholds g i are chosen according to the rule (3.3). We consider the simple quadratic example first. In this example, recursive iterations were always accepted by the test (2.14). As a result, the work consisted only in exactly minimizing (2.7) in the trust region at the coarsest level and SCM smoothing at higher levels. Table <ref type="table">3</ref>.1 gives the problem dimension (n) for each level and the number of smoothing SCM cycles (# fine SCM) at the finest level required to solve the complete problem from scratch. This is, by far, the dominant linear algebra cost. For completeness, we also report the solution time in seconds (as reported by MATLAB) in the line "CPU(s)" of the same table.</p><p>For comparison, we also tested an efficient classical trust-region method using mesh-refinement with cubic interpolation and a TCG solver, where the conjugategradient minimization at iteration (i, k) is terminated as soon as the model gradient falls under the threshold max min 0.1, g i,k g i,k , 0.95 g r (see section 7.5.1 of <ref type="bibr" target="#b12">[13]</ref>, for instance). This algorithm solved the level-7 problem (n = 261, 121) with 657 conjugate-gradient iterations at the finest level in 190.54 seconds, and solved the level-8 problem (n = 1, 046, 529) with 1,307 conjugate-gradient iterations at the finest level in 2,463.33 seconds. (Note that this TCG solver can also be obtained as a special case of our framework by replacing smoothing iterations by TCG ones and disabling the recursive calls to RMTR.) As expected for a typical multigrid algorithm for linear equations, we observe that the number of smoothing cycles is fairly independent of the mesh size and dimension, which indicates that the trust-region machinery does not alter this property. We now consider our nonconvex test problem, for which the same statistics are given in Table <ref type="table">3</ref>.2. As for the quadratic example, the test (2.14) was always satisfied and the algorithm thus never had to use TCG iterations for levels above the coarsest.</p><p>On this example, the mesh-refinement algorithm using the TCG solver solved the level-6 problem (n = 130, 050) with 33,033 conjugate-gradient iterations at level 6 in 3,262.06 seconds, and solved the level-7 problem (n = 522, 242) with 3,926 conjugategradient iterations at level 7 in 6,154.96 seconds.</p><p>Even if these results were obtained by a very simple implementation of our framework, they are nevertheless highly encouraging, as they suggest that speed-ups of one order of magnitude or more could be obtained over (good) contending methods. Moreover, the statistics presented here also suggest that, at least for not too nonlinear problems, performance in CPU time can be essentially proportional to problem size, a very desirable property. The authors are of course aware that only continued experience with more advanced implementations will vindicate those preliminary tests (this work is currently under way) but consider that the potential numerical benefits justify a sound convergence analysis of the algorithm, which is best carried out considering the general RMTR class. This is the purpose of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Global convergence.</head><p>Our exposition of the global convergence properties of our general class of recursive multiscale algorithms starts with the analysis of properties that are specific to our class. The main concepts and developments of section 6.4 in <ref type="bibr" target="#b12">[13]</ref> are subsequently revisited to conclude the case of the multiscale algorithm. Interestingly, the techniques of proof are different and lead to a new complexity result (Theorem 4.10) that is also valid in the classical single-level case.</p><p>We first complete our assumptions by supposing that the Hessians of each h i and their approximations are bounded above by the constant κ H ≥ 1, i.e., more formally, that, for i = 0, . . . , r,</p><formula xml:id="formula_33">1 + ∇ 2 h i (x i ) ≤ κ H (4.1)</formula><p>for all x i ∈ ni , and</p><formula xml:id="formula_34">1 + H i,k ≤ κ H (4.2)</formula><p>for all k. In order to keep our notation simple, we also assume, without loss of generality, that</p><formula xml:id="formula_35">σ i = 1 (4.3) in (2.</formula><p>3) for i = 0, . . . , r (this can be directly obtained from the original form by scaling P i and/or R i ). We also define the constants We also introduce some additional concepts and notation.</p><formula xml:id="formula_36">κ PR def = max 1, max i=1,...,r P i = max 1, max i=1,...,r R i (4.</formula><p>1. If iteration (i, k) is recursive, we say that this iteration initiates a minimization sequence at level i -1, which consists of all successive iterations at this level (starting from the point</p><formula xml:id="formula_37">x i-1,0 = R i x i,k</formula><p>) until a return is made to level i within iteration (i, k). In this case, we also say that iteration (i, k) is the predecessor of the minimization sequence at level i -1. If (i -1, ) belongs to this minimization sequence, this is written as (i, k) = π(i -1, ). 2. To a given iteration (i, k), we associate the set</p><formula xml:id="formula_38">R(i, k) def = {(j, ) | iteration (j, ) occurs within iteration (i, k)}. (4.7)</formula><p>The set R(i, k) always contains the pair (i, k) and contains only that pair if</p><p>Step 3 is used at iteration (i, k). If Step 2 is used instead of Step 3, then it additionally contains the pairs of level and iteration numbers of all iterations that occur in the potential recursion started in Step 2 and terminating on return within iteration (i, k). Because R(i, k) is defined in terms of iterations, it does not contain the pairs of indices corresponding to the terminating iterates (j, * ) of its (internal) minimization sequences. One easily verifies that j ≤ i for every j such that (j, ) ∈ R(i, k) for some nonnegative k and . The mechanism of the algorithm also ensures that Δ j, ≤ Δ i,k whenever (j, ) ∈ R(i, k), <ref type="bibr">(4.8)</ref> because of the choice of Δ j,0 in Step 0 and (2.17). Note that R(i, k) contains at most one minimization sequence at level i -1 but may contain more than one at level i -2, since each iteration at level i -1 may generate its own. 3. For any iteration (j, ) ∈ R(i, k), there exists a unique path from (j, ) to (i, k) defined by taking the predecessor of iteration (j, ), say, (j + 1, q) = π(j, ), and then the predecessor of (j + 1, q) and so on until iteration (i, k). We also define</p><formula xml:id="formula_39">d(i, k) = min (j, )∈R(i,k) j, (4.9)</formula><p>which is the index of the deepest level reached by the potential recursion of iteration (i, k). The path from (d(i, k), ) to (i, k) is the longest in R(i, k). 4. We use the symbol</p><formula xml:id="formula_40">T (i, k) def = {(j, ) ∈ R(i, k) | iteration (j, ) uses Step 3}</formula><p>to denote the subset of Taylor iterations in R(i, k), that is, iterations at which Taylor's model m j, (x j, + s j ) is chosen. Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>We start the analysis of Algorithm RMTR by proving that it has a central property of trust-region methods, namely, that the steps remain in the trust region.</p><p>Lemma 4.1. For each iteration (i, k), we have that</p><formula xml:id="formula_41">s i,k i ≤ Δ i,k . (4.10)</formula><p>Moreover, if Δ j+1,q is the trust-region radius of iteration (j + 1, q) = π(j, ), we have that, for each (j, ) ∈ R(i, k), x j,x j,0 j ≤ Δ j+1,q and x j, *x j,0 j ≤ Δ j+1,q . (4.11)</p><p>Proof. The constraint (4.10) is explicit for Taylor iterations. We therefore have to verify that it holds only if Step 2 is chosen at iteration (i, k). If this is the case, consider j = d(i, k), and consider the first time it occurs in R(i, k). Assume, furthermore, that x j, * = x j,p . Because no recursion occurs to a level lower than j, one must have (from</p><p>Step 3) that s j, j ≤ Δ j, ( = 0, . . . , p -1). (4.12)</p><p>Then we obtain, for = 1, . . . , p, that, if iteration (j, -1) is successful,</p><formula xml:id="formula_42">x j, -x j,0 j = x j, -1 -x j,0 + s j, -1 j ≤ x j, -1 -x j,0 j + s j, -1 j</formula><p>because of the triangle inequality, while x j,x j,0 j = x j, -1x j,0 j ≤ x j, -1x j,0 j + s j, -1 j if it is unsuccessful. Combining these two bounds and (4.12), we have that x j,x j,0 j ≤ x j, -1x j,0 j + Δ j, -1 ≤ x j, -1x j,0 j + Δ j+1,qx j, -1x j,0 j = Δ j+1,q (4.13) for = 2, . . . , p, where the last inequality results from (2.17). The same result also holds for = 1, since x j,1 -x j,0 j ≤ Δ j,0 ≤ Δ j+1,q because of Step 0 in the algorithm. We then verify, using (2.10), that s j+1,q j+1 = P j+1 (x j, *x j,0 ) j+1 = x j, *x j,0 j = x j,px j,0 j ≤ Δ j+1,q , which is nothing but inequality (4.12) at iteration (j + 1, q). The same reasoning may then be applied to each iteration at level j + 1 that uses Step 2. Since inequality (4.12) is guaranteed for all other iterations of that level by Step 3, we obtain that (4.12) also holds with j replaced by j + 1. The same must therefore be true for (4.13). The induction can then be continued up to level i, yielding both (4.10) and (4.11) (for which the case = 0 is obvious).</p><p>In the same vein, the algorithm also ensures the following two properties. Lemma 4.2. The mechanism of Algorithm RMTR guarantees that, for each iterate of index (j, ) such that (j, ) = (j, * ) (i.e., for all iterates at level j but the last one), g j, &gt; g j (4.14) Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php S. GRATTON, A. SARTENAER, AND P. L. TOINT and x j,x j,0 j ≤ (1 -Δ j )Δ j+1,q , (4. <ref type="bibr" target="#b14">15)</ref> where Δ j+1,q is the trust-region radius of iteration (j + 1, q) = π(j, ).</p><p>Proof. These bounds directly follow from the stopping criteria for minimization at level j, in Step 5 of the algorithm.</p><p>We now prove some useful bounds on the gradient norms for all iterates that belong to a recursion process initiated within a sufficiently small trust region.</p><p>Lemma 4.3. Assume that, for some iteration (i, k),</p><formula xml:id="formula_43">Δ i,k ≤ √ κ σ κ r g 2rκ H g i,k def = κ 1 g i,k , (4.16)</formula><p>where κ 1 ∈ (0, 1). Then one has that, for all (j, ) ∈ R(i, k),</p><formula xml:id="formula_44">1 2 κ r g g i,k ≤ g j, ≤ κ r PR (1 + 1 2 κ r g ) g i,k . (4.17)</formula><p>Proof. The result is obvious for (j, ) = (i, k) since, by definition, κ g &lt; 1 and κ PR ≥ 1. Let us now consider some iteration (j, ) ∈ R(i, k) with j &lt; i. From the mean-value theorem, we know that, for any iteration (j, ), g j, = g j,0 + G j, (x j,x j,0 ), <ref type="bibr">(4.18)</ref> where</p><formula xml:id="formula_45">G j, = 1 0 ∇ 2 h j (x j,0 + t(x j, -x j,0 )) dt. (4.19) But G j, ≤ max t∈[0,1] ∇ 2 h j (x j,0 + t(x j, -x j,0 )) ≤ κ H , (4.20)</formula><p>and hence, by definition of the norms and (4.5),</p><formula xml:id="formula_46">g j, ≥ g j,0 -κ H x j, -x j,0 ≥ g j,0 - κ H √ κ σ x j, -x j,0 j (4.21)</formula><p>for all (j, ). On the other hand, if (j + 1, q) = π(j, ), we have also that, for all (j, ) ∈ R(i, k), x j,x j,0 j ≤ Δ j+1,q ≤ Δ i,k <ref type="bibr">(4.22)</ref> because of (4.11) and (4.8) (as (j + 1, q) ∈ R(i, k)). Combining (4.21) and (4.22), we obtain that, for all (j, ) ∈ R(i, k),</p><formula xml:id="formula_47">g j, ≥ g j,0 - κ H √ κ σ Δ i,k . (4.23)</formula><p>Consider now the path from (j, ) to (i, k) in R(i, k). Let this path consist of the iterations (j, ), (j + u, t j+u ) for u = 1, . . . , ij -1, and (i, k). We then have that  <ref type="bibr">(2.5)</ref>, the first part of (2.14), and the inequality κ g &lt; 1. We then deduce the first inequality of (4.17) from (4.16).</p><formula xml:id="formula_48">g j, ≥ g j,0 -κ H √ κ σ Δ i,k ≥ κ g g j+1,tj+1 -κ H √ κ σ Δ i,k ≥ κ g g j+1,0 -2 κ H √ κ σ Δ i,k ≥ κ 2 g g j+2,tj+2 -2 κ H √ κ σ Δ i,k ≥ κ r g g i,k -r κ H √ κ σ Δ i,</formula><p>To prove the second, we reuse (4.18)-(4.20) to obtain that</p><formula xml:id="formula_49">g j, ≤ g j,0 + κ H x j, -x j,0 ≤ g j,0 + κ H √ κ σ x j, -x j,0 j . (4.24)</formula><p>Combining this with (4.22), we conclude that</p><formula xml:id="formula_50">g j, ≤ g j,0 + κ H √ κ σ Δ i,k . (4.25)</formula><p>We now retrack the iteration path from (j, ) back to (i, k) as above and successively deduce from (4.25), (2.5), and (4.4) that</p><formula xml:id="formula_51">g j, ≤ g j,0 + κ H √ κ σ Δ i,k ≤ κ PR g j+1,tj+1 + κ H √ κ σ Δ i,k ≤ κ PR g j+1,0 + (κ PR + 1) κ H √ κ σ Δ i,k ≤ κ 2 PR g j+2,tj+2 + 2 κ PR κ H √ κ σ Δ i,k ≤ κ r PR g i,k + r κ r-1 PR κ H √ κ σ Δ i,k ≤ κ r PR g i,k + r κ H √ κ σ Δ i,k , using κ PR ≥ 1.</formula><p>We may now use the bound (4.16) to conclude that the second inequality of (4.17) must hold.</p><p>We now investigate what happens at noncritical points if the trust-region radius Δ i,k is small enough. This investigation is conducted by considering the subset</p><formula xml:id="formula_52">V(i, k) of R(i, k) defined by V(i, k) = (j, ) ∈ R(i, k) | δ j, ≥ 1 2 κ red κ r g κ j-d(i,k) g i,k Δ j, ,<label>(4.26)</label></formula><p>where</p><formula xml:id="formula_53">κ def = η 2 Δ min &lt; 1. (4.27)</formula><p>V(i, k) is the subset of iterations within the recursion at iteration (i, k) for which the model decrease is bounded below by a (level-dependent) factor times the product of the gradient norm g i,k and the trust-region radius Δ j, . Note that, if iteration (j, ) belongs to V(i, k), this implies that δ j, can be computed in a finite number of iterations, and thus that R(j, ) is finite. The idea of the next two results is to show that V(i, k) and R(i, k) coincide for a sufficiently small radius Δ i,k .</p><p>Theorem 4.4. Consider an iteration (i, k) for which g i,k &gt; 0 and</p><formula xml:id="formula_54">Δ i,k ≤ min Δ s min , min κ 1 , κ red κ σ κ r g κ r (1 -η 2 ) 2κ H g i,k def = min[Δ s min , κ 2 g i,k ],<label>(4.28)</label></formula><p>where κ 2 ∈ (0, 1). Then the following conclusions hold: 1. every iteration using Taylor's model belongs to <ref type="bibr">(4.26)</ref>, that is,</p><formula xml:id="formula_55">T (i, k) ⊆ V(i, k),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and (4.29)</head><p>Moreover, if all iterations (j, ) of a minimization sequence at level j &lt; i belong to V(i, k) and if π(j, ) = (j + 1, q), then 3. the decrease in the objective function at level j satisfies, for each &gt; 0,</p><formula xml:id="formula_56">h j (x j,0 ) -h j (x j, ) ≥ 1 2 κ red κ r g κ j-d(i,k)+1 g i,k Δ j+1,q , (4.30)</formula><p>4. there are at most</p><formula xml:id="formula_57">p * def = κ r PR √ κ σ (2 + κ r g ) + κ 2 κ H κ red κ σ κ r g κ r (4.31)</formula><p>iterations in the minimization sequence at level j, and 5. we have that</p><formula xml:id="formula_58">(j + 1, q) ∈ V(i, k). (4.32)</formula><p>Proof. 1. We start by proving (4.29). Note that, for (j, ) ∈ R(i, k), (4.8), the fact that the positive constants κ red , κ σ , κ , and η 2 are all bounded above by one, (4.28), the left inequality in (4.17), and (4.2) allow us to conclude that</p><formula xml:id="formula_59">Δ j, ≤ Δ i,k ≤ κ r g 2κ H g i,k ≤ g j, 1 + H j, . (4.33)</formula><p>If we now assume that (j, ) ∈ T (i, k), the decrease condition (2.9) must hold at this iteration, which, together with the left part of (4.17) and (4.33), gives that</p><formula xml:id="formula_60">δ j, = m j, (x j, ) -m j, (x j, + s j, ) ≥ κ red g j, Δ j, ≥ 1 2 κ red κ r g g i,k Δ j, ,<label>(4.34)</label></formula><p>which then implies (4.29) since κ &lt; 1.</p><p>2. We prove item 2 separately for (j, ) ∈ T (i, k) and for (j, ) ∈ V(i, k) \ T (i, k). Consider the case where (j, ) ∈ T (i, k) first. We deduce from Taylor's theorem that, for (j, ) ∈ T (i, k),</p><formula xml:id="formula_61">|h j (x j, + s j, ) -m j, (x j, + s j, )| ≤ κ H s j, s j, j 2 Δ 2 j, (<label>4.35)</label></formula><p>(see, for instance, Theorem 6.4.1 on p. 133 of <ref type="bibr" target="#b12">[13]</ref>). But, by definition of the norms and (4.5), we know that s j, j ≥ √ κ σ s j, . Hence, (4.35) becomes</p><formula xml:id="formula_62">|h j (x j, + s j, ) -m j, (x j, + s j, )| ≤ κ H κ σ Δ 2 j, .</formula><p>Combining this last bound with (4.34), we obtain from (2.15) that</p><formula xml:id="formula_63">|ρ j, -1| ≤ h j (x j, + s j, ) -m j, (x j, + s j, ) m j, (x j, ) -m j, (x j, + s j, ) ≤ 2κ H κ red κ σ κ r g g i,k Δ j, ≤ 1 -η 2 ,</formula><p>where the last inequality is deduced from (4.8) and the fact that (4.28) implies the bound We next prove item 2 for (j, ) ∈ V(i, k) \ T (i, k), which implies, in particular, that R(j, ) is finite and x j-1, * is well defined. If we consider iteration (j, ), we may still deduce from the mean-value theorem that h j (x j, )h j (x j, + s j, ) =g j, , s j, -1 2 s j, , ∇ 2 h j (ξ j ) s j, for some ξ j ∈ [x j, , x j, + s j, ] and also that</p><formula xml:id="formula_64">Δ i,k ≤ κ red κ σ κ r g g i,k (1 -η 2 )/</formula><formula xml:id="formula_65">h j-1 (x j-1,0 ) -h j-1 (x j-1, * ) = -g j-1,0 , z j-1 -1 2 z j-1 , ∇ 2 h j-1 (ξ j-1 ) z j-1 for some ξ j-1 ∈ [x j-1,0 , x j-1,0 + z j-1 ], where z j-1 = x j-1, * -x j-1,0 = x j-1, * - R j x j,</formula><p>. Now, because s j, = P j z j-1 , we deduce from (2.6) and ( <ref type="formula" target="#formula_52">4</ref>.3) that g j, , s j, = g j-1,0 , z j-1 and therefore that</p><formula xml:id="formula_66">h j (x j, ) -h j (x j, + s j, ) = h j-1 (x j-1,0 ) -h j-1 (x j-1, * ) -1 2 s j, , ∇ 2 h j (ξ j ) s j, + 1 2 z j-1 , ∇ 2 h j-1 (ξ j-1 ) z j-1 .</formula><p>(4.36)</p><p>But Lemma 4.1 implies that s j, j ≤ Δ j, and z j-1 j-1 ≤ Δ j, , which, in turn with the Cauchy-Schwarz inequality, gives that</p><formula xml:id="formula_67">| s j, , ∇ 2 h j (ξ j ) s j, | ≤ κ H s j, 2 ≤ κ H s j, s j, j 2 Δ 2 j, ≤ κ H κ σ Δ 2 j, . (4.37) Similarly, | z j-1 , ∇ 2 h j-1 (ξ j-1 ) z j-1 | ≤ κ H κ σ Δ 2 j, . (4.38)</formula><p>Combining (4.36), (4.37), <ref type="bibr">(4.38)</ref>, and the definition of δ j, , we obtain that</p><formula xml:id="formula_68">h j (x j, ) -h j (x j, + s j, ) ≥ δ j, - κ H κ σ Δ 2 j, . (4.39)</formula><p>But since (j, ) ∈ V(i, k) and κ &lt; 1, we have that</p><formula xml:id="formula_69">δ j, ≥ 1 2 κ red κ r g κ j-d(i,k) g i,k Δ j, ≥ 1 2 κ red κ r g κ r g i,k Δ j, &gt; 0,</formula><p>and we conclude from (4.39), the definition of ρ j, , and this last bound that</p><formula xml:id="formula_70">ρ j, = h j (x j, ) -h j (x j, + s j, ) δ j, ≥ 1 - κ H Δ 2 j, κ σ δ j, ≥ 1 - 2κ H Δ j, κ red κ σ κ r g κ r g i,k</formula><p>.</p><p>Noting now that (4.28) implies the inequality</p><formula xml:id="formula_71">Δ i,k ≤ 1 2 κ red κ σ κ r g κ r g i,k (1 -η 2 )</formula><p>and using the bound (4.8), we obtain that ρ j, ≥ η 2 . Iteration (j, ) is thus very successful, which completes the proof of item 2.</p><p>3. We now assume that all iterations (j, ) of a minimization sequence at level j &lt; i belong to V(i, k) with (j +1, q) = π(j, ). We first notice that (j +1, q) ∈ R(i, k), (4.8), (4.28), and (4.6) imply that Δ j+1,q ≤ Δ i,k ≤ Δ s min ≤ Δ s j . Hence Step 0 gives Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php that Δ j,0 = Δ j+1,q , and since all iterations at level j are very successful because of item 2, we have from Step 6 that, for all (j, ) with &gt; 0, Δ j, = min Δ + j, -1 , Δ j+1,qx j,x j,0 j ≥ min Δ j, -1 , Δ j+1,qx j,x j,0 j = min min[Δ + j, -2 , Δ j+1,qx j, -1x j,0 j ], Δ j+1,qx j,x j,0 j</p><formula xml:id="formula_72">≥ min Δ j, -2 , Δ j+1,q -max p= -1,</formula><p>x j,px j,0 j ≥ min Δ j,0 , Δ j+1,qmax p=1,...,</p><p>x j,px j,0 j = Δ j+1,qmax p=1,...,</p><p>x j,px j,0 j ≥ Δ j Δ j+1,q , where we used (4.15) to deduce the last inequality. Note that Δ j,0 = Δ j+1,q &gt; Δ j Δ j+1,q , covering the case where = 0. Combining these bounds with the very successful nature of each iteration at level j, we obtain that, for each (j, p) with p = 0, . . . , -1,</p><formula xml:id="formula_73">h j (x j,p ) -h j (x j,p + s j,p ) ≥ η 2 δ j,p ≥ 1 2 η 2 κ red κ r g κ j-d(i,k) g i,k Δ j,p ≥ 1 2 κ red κ r g κ j-d(i,k) η 2 Δ j g i,k Δ j+1,q ≥ 1 2 κ red κ r g κ j-d(i,k)+1 g i,k Δ j+1,q ,</formula><p>where we used (4.6) and (4.27) to obtain the last inequality. Summing now over iterations p = 0, . . . , -1 at level j, we obtain that</p><formula xml:id="formula_74">h j (x j,0 ) -h j (x j, ) = -1 p=0 [h j (x j,p ) -h j (x j,p + s j,p )] ≥ 1 2 κ red κ r g κ j-d(i,k)+1 g i,k Δ j+1,q ,</formula><p>yielding (4.30). 4. In order to prove item 4, we start by proving that the total decrease in h j (the objective function for the considered minimization sequence at the jth level) is bounded above by some multiple of g i,k and Δ j+1,q . We first note that the mean-value theorem gives that h j (x j,0 + s j,min ) = h j (x j,0 ) + g j,0 , s j,min + 1 2 s j,min , ∇ 2 h j (ξ j ) s j,min for some ξ j ∈ [x j,0 , x j,0 + s j,min ], where we have defined s j,min = arg min sj j ≤Δj+1,q h j (x j,0 + s j ).</p><p>Hence, we obtain that, for all s j such that s j j ≤ Δ j+1,q , h j (x j,0 )h j (x j,0 + s j ) ≤ h j (x j,0 )h j (x j,0 + s j,min ) ≤ g j,0 √ κ σ Δ j+1,q + κ H 2κ σ Δ 2 j+1,q . Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php But we have that x j,x j,0 j ≤ Δ j+1,q because of (4.11), and therefore the right inequalities of (4.17), (4.8), and (4.28) now give that</p><formula xml:id="formula_75">h j (x j,0 ) -h j (x j, ) ≤ κ r PR + 1 2 κ r PR κ r g √ κ σ + κ 2 κ H 2κ σ g i,k Δ j+1,q (4.40)</formula><p>for all (j, ) with ≥ 0. Combining now this bound with (4.30) and remembering that κ &lt; 1, we deduce that item 4 must hold with (4.31).</p><p>5. Finally, since the minimization sequence at level j is guaranteed to terminate after a finite number of iterations 1 ≤ ≤ p * , we deduce from (4.30) and the definition of δ j+1,q that δ j+1,q ≥ 1 2 κ red κ r g κ j+1-d(i,k) g i,k Δ j+1,q , and (4.32) then immediately follows.</p><p>We may deduce the following important corollary from this theorem. Corollary 4.5. Assume (4.28) holds for some iteration (i, k) for which g i,k &gt; 0. Then all iterations (j, ) ∈ R(i, k) are very successful. Moreover, the total number of iterations in R(i, k) is finite and</p><formula xml:id="formula_76">Δ + i,k ≥ Δ i,k . Proof.</formula><p>As suggested above, we proceed by showing that V(i, k) = R(i, k), working from the deepest recursion level upward. Thus consider level j = d(i, k) first. At this level, all iterations (j, ) belong to T (i, k) and thus, by (4.29), to V(i, k). If j = i, we have achieved our objective. Assume, therefore, that j &lt; i and consider level j + 1. Using (4.32), we see that all iterations involving a recursion to level j must belong to V(i, k), while the other (Taylor) iterations again belong to V(i, k) by (4.29). If j + 1 = i, we have thus proved that V(i, k) = R(i, k). If j + 1 &lt; i, we may then apply the same reasoning to level j + 2, and so on, until level i is reached. We may thus conclude that V(i, k) and R(i, k) always coincide and, because of item 2 of Theorem 4.4, contain only very successful iterations. Furthermore, using item 4 of Theorem 4.4, we see that the total number of iterations in R(i, k) is bounded above by</p><formula xml:id="formula_77">r l=0 p * ≤ rp r * + 1.</formula><p>Finally, the fact that Δ + i,k ≥ Δ i,k then results from the mechanism of Step 6 of the algorithm and the very successful nature of iteration (i, k) ∈ R(i, k).</p><p>This last result guarantees the finiteness of the recursion at iteration (i, k) (and thus the finiteness of the computation of s i,k ) if Δ i,k is small enough. It also ensures the following useful consequence.</p><p>Lemma 4.6. Each minimization sequence contains at least one successful iteration.</p><p>Proof. This follows from the fact that unsuccessful iterations cause the trustregion radius to decrease, until (4.28) is eventually satisfied and a (very) successful iteration occurs because of Corollary 4.5.</p><p>We now investigate the consequence of the above results on the trust-region radius at each minimization level.</p><p>Lemma 4.7. For every iteration (j, ), with j = 0, . . . , r and ≥ 0, we have that</p><formula xml:id="formula_78">Δ j, ≥ γ 1 min Δ s min , κ 2 g j , Δ j Δ j+1,q , (4.41)</formula><p>where (j + 1, q) = π(j, ). Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php S. GRATTON, A. SARTENAER, AND P. L. TOINT Proof. Consider the minimization sequence at level j ≤ r initiated from iteration (j + 1, q), and assume, for the purpose of obtaining a contradiction, that iteration (j, ) is the first such that</p><formula xml:id="formula_79">Δ j, &lt; γ 1 min Δ s min , κ 2 g j , Δ j Δ j+1,q . (4.42) Note that, because Δ j &lt; 1 and γ 1 &lt; 1, Δ j,0 = min[Δ s j , Δ j+1,q ] ≥ min[Δ s min , Δ j Δ j+1,q ] &gt; γ 1 min Δ s min , κ 2 g j , Δ j Δ j+1,q ,</formula><p>which ensures that &gt; 0 and hence that Δ j, is computed by applying Step 6 of the algorithm at iteration (j, -1). Suppose now that Δ j, = Δ j+1,qx j,x j,0 j ; (4. <ref type="bibr" target="#b44">43)</ref> i.e., the second term is active in (2.17). Our definition of Δ r+1,0 = ∞ and (4.42) then ensure that j &lt; r. Then, using (4.15), the definition of γ 1 , and (4.42), we deduce that, for j &lt; r,</p><formula xml:id="formula_80">Δ j, ≥ Δ j+1,q -(1 -Δ j )Δ j+1,q = Δ j Δ j+1,q &gt; γ 1 Δ j Δ j+1,q &gt; Δ j, ,</formula><p>which is impossible. Hence (4.43) cannot hold, and we obtain from (2.17</p><formula xml:id="formula_81">) that Δ j, = Δ + j, -1 ≥ γ 1 Δ j, -1</formula><p>, where the last inequality results from <ref type="bibr">(2.16)</ref>. Combining this bound with (4.42) and (4.14), we deduce that</p><formula xml:id="formula_82">Δ j, -1 ≤ min Δ s min , κ 2 g j , Δ j Δ j+1,q ≤ min [Δ s min , κ 2 g j, -1 ] .</formula><p>Hence we may apply Corollary 4.5 and conclude that iteration (j, -1) is very successful and that Δ j, -1 ≤ Δ + j, -1 = Δ j, . As a consequence, iteration (j, ) cannot be the first such that (4.42) holds. This contradiction now implies that (4.42) is impossible, which completes the proof.</p><p>Thus trust-region radii are bounded away from zero by a level-dependent factor. We now verify that this factor may be made independent of the level.</p><p>Theorem 4.8. There exists a constant Δ min ∈ (0, min[Δ s min , 1]) such that</p><formula xml:id="formula_83">Δ j, ≥ Δ min (4.44)</formula><p>for every iteration (j, ).</p><p>Proof. Observe first that Lemma 4.7 ensures the bound</p><formula xml:id="formula_84">Δ r,k ≥ γ 1 min[Δ s min , κ 2 g r ] def = γ 1 μ (4.45)</formula><p>for all k ≥ 0, because we have assumed that the call to the uppermost level is made with an infinite trust-region radius. Note that μ ∈ (0, 1) because κ 2 and g r both belong to (0, 1). Suppose now that, for some iteration (j, ),</p><formula xml:id="formula_85">Δ j, &lt; γ r+2 1 ( Δ min ) r μ. (4.46)</formula><p>If j = r, this contradicts (4.45); hence 0 ≤ j &lt; r. Lemma 4.7 and the definition of μ in (4.45) then imply that min[ μ, Δ j Δ j+1,q ] &lt; γ r+1 1 ( Δ min ) r μ, Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where, as above, iteration (j + 1, q) = π(j,</p><formula xml:id="formula_86">). If min[ μ, Δ j Δ j+1,q ] = μ, then μ &lt; γ r+1 1 ( Δ min ) r μ, which is impossible because γ r+1 1 ( Δ min ) r &lt; 1. As a consequence, Δ j Δ j+1,q = min[ μ, Δ j Δ j+1,q ] &lt; γ r+1 1 ( Δ min ) r μ ≤ γ r+1 1 ( Δ min ) r-1 Δ j μ,</formula><p>because of (4.6), and hence</p><formula xml:id="formula_87">Δ j+1,q &lt; γ r+1 1 ( Δ min ) r-1 μ.</formula><p>This condition is entirely similar to (4.46) but one level higher. We may therefore repeat the reasoning at levels j + 1, . . . , r -1, yielding the bound</p><formula xml:id="formula_88">Δ r,k &lt; γ r+2-(r-j) 1 ( Δ min ) r-(r-j) μ = γ j+2 1 ( Δ min ) j μ &lt; γ 1 μ.</formula><p>But this last inequality contradicts (4.45), and we therefore deduce that (4.46) never holds. This proves (4.44) with</p><formula xml:id="formula_89">Δ min def = γ r+2 1 ( Δ min ) r min[Δ s min , κ 2 g r ],<label>(4.47)</label></formula><p>and the bounds 1 ∈ (0, 1), Δ min ∈ (0, 1), κ 2 ∈ (0, 1), and g r ∈ (0, 1) together imply that Δ min ∈ (0, min[Δ s min , 1]), as requested. This result must be compared to Theorem 6.4.3 on p. 135 of <ref type="bibr" target="#b12">[13]</ref>, keeping (4.14) in mind along with the fact that we have called the uppermost minimization level with some nonzero tolerance g r . Also note in (4.47) that Δ min is linearly proportional to g r for small enough values of this threshold. The next crucial step of our analysis is to show that the algorithm is well defined in that all the recursions are finite.</p><p>Theorem 4.9. The number of iterations at each level is finite. Moreover, there exists κ h ∈ (0, 1) such that, for every minimization sequence at level i = 0, . . . , r,</p><formula xml:id="formula_90">h i (x i,0 ) -h i (x i,p+1 ) ≥ τ i,p η i+1 1 κ h ,</formula><p>where τ i,p is the total number of successful iterations in p =0 T (i, ). Proof. We prove the desired result by induction on higher and higher levels from 0 to r. We start by defining ω i, to be the number of successful iterations in T (i, ), as well as the number of successful iterations in the set p =0 T (i, ):</p><formula xml:id="formula_91">τ i,p = p =0 ω i, . (4.48) Note that ω i, ≥ 1 if iteration (i, ) is successful.</formula><p>Consider first an arbitrary minimization sequence at level 0 (if any), and assume, without loss of generality, that it belongs to R(r, k) for some k ≥ 0. Every iteration in this minimization sequence must be a Taylor iteration, which means that every successful iteration in the sequence satisfies</p><formula xml:id="formula_92">h 0 (x 0, ) -h 0 (x 0, +1 ) ≥ η 1 κ red g 0 min g 0 κ H , Δ min ≥ ω 0, η 1 κ red g min min g min κ H , Δ min , (4.49)</formula><p>where we have used (2.9), (4.14), (4.2), Theorem 4.8, (4.6), and the fact that ω 0, = 1 for every successful iteration (0, ) because T (0, ) = {(0, )}. Since we know from Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php S. GRATTON, A. SARTENAER, AND P. L. TOINT Lemma 4.6 that there is at least one such iteration for every minimization sequence, we may sum the objective decreases at level 0 and obtain from (4.49) that h 0 (x 0,0 )h 0 (x 0,p+1 ) = p =0 (S) [h 0 (x 0, )h 0 (x 0, +1 )] ≥ τ 0,p η 1 κ h , <ref type="bibr">(4.50)</ref> where the sum with superscript (S) is restricted to successful iterations and where</p><formula xml:id="formula_93">κ h def = κ red g min min g min κ H</formula><p>, Δ min ∈ (0, 1). (4.51)</p><p>If r = 0, we know that h 0 = f is bounded below by assumption, and (4.50) implies that τ 0,p must be finite. If r &gt; 0, our assumption that f 0 is continuous implies that h 0 is also continuous and hence bounded below on the set {x ∈ n0 | xx 0,0 0 ≤ Δ r,k }. The relation (4.50), Lemma 4.1, and (4.8) therefore again impose the finiteness of τ 0,p . Since τ 0,p accounts for all successful iterations in the minimization sequence, we obtain that there must be a last finite successful iteration (0, 0 ). If the sequence were nevertheless infinite, this would mean that every iteration (0, ) is unsuccessful for all &gt; 0 , causing Δ j, to converge to zero, which is impossible in view of Theorem 4.8. Hence the minimization sequence is finite. The same reasoning may be applied to every such sequence at level 0. Now consider an arbitrary minimization sequence at level i (again, without loss of generality, within R(r, k) for some k ≥ 0) and assume that each minimization sequence at level i -1 is finite and also that each successful iteration (i -1, u) in every minimization sequence at this lower level satisfies</p><formula xml:id="formula_94">h i-1 (x i-1,u ) -h i-1 (x i-1,u+1 ) ≥ ω i-1,u η i 1 κ h , (4.52)</formula><p>which is the direct generalization of (4.49) at level i-1. Consider a successful iteration (i, ), whose existence is ensured by Lemma 4.6. If it is a Taylor iteration (i.e., if (i, ) ∈ T (i, )), we obtain as above that</p><formula xml:id="formula_95">h i (x i, ) -h i (x i, +1 ) ≥ η 1 κ h ≥ η i+1 1 κ h = ω i, η i+1 1 κ h (4.53)</formula><p>since η 1 ∈ (0, 1) and ω i, = 1 for every successful Taylor iteration. If, on the other hand, iteration (i, ) uses Step 2, then, assuming x i-1, * = x i-1,t+1 , we obtain that</p><formula xml:id="formula_96">h i (x i, ) -h i (x i, +1 ) ≥ η 1 [h i-1 (x i-1,0 ) -h i-1 (x i-1, * )] = η 1 t u=0 (S) [h i-1 (x i-1,u ) -h i-1 (x i-1,u+1 )].</formula><p>Observing that ω i, = τ i-1,t , (4.52) and (4.48) then give that</p><formula xml:id="formula_97">h i (x i, ) -h i (x i, +1 ) ≥ η i+1 1 κ h t u=0 ω i-1,u = τ i-1,t η i+1 1 κ h = ω i, η i+1 1 κ h . (4.54)</formula><p>Combining (4.53) and (4.54), we see that (4.52) again holds at level i instead of i -1. Moreover, as above,</p><formula xml:id="formula_98">h i (x i,0 ) -h i (x i,p+1 ) = p =0 (S) [h i (x i, ) -h i (x i, +1 )] ≥ τ i,p η i+1</formula><p>1 κ h (4.55) Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php for the minimization sequence including iteration (i, ). If i = r, h i = f is bounded below by assumption and (4.55) imposes that the number of successful iterations in this sequence must again be finite. The same conclusion holds if i &lt; r, since h i is continuous and hence bounded below on the set {x ∈ ni | xx i,0 i ≤ Δ r,k }, which contains x i,p+1 because of Lemma 4.1 and (4.8). As for level 0, we may then conclude that the number of iterations (both successful and unsuccessful) in the minimization sequence is finite. Moreover, the same reasoning holds for every minimization sequence at level i, and the induction is complete.</p><p>A first remarkable consequence of this theorem is an upper bound on the number of iterations needed by the trust-region algorithm to reduce the gradient norm at level r below a given threshold value.</p><p>Theorem 4.10. Assume that one knows a constant f low such that h r (x r ) = f (x) ≥ f low for every x ∈ n . Then Algorithm RMTR needs at most</p><formula xml:id="formula_99">f (x r,0 ) -f low θ( g min )</formula><p>successful Taylor iterations at any level to obtain an iterate x r,k such that g r,k ≤ g r , where</p><formula xml:id="formula_100">θ( ) = η r+1 1 κ red min κ H , γ r+2 1 ( Δ min ) r min[Δ s min , κ 2 ] .</formula><p>Proof. The desired bound directly follows from Theorem 4.9, (4.51), (4.47), and the definition of g min . (To keep the expression manageable, we have refrained from substituting the value of κ 2 from (4.28) and, in this value, that of κ 1 from (4.16), all these values being independent of .)</p><p>Of course, the bound provided by this theorem may be very pessimistic and not all the constants in the definition of θ( ) may be known in practice, but this loose complexity result is nevertheless theoretically interesting as it applies to general nonconvex problems. One should note that the bound is in terms of iteration numbers, and implicitly accounts only for the cost of computing a Taylor step satisfying (2.9). Theorem 4.10 suggests several comments.</p><p>1. The bound involves the number of successful Taylor iterations, that is, successful iterations where the trial step is computed without resorting to further recursion. This provides an adequate measure of the linear algebra effort for all successful iterations, since successful iterations using the recursion of Step 2 cost little beyond the evaluation of the level-dependent objective function and its gradient. Moreover, the number of such iterations is, by construction, at most equal to r times that of Taylor iterations (in the worst case, where each iteration at level r includes a full recursion to level 0 with a single successful iteration at each level j &gt; 0). Hence the result shows that the number of necessary successful iterations, all levels included, is of order 1/ 2 for small values of . This order is not qualitatively altered by the inclusion of unsuccessful iterations either, provided we replace the very successful trust-region radius update (top case in (2.16)) by</p><formula xml:id="formula_101">Δ + i,k ∈ [Δ i,k , γ 3 Δ i,k ] if ρ i,k ≥ η 2</formula><p>for some γ 3 &gt; 1. Indeed, Theorem 4.8 imposes that the decrease in radius caused by unsuccessful iterations must asymptotically be compensated for by Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php an increase at successful iterations, irrespective of the fact that Δ min depends on by <ref type="bibr">(4.47)</ref>. This is to say that, if α is the average number of unsuccessful iterations per successful one at any level, then one must have that γ 3 γ α 2 ≥ 1 and therefore that α ≤log(γ 3 )/ log(γ 2 ). Thus the complexity bound in 1/ 2 for small is modified by a constant factor only if all iterations (successful and unsuccessful) are considered. This therefore also gives a worst-case upper bound on the number of function and gradient evaluations. 2. This complexity bound is of the same order as the corresponding bound for the pure gradient method (see <ref type="bibr">[31, p. 29]</ref>). This is not surprising given that it is based on the Cauchy condition, which itself results from a step in the steepest-descent direction. 3. The bound involves the number of successful Taylor iterations summed up on all levels (as a result of Theorem 4.9). Thus successful such iterations at cheap low levels decrease the number of necessary expensive ones at higher levels, and the multiscale algorithm requires (at least in the theoretical worst case) fewer Taylor iterations at the upper level than the single-level variant. This provides theoretical backing for the practical observation that the structure of multiscale unconstrained optimization problems can be used to advantage. 4. The constants involved in the definition of θ( ) do not depend on the problem dimension but rather on the properties of the problem (r, κ H , κ σ ) or of the algorithm itself (κ red , κ g , γ 1 , η 1 , η 2 , Δ min , Δ s min ). If we consider the case where different levels correspond to different discretization meshes and make the mild assumption that r and κ H are uniformly bounded above and that κ σ is uniformly bounded below, we observe that our complexity bound is mesh-independent. A second important consequence of Theorem 4.9 is that the algorithm is globally convergent in the sense that it generates a subsequence of iterates whose gradients converge to zero if run with g r = 0. Proof. We first observe that the sequence of iterates {x r,k } generated by the algorithm called with g r = 0 is identical to that generated as follows. We consider, at level r, a sequence of gradient tolerances { g r,j } ∈ (0, 1) monotonically converging to zero, start the algorithm with g r = g r,0 , and slightly alter the mechanism of Step 5 (at level r only) to reduce g r from g r,j to g r,j+1 as soon as g r,k+1 ≤ g r,j . The calculation is then continued with this more stringent threshold until it is also attained, g r is then again reduced, and so on. Since Δ r+1,0 = ∞, each successive minimization at level r can stop at iteration k only if g r,k+1 ≤ g r,j . (4.57) Theorem 4.9 then implies that there are only finitely many successful iterations between two reductions of g r . We therefore obtain that for each g r,j there is an arbitrarily large k such that (4.57) holds. The desired result then follows immediately from our assumption that { g r,j } converges to zero. The interest of this result is mostly theoretical, since most practical applications of Algorithm RMTR consider a nonzero gradient tolerance g r . Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>The reader may have noticed that our theory still applies when we modify the technique described at the start of Corollary 4.11 by allowing a reduction of all the g i to zero at the same time, 4 instead of merely reducing the uppermost one. If this modified technique is used, and assuming the trust region becomes asymptotically inactive at every level (as is most often the case in practice), each minimization sequence in the algorithm becomes infinite (as if it were initiated with a zero gradient threshold and an infinite initial radius). Recursion to lower levels then remains possible for arbitrarily small gradients and may therefore occur arbitrarily far in the sequence of iterates. Moreover, we may still apply Corollary 4.11 at each level and deduce that, if the trust region becomes asymptotically inactive, lim inf k→∞ g i,k = 0 (4.58) for all i = 0, . . . , r.</p><p>As is the case for single-level trust-region algorithms, we now would like to prove that the limit inferior in (4.56) (and possibly (4.58)) can be replaced by a true limit, while still allowing recursion for very small gradients. We start by deriving a variant of Theorem 4.9 that does not assume that all gradient norms remain above some threshold to obtain a measure of the predicted decrease at some iteration (i, k).</p><p>Lemma 4.12. There exists a constant κ 3 ∈ (0, 1) such that, for all (i, k) such that g i,k &gt; 0,</p><formula xml:id="formula_102">δ i,k ≥ κ red η r 1 γ r 1 κ r g g i,k min [ Δ s min , κ 3 g i,k , Δ i,k ] . (4.59) Proof. Consider iteration (i, k). If it is a Taylor iteration, then, if we set κ 3 = min κ r g κ H , κ 2 κ r g = κ 2 κ r g ∈ (0,<label>1</label></formula><p>), (4.60) (4.59) immediately follows from (2.9), (4.2), and the bounds κ g ∈ (0, 1), η 1 ∈ (0, 1), and γ 1 ∈ (0, 1). Otherwise, define the iteration (j, ) (with j &lt; i) to be the deepest successful iteration in R(i, k) such that g j,0 = g</p><formula xml:id="formula_103">j,1 = • • • = g j, = R j+1 . . . R i g i,k</formula><p>and such that all iterations (j + 1, t j+1 ), (j + 2, t j+2 ), . . . , up to (i -1, t i-1 ) of the path from (j, ) to (i, k), are successful (meaning that iterations (j, u) are unsuccessful for u = 0, . . . , -1, if any, and that iterations (p, u) are also unsuccessful for p = j + 1, . . . , i -1 and u = 0, . . . , t p -1, if any). Note that such a path is guaranteed to exist because of Lemma 4.6. Using the first part of (2.14), we then obtain that</p><formula xml:id="formula_104">g j,0 = g j,1 = • • • = g j, = R j+1 . . . R i g i,k ≥ κ r g g i,k &gt; 0. (4.61) If = 0, then Δ j, = min[Δ s j , Δ j+1,tj+1 ] ≥ min[Δ s min , Δ j+1,tj+1 ]. (4.62)</formula><p>If, on the other hand, &gt; 0, we know that iterations (j, 0) to (j, -1) are unsuccessful. Corollary 4.5 then implies that (4.28) cannot hold for iteration (j, -1), and thus that Proof. The proof is identical to that of Theorem 6.4.6 on p. 137 of <ref type="bibr" target="#b12">[13]</ref>, with (4.59) (with i = r) now playing the role of the sufficient model reduction condition AA.1 at level r.</p><formula xml:id="formula_105">Δ j, -1 &gt; min[ Δ s min , κ 2 g j, -1 ] = min[ Δ s min , κ 2 g j,0 ].</formula><p>This last result implies, in particular, that any limit point of the infinite sequence {x r,k } is first-order critical for problem (2.1). But we may draw stronger conclusions. If we assume that the trust region becomes asymptotically inactive at all levels and that all g i (i = 0, . . . , r -1) are driven down to zero together with g r (thus allowing recursion even for very small gradients), then, as explained above, each minimization sequence in the algorithm becomes infinite, and we may apply Theorem 4.13 to each of them, concluding that, if the trust region becomes asymptotically inactive, lim k→∞ g i,k = 0 for every level i = 0, . . . , r. The behavior of Algorithm RMTR is therefore truly coherent with its multiscale formulation, since the same convergence results hold for each level.</p><p>The convergence results at the upper level are unaffected if minimization sequences at lower levels are "prematurely" terminated, provided each such sequence contains at least one successful iteration. Indeed, Lemmas 4.1 and 4.2 do not depend on the actual stopping criterion used, and all subsequent proofs do not depend on it either. Thus, one might think of stopping a minimization sequence after a preset number of successful iterations: in combination with the freedom left at Step 1 to choose the model whenever (2.14) holds, this strategy allows a straightforward implementation of fixed lower-iterations patterns, like the V or W cycles in multigrid methods. This is what we have done in section 3.</p><p>Our theory also remains essentially unchanged if we merely insist on first-order coherence (i.e., conditions (2.5) and (2.6)) to hold only for small enough trust-region radii Δ i,k , or only up to a perturbation of the order of Δ i,k or g i,k Δ i,k . Other generalizations may be possible. Similarly, although we have assumed for motivation purposes that each f i is "more costly" to minimize that f i-1 , we have not used this feature in the theory presented above, nor have we used the form of the lower levels objective functions. In particular, our choice of section 3 to define f i as identically zero for i = 0, . . . , r -1 satisfies all our assumptions. Nonconstant prolongation and restriction operators of the form P i (x i,k ) and R i (x i,k ) may also be considered, provided the singular values of these operators remain uniformly bounded.</p><p>In its full generality, convergence to second-order critical points appears to be out of reach unless one is able to guarantee some "eigenpoint condition." Such a condition imposes that, if τ i,k , the smallest eigenvalue of H i,k , is negative, then</p><formula xml:id="formula_106">m i,k (x i,k ) -m i,k (x i,k + s i,k ) ≥ κ eip |τ i,k | min[τ 2 i,k , Δ 2 i,k ]</formula><p>for some constant κ eip ∈ (0, 1 S. GRATTON, A. SARTENAER, AND P. L. TOINT to critical points that satisfy second-order optimality conditions at the coarsest level. This results from the application of the Moré-Sorensen exact trust-region subproblem solver at that level, for which this property is well known (see section 6.6 of <ref type="bibr" target="#b12">[13]</ref>, for instance). The idea of imposing an eigenpoint condition at the coarsest level to obtain second-order criticality at that level is also at the core of the globalization proposal in <ref type="bibr" target="#b8">[9]</ref>, but it can be verified <ref type="bibr" target="#b22">[21]</ref> that this technique does not enforce second-order convergence at finer levels. However, imposing an eigenpoint condition at fine levels may be judged impractical: for instance, the SCM smoothing strategy described above does not guarantee such a condition but merely that</p><formula xml:id="formula_107">m i,k (x i,k ) -m i,k (x i,k + s i,k ) ≥ 1 2 |μ i,k |Δ 2 i,k ,</formula><p>where μ i,k is the most negative diagonal element of H i,k . This weaker result is caused by the fact that SCM limits its exploration of the model's curvature to the coordinate axes, at variance with the TCG and GLTR methods, which implicitly construct Lanczos approximations to Hessian eigenvalues. Convergence to fine-level first-order critical points satisfying a weak version of second-order optimality can, however, be expected in this case. In particular, the diagonal elements of the objective function's Hessian have to be nonnegative at such limit points (see <ref type="bibr" target="#b22">[21]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comments and perspectives.</head><p>We have defined a class of recursive trustregion algorithms whose members are able to exploit cheap lower-level models in a multiscale optimization problem. This class has been proved to be well defined and globally convergent to first order; preliminary numerical experience suggests that it may have strong potential. We have also presented a theoretical complexity result giving a bound on the number of iterations that are required by the algorithms of our class to find an approximate critical point of the objective function within prescribed accuracy. This last result also shows that the total complexity of solving an unconstrained multiscale problem can be shared amongst the levels, exploiting the structure to advantage.</p><p>Although the example of discretized problems has been used as a major motivation for our work, this is not the only case where our theory can be applied. We think, in particular, of cases where different models of the true objective function might live in the same space but involve different levels of complexity and/or cost. This is of interest, for instance, in a number of problems arising from physics, like data assimilation in weather forecasting <ref type="bibr" target="#b14">[15]</ref>, where different models may involve different levels of sophistication in the physical modeling itself. More generally, the algorithms and theory presented here are relevant in most areas where simplified models are considered, such as multidisciplinary optimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> or PDE-constrained problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>We may also think of investigating even more efficient algorithms combining the trust-region framework developed here with other globalization techniques, like linesearches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b40">39]</ref>, nonmonotone techniques <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b45">44]</ref>, or filter methods <ref type="bibr" target="#b21">[20]</ref>. While this might add yet another level of technicality to the convergence proofs, we expect such extensions to be possible and the resulting algorithms to be of practical interest.</p><p>Another important research direction is to investigate what kinds of Hessian (and possibly gradient) approximations are practically efficient within our framework, especially at the fine levels. Various options are possible, ranging from specialized finite differences to secant approximations.</p><p>Applying recursive trust-region methods of the type discussed here to constrained problems is another obvious avenue of research. Although we anticipate the associated Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php convergence theory to be again more technically difficult, intuition and limited numerical experience suggest that the power of such methods should also be exploitable in this case.</p><p>A number of practical issues related to Algorithm RMTR (such as alternative gradient smoothing and choice of cycle patterns) have not been discussed, although they may be crucial in practice. We investigate these issues in a forthcoming paper describing (so far encouraging) numerical experience with Algorithm RMTR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>Increment k by one and go to Step 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php (where we used (2.3) and (4.3) to deduce the second equality) andκ σ def = min 1, min i=0,...,r σ min (M i ) &gt; 0, (4.5)where σ min (A) denotes the smallest singular value of the matrix A. We finally define</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 4 . 11 .</head><label>411</label><figDesc>Assume that Algorithm RMTR is called at the uppermost level with g r = 0. Then lim inf k→∞ g r,k = 0. (4.56)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>php Algorithm 2.1: RMTR(i, x i,0 , g i,0 , Δ i+1 , g i , Δ i , Δ s i ) Step 0: Initialization. Compute</head><label></label><figDesc>Step 5 are Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.</figDesc><table><row><cell>v i = g i,0 -∇f i (x i,0 ) and h i (x i,0 ). Set Δ i,0 =</cell></row><row><cell>min[Δ s i , Δ i+1 ] and k = 0.</cell></row><row><cell>Step 1: Model choice. If i = 0 or if (2.14) fails, go to Step 3. Otherwise,</cell></row><row><cell>choose to go to Step 2 (recursive step) or to Step 3 (Taylor step).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Step 2: Recursive step computation. Call Algorithm RMTR</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>2 ,</head><label>2</label><figDesc>Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . 1</head><label>31</label><figDesc>Performance on the simple quadratic example. Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>Level</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>n</cell><cell>9</cell><cell>49</cell><cell>225</cell><cell cols="6">961 3,969 16,129 65,025 261,121 1,046,529</cell></row><row><cell># fine SCM</cell><cell>-</cell><cell>11</cell><cell>11</cell><cell>11</cell><cell>9</cell><cell>8</cell><cell>6</cell><cell>5</cell><cell>3</cell></row><row><cell>CPU(s)</cell><cell cols="4">-0.05 0.14 0.37</cell><cell>0.97</cell><cell>2.84</cell><cell>9.4</cell><cell>38.4</cell><cell>150.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 . 2</head><label>32</label><figDesc>Performance on the nonconvex example.</figDesc><table><row><cell>Level</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>n</cell><cell>18</cell><cell>98</cell><cell cols="6">450 1,922 7,938 32,258 130,050 522,242</cell></row><row><cell># fine SCM</cell><cell>-</cell><cell>21</cell><cell>19</cell><cell>21</cell><cell>28</cell><cell>32</cell><cell>14</cell><cell>9</cell></row><row><cell>CPU(s)</cell><cell cols="3">-0.43 1.05</cell><cell cols="2">3.60 14.90</cell><cell>73.63</cell><cell>151.53</cell><cell>560.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>k , Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where we successively used (4.23),</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /><note><p>2κ H since κ &lt; 1. Hence ρ j, ≥ η 2 and iteration (j, ) ∈ T (i, k) is very successful, as requested in item 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php All the elements are now in place to show that, if the algorithm is run with g r = 0, then gradients at level r converge to zero.</figDesc><table><row><cell cols="3">Theorem 4.13. Assume that Algorithm RMTR is called at the uppermost level</cell></row><row><cell>with g r = 0. Then</cell><cell></cell><cell></cell></row><row><cell>(4.66)</cell><cell>lim k→∞</cell><cell>g r,k = 0.</cell></row></table><note><p><p>4 </p>The ratios g i / g r could, for instance, be fixed or kept within prescribed bounds.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We are well aware that this creates some ambiguities, since a sequence of indices i, k can occur more than once if level i (i &lt; r) is used more than once, implying the existence of more than one starting iterate at this level. This ambiguity is resolved by the context. Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Observe that this is the only possible choice for i = 0. Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>See[12, p. 10], or[18, p. 510], or[33, p. 214]  amongst many others. Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>iteration (j, ) is very successful for every (j, ) ∈ V(i, k). Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>S. GRATTON, A. SARTENAER, AND P. L. TOINT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>) (see AA.2 in[13, p. 153]). This is easy to obtain at relatively coarse levels, where the cost of an eigenvalue computation or of a factorization remains acceptable. For instance, the algorithm considered in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_6"><p>section 3 is convergent Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors are indebted to Nick Gould for his comments on a draft of the manuscript and to Natalia Alexandrov for stimulating discussion.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S. GRATTON, A. SARTENAER, AND P. L. TOINT</head><p>But this inequality, (2.16), (2.17), the unsuccessful nature of the first iterations at level j, (4.61), and the bound γ 1 &lt; 1 then yield that</p><p>]. Combining this last inequality with (4.62), we conclude that, for ≥ 0,</p><p>]. Our choice of iteration (j, ) also ensures that the same reasoning can now be applied not only to iteration (j, ) but also to every iteration in the path (j + 1, t j+1 ), . . . , (i -1, t i-1 ), because the first part of (2.14) implies that</p><p>] for u = 0, . . . , ij -1 (where we identify t i = k for u = ij -1). We may then use these bounds recursively level by level and deduce that</p><p>because γ 1 &lt; 1. On the other hand, (j, ) ∈ T (i, k) by construction, and we therefore obtain from (2.9) and (4.2) that δ j, ≥ κ red g j, min g j, κ H , Δ j, . (4.64) Gathering now (4.61), (4.63), and (4.64), we obtain that</p><p>and thus, using (4.60), that</p><p>But the fact that all iterations on the path from (j, ) to (i, k) are successful also implies that</p><p>The bound (4.59) then follows from this last inequality and (4.65). Downloaded 01/02/13 to 138.26.31.3. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A trust region framework for managing the use of approximation models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Optimization</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="16" to="23" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An overview of first-order model management for engineering optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="413" to="430" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximation and model management in aerodynamic optimization with variable fidelity models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Gumbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Aircraft</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1093" to="1101" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Trust-Region Proper Orthogonal Decomposition for Flow Control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Sachs</surname></persName>
		</author>
		<idno>2000-25</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>NASA Langley Research Center</publisher>
			<pubPlace>Hampton, VA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Computer Applications in Science and Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interior point methods for a class of elliptic variational inequalities, in High Performance Algorithms and Software for Nonlinear Optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Marcia</surname></persName>
		</author>
		<editor>Biegler et al.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="218" to="235" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable Algorithms in Optimization: Computational Experiments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarich</surname></persName>
		</author>
		<idno>ANL/MCS-P1175-0604</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization (MA&amp;O) Conference</title>
		<meeting>the 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization (MA&amp;O) Conference<address><addrLine>Argonne, IL; Albany, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science, Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal low thrust trajectory to the moon</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Betts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Erb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Dyn. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="144" to="170" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">High Performance Algorithms and Software for Nonlinear Optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ghattas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<editor>Heinkenschloss, and B. Van Bloemen Waanders</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A globalisation strategy for the multigrid solution of elliptic optimal control problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="445" to="459" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Bramble</surname></persName>
		</author>
		<title level="m">Multigrid Methods, Longman Scientific and Technical</title>
		<meeting><address><addrLine>Harlow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-level adaptative solutions to boundary value problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="333" to="390" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multigrid</forename><surname>Tutorial</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>2nd ed., SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trust-Region Methods</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2000">2000</date>
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
	<note>MPS-SIAM Ser. Optim.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reduced order modelling approaches to PDE-constrained optimization based on proper orthogonal decomposition, in High Performance Algorithms and Software for Nonlinear Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sachs</surname></persName>
		</author>
		<editor>Biegler et al.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="268" to="281" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimization algorithms for variational data assimilation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECMWF Seminar on Recent Developments in Numerical Methods for Atmospheric Modelling</title>
		<meeting>the ECMWF Seminar on Recent Developments in Numerical Methods for Atmospheric Modelling<address><addrLine>Reading, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="364" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On multilevel iterative methods for optimization problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combination Trust-Region Line-Search Methods for Unconstrained Optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Gertz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics, University of California</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 01/02/13 to 138.26.31.3</idno>
		<ptr target="http://www.siam.org/journals/ojsa.phpCopyright©bySIAM" />
		<imprint/>
	</monogr>
	<note>Unauthorized reproduction of this article is prohibited</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Gratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sartenaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Solving the trust-region subproblem using the Lanczos method</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="504" to="525" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A filter-trust-region method for unconstrained optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sainvitu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="341" to="357" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Second-order convergence properties of trustregion methods using incomplete curvature information, with an application to multigrid optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sartenaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="676" to="692" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local convergence analysis for partitioned quasi-Newton updates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="429" to="448" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterative Solution of Large Sparse Systems of Equations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Sci</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<date type="published" when="1994">1994</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of a damped nonlinear multilevel method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reusken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multigrid approach to Euler equations</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multigrid Methods, Frontiers Appl. Math</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Mccormick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Siam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philadelphia</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="57" to="72" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Practical aspects of multiscale optimization methods for VLSI-CAD</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nash</surname></persName>
		</author>
		<editor>Multiscale Optimization and VLSI/CAD, J. Cong and J. R. Shinnerl</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="265" to="291" />
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Model problems for the multigrid optimization of systems governed by differential equations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1811" to="1837" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Terascale optimal PDE solvers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAM 2003 Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the use of directions of negative curvature in a modified Newton method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multigrid approach to discretized optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="99" to="116" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introductory Lectures on Convex Optimization, Appl. Optim</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining trust region and line search techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Nonlinear Programming</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="153" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Iterative Solution of Nonlinear Equations in Several Variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Rheinboldt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Fortran Subroutine for Unconstrained Minimization Requiring First Derivatives of the Objective Function</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<pubPlace>Harwell, Oxfordshire, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>AERE Harwell Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report R-6469</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A new algorithm for unconstrained optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
		<editor>Nonlinear Programming, J. B. Rosen, O. L. Mangasarian, and K. Ritter</editor>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="31" to="65" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic determination of an initial trust region in nonlinear programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sartenaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1788" to="1803" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The conjugate gradient method and trust regions in large scale optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Steihaug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="626" to="637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards an efficient sparsity exploiting Newton method for minimization</title>
		<author>
			<persName><forename type="middle">L</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse Matrices and Their Uses</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Duff</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="57" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">VE08AD, a routine for partially separable optimization with bounded variables</title>
		<author>
			<persName><forename type="middle">L</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harwell Subroutine Library</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-monotone trust-region algorithms for nonlinear optimization subject to convex constraints</title>
		<author>
			<persName><forename type="middle">L</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="69" to="94" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">U</forename><surname>Trottenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multigrid</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonmonotone trust-region methods for bound-constrained semismooth equations with applications to nonlinear mixed complementarity problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ulbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="889" to="917" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An Introduction to Multigrid Methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wesseling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Wiley and Sons</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Chichester, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Nonmonotone trust region methods with curvilinear path in unconstrained optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="303" to="317" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A multilevel nonlinear method</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yavneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dardyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="24" to="46" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
