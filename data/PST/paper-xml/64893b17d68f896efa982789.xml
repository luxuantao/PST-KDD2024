<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-13">13 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omkar</forename><surname>Thawkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abdelrahman</forename><surname>Shaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaji</forename><surname>Sahal</surname></persName>
						</author>
						<author>
							<persName><surname>Mullappilly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rao</forename><forename type="middle">Muhammad</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-13">13 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.07971v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The latest breakthroughs in large visionlanguage models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on taskspecific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate 217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https: //github.com/mbzuai-oryx/XrayGPT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Large-scale Vision-Language models have emerged as a transformative area of research at the intersection of computer vision and natural language processing, enabling machines to understand and generate information from both visual and textual modalities. These models represent a significant advancement in the field, bridging the gap between visual perception and language comprehension, and have demonstrated remarkable capabilities across various tasks, including but not limited to image captioning <ref type="bibr" target="#b5">(Hossain et al., 2019)</ref>, visual question answering <ref type="bibr" target="#b9">(Lu et al., 2023)</ref>, and visual commonsense reasoning <ref type="bibr" target="#b19">(Zellers et al., 2019)</ref>. Training these models requires vast amounts of image and text data, enabling them to learn rich representations that capture the intricacies of both modalities. Additionally, fine-tuning can be employed using task-specific data to better align the models with specific end tasks and user preferences. Recently, Bard and GPT-4 have demonstrated impressive capabilities in various tasks, raising excitement within the research community and industry. However, it is important to note that the models of Bard and GPT-4 are not currently available as open-source, limiting access to their underlying architecture and implementation details.</p><p>Recently, Mini-GPT <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref> demonstrates a range of impressive capabilities by aligning both vision and language models. It excels at generating contextual descriptions based on the given image. However, it is not as effective in medical scenarios due to the significant differences between medical image-text pairs and general web content. Adopting vision-text pre-training in the medical domain is a challenging task because of two factors: (1) Lack of data, Mini-GPT has trained the projection layer on a dataset of 5M image-text pairs, while the total number of publicly available medical images and reports is orders of magnitude below. (2) Different modalities and domains, while Mini-GPT may involve distinguishing between broad categories like "Person" and "Car" the distinctions within medical domains are much more subtle and fine-grained. For instance, differentiating between terms like "Pneumonia" and "Pleural Effusion" requires more precision by capturing and aligning crucial medical domain knowledge.</p><p>Chest radiographs are vital for clinical decisionmaking as they offer essential diagnostic and prognostic insights about the health of the patients. Text summarization tasks can partially address this challenge by providing meaningful information and summaries based on the given radiology reports. In our approach, we go beyond traditional summarization techniques by providing concise summaries that highlight the key findings and the overall impression based on the given X-ray. Additionally, our model allows for interactive engagement, enabling users to ask follow-up questions based on the provided answers. In this study, we stimulate the research around the automated analysis of chest radiographs based on X-ray images. Also, we argue that based on the visual and large language models, the majority of knowledge acquired during the pertaining stage of these models requires a domain-specific high-quality instruction set derived from task-specific data to achieve promising results. The main contributions of our work are:-? The LLM (Vicuna) is fine-tuned on medical data (100k real conversations between patients and doctors) and 20k radiology conversations to acquire domain-specific and relevant features.</p><p>? We generate interactive and clean summaries ( 217k) from free-text radiology reports of two datasets: MIMIC-CXR <ref type="bibr" target="#b6">(Johnson et al., 2019)</ref> and OpenI <ref type="bibr" target="#b3">(Demner-Fushman et al., 2015)</ref>. These summaries serve to enhance the performance of LLMs by fine-tuning the linear transformation layer on high-quality data.</p><p>? We align the frozen specialized medical visual encoder (MedClip) with a fine-tuned LLM (Vicuna), using a simple linear transformation to understand medical meanings and acquire remarkable visual conversation abilities.</p><p>? To advance the research in biomedical multimodal learning, we open-source our assets to the community: The codebase, the finetuned models, the high-quality instruction-set, and the raining recipe for data generation and model training are publically released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Medical Chatbot Medical chatbots have emerged as valuable tools in healthcare, providing personalized support, information, and assistance to patients and healthcare professionals. The recently introduced Chatdoctor <ref type="bibr" target="#b7">(Li et al., 2023</ref>) is a nextgeneration AI doctor model that is based on the LLaMA <ref type="bibr" target="#b15">(Touvron et al., 2023)</ref> model. The goal of this project is to provide patients with an intelligent and reliable healthcare companion that can answer their medical queries and provide them with personalized medical advice. After success of <ref type="bibr">ChatGPT (OpenAI, 2022)</ref>, <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and other open source LLM's <ref type="bibr" target="#b15">(Touvron et al., 2023;</ref><ref type="bibr" target="#b2">Chiang et al., 2023;</ref><ref type="bibr" target="#b14">Taori et al., 2023)</ref>, many medical chatbots were introduced recently such as Med-Alpaca <ref type="bibr" target="#b4">(Han et al., 2023)</ref>, PMC-LLaMA <ref type="bibr" target="#b17">(Wu et al., 2023)</ref>, and DoctorGLM <ref type="bibr" target="#b18">(Xiong et al., 2023)</ref>  <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref>, visual question answering <ref type="bibr" target="#b0">(Bazi et al., 2023;</ref><ref type="bibr" target="#b8">Liu et al., 2023;</ref><ref type="bibr" target="#b10">Muhammad Maaz and Khan, 2023)</ref>, and image generation <ref type="bibr" target="#b21">(Zhang and Agrawala, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>XrayGPT is an innovative conversational medical vision-language model specifically developed for analyzing chest radiographs. The core concept revolves around aligning medical visual and textual representations to enable the generation of meaningful conversations about these radiographs using our generated high-quality data. Our approach draws inspiration from the design of visionlanguage models in general, but with a specific focus on the medical domain. Due to the limited availability of medical image-summary pairs, we adopt a similar methodology by building upon a pre-trained medical vision encoder (VLM) and medical large language model (LLM), as our foun-dation. The fine-tuning process involves aligning both modalities using high-quality image-summary pairs through a simple transformation layer. This alignment enables XrayGPT to possess the capability of generating insightful conversations about chest radiographs, providing valuable insights for medical professionals. By leveraging pre-existing resources and fine-tuning specific components, we optimize the model's performance while minimizing the need for extensive training on scarce data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>We show in Fig. <ref type="figure" target="#fig_1">1</ref> an overview of our XrayGPT.</p><p>Given the X-ray, we align both visual features and textual information from a pre-trained medical vision encoder (VLM), and medical large language model (LLM). Specifically, we utilize Med-Clip <ref type="bibr" target="#b16">(Wang et al., 2022)</ref> as a visual encoder and our large language model (LLM) is built upon the recent Vicuna <ref type="bibr" target="#b2">(Chiang et al., 2023)</ref>. Given X-ray x ? R H?W ?C , the visual encoder is used to encode the image into embeddings using a vision encoder E img . Then, the raw embeddings are mapped to an output dimension of 512 using a linear projection head.</p><formula xml:id="formula_0">V p = f v (E img (x)) (1)</formula><p>where E img is the vision encoder, f v is the projection head.</p><p>To bridge the gap between image-level features and the language decoder's embedding space, we employ a trainable linear transformation layer, denoted as t. This layer projects the image-level features, represented by V p , into corresponding language embedding tokens, denoted as L v :</p><formula xml:id="formula_1">L v = t(v p ),<label>(2)</label></formula><p>We have two text queries in our overall architecture. The first query, denoted as ###Assistant, serves the purpose of determining the system role, which in our case is defined as "You are a helpful healthcare virtual assistant." The second text query, ###Doctor, corresponds to the prompt itself. To ensure consistency, both queries undergo tokenization, resulting in dimensions represented by L t . Finally, L v is concatenated with L t and fed into the medical LLM, Fine-tuned Vicuna, which generates the summary of the chest x-ray.</p><p>Our XrayGPT follows a two-stage training approach. In the first stage, we train the model using interactive summaries from MIMIC-CXR <ref type="bibr" target="#b6">(Johnson et al., 2019)</ref> reports. While in the second stage, we use the high-quality curated interactive summaries of OpenI <ref type="bibr" target="#b3">(Demner-Fushman et al., 2015)</ref> reports. MIMIC-CXR report findings contain information about patient history, which adds noise to the data. To mitigate this effect, we use small yet effective Interactive OpenI report summaries in the second stage to make our model robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image-text alignment</head><p>To align the generated high-quality summaries with the given x-ray, we use similar conversational format of the Vicuna <ref type="bibr" target="#b2">(Chiang et al., 2023)</ref> language model as follows:</p><formula xml:id="formula_2">###Doctor: X R X Q ###Assistant: X S</formula><p>where X R is the visual representation produced by the linear transformation layer for image X, X Q is a sampled question (e.g. What are the main findings and impression of the given X-ray?), and X S is the associated summary for image X. In this way, we curate image-text pairs with detailed and informative interactive summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Curating high-quality data</head><p>Datasets: The MIMIC-CXR consists of a collection of chest radiographs associated with free-text radiology reports. It consists of 377,110 images and 227,827 associated reports, which are used for both training and testing purposes. The dataset is de-identified by removing the health information to satisfy health insurance and privacy requirements. The OpenI dataset is a collection of chest X-ray images from the Indiana University hospital network, composing 6,459 images and 3,955 reports. High Quality and Interactive Summaries: To generate concise and coherent medical summaries from the unstructured reports, we perform the following pre-processing steps for both datasets: (1) Removal of incomplete reports lacking finding or impression sections. (2) Elimination of reports that have finding sections containing less than 10 words.</p><p>(3) Exclusion of reports with impression sections containing less than 2 words.</p><p>In addition, utilizing the power of gpt-3.5-turbo model, we further implement the following preprocessing techniques to ensure high-quality summaries per image: (1) Elimination of sentences containing comparisons to the patient's prior medical history. (2) Removal of de-defined symbols "__", while preserving the original meaning. (3) As our training relies on image-text pairs, we ex-</p><p>The chest x-ray shows bilateral lung hyperinflation with flattening of the thoracic spine. This may suggest emphysema or asthma. The heart is mildly enlarged and there is mild asymmetry of the hila with mild vascular congestion, indicative of early congestive cardiac failure (CCF). However, there are no signs of consolidation, pulmonary edema, pleural effusion or pneumothorax. The upper abdomen is normal, and there is no bony abnormality. The impression of the chest x-ray is mild CCF with flattening of the thoracic spine.  cluded the provided view from the summary. ( <ref type="formula">4</ref>) We combine the clean findings and impressions to generate an interactive and high-quality summary.</p><p>Following these steps, we obtained a set of filtered training reports consisting of 114,690 reports associated with 241k training images based on Mimic-CXR dataset. Also, we obtained 3,403 highquality summaries that used for training based on the OpenI dataset.</p><p>Here is an example before and after the proposed pre-processing. Input findings: PA and lateral views of the chest were provided demonstrating no focal consolidation, effusion, or pneumothorax. Cardiomediastinal silhouette appears normal and stable. There is a compression deformity involving a mid thoracic vertebral body, which appears new from the prior chest radiograph of ___. No free air below the right hemidiaphragm. There are tiny surgical clips in the left base of neck, likely indicating prior thyroid surgery. Input Impression: No acute intrathoracic process. Interval development of a mid thoracic spine compression fracture. Highquality and interactive summary: The chest x-ray findings reveal no evidence of focal consolidation, effusion, or pneumothorax. The cardiomediastinal silhouette appears stable and normal. There is a newly developed mid thoracic spine compression fracture but no free air below the right hemidiaphragm. The presence of surgical clips in the left base of the neck suggests prior thyroid surgery. The impression suggests that there is no acute intrathoracic condition detected in the x-ray aside from the new development of mid thoracic spine compression fracture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Stage-1 Training: In stage-1 training, the model is designed to gain understanding of how Xray image features and corresponding reports are interconnected by analysing a large set of image-text pairs. The result obtained from the injected projection layer is considered as a gentle cue for our medically tuned LLM model, guiding it to produce the appropriate report based on the finding and impression that match the given x-ray images. We use high quality interactive report summary as described in sec. In both stage-1 and stage-2 of training, we utilize predetermined prompts in the given format: ###Doctor: &lt;Img&gt;&lt;ImageFeature&gt;&lt;/Img&gt; &lt;In-struction&gt; ###Assistant:</p><p>Here, &lt;Instruction&gt; refers to a randomly selected instruction from our pre-established set of instructions, which includes different forms of instructions like "Describe the given chest x-ray image in <ref type="bibr">detail."</ref> or "Are there any potential complications or risks associated with the observed abnormalities in this chest x-ray image? or the x-ray is normal." or "Is the overall impression provided by this chest x-ray image normal or abnormal? Answer based on the observed findings.". Similar to the baseline <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref>, we do not compute the regression loss for this particular text-image prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We used the Rogue Score as an evaluation metric to compare the contribution of our components over the baseline <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref>. Rogue score has been commonly used <ref type="bibr" target="#b1">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b11">Nallapati et al., 2017;</ref><ref type="bibr" target="#b20">Zhang et al., 2020)</ref> to assess the quality of generated text, particularly in the field of natural language processing and text generation. It measures the overlap between the generated text and a set of reference texts, typically generated by human experts. The Rogue Score calculates precision, recall, and F1-score, taking into account the presence and ordering of n-grams (contiguous sequences of words). A higher Rogue Score indicates a better alignment between the generated text and the reference texts, indicating a higher level of quality and coherence. The Rogue Score serves as a valuable quantitative measure to objectively compare and assess the performance of different text generation models and techniques.</p><p>GPT-based evaluation of LMM (Language Model Mediated) generated text refers to the use of GPT (Generative Pre-trained Transformer) models to assess the quality and coherence of text generated by LMM approaches. LMM combines the power of pre-trained language models, such as GPT, with explicit control mechanisms to guide text generation. GPT-based evaluation involves using a fine-tuned GPT model to generate a set of reference texts, which can then be compared with the LMM-generated texts. Metrics such as perplexity, BLEU score, or the recently proposed Self-BLEU score can be used to quantitatively evaluate the similarity between the reference and generated texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Measures</head><p>In this section, we highlight a key contribution of our XrayGPT compared to our baseline <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref>. We conduct quantitative evaluation using ad-  <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref> using Rogue score on MIMIC-CXR <ref type="bibr" target="#b6">(Johnson et al., 2019)</ref> Test set. All results are reported by progressively adding our components in the baseline Minigpt-4 <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref>. Our approach outperforms the recent Minigpt-4 with an absolute gain of 19% in terms of R-1 score. Best results are in bold.</p><formula xml:id="formula_3">Method R-1 R-2 R-L Baseline 0.</formula><p>vanced metrics such as Rogue score and GPT-based evaluation as described in sec. 5.2. Tab. 1 shows comparison of our key components when progressively integrated into our baseline <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref> frame. From Tab. 1 our XrayGPT (row 4) has a significant improvement of 19% over the state-ofthe-art baseline <ref type="bibr" target="#b22">(Zhu et al., 2023)</ref> on the MIMIC-CXR test set. Also, we did LLM's based evaluation by asking ChatGPT model to choose "which response is closer to reference between baseline vs XrayGPT" where our model scored 82% compared to baseline 6% showing the superiority of our XrayGPT for radiology-specific summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Examples</head><p>We performed an extensive evaluation of our xrayGPT on a various Chest Xray samples where our model not only provides detailed findings and impression for given chest xray but also shows the conversation capability like radiologist. Please refer Figure <ref type="figure">2</ref> to 5 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To conclude, we present XrayGPT, a novel conversational medical vision-language model that combines both modalities to analyze and answer questions about chest radiographs. By aligning these models and leveraging our proposed interactive summaries from free-text radiology reports, XrayGPT demonstrates exceptional visual conversation abilities grounded in a deep understanding of chest radiographs in terms of findings and impressions. We open-source demos, models, and instruction sets. This work opens up new possibilities for advancing the automated analysis of chest radiographs and enhances the performance of large language models in the medical context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chest X-Ray Diagnosis</head><p>Describe the given chest x-ray image as detailed as possible.</p><p>The chest x-ray shows a normal cardiomediastinal silhouette and well-aerated lungs without focal consolidation, pleural effusion, or pneumothorax. The upper abdomen appears normal with no bony abnormality identified. The impression is that there is no acute cardiopulmonary process identified.</p><p>Figure <ref type="figure">2</ref>: XrayGPT: Delivering Comprehensive and Succinct Findings and Impressions for Every X-ray Image. Our XrayGPT provides detailed radiology specific summary as well as concise findings and impression for given xray image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chest X-Ray Diagnosis</head><p>Is there any sign of pneumonia in given chest x-ray ?</p><p>The chest x-ray shows bilateral lung hyperinflation with flattening of the thoracic spine. This may suggest emphysema or asthma. The heart is mildly enlarged and there is mild asymmetry of the hila with mild vascular congestion, indicative of early congestive cardiac failure (CCF). However, there are no signs of consolidation, pulmonary edema, pleural effusion or pneumothorax. The upper abdomen is normal, and there is no bony abnormality. The impression of the chest x-ray is mild CCF with flattening of the thoracic spine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What further diagnosis and treatment do you recommend based on given x-ray?</head><p>The findings are compatible with early congestive cardiac failure, but this cannot be confirmed until additional cardiac imaging studies, such as an echocardiogram or stress nuclear test, are performed. Depending on the presence of heart failure indicators, such as cardiac enzyme elevation, chest pain, shortness of breath and fatigue, the need for further treatment will be determined. If heart failure indicators are not present, and the chest x-ray is taken in a resting state, then the patient may be observed without treatment. However, if any new or worsening symptoms occur, then the patient should be assessed with further diagnostic tests and/or medical or interventional therapy. If there is fluid overload, diuretics may be considered. Otherwise, the management should focus on optimizing heart failure indicators, optimizing cardiac disease management, such as coronary artery disease and arrhythmia, and avoiding other potential contributing factors, such as fluid overload, hypertension, diabetes, renal disease, infection, etc.   The chest x-ray shows bibasilar opacity with overlying edema, indicating pulmonary edema. There is also a moderate left-sided pleural effusion. The heart size and cardiomediastinal contour are unremarkable. Based on these findings, the impression is pulmonary edema with pleural effusion. The patient may also have underlying pulmonary disease.</p><p>Based on findings, does the x-ray have any serious issues ? Yes, it shows pulmonary edema with pleural effusion.</p><p>Suggest treatment to cure pleural effusion.</p><p>The best way to start treatment is to discuss the pleural effusion with a general physician, since the underlying cause may be infectious and could require removal via thoracoscopic technique. A chest CT scan is recommended to confirm the effusion, and to determine if it is superimposed upon other issues that could be causing the effusion. Please consult your physician to determine the best treatment strategy for your condition. What medical treatment do you suggest for pulmonary edema ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chest X-Ray Diagnosis</head><p>The treatment is contingent on underlying diagnosis. In case of congestive heart failure, diuretics or other cardiac medications may be prescribed along with nebivolol or amlodipine to control cardiac hypertension. In case of ischemic heart disease, nitrates or other cardiac vasodilators may be prescribed. In case of coronary artery disease, stent placement or bypass surgery may be indicated. In case of pulmonary edema caused by atrial fibrillation or other arrhythmias, rate control should be done. If the patient is stable and pulmonary edema is improving, conservative medical therapy may be continued with nebivolol or amlodipine, nitrates or diuretics. If the patient is unstable, angioplasty or bypass surgery may be indicated. In case of ischemic heart disease, angioplasty or bypass surgery may be indicated if angiography reveals new coronary blocks or other coronary disease. In case of pulmonary edema caused by atrial fibrillation or other arrhythmias, rate control should be done before starting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>#</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our XrayGPT framework. The input X-ray is passed sequentially to three components. (1) Frozen medical visual encoder to extract relevant features pertaining to the chest diagnosis. (2) Leanable linear transformation layer to align the medical visual features with the Medical LLM together to learn extensive medical visual-text alignment. (3) Frozen Medical LLM to generate a detailed summary of the X-ray based on the encoded features and the given prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 of MIMIC-CXR (Johnson et al., 2019) train set with 213,514 image text pairs for training. During training the model trained for 320k total training steps with a total batch size of 128 using 4 AMD MI250X (128GB) GPUS. Stage-2 Training: In stage-2 training, the pretrained stage-1 model is enforced to gain radiology specific summary of how xray image features by examining set of highly curated image-text pairs from OpenI dataset. As a result, our medically tuned LLM can produce more natural and high quality radiology specific responses of given chest Xray images. We use high quality interactive report summary as described in sec. 4 from OpenI (Demner-Fushman et al., 2015) set with 3k image text pairs for training. During training the model trained for 5k total training steps with a total batch size of 32 using single AMD MI250X (128GB) GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: XrayGPT: Empowering Users with Concise Insights -Explore the Comprehensive Analysis Derived from the xray Image. Our XrayGPT provides a brief explanation for users quaries based on the overall findings from the given xray image.</figDesc><graphic url="image-18.png" coords="6,109.04,589.88,76.32,91.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: XrayGPT: Unlocking the Power of Precision, Providing Specific Key Details in Response to User Queries. Our XrayGPT is able to answer specific key detail in response to user queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Chest X-Ray DiagnosisAre there any visible indications of enlargement or abnormalities in the patient's lymph nodes in this chest x-ray image?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: XrayGPT: The Conversational AI Revolutionizing Radiological Interactions. Our XrayGPT has radiological conversational capabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: XrayGPT: Medical Treatment Recommendation. Our XrayGPT has the capability to suggest treatment based on the diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our XrayGPT components with Baseline</figDesc><table><row><cell></cell><cell>1313 0.0221 0.0879</cell></row><row><cell>+ MedCLIP</cell><cell>0.1517 0.0308 0.0973</cell></row><row><cell cols="2">+ MedVicuna 0.2099 0.0551 0.1284</cell></row><row><cell cols="2">+ RadVicuna 0.3213 0.0912 0.1997</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visionlanguage model for visual question answering in medical imagery</title>
		<author>
			<persName><forename type="first">Yakoub</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Rahhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laila</forename><surname>Bashmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansour</forename><surname>Zuair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioengineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">380</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<title level="m">Neural summarization by extracting sentences and words</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vicuna: An opensource chatbot impressing gpt-4</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>with 90%* chatgpt quality</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">B</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonya</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laritza</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocv080</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>L?ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keno</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08247</idno>
		<title level="m">Medalpaca-an open-source collection of medical conversational ai models and training data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName><forename type="first">Ferdous</forename><surname>Md Zakir Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohd</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Fairuz Shiratuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Laga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CsUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">R</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ying</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Horng</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0322-0</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Scientific Data</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Q2atransformer: Improving medical vqa via an answer querying decoder</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01611</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale feature extraction and fusion of image and text in vqa</title>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengtong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Video-chatgpt: Towards detailed video understanding via large vision and language models</title>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<idno>ArXiv 2306.05424</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Chatgpt</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">Medclip: Contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14454</idno>
		<title level="m">Pmc-llama: Further finetuning llama on medical papers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Doctorglm: Fine-tuning your chinese doctor is not a herculean task</title>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01097</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
