<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Flexpad: Highly Flexible Bending Interactions for Projected Handheld Displays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jürgen</forename><surname>Steimle</surname></persName>
							<email>steimle@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Jordt</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Christian-Albrechts-Universität Kiel</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
							<email>pattie@media.mit.edu</email>
							<affiliation key="aff2">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Flexpad: Highly Flexible Bending Interactions for Projected Handheld Displays</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8F7C8BAAFB7877045C6ACF958010DC7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Flexible display</term>
					<term>handheld display</term>
					<term>tracking</term>
					<term>projection</term>
					<term>depth camera</term>
					<term>deformation</term>
					<term>bending</term>
					<term>volumetric data H5.2. User interfaces: Graphical user interfaces (GUI), Input devices and strategies, Interaction styles</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Flexpad is an interactive system that combines a depth camera and a projector to transform sheets of plain paper or foam into flexible, highly deformable, and spatially aware handheld displays. We present a novel approach for tracking deformed surfaces from depth images in real time. It captures deformations in high detail, is very robust to occlusions created by the user's hands and fingers, and does not require any kind of markers or visible texture. As a result, the display is considerably more deformable than in previous work on flexible handheld displays, enabling novel applications that leverage the high expressiveness of detailed deformation. We illustrate these unique capabilities through three application examples: curved cross-cuts in volumetric images, deforming virtual paper characters, and slicing through time in videos. Results from two user studies show that our system is capable of detecting complex deformations and that users are able to perform them quickly and precisely.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Projecting visual interfaces onto movable real-world objects has been an ongoing area of research, e.g. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b11">11]</ref>. By closely integrating physical and digital information spaces, they leverage people's intuitive understanding of how to manipulate real-world objects for interaction with computer systems. Based on inexpensive depth sensors, a stream of recent research presented elegant trackingprojection approaches for transforming real-world objects into displays, without requiring any instrumentation of these objects <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b11">11]</ref>. None of these approaches, however, interpret the deformation of flexible objects.</p><p>Flexible deformation can expand the potential of projected interfaces. Deformation of everyday objects allows for a surprisingly rich set of interaction possibilities, involving many degrees of freedom, yet with very intuitive interaction: people bend pages in books, squeeze balls, model clay, and fold origami, to cite only a few examples. Adding deformation as another degree of freedom to handheld displays has great potential to add to the richness and expressiveness of interaction.</p><p>We present Flexpad, a system that supports highly flexible bending interactions for projected handheld displays. A Kinect depth camera and a projector form a cameraprojection unit that lets people use blank sheets of paper, foam or acrylic of different sizes and shapes as flexible displays. Flexpad has two main technical contributions: first, we contribute an algorithm for capturing even complex deformations in high detail and in real time. It does not require any instrumentation of the deformable handheld material. Hence, unlike in previous work that requires markers, visible texture, or embedded electronics <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>, virtually any sheet at hand can be used as a deformable projection surface for interactive applications. Second, we contribute a novel robust method for detecting hands and fingers with a Kinect camera using optical analysis of the surface material. This is crucial for robust captur- ing of deformable surfaces in realistic conditions, where the user occludes significant parts of the surface while deforming it. Since the solution is inexpensive, requiring only standard hardware components, it can be envisioned that deformable handheld displays become widespread and common.</p><p>The highly flexible handheld display creates unique novel application possibilities. Prior work on immobile flexible displays has shown the effectiveness of highly detailed deformations, e.g. for the purposes of 2.5D modeling and multi-dimensional navigation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b7">7]</ref>. Yet, the existing inventory of interactions with flexible handheld displays <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b18">18]</ref> is restricted to only low detail deformations, mostly due to the very limited flexibility of the displays and limited resolution of deformation capturing. Flexpad significantly extends the existing inventory by adding highly flexible and multi-dimensional deformations.</p><p>We present three application examples that each leverage rich and expressive, highly flexible deformations. The first application supports exploration and analysis of volumetric data sets (Fig. <ref type="figure" target="#fig_0">1 a,</ref><ref type="figure">b</ref>). We show how highly flexible displays support intuitive and effective exploration of the volumetric data set by allowing the user to easily define curved cross-sections, to get an overview, and to compare contents.</p><p>The second application enables children to deform and animate 2D characters (Fig <ref type="figure" target="#fig_0">1 c</ref>). A third application shows how detailed deformation can be used for slicing through time in videos (Fig. <ref type="figure" target="#fig_0">1 d</ref>). These applications demonstrate the utility of Flexpad and introduce transferrable ideas for future handheld devices that use active flexible displays.</p><p>To evaluate the feasibility of our approach, both with respect to technology and human factors, we conducted two evaluation studies. They show that the tracking provides accurate results even for complex deformations and confirm that people can perform them fast and precisely.</p><p>The remainder of this paper is structured as follows. After reviewing related work, we present the concept and implementation of Flexpad. This is followed by an overview of applications. Finally, we present the results of two evaluation studies and conclude with an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK Deformation Capturing</head><p>Our work relates to a body of research that realizes handheld displays using optical capturing of the projection surface and spatially aligned projection, either with a static or a handheld projector, e.g. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Many methods exist for capturing the pose and deformation of a mobile projection surface -information required for accurate texture mapping. Existing methods include passive or active markers, stereo cameras, structured light, time of flight, analysis of visible texture via features <ref type="bibr" target="#b36">[36]</ref>, color distances <ref type="bibr" target="#b6">[6]</ref> or optical flow <ref type="bibr" target="#b14">[14]</ref> as well as analysis of the object's contour <ref type="bibr" target="#b8">[8]</ref>. The commercial availability of inexpensive depth cameras has recently stimulated a lot of re-search that produces a detailed geometrical model of a realworld scene for enabling novel forms of HCI. LightSpace <ref type="bibr" target="#b37">[37]</ref> uses several depth cameras and projectors to augment walls and tables for touch and gesture-based interaction with projected interfaces. KinectFusion <ref type="bibr" target="#b16">[16]</ref> automatically creates a very detailed model of a static scene, enabling physics-based interactions on and above surfaces and objects. Omnitouch <ref type="bibr" target="#b11">[11]</ref> uses a depth sensor to support multitouch input on projected interfaces on virtually any noninstrumented surface. However, none of these approaches model real-time deformation of objects. Very recent work <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b5">5]</ref> uses proxy particles to model deformable objects, allowing for a range of physics-based interactions that are based on collision and friction forces. However, tracking particles over a sequence of frames requires optical flow, which is not possible with untextured surfaces. Other work <ref type="bibr" target="#b17">[17]</ref> captures deformable surfaces from Kinect depth data, but the approach does not support capturing detailed deformations in real time, which is critical for interactive applications. We present an approach that is capable of capturing the pose and detailed deformation of a deformable surface from depth data in real-time, to support very fine-grained interactions and natural deformations. Due to a novel detection of hands and fingers, our approach is very robust to occlusions, which is crucial when users deform objects in natural settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flexible Display Interfaces</head><p>Our research is also influenced by existing work on deformation-based handheld interfaces. Here, two streams of work can be distinguished:</p><p>First, prior work proposed using a deformable tape or sheet as an external input device, to control GUI applications by deformation. Output is given on a separate, inflexible display. An early and influential work is ShapeTape <ref type="bibr" target="#b2">[2]</ref> that allows users to create and modify 3D models by bending a tape that is augmented by a deformation sensor. Other applications use a deformable sheet for controlling windows on the computer desktop <ref type="bibr" target="#b10">[10]</ref> and for AR purposes <ref type="bibr" target="#b26">[26]</ref>.</p><p>A second type of research, more directly related to our work, studies deformation of handheld displays. Prior work has investigated interactions with interfaces of flexible smart phones and e-readers <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b20">20]</ref> and for volumetric datasets <ref type="bibr" target="#b22">[22]</ref>. Most of this work uses projection or simulates flexible displays with existing inflexible displays; PaperPhone <ref type="bibr" target="#b23">[23]</ref> and Kinetic <ref type="bibr" target="#b20">[20]</ref> use an active flexible display. Lee et al. <ref type="bibr" target="#b25">[25]</ref> elicited user-defined gestures for flexible displays. A recent study <ref type="bibr" target="#b21">[21]</ref> examined how device stiffness and the extent of deformation influenced deformation precision. While no pronounced influence was identified, users preferred softer rather than rigid materials.</p><p>This stream of work represents an important step towards fully flexible displays, yet existing work is limited by quite restricted deformability. It supports only slight bending and captures only simple deformations, which ultimately limits the expressiveness of deformation to mostly one-dimensional interactions. We contribute novel interactions that extend the inventory of interactions by leveraging more flexible deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLEXPAD OVERVIEW</head><p>Flexpad enables users to interact with highly flexible projected displays in interactive real-time applications. The setup consists of a Kinect camera, a projector, and a sheet of paper, foam or acrylic that is used as projection surface.</p><p>Our current setup is illustrated in Fig. <ref type="figure">2</ref>. A Kinect camera and a full HD projector (ViewSonic Pro 8200) are mounted to the ceiling above the user, next to each other, and are calibrated to define one coordinate space. The setup creates an interaction volume of approximately 110 x 55 x 43 cm within which users can freely move, rotate, and deform the projection surface. The average resolution of the projection is 54 dpi. The user can either be seated at a table or standing. The table surface is located at a distance of 160 cm from the camera and the projector. Our implementation uses a standard desktop PC with an Intel i7 processor, 8 GB RAM, and an AMD Radeon HD 6800 graphics engine. While we currently use a fixed setup, the Kinect camera could be shoulder-worn with a mobile projector attached to it, as presented in <ref type="bibr" target="#b11">[11]</ref>, allowing for mobile or nomadic use.</p><p>The key contribution of Flexpad is in its approach for markerless capture of a high-resolution 3D model of a deformable surface, including its pose and deformation (see section about the model below). The system is further capable of grabbing 2D image contents from any window in Microsoft Windows and projecting it correctly warped onto the deformable display. Alternatively, it can use 3D volume data as input for the projection. The detailed model of deformation enables novel applications that make use of varied deformations of the display. We will present some example applications below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flexible display materials</head><p>Sheets of many different sizes and materials can be used as passive displays, including standard office paper. In our experiments, we used letter-sized sheets of two materials that have demonstrated good haptic characteristics:</p><p>The fully flexible display is 2 mm thick white foam (Darice Foamies crafting foam). It allows the user to change between different deformations very easily and quickly. The bending stiffness index S b * of this material was identified as 1.12 Nm 7 /kg 3 , which is comparable to the stiffness of 160g/m 2 paper. (S b * = S b /w 3 is the bending stiffness nor-malized w.r.t. the sheet's basis weight w in kg/m 2 <ref type="bibr" target="#b27">[27]</ref>.) The display's weight is 12 grams.</p><p>The shape-retaining display retains its deformation even when the user is not holding it. It consists of 2 mm thick white foam with Amalog 1/16" armature wire attached to its backside. It allows for more detailed modeling of the deformation, for easy modifications, as well as for easy holding. To characterize its deformability, we measured the minimal force required to permanently deform the material. We affixed the sheet at one side; a torque as small as 0.1Nm is enough to permanently deform the material, which shows that the material is very easy to deform. The sheets' weight is 25g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLEXPAD IMPLEMENTATION</head><p>Existing image based tracking algorithms of deformable surfaces <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b14">14]</ref> rely on visible features. In this section, we present an approach that requires only image data from a Kinect depth sensor. This allows for tracking an entirely blank (or arbitrarily textured) deformable projection surface in high detail. Moreover, since the depth sensor operates in the infrared spectrum, it does not interfere with the visible projection, no matter what content is projected.</p><p>A challenge of depth data is that it does not provide reliable image cues on local movement, which renders traditional tracking methods useless. The solution to this problem is a more global, model-based approach: We introduce a parameterized, deformable object model that is fit into the depth image data by a semi-global search algorithm, accounting for occlusion by the user's hands and fingers.</p><p>In the following, we explain 1) the removal of hands and fingers from the input data, 2) the global deformation model and 3) how a global optimization method is used to fit the deformation model to the input data in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Surface Material Analysis for Hand and Finger Detection</head><p>While the user is interacting with the display, hands and fingers partially occlude the surface in the depth image. For a robust deformation tracking, it is important to classify these image pixels as not belonging to the deformable surface, so the model is not fit into wrong depth values. The proposed tracking method is able to handle missing data, so the solution lies in removing those occluded parts from the input images in a preprocessing step.</p><p>Due to the low resolution of the Kinect depth sensor it is difficult to perform a shape-based classification of fingers and hands at larger distances. In particular, when the finger or the flat hand is touching the surface, the resolution is insufficient to differentiate it reliably from the underlying surface. Existing approaches use heuristics that work reliably as long as the back of the hand is kept at some distance above the surface <ref type="bibr" target="#b11">[11]</ref>. These restrictions do not hold in our more general case, where people use their hands and fingers right on the surface for deforming it. We introduce optical surface material analysis as a novel method of distinguishing between skin and the projection surface using the Kinect sensor. It is based on the observation that different materials have different reflectivity and translucency properties. Some materials, e.g. skin, have translucency properties leading to subsurface scattering of light. This property causes the point pattern, which is projected by the Kinect, to blur. This reduces the image gradients in the raw infrared image, which is given by the Freenect driver (see Fig. <ref type="figure">3</ref>). The display surface, made out of paper, cardboard, foam or any other material with highly diffuse reflection, varies from skin in reflectivity as well as translucency; so low peak values or low gradient values provide a stable scheme to classify non-display areas.</p><p>The projector inside the Kinect has a constant intensity and the infrared camera has a constant shutter time and aperture. Hence, the brightness of the point pattern is constant for constant material properties and object distances and angles. As a consequence, the surface reflectivity of an object with diffuse material can be determined by looking at the local point brightness. However, because the brightness in the infrared image decreases with increasing distance, the local gradients and peak values have to be regarded with respect to the depth value of the corresponding pixel in the depth image. Pixels that cannot be associated to a dot of the pattern are not classified directly, but receive the same classification as the surrounding pixels by iterative filtering. Results from an evaluation, reported below, show that this classification scheme works reliably at distances up to 1.50 m from the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling the Deformable Surface</head><p>Without local movement cues in the image, such as feature movement or optical flow, and with incomplete depth data, it is important to formulate very precisely what the tracking method should find in the image, i.e. to define a deformation model. We present a model that is generic enough to approximate a wide variety of deformations, including complex ones, while still being efficient to calculate.</p><p>The model is based on a rectangular 25 x 25 vertex plane at the size of the actual display surface. Using 25 vertices per dimension proved to provide stable tracking results while keeping the computational costs small. We then define a set of 8 basic deformations, 4 diagonal and 4 axis-aligned bends, each deforming half the surface plane (Fig. <ref type="figure" target="#fig_2">4</ref> upper left). It is important to note that all of these basic deformations are combined by using weighted factors, such that the model supports complex curvatures. A drawback of this model is that it lacks the ability to shift the starting point of a deformation, e.g. for distinguishing between a smaller and a larger dog-ear. To add this ability to the deformation model while keeping it computationally efficient and robust through low dimensionality, an additional mapping function for the z components of the vertex position is applied after the deformed vertex positions are calculated. The mapping can change from a square-like function, via the identity function to a square root-like function. This parameterized function is applied to every zcomponent of each surface vertex to change the shape of the deformation. This largely increases the set of deformations in the model by only adding one additional deformation parameter.</p><p>In summary, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the deformation model is a 15 dimensional vector, containing the angles of the 8 basic deformations, the z mapping parameter as well as 6 variables for the degrees of freedom that are required for the affine 3D transformations. Figure <ref type="figure" target="#fig_2">4</ref> (right) gives some examples of the complex deformations the model supports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-time Calculation</head><p>Given this deformation model, the overall tracking goal can be defined as finding the parameters of the model that describe an object that, when synthesized as a depth image, matches the input depth image best. Such approaches are known as Analysis by Synthesis (AbS) or direct tracking methods <ref type="bibr" target="#b17">[17]</ref>. The advantage of such methods is the ability to work completely without information on feature movement or optical flow. Such evaluation allows for formulating the tracking as an optimization problem, turning the tracking task into a mathematically well-defined objective of finding the parameter vector that produces the most suitable deformation.</p><p>To determine the deformation parameters, the AbS approach synthesizes the depth measurements of the 25x25 pixels to which the model vertices would be projected and compares the vertex-camera distance to the real input depth image. Thereby, areas that were identified as skin by the occlusion handling step are ignored. The average difference over all vertices provides an error value that examines the quality of the parameter guess to model the deformed object in the image. Due to the uneven measurements and the noise in the Kinect image, this error function contains many local minima, making it intractable for common local least squares optimization. Hence, we apply the CMA-ES optimization scheme <ref type="bibr" target="#b29">[29]</ref> to find an approximation of the optimum since CMA-ES combines two important properties: it is a global optimization algorithm, which allows for coping with large numbers of local minima, and it applies a smart distribution scheme minimizing the necessary number of function evaluations to find the optimum.</p><p>The optimization process is initialized at the first frame with a vertex set describing a planar surface of the display size and shape, placed at the center in the depth image. In the beginning of each following frame, the optimization is initialized with the optimization result of the preceding frame. This allows for initially finding the sheet not only at this position, but also at different locations and orientation within the instrumented volume. Should the depth error of the optimization result rise above a given limit, it is most likely that the tracking failed, e.g. because the user has removed the deformable sheet from the camera's viewport. In this case, the initialization step is automatically executed again, until the depth error falls below this limit.</p><p>To further increase the performance, our approach leverages the fact that the user cannot make high precision deformations while moving the object quickly, but only when the object is held steadily or slowly moved. The number of iterative optimization steps per frame is automatically adjusted by the pose difference of the preceding frames. During large movements, the number of iterations is reduced (on our hardware, 50 iterations yield 25 fps). During very slow movements, the number is increased, smoothly reducing the frame rate (200 iterations yield 8 fps), allowing for the highest possible accuracy of deformation capturing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our approach allows for robust real-time tracking of a large variety of deformations with a variety of deformable surfaces. However, due to the real-time constraint and restrictions of the depth sensor, several limitations apply. Obviously, it is only possible to track deformations that can be observed directly by the Kinect camera. Folding, which occludes large parts of an object, as well as very steep bending angles that occlude parts of the object to the camera cannot be handled by the presented system. For the cost of installing and calibrating additional Kinect sensors and projectors, this limitation can be overcome. Moreover, the presented deformation model makes a trade-off between tracking stability and the set of detectable deformations. Hence, very sharp bends, in particular folds, and stretchable materials cannot be reliably captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPLICATION EXAMPLES</head><p>Flexpad supports many application and interaction possibilities. Its unique strength is the high flexibility of the display and the detailed capturing of its deformation. A direct 1:1 mapping between the deformed physical shape and virtual model space enables novel applications with expressive interactions that were not possible with rigid or slightly flexible handheld displays. It is worth emphasizing that all proposed interactions also transfer to future active displays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring and Analyzing Volumetric Datasets</head><p>Analysis of volumetric images and datasets is important in many fields, such as medicine (CT and MRI scans), geology and earth sciences (atmospheric and submarine datasets).</p><p>Prior research has shown that curved cross-sections are required for analyzing important medical phenomena <ref type="bibr" target="#b31">[31]</ref>.</p><p>We contribute interactions that significantly go beyond existing work on visualization of volumetric datasets, which support only planar cross-cuts <ref type="bibr" target="#b35">[35]</ref>, only very restricted curvatures <ref type="bibr" target="#b22">[22]</ref> or make curvature very hard to control <ref type="bibr" target="#b31">[31]</ref>.</p><p>Our application maps the volumetric dataset to a 3D volume in physical space and lets the user slice through that volume with the display. By deforming the highly flexible display, the user can easily create non-planar cross-sections to analyze a large variety of curved structures that would not be visible on planar or only slightly deformed crosssections. Figures <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_3">5</ref> illustrate several examples: the spine (Fig. <ref type="figure" target="#fig_0">1a</ref>) and the pelvis (Fig. <ref type="figure" target="#fig_3">5a</ref>). For detailed analysis of a cross-section, the user can lock the view by pressing a foot button. When locked, the display can be moved and deformed without affecting the visualization (Fig. <ref type="figure" target="#fig_3">5b</ref>). For instance, this allows for flattening the view and easy measurement of distances that were on a curve in the original view. It also allows for handing the view over to a colleague. It is worth noting that this application is not only helpful for medical experts, but also supports non-experts who are interested in exploring the inner workings of the human body in an intuitive way. In addition to following a layer, deformation also proves powerful in supporting curved cuts across layers. Consider the example of a reservoir engineer exploring the best locations for a new well in an oil field. The engineer can select the desired curvature of the well (nowadays many wells are curved) by bending the display and then skim through the entire volume.</p><p>Deformation also supports better orientation and overview in the dataset. For instance, one part of the display remains at a visually salient feature while the user curves the other part to explore the surrounding area in the dataset (Fig. <ref type="figure" target="#fig_0">1b</ref>). Furthermore, deformation of the highly flexible display provides an intuitive way of comparing data across layers. By deforming the display into a wave bend, the user sepa- Our proposed interactions can be easily integrated with functionality for cross-cuts of volumetric datasets that have been presented in prior work, such as additional views on external displays <ref type="bibr" target="#b35">[35]</ref>, panning and zooming, or saving of 2D snapshots for subsequent analysis and presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animating Virtual Paper Characters</head><p>To demonstrate the wide applicability of highly deformable handheld displays, we next present an application for children that leverages deformation as a simple and intuitive means for animating paper characters in an interactive picture. By deforming the display and moving it in space (Fig. <ref type="figure" target="#fig_0">1c</ref> and<ref type="figure">6a</ref>), the creature becomes animated. High-resolution deformation allows very individualized and varied animation patterns. Once animated, the creature is released into the virtual world of the animated picture (Fig. <ref type="figure">6 b</ref>). For example, fish may move with different speeds and move their fins. A sea star may lift some of its tentacles. A sea worm may creep on the ground. A flatfish may meander with sophisticated wave-form movements and seaweed may slowly bend in the water.</p><p>Such rich deformation capabilities can be easily combined with concepts from previous research on animating paper cut-outs <ref type="bibr" target="#b4">[4]</ref> that address skeleton animation, eye movement, and scene lighting. Also 3D models could get animated by incorporating as-rigid-as-possible deformation <ref type="bibr" target="#b33">[33]</ref>. It is worth noting that our concept can be readily deployed with standard hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slicing through Time in Videos</head><p>A third application of Flexpad enables people to create slices through time in videos, inspired by the ingenious Khronos projector <ref type="bibr" target="#b7">[7]</ref>. In contrast to Khronos, which required a very bulky, several cubic meters big, immobile setup with a fixed screen, Flexpad brings similar functionality to virtually any sheet of paper. The user can load a video from YouTube, which is automatically mapped to a virtual volume, whereby time is mapped to the z dimension. By moving and deforming the display within this volume, the user creates ever new combinations of different moments in time that open up new artistic perspectives on the video. Figure <ref type="figure">7</ref> depicts an example, in which a tree is "moved" in time to glow in the sunset. However, this functionality is best illustrated in the video that accompanies this paper.</p><p>In contrast to the fixed display of Khronos, the flexible handheld display allows for defining a curvature, i.e. a time gradient, and then move this gradient to different areas within one frame or across frames. Moreover, the handheld display allows for selecting any frame of the video to start with deforming, whereas deformations in Khronos always start at the topmost frame, going in only one direction. This allows for novel, expressive interactions with videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>To evaluate the feasibility of the Flexpad approach, we conducted two evaluations, examining both system performance and the users' ability of precise interaction with highly deformable displays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking Performance</head><p>To evaluate the precision of tracking in a natural real-time scenario, we recruited 10 volunteer participants (5f, 5m, median age 26). Their task was to use the slicing-throughtime application and freely create interesting renderings by deforming the display. This application is particularly well suited for a technical evaluation of the system, for the unstructured nature of the interface stimulates users to deform the display in highly varied, complex ways. We recorded the raw video stream from the Kinect camera while the user performed the task, overall 35 minutes of footage. We used this data as an input for our algorithm, calculating the RMS error in each frame from the distance between the model surface to the corresponding depth image values. The average RMS error over all 20,217 frames is 6.47 mm (SD 3.33 mm). This shows that the tracking performs very adequately even in challenging realistic tasks.</p><p>To test the abilities of the tracking method in a more distinct way, we additionally picked a set of 8 deformations, ranging from simple bends to complex deformations. Each deformation was recorded 20 times at 90 and 150 cm distance to the Kinect camera. Figure <ref type="figure">8</ref> shows the deformations and the average RMS error for each deformation.</p><p>In addition, we evaluated the preprocessing step, which removes hands and fingers from the input data, in an informal study. The Kinect data of hands from 10 users (aged 25 to 54) occluding three different display materials (paper, card board, plastic) was recorded. Analysis showed that independently of skin color, the preprocessing step correctly removed over 99% of the skin pixels without the need to adjust the thresholds for reflection or translucency. Figure <ref type="figure" target="#fig_5">9</ref> shows data from six users and the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Performance of Deformation</head><p>For the feasibility of the Flexpad approach, it is essential that users are able to perform deformations with the display with ease and sufficiently high precision. To evaluate these human factors, we conducted a controlled experiment with users. Our aim was to evaluate how fast and precise users are able to perform deformations and to examine the influence of flexible vs. shape-retaining display materials. Our hypothesis was that the task completion time increases with the complexity of the deformation and the level of precision required. Moreover, we hypothesized that the shaperetaining material would increase task completion times. We aimed at quantifying this influence. As such, our study adds to an emerging body of experiments on the manipulation of paper-like displays, including target acquisition performance in 3D space with rigid paper displays <ref type="bibr" target="#b34">[34]</ref> and performance of deforming slightly flexible displays <ref type="bibr" target="#b21">[21]</ref>.</p><p>We recruited 10 volunteer participants (5f, 5m, all righthanded, median age 26). The two participants who performed the task fastest were offered a $10 gift card. The experiment consisted of a series of trials that required the participant to deform the display surface as quickly as possible to match a given target shape within a specific precision level (see below). A perspective rendering of the target shape was displayed before each trial. During the trial, a real-time visualization on the display guided the participant on how to deform it to match the shape; to match the shape, all red areas had to turn green (Fig. <ref type="figure" target="#fig_0">10</ref>). The trial was solved when the target shape was held during 250ms. The completion time for each trial was measured. After each trial, the participant had to rate the perceived difficulty on a five-point Likert scale.</p><p>We selected five deformation classes: two one-sided deformations, which are oriented with the corner (Fig. <ref type="figure">8a</ref>) and oriented with the edge (Fig. <ref type="figure">8b</ref>), and three two-sided ones: center (Fig. <ref type="figure">8c</ref>), symmetric wave (Fig. <ref type="figure">8d</ref>) and asymmetric wave (Fig. <ref type="figure">8e</ref>). Each of these classes contains not only the basic deformation, but all variations of it, on all four sides of the display, and each also flipped vertically. This set of deformations was informed by the deformations made in our application examples, and for reasons of feasibility of this study, focused on deformations along one dimension in landscape format. From each deformation class, we randomly selected 2 deformations for each participant.</p><p>Each deformation had to be performed with two precision levels: +/-8 degrees and +/-6 degrees. In a pre-study with 5 participants, we identified these precision levels as challenging, but still feasible, whereas a level of +/-4 degrees turned out not to be reliably reachable. The sequence of trials was randomized. To compare influences of the display material, participants performed the tasks with the fully flexible and with the shape-retaining display, as described in the system overview section. The order was counterbalanced. In summary, the within-subject, multifactorial experimental design was as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">deformation classes x 2 instances per class x 2 precision levels x 2 materials x 10 participants = 400 trials</head><p>Before the experiment, participants could practice the tasks with both materials until they felt fully confident. After the experiment, participants freely explored a medical dataset in the volumetric application as well as the slicing-throughtime application. This was followed by a semi-structured interview. Each session lasted approximately 1 hour.  We first performed a multifactorial repeated measures ANOVA with all factors, including rotation and vertical flip of the deformations. We found no significant effect of rotation and vertical flip; therefore, in the following analyses, we group the trials into the five basic deformations, regardless of their rotation and vertical flip.</p><p>To identify systematic dependencies, we performed a r-ANOVA with the factors deformation, precision level, and material. Significant main effects were found for deformation (F (4,36) = 16.13; p &lt; 0.001), precision level (F (1,9) = 13.17; p = 0.005) and material (F (1,9) = 8.86; p = 0.016). As expected, more complex deformations, a higher precision level, and the shape-retaining material resulted in higher task performance time. On average, completion time of trials that were performed with the shape-retaining material was 33% longer than of trials with the fully flexible material. When the higher precision level was required, completion time of trials was 46% longer than of trials performed at the lower precision level.</p><p>To further analyze differences between the five deformation classes, we conducted post-hoc tests with Bonferroniadjusted alpha-levels of .01. Results showed highly significant differences between: edge and center (mean difference = 8.77; p = .004), edge and asymmetric wave (MD = 9.36; p = .006); corner and center (MD = 8.53; p = .001), as well as corner and asymmetric wave (MD = 9.12; p = .004). This shows a distinctive difference between deformations requiring bending only one versus both sides. The symmetric wave deformation does not differ statistically from both groups due to a highly material-dependent behavior, which is visible in Fig. <ref type="figure" target="#fig_7">11</ref>.</p><p>Simple (edge or corner) deformations had an average task completion time of 3.67 seconds (SD = 1.72). Even the most difficult deformations, in high precision level, resulted in an average task performance time of 15.6 seconds (SD = 9.54). The average rating of all trials was 2.06 (SD = 0.88).</p><p>Analyzed separately for all combinations of deformation, precision level and material, the average ratings range from 1.2 to 3.1, i.e. from very easy to medium. Perceived difficulty and completion times of the trials showed a very high bivariate correlation (Pearson's r = .75, p &lt; .001). Therefore, our following analyses are based on the completion times, but transfer to the difficulty of the task.</p><p>We identified one exception to the general finding that the deformations were performed without difficulties: the asymmetric wave deformation in the higher precision level, performed with the shape-retaining material. While average time is in line with those of the other complex deformation, the standard deviation is much higher, as can be seen in Fig. <ref type="figure" target="#fig_7">11</ref> (rightmost column). Statistical testing using Grubbs' Test for outliers showed that this SD is indeed an outlier (G = 2.68, p = .005.) This high SD shows that some participants were able to perform this deformation rather quickly, whereas other participants could only perform it reliably within a disproportionately long time.</p><p>As presented above, and in accordance with our hypothesis, the ANOVA found a significant effect of material. Because of the advantages of the shape-retaining material for complex deformations that were stated above, we will now analyze this effect in more detail for double-sided deformations (symmetric wave, center, and asymmetric wave).</p><p>The penalty on completion time in the lower precision level that is due to the shape-retaining material averages out at 95% with symmetric wave, 56% with center and 24% with asymmetric wave. For the higher precision level, the penalties introduced by the shape-retaining material are 77%, Precision +/-8 degrees Precision +/-6 degrees 21%, and 6%, for the three deformations respectively. Most notably, the penalty introduced by the shape-retaining decreases with increasing difficulty of the deformation and increasing level of precision. This relationship is illustrated by significant medium-sized negative correlations between the difficulty of the deformation (symmetric wave, center, asymmetric wave) and the penalty on completion time, separated for precision level (lower precision level: r = -.38, p = .018; higher precision level: r = -.39, p = .016). A similar result was obtained when difficulty of the deformation and level of precision combined were correlated with the penalty on completion time (r = -.39, p = .001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND CONCLUSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of study findings</head><p>The results of the controlled experiment show that users are capable of using highly flexible displays with ease. Users can make various single and dual deformations with both materials quickly and easily, even with a high precision level of +/-6 degrees. The single exception is the asymmetric dual deformation, which, under high precision level, some participants had difficulties performing reliably with the shape-retaining material. Therefore, we suggest for this deformation a lower bound of +/-8 degrees as the maximum precision if time is an issue in the application.</p><p>The results further showed that use of a shape-retaining material has only a modest average influence on task performance. While the fully flexible material is better suited for applications with only simple deformations, the shaperetaining material is well suited for use in applications with complex deformations. Notably, the time penalty introduced by the shape-retaining material decreases with increasing complexity of the deformation and the level of precision required. For the most complex deformation, it induces a temporal penalty of only 6 %, while it allows for easily holding deformations over time, for easily modifying them, and for modeling curvatures that involve more than two deformations. These quantitative findings are underpinned by qualitative observations. In the interviews, many participants commented that they preferred the fully flexible material for the task of the controlled experiment, whereas they preferred the shape-retaining material for analyzing curved phenomena in the medical dataset. Results from the technical evaluation show that our tracking approach performs with high accuracy to support all of the proposed applications. Even in a technically very challenging task that encouraged participants to make very complex arbitrary deformations, the average error was below 7 mm. These values can be further decreased in a mobile setup, where the Kinect camera is closer to the projection surface. Then, the average RMS error for many of the deformations is close to the random noise of the Kinect sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Touch input on deformable displays</head><p>A logical extension of Flexpad is touch input. With respect to this question, future work should address several challenges. Technically, this involves developing solutions for Kinect sensors to detect touch input reliably on real-time deformable surfaces. On a conceptual level and building on Dijkstra et al.'s recent work <ref type="bibr" target="#b9">[9]</ref>, this also involves an understanding of where users hold the display and where they deform it. This is required to identify areas that are reachable for touch input and to inform techniques that differentiate between touches stemming from desired touch input and false positives that result from the user touching the display while deforming it. An informal analysis from our study data showed that users made single deformations almost exclusively by touching the display close to the edges and on its backside. This also held true for dual deformations with the fully flexible display material. This suggests that touches in the center area of the display can be reliably interpreted as desired touch input. In contrast, the participants touched all over the display for dual deformations with the shape-retaining material. A simple spatial differentiation is not sufficient in this case; more advanced techniques need to be developed, for instance based on the shape of the touch point or on the normal force involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active flexible displays</head><p>While this work focuses on projected displays, all of our application examples transfer to future active flexible displays. Currently available prototypes are still very limited in their flexibility, so that they cannot be used to realize our concepts. Given the rapid advancements in display technology, this is very likely to change in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smart materials: programmable stiffness and stretchability</head><p>Future work should investigate smart materials for flexible displays. As discussed above, both fully flexible and shaperetaining displays have unique capabilities. A material that can programmatically switch between both of these states would combine all these advantages. Moreover, future work should examine handheld displays that, in addition to being deformable, are stretchable. This will further increase the expressiveness of interactions with flexible displays.</p><p>courtesy of Dan Heller. This work has partially been funded by the Cluster of Excellence Multimodal Computing and Interaction within the German Federal Excellence Initiative.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Application examples of Flexpad: Curved cross-cuts in volumetric images (a, b); animation of characters (c); slicing through time in videos (d)</figDesc><graphic coords="1,318.24,194.40,239.76,180.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Figure 3 :</head><label>23</label><figDesc>Fig. 2: Physical setup</figDesc><graphic coords="3,54.78,255.00,98.80,151.02" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dimensions of the deformation model (left) and examples of deformations it can express (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Exploring curved cross-sections (a), flattening the view (b), comparing contents across layers (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 : 7 :Figure 8 :</head><label>678</label><figDesc>Figure 6: Animating virtual paper characters Figure 7: Slicing through time in a video by deformation</figDesc><graphic coords="6,55.98,63.78,238.56,97.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Classification of skin. Top: depth input. Center: infrared input. Bottom: depth image after classification with skin parts removed.</figDesc><graphic coords="7,317.76,60.54,239.16,114.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 (</head><label>1011</label><figDesc>Figure 10: Interactive visualization during a trial</figDesc><graphic coords="7,54.78,603.12,122.16,85.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average trial completion time in seconds. Error bars show the standard deviation.</figDesc><graphic coords="8,87.72,69.06,219.42,156.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For instance, P3 commented: "The flexible one is easier to do the first assignment. But this one [the shape-retaining material] is easier to keep. It is more reliable. I can hold it even with just one hand. And I know that I gonna have what I want. It's way easier." P4 stated: "The less flexible material allows me to freeze a structure. (…) I don't have to bother with holding the shape." P7 commented: "I prefer the sensation of the flexible material, but the other one gives me more precision." In future work, we plan to perform a qualitative analysis of our video recordings to analyze which deformations participants spontaneously made with both materials.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We gratefully acknowledge Reinhard Koch, James D. Hollan and Simon Olberding for helpful discussions and feedback on earlier versions of this paper. We thank Clemens Winkler and Cassandra Xia for help with the video. The time lapse video used in one application was provided by</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">mm (1.7) RMS90: 2.67 mm (1.6) RMS150: 6.1 mm</title>
		<idno>4.2) RMS90: 4.58 mm (1.9) RMS150: 5.45 mm (2.7) RMS90: 4.82 mm (2.2) RMS150: 5.15 mm (2.5) RMS90: 4.93 mm (2.1) RMS150: 6.38 mm (3.8) RMS90: 5.39 mm (2.2) RMS150: 7.03 mm (4.1) RMS90: 2.41</idno>
		<imprint/>
	</monogr>
	<note>mm (1.2) RMS150: 4.56 mm (2.3) REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tilt displays: designing display surfaces with multi-axis tilting and actuation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MobileHCI&apos;12</title>
		<meeting>MobileHCI&apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring interactive curve and surface manipulation using a bend and twist sensitive input strip</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurtenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. I3D &apos;99</title>
		<meeting>I3D &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic shader lamps: Painting on movable objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. Augmented Reality</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video puppetry: a performative interface for cutout animation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia&apos;08</title>
		<meeting>SIGGRAPH Asia&apos;08</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MirageTable: freehand interaction on a projected augmented reality tabletop</title>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jota</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;12</title>
		<meeting>CHI &apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic deformable surface tracking from multiple videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV&apos;10</title>
		<meeting>ECCV&apos;10</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Khronos projector</title>
		<author>
			<persName><surname>Cassinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH&apos;05</title>
		<meeting>SIGGRAPH&apos;05</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multicue HMM-UKF for real-time contour tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating effects of structural holds on pointing and dragging performance with flexible displays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dijkstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;11</title>
		<meeting>CHI &apos;11</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards more paper-like input: flexible input devices for foldable interaction styles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Seniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;08</title>
		<meeting>UIST &apos;08</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OmniTouch: wearable multitouch interaction everywhere</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;11</title>
		<meeting>UIST &apos;11</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twend: twisting and bending as new interaction gesture in mobile devices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Herkenrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;08 Extended Abstracts</title>
		<meeting>CHI &apos;08 Extended Abstracts</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HoloDesk: direct 3d interactions with a situated see-through display</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;12</title>
		<meeting>CHI &apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realistic cloth augmentation in single view video under occlusions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hilsmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eisert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Paper windows: interaction techniques for digital paper</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;05</title>
		<meeting>CHI &apos;05</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">KinectFusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;11</title>
		<meeting>UIST &apos;11</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Direct model-based tracking of 3d object deformations in depth and color video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jordt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FoldMe: interacting with double-sided foldable displays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khalilbeigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lissermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kleine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steimle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TEI &apos;12</title>
		<meeting>TEI &apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interaction techniques for rollable displays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khalilbeigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lissermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mühlhäuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steimle</surname></persName>
		</author>
		<author>
			<persName><surname>Xpaaand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;11</title>
		<meeting>CHI &apos;11</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kinetic device: designing interactions with a deformable mobile interface</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kildal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paasovaara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aaltonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI 2012 Extended Abstracts</title>
		<meeting>CHI 2012 Extended Abstracts</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feeling it: the roles of stiffness, deformation range and feedback in the control of deformable UI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kildal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI&apos;12</title>
		<meeting>ICMI&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A handheld flexible display system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konieczny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'nardo</forename><surname>Colucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization 2005</title>
		<meeting>IEEE Visualization 2005</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PaperPhone: understanding the use of bend gestures in mobile devices with flexible electronic paper displays</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;11</title>
		<meeting>CHI &apos;11</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Foldable interactive displays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;08</title>
		<meeting>UIST &apos;08</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How users manipulate deformable displays as input devices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;10</title>
		<meeting>CHI &apos;10</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 3D flexible and tangible magic lens in augmented reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Looser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grasset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMAR &apos;07</title>
		<meeting>ISMAR &apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Habeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lyne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WUW -wear ur world: a wearable gestural interface</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;09 EA</title>
		<meeting>CHI &apos;09 EA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An evolution strategy with coordinate system invariant adaptation of arbitrary normal mutation distributions within the concept of mutative strategy parameter control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Genetic and Evolutionary Comp. Conf</title>
		<meeting>Genetic and Evolutionary Comp. Conf</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Illuminating clay: a 3-d tangible interface for landscape analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Piper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;02</title>
		<meeting>CHI&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring curved anatomic structures with surface sections</title>
		<author>
			<persName><forename type="first">L</forename><surname>Saroul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gummi: a bendable computer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schwesig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;04</title>
		<meeting>CHI &apos;04</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">As-rigid-as-possible surface modeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
		<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going beyond the surface: studying multi-layer interaction above the tabletop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;12</title>
		<meeting>CHI&apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PaperLens: advanced magic lens interaction above the tabletop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stellmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ITS&apos;09</title>
		<meeting>ITS&apos;09</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Template-free monocular reconstruction of deformable surfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision</title>
		<meeting>Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining multiple depth cameras and projectors for interactions on, above and between surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;10</title>
		<meeting>UIST &apos;10</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
