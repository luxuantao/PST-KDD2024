<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieval-Based Face Annotation by Weak Label Regularized Local Coordinate Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dayong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
							<email>jkzhu@zju.edu.cn..</email>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
						</author>
						<author>
							<persName><forename type="middle">J</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University Singapore</orgName>
								<address>
									<postCode>N4-02a-08, 639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<addrLine>RM 502, Cao Guangbiao Main Building, No. 38 Zheda Road, Yuquan Campus</addrLine>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<addrLine>Building 2, No. 5, Dan Ling Street, Haidian District</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Studies Building</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>611, 14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retrieval-Based Face Annotation by Weak Label Regularized Local Coordinate Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F2B326908B2E65EA1A5571D84145114</idno>
					<idno type="DOI">10.1109/TPAMI.2013.145</idno>
					<note type="submission">received 23 July 2012; revised 12 Jan. 2013; accepted 12 July 2013; published online 2 Aug. 2013. Recommended for acceptance by D. Forsyth.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face annotation</term>
					<term>content-based image retrieval</term>
					<term>machine learning</term>
					<term>label refinement</term>
					<term>web facial images</term>
					<term>weak label</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Auto face annotation, which aims to detect human faces from a facial image and assign them proper human names, is a fundamental research problem and beneficial to many real-world applications. In this work, we address this problem by investigating a retrieval-based annotation scheme of mining massive web facial images that are freely available over the Internet. In particular, given a facial image, we first retrieve the top n similar instances from a large-scale web facial image database using content-based image retrieval techniques, and then use their labels for auto annotation. Such a scheme has two major challenges: 1) how to retrieve the similar facial images that truly match the query, and 2) how to exploit the noisy labels of the top similar facial images, which may be incorrect or incomplete due to the nature of web images. In this paper, we propose an effective Weak Label Regularized Local Coordinate Coding (WLRLCC) technique, which exploits the principle of local coordinate coding by learning sparse features, and employs the idea of graph-based weak label regularization to enhance the weak labels of the similar facial images. An efficient optimization algorithm is proposed to solve the WLRLCC problem. Moreover, an effective sparse reconstruction scheme is developed to perform the face annotation task. We conduct extensive empirical studies on several web facial image databases to evaluate the proposed WLRLCC algorithm from different aspects. The experimental results validate its efficacy. We share the two constructed databases "WDB" (714,454 images of 6,025 people) and "ADB" (126,070 images of 1,200 people) with the public. To further improve the efficiency and scalability, we also propose an offline approximation scheme (AWLRLCC) which generally maintains comparable results but significantly reduces the annotation time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A UTO face annotation, which aims to detect human faces from a photo image and to tag the facial image with the human names, is a fundamental research problem and beneficial to many real-world applications. Typically, face annotation is formulated as an extended face recognition problem in which face classification models are trained from a collection of well-controlled labeled facial images using supervised machine learning techniques <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Recent studies <ref type="bibr" target="#b2">[3]</ref> have attempted to explore a promising retrieval-based face annotation (RBFA) paradigm for facial image annotation by mining the world wide web (WWW), where a massive number of weakly labeled facial images are freely available.</p><p>Generally, it is easy to construct a large-scale web facial image database by exploring web search engine with names as queries. All the returned images can be directly tagged with the query names. Due to the noisy nature of web images, the initial name labels of such a facial image database may be incorrect or incomplete without extra manual refinement effort. For example, for Google search engine the precision of the top 200 returned images is around 0.4 with "Nick Lachey" as a query <ref type="bibr" target="#b3">[4]</ref>. It is also extremely time consuming to manually label (tag) all the images with the correct names. In this work, we refer to this kind of web facial images with noisy/incomplete names as weakly labeled facial images. In the RBFA framework, given a query (test) facial image, first, we retrieve its top n similar instances from weakly labeled facial image database by exploiting content-based image retrieval (CBIR) techniques <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Second, we tag the query image with human names based on the weak (noisy) name labels of the top-ranking similar images. Such a paradigm is inspired by search-based general image annotation techniques because face annotation can be viewed as a special case of generic image annotation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, which has been extensively studied yet remains a challenging problem.</p><p>In general, there are two key problems for the RBFA technique: The first problem is how to efficiently retrieve a short list of the most similar facial images from a large facial image database for a query facial image. It typically relies on an effective CBIR solution. The recent work <ref type="bibr" target="#b5">[6]</ref> mainly addresses this problem in which an effective image representation technique is proposed for facial image retrieval by employing both the local and global features.</p><p>The second problem is how to effectively exploit the short list of candidate facial images and their weak label information for the face annotation task. This is critical because the associated labels of web facial images are noisy. To address this critical problem, we propose a novel Weak Label Regularized Local Coordinate Coding (WLRLCC) algorithm to boost the annotation performance by a unified learning scheme, which exploits the local coordinate coding (LCC) principle for learning more discriminative features and makes use of the graph-based regularization for enhancing the weak labels simultaneously. The main contributions of this paper are as follows:</p><p>. We propose a novel WLRLCC algorithm for the retrieval-based face annotation paradigm. . We conduct extensive experiments to evaluate the proposed algorithm for automated face annotation on two large-scale web facial image databases, which are also shared to facilitate related research for other researchers. . We propose an offline approximation scheme AWLRLCC to further reduce the running time without introducing much degradation of annotation performance. The remainder of this paper is organized as follows: Section 2 reviews related work. Section 3 briefly introduces the proposed RBFA framework. Section 4 presents the proposed WLRLCC scheme together with an effective face name annotation solution based on sparse reconstruction. Section 5 shows the offline approximation algorithm for WLRLCC. Section 6 introduces the construction of our weakly labeled retrieval database. Section 7 shows the experimental results of our empirical studies. Section 8 discusses the limitations, and Section 9 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is closely related to several groups of research work. The first group of related work is face recognition and verification, which are classic research problems in computer vision and pattern recognition and have been extensively studied for many years <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr">[12]</ref>. Although traditional face recognition methods can be extended for automatic face annotation <ref type="bibr" target="#b1">[2]</ref>, they usually suffer from a few common drawbacks. For example, they usually require high-quality facial image databases collected in well-controlled environments. This drawback has been partially addressed in recent benchmark studies of unconstrained face detection and verification techniques on the facial images collected from the web, such as the LFW benchmark <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b12">[14]</ref>.</p><p>The second group is related to generic image annotation <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. The common techniques usually apply existing object recognition techniques to train classification models from human-labeled training images, or attempt to infer the correlation or joint probabilities between query images and annotation keywords <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b17">[19]</ref>. Given limited training data, semi-supervised learning methods have been widely used for image annotation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[20]</ref>. For example, Wang et al. <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref> proposed to refine the model-based annotation results with a label similarity graph by following a random walk approach. Although semi-supervised learning approaches can leverage both labeled and unlabeled data, its performance still quite depends on the amount of labeled data. It is usually fairly time consuming and expensive to collect enough welllabeled training data to achieve satisfying performance in large-scale scenarios. Recently, the retrieval-based image annotation paradigm by mining web images has attracted more and more attention <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b21">[23]</ref>. A few studies in this area have attempted to develop efficient content-based indexing and search techniques to facilitate annotation/ recognition tasks. For example, Russell et al. <ref type="bibr" target="#b20">[22]</ref> developed a large collection of web images with ground-truth labels to facilitate object recognition tasks. There are also several studies that aim to address the final annotation process by exploring effective label propagation. For example, Wright et al. <ref type="bibr" target="#b22">[24]</ref> proposed a classification algorithm based on sparse representation, which predicts the label information based on the class-based feature reconstruction. Tang et al. presented a sparse graph-based semi-supervised learning (SGSSL) approach to annotate web images <ref type="bibr" target="#b7">[8]</ref>. Wu et al. <ref type="bibr" target="#b8">[9]</ref> proposed to select heterogeneous features with structural grouping sparsity and suggested a multilabel boosting scheme (denoted as "MtBGS" for short) for feature regression, where a group sparse coefficient vector is obtained for each class (category) and further used for predicting new instances. Wu et al. <ref type="bibr" target="#b5">[6]</ref> proposed a multireference re-ranking scheme (denoted as "MRR" for short) for improving the retrieval process.</p><p>The third group is face annotation on the collections of personal/family photos. Several studies have mainly focused on the annotation task on collections of personal/ family photos <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b25">[27]</ref>, which often contain rich context clues, such as personal/family names, social context, GPS tags, time stamps, and so on. In addition, the number of persons/classes is usually quite small, making such annotation tasks less challenging. These techniques usually achieve fairly impressive annotation results. Some techniques have been successfully deployed in commercial applications, for example, Apple iPhoto, 1  Google Picasa, 2 Microsoft easyAlbum <ref type="bibr" target="#b24">[26]</ref>, and Facebook face auto-tagging solution. 3  The fourth group deals with face annotation by mining weakly labeled facial images on the web. A few studies consider a human name as an input query, and mainly aim to refine the text-based search results by exploiting visual consistency of facial images <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[28]</ref>, which is closely related to automatic image re-ranking problems. For example, Ozkan and Duygulu <ref type="bibr" target="#b26">[28]</ref> proposed a graph-based model for finding the densest subgraph as the most related result. Following the graph-based approach, Le and Satoh <ref type="bibr" target="#b27">[29]</ref> proposed a new local density score to represent the importance of each returned image. The generative approach such as the Gaussian mixture model had also been adopted to the name-based search scheme and achieved comparable results <ref type="bibr" target="#b0">[1]</ref>. Recently, a discriminant approach was proposed in <ref type="bibr" target="#b28">[30]</ref> to improve the generative approach and avoid to explicitly compute the pairwise similarities in a graph-based approach. Inspired by query expansion <ref type="bibr" target="#b29">[31]</ref>, the performance of name-based scheme can be further improved by introducing the images of "friends" of the query name. Unlike these studies of filtering the text-based retrieval results, some studies have attempted to directly annotate each facial image with the names extracted from its caption information. For example, Berg et al. <ref type="bibr" target="#b30">[32]</ref> proposed a probability model which is combined with a clustering algorithm to estimate the relationship between the facial images and the names in their captions. For the facial images and the detected names in the same document, Guillaumin et al. <ref type="bibr" target="#b28">[30]</ref> proposed to iteratively update the assignment based on a minimum cost matching algorithm. To further improve the annotation performance, they adopted supervised distance metric learning techniques to grasp the important discriminative features in lowdimensional spaces.</p><p>Our work is fundamentally different from the previous studies of "text-based face annotation" and "caption-based face annotation." The key difference lies in two major aspects. First, our work aims to solve the generic content-based face annotation problem, where the facial images are directly used as the input query images, and the task is to return the corresponding names in the query images. Second, for the top-ranking similar candidate images, the proposed WLRLCC algorithm aims to achieve a new discriminative feature representation and refined label information in a unified learning scheme. However, the caption-based annotation scheme only considers the assignment between the facial images and the names appearing in their corresponding surrounding text. Therefore, the captionbased annotation scheme is only suitable for the scenario where both images and their captions are available, and cannot be employed in our RBFA framework due to the shortage of caption information. In addition, this work is related to our previous work in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b31">[33]</ref>, which proposed an unsupervised label refinement (ULR) technique to enhance the label information over the entire facial image database. This paper is different from the ULR algorithm in several aspects. First, it is difficult for the ULR to handle huge databases due to its high computational cost, while the WLRLCC algorithm is only applied to a short list of the most similar images for each query image and, therefore, is independent of the entire retrieval database size. Second, ULR focuses on refining the label information over the whole database, and its simple majority voting scheme may not be effective enough in exploiting the short list of the most similar images. In contrast, the proposed WLRLCC algorithm comprehensively resolves this problem by fully exploiting the short list of top similar images via a unified optimization scheme, which learns both sparse features and enhanced labels. In addition, according to recent research work <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b33">[35]</ref>, the face annotation performance could be further improved by combining the proposed WLRLCC algorithm with supervised inductive learning techniques in a unified framework.</p><p>The proposed learning methodology for WLRLCC is partially inspired by several groups of existing work in machine learning, including local coordinate coding <ref type="bibr" target="#b34">[36]</ref>, <ref type="bibr" target="#b35">[37]</ref>, graph-based semi-supervised learning <ref type="bibr" target="#b36">[38]</ref>, and multilabel learning <ref type="bibr" target="#b37">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RETRIEVAL-BASED FACE ANNOTATION FRAMEWORK</head><p>In this section, we briefly introduce the proposed RBFA paradigm. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the proposed framework, which consists of the following four major stages:</p><p>1. data collection of facial images from WWW, 2. facial image preprocessing and high-dimensional facial feature indexing, 3. content-based facial image retrieval for a query facial image, and 4. face annotation by the proposed WLRLCC algorithm. The details of each stage are described as follows.</p><p>The first stage, as shown in Fig. <ref type="figure" target="#fig_0">1a</ref>, collects a database of weakly labeled facial images which can be crawled from the web. Specifically, we can choose a list of desired human names and submit them to existing web search engines (e.g., Google) to crawl their related web facial images. As the output of this crawling process, we obtain a collection of web facial images, each of them is associated with some human names. The collected web images are useful for We collect weakly labeled facial images from WWW using a web search engine; (b) we perform face detection and alignment, then extract GIST features from the detected faces, and finally apply LSH <ref type="bibr" target="#b9">[10]</ref> to index the high-dimensional facial features; (c) a query facial image uploaded by the user is transformed into a feature vector with the same preprocessing step; using our contentbased facial image retrieval engine, a short list of the top-n most similar facial images and their associated names are retrieved and passed to the subsequent learning and annotation stage; (d) the proposed WLRLCC scheme is applied to return the final list of (ranked) annotated face names. many applications, for example, object classification <ref type="bibr" target="#b38">[40]</ref> and animal classification <ref type="bibr" target="#b39">[41]</ref>. In our framework, we use them as the retrieval database in a data-driven scheme. Since the labels of these web images are usually noisy, we refer to such web facial images with noisy names as weakly labeled facial images.</p><p>The second stage, as shown in Fig. <ref type="figure" target="#fig_0">1b</ref>, preprocesses the weakly label facial image database, including face detection, face alignment, facial feature representation, and highdimensional feature indexing. For facial region detection and alignment, we adopt OpenCV and the unsupervised face alignment technique DLK proposed in <ref type="bibr" target="#b40">[42]</ref>. Generally, any facial feature which is represented in the vector format could be used. In our system, we extract the GIST features <ref type="bibr" target="#b41">[43]</ref> to represent the aligned facial regions. Finally, we apply the Locality-Sensitive Hashing (LSH) to index the facial features in our solution <ref type="bibr" target="#b9">[10]</ref>.</p><p>The first two stages must be done before annotating a query facial image. The next two stages are related to online processes of annotating a query facial image. As shown in Fig. <ref type="figure" target="#fig_0">1c</ref>, given a query facial image, we employ a similar face retrieval process (kNN with L2 distance) to find a short list of the most similar faces (e.g., top-n similar faces) from the indexed face databases using the LSH technique.</p><p>After obtaining a set of the similar faces for the query image, the last stage applies the proposed WLRLCC algorithm for name annotation, as shown in Fig. <ref type="figure" target="#fig_0">1d</ref>. Specifically, WLRLCC learns local coordinate coding for each of the similar facial images and enhances the weak label matrix via an iterative optimization process. Based on the learning results, a sparse reconstruction algorithm is applied to perform the final face name annotation. Next, we present the details of WLRLCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEAK LABEL REGULARIZED LOCAL COORDINATE CODING</head><p>In this section, we present the proposed WLRLCC for the face annotation task based on a list of the similar facial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>Throughout the paper, we denote the matrices by upper case letters, for example, X; D; we denote the vectors by bold lower case letters, for example, x; x i ; we denote the scalars by the normal letters, for example, x i , x ij , X ij , where x i is the ith element of the vector x, x ij is the jth element of the vector x i , and X ij is the element in the i-row and j-column of the matrix X. Consider a query facial image x q 2 IR d in a d-dimensional feature space, which is associated with an unknown class denoted by a class label vector y q . Assume the n retrieval results of the query image x q are fðx i ; y i Þ j i ¼ 1; 2; . . . ; ng, where y i 2 f0; 1g m is the name label vector of its corresponding facial image x i and ky i k 0 ¼ 1, and m is the total number of classes (names) among all the similar facial images. Let X ¼ ½x 1 ; x 2 ; . . . ; x n denote the feature matrix of the retrieval results. We represent the initial name information with a label matrix Ỹ 2 IR nÂm , where Ỹi? ¼ y i , the ith row of the matrix, denotes the class label vector for x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sparse Features via Local Coordinate Coding</head><p>Sparse representation has been successfully applied in many applications. Recently, it has been argued from both theoretical and empirical perspectives that it is important to exploit the locality information for the linear embedding of high-dimensional data on the manifold in LCC <ref type="bibr" target="#b34">[36]</ref>. In detail, the goal of LCC is to develop a new representation of feature vectors in which each feature vector is described as a linear combination of several nearby items among the basis vectors. The sparse representation of each vector is locally adapted; hence the name of "Local Coordinate Coding."</p><p>In our framework, we adopted the LCC algorithm for the coding step of WLRLCC. Specifically, we reconstruct the locality coding s i of the ith facial image x i in the retrieval results with the dictionary B ¼ ½X; I 2 IR dÂðnþdÞ as follows:</p><formula xml:id="formula_0">min ŝi 1 2 kx i À Bŝ i k 2 þ X nþd k¼1 jŝ ik jkB ?k À x i k 2 s:t: ŝii ¼ 0;<label>ð1Þ</label></formula><p>where s i is a subvector of ŝi with its first n elements: ŝi ¼ ½s i ; i , i is related to the noise information in our framework, I is the identity matrix, and B ?k is the kth column of dictionary B. A similar leave-one-out representation model is adopted in sparse subspace clustering <ref type="bibr" target="#b42">[44]</ref>. For simplicity, we also denote the coding problem for x i by eðŝ i ; x i Þ.</p><p>To make sure the neighbor samples of each x i are included in the retrieval results X, we apply query expansion for the query image x q by including the top n 0 nearest samples of each query result into the final retrieval database. In our experiment, we simply fix n 0 to 3 for all cases. One difference between our coding algorithm and the original LCC algorithm is that we directly construct the dictionary B instead of learning it by optimization, which is proposed in <ref type="bibr" target="#b43">[45]</ref> and achieves excellent experiment results on the face recognition task. This method is also well known as "cross-and-bouquet" model for dense error correction <ref type="bibr" target="#b44">[46]</ref>.</p><p>According to our formulation, the jth element of s i tries to measure the similarity between the facial images x i and x j , so a negative value is contrary to this intuitive notion and does not make sense in our scenario. As a result, we introduce an extra nonnegative constraint to the previous formulation following monnegative sparse coding <ref type="bibr" target="#b35">[37]</ref>, in which all the elements in ŝi are forced to be nonnegative: ŝij ! 0; j ¼ 1; 2; . . . ; n.</p><p>Finally, we can give the formulation for all the n facial images in the retrieval results:</p><formula xml:id="formula_1">E 1 ð Ŝ; XÞ ¼ X n i¼1 eðŝ i ; x i Þ;<label>ð2Þ</label></formula><p>where Ŝ 2 IR ðnþdÞÂn ¼ ½S; Ä, S 2 IR nÂn is the nonnegative local coordinate coding of X, and Ä 2 IR dÂn is the noise matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Weak Label Enhancement</head><p>The previous formulation shows that the jth local coefficient s ij of facial image x i essentially encodes the locality information between x i and x j , where j 6 ¼ i. Specifically, a larger value of s ij indicates that x j is more representative of x i . In addition, from a view of graph-based semi-supervised learning, for any two facial images, the smaller their local distance, the more likely they should belong to the same person. As a result, a larger value of s ij implies that the name labels of x i and x j are more likely to be the same. Based on the above motivation, we can give the following formulation to enhance the initial weak label matrix Ỹ as follows:</p><formula xml:id="formula_2">min Y !0 1 2 X i;j s ij kY i? À Y j? k 2 þ kðY À Ỹ Þ Mk 2 F ;<label>ð3Þ</label></formula><formula xml:id="formula_3">where M ¼ ½hð Ỹij Þ is an indicator matrix: hðxÞ ¼ 1 if x &gt; 0</formula><p>and otherwise hðxÞ ¼ 0, and denotes the Hadamard product of two matrices. In the above objective function, the first term enforces that the class labels of two facial images x i and x j to be similar if the local sparse coefficient s ij is large, and the second term is a regularization term that prevents the refined label matrix being deviated too much from the initial weak matrix. Since the initial label matrix is noisy and incomplete, we apply the regularization of the second term on only these nonzero elements in Ỹ .</p><p>In general, the optimal solution to the problem in (3) is dense; however, the ideal true label matrix is often very sparse. We introduce some convex sparsity constraints, i.e., kY i? k 1 "; " ! 1, where i ¼ 1; 2; . . . ; n (we choose " ¼ 1 in this work). These constraints are included to limit the number of name labels assigned to each facial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Weak Label Regularized Local Coordinate Coding</head><p>The above two optimization tasks of "sparse feature learning" and "label enhancement" are performed separately. Specifically, the sparse features S are first learned from the optimization in (1), and then used by the optimization in (3) to refine the label matrix Y . To better exploit the potential of the two learning approaches, we propose the WLRLCC scheme, which aims to reinforce the two learning tasks via a unified optimization framework. Specifically, the optimization of WLRLCC can be formulated as follows:</p><formula xml:id="formula_4">min Ŝ;Y E 1 ð Ŝ; XÞ þ E 2 ðY ; SÞ ¼ min Ŝ;Y 1 2 kB Ŝ À Xk 2 F þ 1 trð1 Á ð Ŝ V ÞÞ þ 2 trðY &gt; LY Þ þ 3 kðY À Ỹ Þ Mk 2 F s:t: Ŝii ¼ 0; kY i? k 1 1; i ¼ 1; 2; . . . ; n; Ŝ ! 0; Y ! 0;<label>ð4Þ</label></formula><formula xml:id="formula_5">where V 2 IR ðnþdÞÂn , V ij ¼ kB ?i À X ?j k 2 , L ¼ D À S, D is a diagonal matrix, with D ii ¼ P S i? þ P S ?i 2 ; Y 2 IR nÂm ;</formula><p>1 is all-one-element matrix with dimension n Â ðn þ dÞ, and trðÁÞ denotes a trace function. In the above, 2 trðY &gt; LY Þ is a label smoothness regularizer that connects the label matrix and the sparse features. In Fig. <ref type="figure" target="#fig_1">2</ref>, we show the whole process of the proposed WLRLCC algorithm with a simple example by using the top n ¼ 8 retrieval results and m ¼ 4 candidate names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>The optimization problem in ( <ref type="formula" target="#formula_4">4</ref>) is generally nonconvex. To solve this challenging optimization, we propose to solve Ŝ and Y alternatively by iteratively solving two optimization steps: 1) Code Learning, and 2) Label Learning. The step of updating Ŝ can be transformed into a weighted nonnegative sparse coding problem, and the step of updating Y is a quadratic programming problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Code Learning</head><p>By first fixing the label matrix Y and ignoring the constant terms, the optimization problem in (4) can be reformulated as follows:</p><formula xml:id="formula_6">Q Y ð ŜÞ ¼ min Ŝ 1 2 kB Ŝ À Xk 2 F þ trð1 Á ð Ŝ ZÞÞ s:t: Ŝii ¼ 0; i ¼ 1; 2; . . . ; n; and Ŝ ! 0;<label>ð5Þ</label></formula><formula xml:id="formula_7">where Z ¼ 1 V þ 2 U, U 2 IR ðnþdÞÂn , for all j ¼ 1; 2; . . . ; n, if i n, U ij ¼ 1 2 kY i? À Y j? k 2 ; otherwise, U ij ¼ 0.</formula><p>The other variables follow the same definitions in <ref type="bibr" target="#b3">(4)</ref>.</p><p>The optimization problem in ( <ref type="formula" target="#formula_6">5</ref>) can be further separated into a series of subproblems for each coding coefficient Ŝ?i of facial image X ?i . Each subproblem is a weighted nonnegative sparse coding problem, which can be written into the following optimization:</p><formula xml:id="formula_8">min ŝ!0 1 2 kBŝ À X ?i k 2 þ X nþd k¼1 ŝk Z ki s:t: ŝi ¼ 0:<label>ð6Þ</label></formula><p>In our approach, we adopt the Fast Iterative Shrinkage and Thresholding Algorithm (FISTA) <ref type="bibr" target="#b45">[47]</ref>, a popular and efficient algorithm for the linear inverse problem that has been already implemented for sparse learning in <ref type="bibr" target="#b46">[48]</ref>. Since we aim to solve the problem related to only the top-n images in the retrieval results, where n is usually small, FISTA is efficient enough for our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Label Learning</head><p>Similarly, by fixing Ŝ and ignoring the constant terms, the optimization problem in (4) can be reformulated as follows:</p><formula xml:id="formula_9">Q Ŝ ðY Þ ¼ min Y trðY &gt; LY Þ þ kðY À Ŷ Þ Mk 2 F s:t: Y ! 0; kY i? k 1 1; i ¼ 1; 2; . . . ; n;<label>ð7Þ</label></formula><p>where ¼ 3  2 and the other variables follow the same definitions in <ref type="bibr" target="#b3">(4)</ref>. The optimization problem is a quadratic program with a series of closed and convex constraints (' 1 ball). To efficiently solve this problem, we first vectorize the label matrix Y 2 IR nÂm into a column vector " y ¼ vecðY Þ 2 IR ðnÁmÞÂ1 , and then rewrite the previous optimization problem as follows:</p><formula xml:id="formula_10">Qð" yÞ ¼ min " y!0 " y &gt; É" y þ c &gt; " y; s:t: 8i; X mÀ1 k¼0 " y kÁnþi 1;<label>ð8Þ</label></formula><formula xml:id="formula_11">where É ¼ I m L &gt; þ R, I m is an identity matrix with dimension m Â m, R ¼ diagðvecðMÞÞ, c ¼ À2R &gt; Á vecð Ỹ Þ,</formula><p>and " y j is the jth element in " y. To solve this problem, we employ the multistep gradient scheme <ref type="bibr" target="#b45">[47]</ref> and the efficient euclidean projection algorithm <ref type="bibr" target="#b47">[49]</ref>.</p><p>Specifically, to achieve the optimal solution " y ? , we recursively update two sequences f" y ðkÞ g and fz ðkÞ g in the multistep gradient scheme, where k is the iteration step. Commonly at each iteration k, the variance z ðkÞ is named as the search point and used to construct the combination of the two previous approximate solutions " y ðkÀ1Þ and " y ðkÀ2Þ . In our problem, the subblock problem is defined as</p><formula xml:id="formula_12">" y ðkþ1Þ ¼ arg min " y!0 t 2 k" y À vk 2 s:t: 8i; X mÀ1 k¼0 " y kÁnþi 1;<label>ð9Þ</label></formula><p>where v ¼ z ðkÞ À 1 t g and g ¼ 2Éz ðkÞ þ c, t can be a fixed positive constant or searched in a backtracking step <ref type="bibr" target="#b45">[47]</ref>.</p><p>The key problem is to solve (9) efficiently. In our work, we employ the Euclidian projection algorithm <ref type="bibr" target="#b47">[49]</ref>, which has been shown as an efficient solution to such kind of ' 1 ball constrained problem with linear time complexity of OðnÞ as compared with other algorithms of Oðn logðnÞÞ.</p><p>Further, to apply the Euclidian projection algorithm, we split " y into a series of m-dimensional subvectors with " y i ¼ ½" y kÁnþi Þ mÀ1 k¼0 , where i ¼ 1; 2; . . . ; n, and similarly we can split the vector v. Specifically, each optimal solution " y i? for subvector " y i can be achieved by solving the following problem:</p><formula xml:id="formula_13">" y i? ¼ arg min " y i !0 t 2 k" y i À v i k 2 s:t: k" y i k 1 1:<label>ð10Þ</label></formula><p>The problem in (10) has a nonnegative constraint, which is different from the one in <ref type="bibr" target="#b47">[49]</ref>. The optimal solution to this problem is given as " y i? j ¼ hðv i j Þmaxðjv i j j À ? ; 0Þ; j ¼ 1; 2; . . . ; m; ð11Þ</p><p>where hðÁÞ is the indicator function in (3), " y i j and v i j are the jth elements in " y i and v i , and ? is the optimal solution for the dual form of <ref type="bibr" target="#b9">(10)</ref>. Suppose S ¼ fjjv i j ! 0g, the value of ? can be computed as follows:</p><formula xml:id="formula_14">? ¼ 0 P k2S v i k 1; " P k2S v i k &gt; 1;<label>ð12Þ</label></formula><p>where " is the unique root of function fðÞ ¼ P k2S maxðjv i k j À ; 0Þ À 1, which is continuous and monotonically decreasing in ðÀ1; 1Þ. The root " can be achieved by a bisection search in linear time complexity of Oðdimð" y i ÞÞ. It is well known that the multistep gradient scheme converges at Oð</p><formula xml:id="formula_15">1 k 2 Þ,</formula><p>where k is the iteration step, indicating our optimization can be solved efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Face Name Annotation</head><p>After obtaining the sparse coding matrix S and the enhanced label matrix Y , the last key step of our framework is to perform the face name annotation. First, the query facial image x q is also projected into the local coordinate coding space. Specifically, the new coding s q w.r.t. x q can be achieved by solving the following optimization problem:</p><formula xml:id="formula_16">½s q ; q ¼ arg min ŝ!0 1 2 kBŝ À x q k 2 þ X k ŝk kB ?k À x q k 2 ;<label>ð13Þ</label></formula><p>where the parameter setting is the same as the previous WLRLCC algorithm.</p><p>Although the manifold structure is supposed to be well fitted in the new local coordinate space, it is still unsuitable to directly use a one-versus-one similarity measure for face name annotation. For the second step, we employ a sparse reconstruction scheme in the local coordinate coding space to recover the potential weighting vector w q for name annotation in the label space. The optimization problem is given as follows:</p><formula xml:id="formula_17">minkw q k 1 s:t: s q ¼ S In Á w q ;<label>ð14Þ</label></formula><p>where S I n ¼ S þ I n , S is the local coordinate codes obtained from the previous step, and I n is an n Â n identity matrix. Finally, the label vector y q can be directly computed by</p><formula xml:id="formula_18">y q ¼ Y &gt; Á w q :</formula><p>The value y qk , k 2 f1; 2; . . . ; mg, measures the confidence of the kth name being assigned to the query facial image x q . Thus, a name with a large confidence value will be ranked on the top position for the final annotation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OFFLINE APPROXIMATION FOR WLRLCC</head><p>To reduce the computational cost time of the proposed WLRLCC algorithm, one straightforward solution is to adopt the PCA dimension reduction techniques over the original high-dimensional feature space. The smaller the new dimension space is, the less time it takes for WLRLCC algorithm. The key limitation for the PCA-based approximation is the information loss during dimension reduction may affect the final annotation performance.</p><p>In this section, we propose an offline approximation scheme for WLRLCC (AWLRLCC for short), which can both significantly reduce the running time and achieve comparable results. In detail, we first precompute the local coordinate coding for each facial image in the retrieval database with its own n neighborhoods by <ref type="bibr" target="#b11">(13)</ref> and save all the coding results. Then, in the annotation step, for each query image we can directly reconstruct the sparse features of its n nearest instances based on the offline coding results without extra computational costs.</p><p>We denote by D ¼ fd 1 ; d 2 ; . . . ; d N g the whole retrieval database, where N is the total number of facial images. For each facial image d i , we denote by N ðd i Þ its n similar images in the retrieval database except itself. Then, we can achieve its encoding result c i with <ref type="bibr" target="#b11">(13)</ref>, where the dictionary is constructed as ½N ðd i Þ; I. For the query image x q , its n most similar facial images X ¼ ½x 1 ; x 2 ; . . . ; x n ¼ ½d I 1 ; d I 2 ; . . . ; d I n is also a subset of the whole retrieval database D, where I is an index vector for the whole retrieval database. In the offline computation step, the sparse coding of x i (or d I i ) is c I i and its jth element c I i j measures the weight of the jth item in the nearest neighbor set N ðd I i Þ. As a result, the key problem in our approximation scheme is to construct x i s (or d I i s) new coding result s i for WLRLCC based on the previous coding results. The new coding results should share the same dictionary " X, which is constructed with all the neighbor sets N ðd I i Þ of d I i ; i ¼ 1; 2; . . . ; n: "</p><formula xml:id="formula_19">X ¼ S n i¼1 N ðd I i Þ S</formula><p>X where the number of dictionary items in " X is " n; " n ! n. Suppose the function Çð "</p><p>X; dÞ returns the index value of the vector d in the dictionary " X, the new coding result s i 2 IR " n of x i can be constructed based on " X with the following rules: 1) For j ¼ 1; 2; . . . ; n, s iÇð "</p><formula xml:id="formula_20">X;N ðd I i Þ j Þ ¼ c I i j .</formula><p>2) The left items in s i are set as zero. Similar to the general WLRLCC algorithm, the local coordinate coding result s q of x q can be achieved by solving the problem in <ref type="bibr" target="#b11">(13)</ref>, where B ¼ ½ " X; I. Fig. <ref type="figure" target="#fig_2">3</ref> shows an example for AWLRLCC, where denotes the query image x q , denotes the database images, and n ¼ 3. As shown in Fig. <ref type="figure" target="#fig_2">3b</ref>, the nearest neighbor set of x q is N ðx q Þ ¼ fx 1 ; x 2 ; x 3 g ¼ fd 4 ; d 3 ; d 5 g, and the index vector is I ¼ ½4; 3; 5. Based on the reconstructed dictionary</p><formula xml:id="formula_21">" X ¼ N ðd 4 Þ S N ðd 3 Þ S N ðd 5 Þ S N ðx q Þ ¼ ½d 1 ; d 2 ;</formula><p>. . . ; d 9 , the value mapping rules between the precomputed coding c i and the reconstructed coding s i are illustrated in Fig. <ref type="figure" target="#fig_2">3c</ref>. For example, for the facial image d 4 with j ¼ 1, the corresponding dictionary item is d 5 and Çð " X; d 5 Þ ¼ 5, so that s 45 ¼ c 41 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RETRIEVAL DATABASE CONSTRUCTION</head><p>Constructing a proper retrieval database is a key step for a retrieval-based face annotation system. In literature, some web facial image databases are available from the previous research work, for example, LFW <ref type="bibr" target="#b11">[13]</ref>, 4 Yahoo!News [30], 5  and FAN-Large <ref type="bibr" target="#b48">[50]</ref>. 6 One issue with these three databases is that although the number of people is quite large as compared to regular face databases, the number of images for each person is quite small, making them inappropriate for retrieval-based face annotation tasks. For example, the LFW database has a total of 13,233 images for 5,749 people, in which each person on average has no more than three facial images. Unlike these database, another database in literature, PubFig <ref type="bibr" target="#b49">[51]</ref>, 7 is more appropriate. It has 200 persons and 58,797 images, as constructed by collecting online news sources. Due to the copyright issue, only image URL addresses were released from the previous work. Since some URL links are no longer available, only a total of 41,609 images were collected by our crawler. For each downloaded image, we crop the face image according the given face position rectangle and resize all the face images into the same size (128 Â 128).</p><p>To evaluate the retrieval-based face annotation scheme in even larger web facial image databases, we construct two new databases: 1) "WLF: Weakly Labeled Faces on the web," which contains 6,025 famous western celebrity and 714,454 web facial images in total; 2) "WLF-cn," an Asian web facial image database, which consists of 1,200 persons and 126,070 images in total. Since there is almost no overlap between "WLF" and "WLF-cn," we adopt both to evaluate the generalization of the system on different types of databases. In general, there are two main steps to build a weakly labeled web facial image database: 1) Construct 4 a name list of popular persons; and 2) query the existing search engine with the names, and then crawl the web images according to the retrieval results. To facilitate future research by other researchers, we have made our databases and their related information publicly available. 8  In the first step, for "WLF" database, we collect a name list consisting of 6,025 names downloaded from the website: IMDb, 9 which covers the actors and actresses who were born between 1950 and 1990. Specifically, we collect these names with the billboard: "Most Popular People Born In yyyy," where yyyy is the year of birth. For example, the webpage 10 presents all the actors and actresses who were born in 1975 in popularity order. For the "WLF-cn" database, we collected a name list consisting of 600 Asian male celebrities and 600 Asian female celebrities from two webpages. 11 In the second step, we submit each name in the aforementioned lists as a query to search for the related web images by Google image search engine. The top 200 retrieved web images are crawled automatically. After that we use the Viola-Jones algorithm to detect the faces and use the Deformable Lucas-Kanade (DLK) algorithm <ref type="bibr" target="#b40">[42]</ref> to align facial images into the same well-defined position. The web images in which no faces are detected are ignored directly. As a result, we collected 714,454 facial images of 6,025 people in the "WLF" database, and 126,070 images of 1,200 people in the "WLF-cn" database.</p><p>For evaluation, we built two "evaluation(test) data sets" by randomly choosing 119 names from the name list "WLF" and 110 names from the name list of "WLF-cn." We issued each of these names as a text query to Google image search and crawled the web images with the top 200th to 400th search results. Note that we did not consider the top 200 retrieval results because they had already been used in the retrieval database. This aims to examine the generalization performance of the proposed technique for the unseen facial images. We requested our staff to manually examine the retrieved facial images and remove those irrelevant facial images for each name. As a result, the evaluation data set for "WLF" contains 1,600 facial images, and the evaluation data set for "WLF-cn" contains 1,300 facial images.</p><p>In the following experiments, we denote the "WLF" database as "WDB-600K" and denote the "WLF-cn" database s "ADB-P100." To evaluate the effect of the number of persons, we build four subsets of the whole database "WDB-600K": "WDB-040K," "WDB-100K," "WDB-200K," and "WDB-400K." For example, WDB-040K includes only all the facial images of 400 celebrities, and there are 53,448 images in WDB-040K. To evaluate the effect of the number of images per person, we build a subset "ADB-P050" for the whole database ADB-P100, which means that only half of the facial images per person are collected into ADB-P050 and there are about 50 facial images per person on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Testbed</head><p>We compare the proposed WLRLCC against a baseline algorithm based on Soft-Max Weighted majority voting ("SMW") and five existing algorithms for face/image annotation, including "S-Recon" <ref type="bibr" target="#b22">[24]</ref>, "SGSSL" <ref type="bibr" target="#b7">[8]</ref>, "LCC" <ref type="bibr" target="#b34">[36]</ref>, "MtBGS" <ref type="bibr" target="#b8">[9]</ref>, and "MRR" <ref type="bibr" target="#b5">[6]</ref>. Most of these algorithms were proposed in recent years, as briefly introduced in Section 2. For facial feature representation, we adopt the 512-dimensional GIST features <ref type="bibr" target="#b41">[43]</ref> for both "WDB" and "ADB" databases. For the "PubFig" database, we adopt the 2,891-dimensional LBP <ref type="bibr" target="#b50">[52]</ref> features by dividing the face images into 7 Â 7 blocks, which is further projected into a lower 500-dimensional feature space using principal component analysis (PCA). More details on feature representation are discussed in Section 7.7. To evaluate the annotation performances, we adopt the hit rate at the top-t annotated results as the performance metric, which measures the likelihood of having the true label among the top-t annotated names for a query facial image. Furthermore, we discuss the cross-validation issue. For all the algorithms, we only conducted it in the experiments on the WDB-040K; as a result, the parameters with the best average hit rate results are directly used in the following experiments based on the other databases. Specifically, we randomly divide the query set into two parts of equal size, in which one part is used as the validation set to find the optimal parameters by a grid search, and the other part is used for performance evaluation. Such a procedure is repeated 10 times and the average performance is computed over the 10 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Impact of the Top-n n and Top-t t Settings</head><p>This experiment aims to evaluate the impact on the final annotation performance under varied settings by varying the values of n and t for Top-n retrieved facial images and Top-t annotated names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Impact of the Top-t Settings</head><p>Table <ref type="table" target="#tab_3">1</ref> shows the hit rate performance of the baseline SMW algorithm (n ¼ 10) and the WLRLCC algorithm (n ¼ 40) under different settings of t values based on the WDB-040K database.</p><p>We observe that given a fixed n value, increasing the value of t generally leads to a better annotation performance and the improvement becomes marginal when t is large. This observation is not surprising as assigning more names certainly gives a better chance to hit the relevant name. Specially, for t ¼ 1 the hit rate measures the precision of the annotation system, and for a large t value, the hit rate is very similar to the recall metric. In practice, we mainly  focus on a small value of t because users usually would not be interested in a long name list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Impact of the Top-n Settings</head><p>Fig. <ref type="figure" target="#fig_3">4a</ref> and Table <ref type="table">2</ref> show the hit rate performance of the baseline SMW algorithm and the WLRLCC algorithm under different settings of n values based on the WDB-040K database. For both algorithms, we fix t to 1. The average running time of WLRLCC algorithm for one query sample is presented in Fig. <ref type="figure" target="#fig_3">4b</ref> and the last row of Table <ref type="table">2</ref>.</p><p>Several observations can be drawn from the results. First of all, for the baseline SMW algorithm, when increasing the value of n, the annotation performance generally decreases. However, for the proposed WLRLCC algorithm, increasing n leads to improve the annotation performance. It is not difficult to explain this observation. In particular, for any two facial images, the smaller their local distance is, the more likely they belong to the same person; as a result, for the top n similar instances, a small n value results in a high precision and a low recall, while a large n value often produces a high recall and a low precision. Because SMW is a simple weighted majority voting algorithm without any re-ranking, it favors the retrieval result of a high precision; this is why a small parameter n gets a better performance. On the other hand, the WLRLCC algorithm adopts a reranking scheme and typically prefers a high recall of the retrieval results such that a potentially larger pool of relevant facial images can be exploited for re-ranking. Second, we observe that the running time of the WLRLCC algorithm increases when increasing value of n due to the cost of encoding more instances and a larger dictionary.</p><p>The above observations are beneficial to determining the appropriate parameters in our rest experiments. For SWM, the parameter n for Top-n retrieved images is set to 10. For the other algorithms that usually carry a re-ranking or sample selection step, we thus set the parameter n to 40, as a tradeoff between annotation performance and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluation of Auto Face Annotation on "WDB,"</head><p>"ADB," and "PubFig"</p><p>In this experiment, we compare the proposed WLRLCC algorithm against the other algorithms on the three databases: WDB-040K, ADB-P100, and PubFig.</p><p>For the two weakly labeled databases WDB-040K and ADB-P100, the average annotation performance with Top-t ¼ 1 are shown in Table <ref type="table">3</ref>. As the Asian celebrity database (ADB-P100) is independent of the Western celebrity database (WDB-040K), we hope the two sets of results are useful to validate the generalization of retrieval-based face annotation system and the proposed WLRLCC algorithms.</p><p>Several observations can be drawn from the results. First, it is clear that the proposed WLRLCC algorithm consistently outperforms the other algorithms on both databases. Considering the database WDB-040K, the performance of the baseline SMW algorithm is about 61.0 percent, while the performance of the proposed WLRLCC algorithm is 76.5 percent. The better annotation performance validates the effectiveness of the proposed WLRLCC algorithm for retrieval-based face annotation. Second, for a small t value (t ¼ 1), compared with SMW, WLRLCC achieves a significant improvement of about þ15:5 points. Further, by examining a larger value of t, for example, t ¼ 5, the hit rate performances of WLRLCC and SMW are 85.1 and 80.1 percent, respectively, leading to an improvement of about þ5:0 points. This result shows that a greater improvement can be achieved by the WLRLCC algorithm for a smaller value of t, which is critical to real-world applications where only top annotated results are concerned. Third, the similar result based on the database ADB-P100 shows that WLRLCC is always helpful to improve the annotation performance given different databases.</p><p>In addition to the above two weakly labeled databases, we also test on the PubFig database. Note that the initial label information of this database is of quite high quality as it has been manually purified and all the facial regions are just for the selected person even if there are multiple faces in the same image. In our experiments, we manually remove the cropped nonface images and some incorrect images for each person. In our experiment, PubFig is adopted as a well-labeled database to help us examine the impact of annotation performance by varied label noise settings. Specifically, regarding the construction of the query set, we randomly sample 10 images per person for all the 200 persons in the testbed. For PubFig, there are a total of 39,609 images in the retrieval database, and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>The Hit Rate Performance of SMW and WLRLCC with Different Top-n, and the Running Time of WLRLCC TABLE <ref type="table">3</ref> The Performance of WLRLCC and the Other Algorithms on Databases WDB-040K and WDB-P100</p><p>2,000 images in the query database. To simulate different noise levels for the retrieval database, we randomly sample p percent (10, 20, 40 percent) of images from each person and change their labels to the other randomly generated names. To fully observe the impact of the noise, one would prefer a higher recall of the face retrieval result and annotation result; we thus set the values of n and t to 40 and 5, respectively, for both WLRLCC and SMW algorithms. The experimental results are shown in Table <ref type="table" target="#tab_4">4</ref>.</p><p>Several observations can be drawn from the results. First of all, for all the cases, the proposed WLRLCC algorithm consistently outperforms the baseline SMW algorithm. Second, when extra noise labels are added into the retrieval database, the task becomes more difficult, and thus the annotation performance decreases. For example, by randomly mislabeling 10 percent images per person, the performance of WLRLCC will drop to 65.1 percent, which is about 96 percent of the result without noise. For comparison, the impact caused by noise labels is more serious for the SMW algorithm, which drops to 79 percent of the result without noise under the same noise level. This indicates that the proposed WLRLCC algorithm is more robust for noisy labels and is able to effectively improve the annotation performance for the weakly labeled databases. Finally, it is interesting to observe the overall annotation performance on the PubFig database is worse than the results on both WDB and ADB. We think three possible reasons may explain such observation: 1) The number of images per person (n ipp ) in the PubFig varies a lot. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, we randomly distribute n ipp of PubFig and WDB-040K along the x-axis. The n ipp of WDB-040K is stable and around 100; however, some values of n ipp of PubFig are very small (e.g., <ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b25">27)</ref>, which are insufficient for datadriven scheme and reduce the annotation performance. 2) It may be because the facial images were cropped from the selected regions on this database without further alignment (instead of adopting the DLK algorithms on the previous databases that would remove some partial face images and reduce the size of retrieval database). 3) The ratio of duplicate images on PubFig is smaller than that of WDB and ADB. To show this, for each query set of the three databases, we try to estimate the ratio of queries that have duplicate images in their retrieval databases. Note that we count a retrieved image is duplicate to the query whenever the distance is small enough (e.g., 10 À4 in our setting). According to statistics, the duplicate ratios are about 8.0 (128 out of 1,600), 5.0 (65 out of 1,300), and 1.1 percent (22 out of 2,000) for WDB-040K, ADB-P100, and PubFig, respectively. This result validates that duplicate images also may affect the face annotation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Evaluation on the Database Size</head><p>This experiment aims to examine the impact of database size on the annotation performance. There are two ways to vary the database size: One is to increase the number of facial images for each person, and the other is to fix the number of images per person but increase the number of unique persons. We will evaluate the impact under both settings in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Varied Numbers of Persons</head><p>We examine the impact of the number of persons on the series of four databases: WDB-040K, WDB-100K, WDB-200K, WDB-400K, and WDB-600K, where the number of perople is increased from 400 to 6,025. The experimental results are presented in Fig. <ref type="figure" target="#fig_5">6</ref>, where for clarity only the annotation results with Top-t ¼ 1 are shown.</p><p>We can draw some observations from the results. First, similar to the previous observations, the WLRLCC algorithm consistently obtains the best annotation performance among all the compared algorithms under different database sizes. Second, it is clear that the annotation performance decreases when increasing the number of people, similar to the observation of some existing work on facial image retrieval in <ref type="bibr" target="#b5">[6]</ref>. This is because increasing the number of people leads to a larger database of more images for the same query, making the retrieval task more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Varied Numbers of Images per Person</head><p>We further examine the impact of the number of images per person collected into the retrieval database based on two database ADB-P050 and ADB-P100, where the number of images per person is increased from about 50 to 100. The experimental results are shown in Fig. <ref type="figure">7</ref> with t ¼ 1. From the results, it is obvious to observe that the larger the   number of facial images per person, the better the average annotation performance achieved. This is also similar to the observation found in the previous work <ref type="bibr" target="#b2">[3]</ref>, where more potential images in the retrieval database are beneficial to the annotation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Evaluation of Approximation for WLRLCC</head><p>In this section, we aim to examine the efficacy of the approximation scheme to trade off between annotation accuracy and time efficiency. To this purpose, we evaluate the running time and annotation performance of the three algorithms: the original WLRLCC algorithm, the PCA-based approximation scheme for WLRLCC, and the offline approximation scheme (AWLRLCC). For the PCA-based approximation, we randomly select 1,000 images from the retrieval database to learn the eigenvectors, from which both the query image x q and the top-n nearest neighbors X would then be projected into a new k-dimensional feature space, which can be further applied by the original WLRLCC algorithm. For the offline approximation, all the facial images in the retrieval database are first encoded based on its top-n neighbors, then the query image x q is annotated with its top-ranked candidate images based on the proposed offline approximation scheme for WLRLCC (AWLRLCC). All the experimental results are presented in Table <ref type="table" target="#tab_5">5</ref>.</p><p>Several observations can be drawn from the results. First, by using PCA, the running time can be significantly reduced. For example, for the new feature "PCA-100," the PCA-based approximation scheme needs only about 0.52 second; however, this is paid by a drop of the annotation performance to about 72.4 percent. Second, we found that the proposed offline approximation algorithm (AWLRLCC) not only takes less time than the PCA-based scheme, but also achieves better annotation results. For example, it achieves the best approximation result 75.6 percent with only about 0.22 second. Although the proposed "AWLRLCC" algorithm achieves a good approximation result with less time cost, we note that its disadvantage is the cost of extra storage (memory) space because all the precomputed results should be saved for the on-the-fly reconstruction.</p><p>To further evaluate the approximation result of AWLRLCC, we compare it with the original WLRLCC algorithm on all the databases, as shown in Fig. <ref type="figure" target="#fig_6">8</ref>. From the results, it is obvious that AWLRLCC always achieves good approximation results on different databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Evaluation on Other Face Annotation Schemes</head><p>As we mentioned previously, the existing algorithms proposed for "text-based face annotation" or "caption-based face annotation" cannot be directly applied to our "retrievalbased face annotation" framework due to the shortage of caption information. However, the proposed WLRLCC algorithm can be generally applied to the previous two schemes. In this section, we aim to apply the proposed WLRLCC technique for the existing two face annotation tasks, and examine if the proposed WLRLCC algorithm is comparable to the state-of-the-art algorithms <ref type="bibr" target="#b28">[30]</ref>.</p><p>Following the same setup and settings as the previous studies, we conduct this experiment on the same database "Labeled Yahoo! News" as used in <ref type="bibr" target="#b28">[30]</ref>, which has over 20,071 documents, 31,147 faces, and 5,873 names. For the feature representation, we adopt PCA to project the highdimensional facial features into a low-dimensional space (100). For the proposed WLRLCC algorithm, we use the whole database as the retrieval database, and initialize the name vector of each face with the detected names in its caption information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.1">Text-Based Face Annotation</head><p>For the "text-based face annotation" task ("Single-person Retrieval"), the goal is to retrieve all facial images of one person for a given name. We adopt the same 23 query names as used in <ref type="bibr" target="#b28">[30]</ref>. For each query name, we collect all the facial images that contain the query name in their corresponding captions, and re-rank the face sets according to the annotation result produced by WLRLCC. The final mAP scores are shown in Table <ref type="table" target="#tab_6">6</ref>. We observe that the proposed WLRLCC algorithm can achieve better results than the "Graph-based" algorithms and the "Generative Model." It is generally competitive to the state-of-the-art "SMLR model," which adopts the original face descriptor and automatically finds which dimensions to use.  Fig. <ref type="figure">7</ref>. Comparison between "ADB-P050" and "ADB-P100," with Top-t ¼ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.2">Caption-Based Face Annotation</head><p>For the "caption-based face annotation" task ("Multiperson Naming"), the goal is to associate a face to one name of its corresponding caption. We first annotated all of the 14,827 facial images in the test set, then restricted the annotation results with names detected in the caption information; at last a threshold value is used to decide whether the topranked name should be assigned. The summary of names and faces association performance and the final naming precision are shown in Table <ref type="table" target="#tab_7">7</ref>, where the results of the compared algorithms are taken from <ref type="bibr" target="#b28">[30]</ref>. Similar to the previous experiment, we observe that the proposed WLRLCC algorithm achieves better result than the "Graph-based" algorithms and highly competitive result as the "Generative Model."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.3">Search-Based Face Annotation</head><p>In this section, we aim to compare with the existing ULR approach <ref type="bibr" target="#b2">[3]</ref> for search-based face annotation. As discussed in Section 2, the ULR algorithm <ref type="bibr" target="#b2">[3]</ref> aims to refine the weak labels over the entire database and may be limited for its simple majority voting-based annotation. In contrast, WLRLCC adopts a re-ranking scheme by fully exploiting the short list of top similar images via a unified optimization scheme. Following the same setting, we first apply WLRLCC on the same data set used in <ref type="bibr" target="#b2">[3]</ref>. The result of WLRLCC is shown in the fourth column of Table <ref type="table" target="#tab_8">8</ref>, in which it is clear to see that WLRLCC achieves a better result. Moreover, we note that the task addressed by WLRLCC is very different from that of ULR. In fact, the WLRLCC algorithm can be benefited from the refined label matrix learned by the ULR algorithm. In particular, we can exploit the refined the label matrix by ULR to construct the initial matrix Ỹ for each query image in (4). To show this, we implement such a combined cascade framework and show the result of this framework in the fifth column of Table <ref type="table" target="#tab_8">8</ref>.</p><p>As a summary, for the retrieval-based face annotation task, the proposed WLRLCC algorithm achieves promising results based on different database sets. It also can be combined with the previous ULR algorithm to further improve the annotation performance. Finally, WLRLCC is not restricted to retrieval-based face annotation tasks, but also can be easily applied to tackle the other existing face annotation tasks in literature with encouraging results highly competitive to the state-of-the-art algorithms on the same public web facial image databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Evaluation of Facial Feature Representation</head><p>In this experiment, we evaluate the impact of different types of facial feature representations for the proposed WLRLCC algorithm based on the database WDB-040K. Table <ref type="table" target="#tab_9">9</ref> shows the annotation performance of different features. All of these features are extracted from the aligned facial images by the DLK algorithm <ref type="bibr" target="#b40">[42]</ref>. The features of "GIST," "Edge," "Color," and "Gabor" are generated by the toolbox in http://goo.gl/BzPPx. For "LBP," each aligned facial image is divided into 7 Â 7 blocks <ref type="bibr" target="#b50">[52]</ref>, leading to 2,891-dimensional features.</p><p>From the results, we observe that the original LBP 2891 feature achieves the best performance, which is consistent with the existing face recognition studies that show LBP is one of the best facial features. However, the high dimensionality of LBP feature leads to an extremely high running time of WLRLCC, which takes about 46 seconds on average for each query sample. By reducing the dimensionality of the original LBP feature to the same dimensionality of GIST (512D) via PCA, denoted as "LBP 512 ," the new feature LBP 512 performs slightly worse than GIST. Based on the result, for the databases aligned by the DLK algorithm (e.g., WDB and ADB), we adopted the GIST features to represent the facial images in our experiments. For the other database (e.g., PubFig), we adopted the LBP features to represent the facial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS</head><p>Despite the encouraging results achieved, our work is limited in some aspects. First, we assume the top ranked web facial images are related to the query name, which is certainly true for celebrities. However, when the query person is not well known, the relevant facial images on the internet may be very limited. This is a common issue of all the existing data-driven face/image annotation techniques. It could be partially resolved by exploiting social contextual information from social networks or exploring a combination with model-based annotation techniques. Second, WLRLCC relies on good facial feature representation techniques and an effective facial image retrieval scheme, which is still technically challenging, despite being studied for many years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>We investigate the retrieval-based face annotation problem and present a promising framework to address the problems in mining massive weakly labeled facial images freely available on the WWW. We proposed a WLRLCC algorithm, which effectively exploits the principles of both local coordinate coding and graph-based weak label regularization. Moreover, a sparse reconstruction scheme is developed to perform face annotation task. We have conducted extensive empirical studies on several web facial image databases, which validate the efficacy of WLRLCC. To further improve the efficiency and scalability, we then proposed an offline approximation scheme (AWLRLCC) to speed up the original WLRLCC algorithm, saving a significant amount of computational time while maintaining comparable performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The system diagram of the RBFA scheme. (a) We collect weakly labeled facial images from WWW using a web search engine; (b) we perform face detection and alignment, then extract GIST features from the detected faces, and finally apply LSH<ref type="bibr" target="#b9">[10]</ref> to index the high-dimensional facial features; (c) a query facial image uploaded by the user is transformed into a feature vector with the same preprocessing step; using our contentbased facial image retrieval engine, a short list of the top-n most similar facial images and their associated names are retrieved and passed to the subsequent learning and annotation stage; (d) the proposed WLRLCC scheme is applied to return the final list of (ranked) annotated face names.</figDesc><graphic coords="3,39.17,51.05,488.18,155.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed WLRLCC process. In this example, we show the top n ¼ 8 retrieved images with m ¼ 4 candidate names for query image x q : (a) gives the input data; (b) illustrates the two key steps of the WLRLCC process; (c) shows the output coding result and final annotation scheme.</figDesc><graphic coords="5,292.97,46.73,244.58,355.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example for AWLRLCC. (a) shows the database images ðfd 1 ; d 2 ; . . . ; d 9 gÞ and query image ðx q Þ; (b) shows the nearest neighbor sets of x q , d 4 , d 3 , and d 5 ; (c) shows the value mapping rules between the precomputed coding c i and the reconstructed coding s i based on the new dictionary " X. As an example, for the facial image d 4 with j ¼ 1, the corresponding dictionary item is d 5 and Çð " X; d 5 Þ ¼ 5, so that s 45 ¼ c 41 .</figDesc><graphic coords="7,54.40,50.26,457.78,129.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The hit rate performance of SMW and WLRLCC with different Top-n, (b) the average running time of WLRLCC for one query sample.</figDesc><graphic coords="9,29.93,51.05,243.74,91.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The distribution of the number of images per person for the database WDB-040K and PubFig.</figDesc><graphic coords="10,60.89,598.25,183.50,122.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison between WLRLCC and the other algorithms on WDB-040K, WDB-100K, WDB-200K, WDB-400K, and WDB-600K.</figDesc><graphic coords="10,323.93,49.85,180.26,85.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison between WLRLCC and AWLRLCC over all databases.</figDesc><graphic coords="11,325.13,47.93,180.26,124.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 The</head><label>1</label><figDesc>Hit Rate Performance of SMW (n ¼ 10) and WLRLCC (n ¼ 40) with Different Top-t</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 The</head><label>4</label><figDesc>Evaluation of Different Noise PercentageBased on the Database "PubFig"</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 The</head><label>5</label><figDesc>Running Time of Different Approximation Schemes, Based on Database WDB-040K</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>The mAP Scores over 23 Queries for the Proposed WLRLCC and Three Other Algorithms in<ref type="bibr" target="#b28">[30]</ref>, Where the Results of the Other Algorithms Were Taken from<ref type="bibr" target="#b28">[30]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 The</head><label>7</label><figDesc>Summary of Names and Faces AssociationPerformance Obtained by Graph-Based Method, Generative Model<ref type="bibr" target="#b28">[30]</ref>, and the Proposed WLRLCC</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 The</head><label>8</label><figDesc>Performance of WLRLCC and the Cascade Framework Combined with ULR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9 The</head><label>9</label><figDesc>Performance of WLRLCC with Different Facial Features Based on Database WDB-040K</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by Singapore MOE Tier-1 research grant (RG33/11), Microsoft Research grant, and IDM National Research Foundation (NRF) Research grant (MDA/IDM/2012/8/8-2 VOL 01). Jianke Zhu was supported by Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dayong Wang received the bachelor's degree in computer science from Tsinghua University, Beijing, P.R. China. He is currently working toward the PhD degree in the School of Computer Engineering, Nanyang Technological University, Singapore. His research interests include machine learning, computer vision, pattern recognition, and multimedia information retrieval.</p><p>Steven C.H. Hoi received the bachelor's degree in computer science from Tsinghua University, Beijing, P.R. China, and the master's and PhD degrees in computer science and engineering from the Chinese University of Hong Kong. He is currently an associate professor in the School of Computer Engineering, Nanyang Technological University, Singapore. His research interests include machine learning, multimedia information retrieval, web search, and data mining. He is a member of the IEEE and ACM.</p><p>Ying He received the BS and MS degrees in electrical engineering from Tsinghua University, and the PhD degree in computer science from Stony Brook University. He is currently an associate professor in the School of Computer Engineering, Nanyang Technological University, Singapore. His research interests fall in the broad areas of visual computing. He is particularly interested in the problems that require geometric computation and analysis. . For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Names and Faces in the News</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="848" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face Annotation by Transductive Kernel Fisher Discriminant</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining Weakly-Labeled Web Facial Images for Search-Based Face Annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Eng</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012">2012</date>
			<publisher>PrePrints</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Clustering for Google Searches of Celebrity Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth IEEE Int&apos;l Conf. Automatic Face &amp; Gesture Recognition (FG &apos;08)</title>
		<meeting>Eighth IEEE Int&apos;l Conf. Automatic Face &amp; Gesture Recognition (FG &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-Supervised SVM Batch Mode Active Learning with Applications to Image Retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Information Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable Face Image Retrieval with Identity-Based Quantization and Multi-Reference Re-Ranking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3469" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval and Clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Computing, Comm., and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image Annotation by kNN-Sparse Graph-Based Label Propagation over Noisily Tagged Web Images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-Label Boosting for Image Annotation by Structural Grouping Sparsity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Int&apos;l Conf. Multimedia</title>
		<imprint>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling LSH for Performance Tuning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th ACM Conf. Information and Knowledge Management (CIKM)</title>
		<meeting>17th ACM Conf. Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face Recognition: A Literature Survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">technical report</title>
		<imprint>
			<biblScope unit="page" from="7" to="49" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
		<respStmt>
			<orgName>UMASS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face Recognition with Learning-Based Descriptor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2707" to="2714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Image Annotation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="617" to="627" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining Social Images with Distance Metric Learning for Automated Image Tagging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Fourth ACM Int&apos;l Conf. Web Search and Data Mining</title>
		<imprint>
			<biblScope unit="page" from="197" to="206" />
			<date type="published" when="2011">2011</date>
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boosting Multi-Kernel Locality-Sensitive Hashing for Scalable Image Retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>35th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh European Conf. Computer Vision (ECCV)</title>
		<meeting>Seventh European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised Learning of Semantic Classes for Image Annotation and Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="394" to="410" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Annotation Refinement Using Random Walk with Restarts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Ann. ACM Int&apos;l Conf. Multimedia (MM)</title>
		<meeting>14th Ann. ACM Int&apos;l Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Pagerank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<imprint>
			<date type="published" when="1999-11">Nov. 1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Labelme: A Database and Web-Based Tool for Image Annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bipartite Graph Reinforcement Model for Web Image Annotation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM 15th Int&apos;l Conf. Multimedia (MM)</title>
		<meeting>ACM 15th Int&apos;l Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Face Recognition via Sparse Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seeing People in Social Context: Recognizing People and Social Relationships</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision (ECCV)</title>
		<meeting>11th European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EasyAlbum: An Interactive Photo Annotation System Based on Face Clustering and Re-Ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conf. Human Factors in Computing Systems (CHI)</title>
		<meeting>SIGCHI Conf. Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative Face Recognition for Improved Face Annotation in Personal Photo Collections Shared on Online Social Networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="28" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Graph Based Approach for Naming Faces in News Photos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1477" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Face Annotation by Mining the Web</title>
		<author>
			<persName><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth IEEE Int&apos;l Conf. Data Mining (ICDM)</title>
		<meeting>Eighth IEEE Int&apos;l Conf. Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face Recognition from Caption-Based Supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="82" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving People Search Using Query Expansions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th European Conf. Computer Vision (ECCV)</title>
		<meeting>10th European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="86" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Who&apos;s in the Picture</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining Weakly Labeled Web Facial Images for Search-Based Face Annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval</title>
		<meeting>34th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Unified Learning Framework for Auto Face Annotation by Mining Web Facial Images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st ACM Int&apos;l Conf. Information and Knowledge Management (CIKM &apos;12)</title>
		<meeting>21st ACM Int&apos;l Conf. Information and Knowledge Management (CIKM &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1392" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to Name Faces: A Multimodal Learning Scheme for Search-Based Face Annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval</title>
		<meeting>36th Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlinear Learning Using Local Coordinate Coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2259" to="2267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-Negative Sparse Coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<idno>cs.NE/ 0202009</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int&apos;l Conf. Machine Learning (ICML)</title>
		<meeting>20th Int&apos;l Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-Label Learning with Weak Label</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th AAAI Conf. Artificial Intelligence (AAAI)</title>
		<meeting>24th AAAI Conf. Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="593" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Object Categories from Google&apos;s Image Search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV &apos;05)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV &apos;05)</meeting>
		<imprint>
			<date type="published" when="2005-10">oct. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1816" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Animals on the Web</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1463" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised Face Alignment by Robust Nonrigid Mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>12th IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse Subspace Clustering: Algorithm, Theory, and Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno>abs/1203.1005</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust Face Recognition via Sparse Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dense Error Correction via l1-Minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3540" to="3560" />
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">SLEP: Sparse Learning with Efficient Projections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Arizona State Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient Euclidean Projections in Linear Time</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Ann. Int&apos;l Conf. Machine Learning (ICML)</title>
		<meeting>26th Ann. Int&apos;l Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Large-Scale Database of Images and Captions for Automatic Face Naming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf. (BMVC)</title>
		<meeting>British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attribute and Simile Classifiers for Face Verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face Recognition with Local Binary Patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="469" to="481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
