<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content-Based Image Retrieval with Compact Deep Convolutional Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Alzu'bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Computing</orgName>
								<orgName type="institution">University of the West of Scotland</orgName>
								<address>
									<postCode>PA1 2BE</postCode>
									<settlement>Paisley</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abbes</forename><surname>Amira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Computing</orgName>
								<orgName type="institution">University of the West of Scotland</orgName>
								<address>
									<postCode>PA1 2BE</postCode>
									<settlement>Paisley</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">KINDI CenterforComputingResearch</orgName>
								<orgName type="institution" key="instit2">Qatar University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naeem</forename><surname>Ramzan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Computing</orgName>
								<orgName type="institution">University of the West of Scotland</orgName>
								<address>
									<postCode>PA1 2BE</postCode>
									<settlement>Paisley</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Content-Based Image Retrieval with Compact Deep Convolutional Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDAC5B029BAFA29EDD1A0D7AB1FEE9C6</idno>
					<idno type="DOI">10.1016/j.neucom.2017.03.072</idno>
					<note type="submission">Received date: 8 August 2016 Revised date: 15 December 2016 Accepted date: 26 March 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CBIR</term>
					<term>Deep learning</term>
					<term>Convolutional neural networks</term>
					<term>Bilinear compact pooling</term>
					<term>Similarity matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) with deep learning have recently achieved a remarkable success with a superior performance in computer vision applications. Most of CNN-based methods extract image features at the last layer using a single CNN architecture with orderless quantization approaches, which limits the utilization of intermediate convolutional layers for identifying image local patterns. As one of the first works in the context of content-based image retrieval (CBIR), this paper proposes a new bilinear CNN-based architecture using two parallel CNNs as feature extractors. The activations of convolutional layers are directly used to extract the image features at various image locations and scales. The network architecture is initialized by deep CNNs sufficiently pre-trained on large generic image dataset then fine-tuned for the CBIR task. Additionally, an efficient bilinear root pooling is proposed and applied to the low-dimensional pooling layer to reduce the dimension of image features to compact but high discriminative image descriptors. Finally, an endto-end training with backpropagation is performed to fine-tune the final architecture and to learn its parameters for the image retrieval task. The experimental results achieved on three standard benchmarking image datasets demonstrate the outstanding performance of the proposed architecture at extracting and learning complex features for the CBIR task without prior knowledge about the semantic meta-data of images. For instance, using a very compact image vector of 16-length, we achieve retrieval accuracy 95.7%(mAP)on Oxford5K and88.6%on Oxford105K; which outperforms the best results reported by state-of-the-art approaches. Additionally, a noticeable reduction is attained in the required extraction time for image features and the memory size required for storage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the domain of content-based image retrieval (CBIR), the retrieval accuracy is essentially based on the discrimination quality of the visual features extracted from images or small patches. Image contents (objects or scenes) may include different deformations and variations, e.g. illumination, scaling, noise, viewpoint, etc, which makes retrieving similar images one of the challenging vision tasks. The typical CBIR approaches consist of three essential steps applied on images: detection of interest points, formulation of image vector, and similarity/dissimilarity matching.</p><p>In order to extract representative image features, the most existing CBIR approaches use some hand-crafted low-level features, e.g. scale-invariant features transform (SIFT) <ref type="bibr" target="#b0">[1]</ref> and speed-up robust features (SURF) <ref type="bibr" target="#b1">[2]</ref> descriptors. Such features are usually encoded by general orderless quantization methods such as vector of locally aggregated descriptors (VLAD) <ref type="bibr" target="#b2">[3]</ref>. The resulting image representations have shown a high capability on preserving the local patterns of image contents by capturing local characteristics of image objects, e.g. edges and corners. Therefore, they are suitable for the image retrieval task and widely used for matching local patterns of objects. However, convolutional neural networks (CNNs) have recently demonstrated a superior performance over hand-crafted features on image classification <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Adopting a deep learning procedure on multiple layers of convolutional filters makes CNNs able to subjectively learn even complex representations for many vision and recognition tasks.</p><p>Many recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> demonstrate that the CNN-based generic features adequately trained on sufficient and diverse image datasets, e.g. ImageNet <ref type="bibr" target="#b8">[9]</ref>, can be successfully applied to other visual recognition tasks. Additionally, performing a proper fine-tuning on CNNs using domain-specific training data can achieve a noticeable performance in common vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>; including object localization and instance image retrieval. Despite the promising results achieved by CNNs so far, there is no exact understanding or common agreement on how these deep learning architectures work; especially at the intermediate hidden layers. Several successful approaches <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> have applied CNNs to extract generic features for image retrieval tasks and obtained promising results. They mainly utilize the power of local features to generate a generic image representation based on some pre-trained CNNs. Nevertheless, many open questions and challenges need more investigation. Foremost, the effectiveness of fine-tuning the CNN models pretrained for specific task, e.g. image classification, on transfer learning to the CBIR task. Secondly, the discrimination quality of image features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>directly extracted from the convolutional layers compared to the features quantized using the traditional generic approaches such as VLAD. Thirdly, the ability of reducing the unfavourable high-dimensional image representations generated by the most of existing CNN-based architectures. Finally, a proper investigation is required on how efficient connections can be made between several CBIR aspects; including query handling, similarity/dissimilarity matching, and retrieval performance in term of search time and memory usage. All of these challenges motivated us to develop and utilize a different deep CNN architecture in order to address the problems associated with features quantization, model fine-tuning, high-dimensionality, and system performance affected by the training procedure and features lengths.</p><p>Accordingly, the main aim of this paper is to propose a new CNN-based learning model in the context of CBIR. The proposed architecture is inspired by the bilinear models proposed by Tenenbaum and Freeman <ref type="bibr" target="#b14">[15]</ref> to model the separation between the "content" and "style" factors of perceptual systems and by the promising results obtained using bilinear CNNs applied to fine-grained categorization <ref type="bibr" target="#b15">[16]</ref>. Specifically, two parallel CNNs are adopted to directly extract image features from the activations of convolutional layers using only the visual contents and without prior knowledge about the semantic meta-data of images, i.e. no tags, annotations, or captions have been used. Image representations are generated by accumulating the extracted features at image locations and scales in order to model local feature correlations. The proposed architecture is initialized by pretrained deep CNN models that adequately fine-tuned in unsupervised manner to learn the parameters for CBIR tasks using several standard retrieval datasets. Moreover, an efficient compact root pooling layer is also proposed based on the compact bilinear pooling recommended by Gao et al. <ref type="bibr" target="#b18">[19]</ref>, which demonstrates a noticeable improvement in the retrieval accuracy. Most critically, the resulting final image vectors are very compact so they reduce the time needed to extract them and reduce the memory size required to index the images and architecture with its parameters. Finally, the discriminatory capability of the image descriptors obtained by the proposed model is examined on different CBIR tasks, e.g. general, object-focused, landmarks image retrieval, and large-scale image retrieval.</p><p>The remaining part of this paper is organized as follows: Section 2 review the related works in literature; Section 3 presents the proposed compact bilinear architecture along with complete retrieval framework; Section 4 demonstrates and discusses the experiments carried out on several standard image retrieval datasets; and Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The most commonly CNNs architectures used in the CBIR are initially trained for classification tasks, where the representations extracted from the higher layers of CNN networks are usually used to capture semantic features for the category-level classification. Transfer learning of generic CNN features, trained on very large classification-based image datasets, to be used for image retrieval has shown a noticeable performance by several works. Wan et al. <ref type="bibr" target="#b10">[11]</ref> applied many existing deep learning methods for learning feature representation from images and their similarity measures with application to CBIR tasks, e.g. object and landmark image retrieval. They concluded that a direct use of features extracted from deep CNN can further boost the retrieval performance and outperform the hand-crafted features. However, authors used very large vocabulary sizes to construct image vectors using the bag-of-word (BOW) encoding approach, which degrades the retrieval performance in terms of training time and memory storage.</p><p>Most recently, the use of VLAD and fisher vectors (FV) quantization methods has been increased due to their effectiveness in image retrieval applications. Several works <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> have improved these approaches in order to aggregate the extracted features from the image into a generic representation. Accordingly, the VLAD and its variants have also been recently applied in the context of CNN-based image retrieval; especially at the higher layers, i.e. the output layers. Gong et al. <ref type="bibr" target="#b11">[12]</ref> extracted CNN feature activations of local image patches at multiple scale levels then they performed the VLAD orderless quantization to pool these features at each level separately and generate a generic image representation, referred as multiple orderless pooling (MOP-CNN). Ng et al. <ref type="bibr" target="#b12">[13]</ref> adopted a similar retrieval scheme by encoding the extracted features from deep CNNs at multiple scales and layers into a single VLAD vector. Yandex and Lempitsky <ref type="bibr" target="#b13">[14]</ref> also aggregated local CNN features into a general compact image vector based on sum pooling, which showed a better performance than other shallow-based and CNN-based models. Razavian et al. <ref type="bibr" target="#b16">[17]</ref> utilized a spatial search based on small and medium image representations extracted from convolutional networks (ConvNets) for visual image retrieval. Paulin et al. <ref type="bibr" target="#b17">[18]</ref> used patch-level descriptors by adapting a convolutional kernel network, i.e. batch-CKN, to perform an unsupervised learning of explicit feature embedding for both batch and image retrieval.</p><p>However, the majority of recent CNN-based approaches used in the CBIR domain have concatenated image features extracted at multiple scales then encoded by VLAD or BOW, and thus they have not considered the direct extraction of image features from the lower CNN layers for CBIR tasks. However, convolutional layers can be much more specialized and efficient than fully connected layers. Additionally, there are no assumptions about image features and patterns extracted from fully connected layersor by general aggregation methods since</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>they act as a general-purpose connection patterns. Moreover, they have an expensive performance cost in terms of memory and computations. The existing CNN-based CBIR approaches have also focused on the number and type of CNN layers used for feature extraction with using conventional pooling, e.g. max and sum pooling, and dimension reduction approaches, e.g. principal component analysis (PCA). Accordingly, more investigation is required on the effectiveness of image representations extracted and pooled directly from convolutional layers and then reduced to very compact lengths.</p><p>Unlike all existing CNNs and other deep models, we introduce a new deep bilinear CNN architecture in the context of visual CBIR. The proposed model is inspired by the successful application of bilinear models, which was first introduced by Tenenbaum and Freeman <ref type="bibr" target="#b14">[15]</ref>, in the context of image classification. Lin et al. <ref type="bibr" target="#b15">[16]</ref> applied a bilinear CNN-based architecture for fine-grained image categorization using two feature extractors. The resulting features are multiplied by outer product at each image location and pooled to form the final image descriptor. However, our proposed architecture and aims are different from their bilinear model in many directions. Firstly, our model is based on unsupervised feature learning extracted only from the visual image content, and therefore it does not depend on class labels, annotations, or bounding boxes. Secondly, the dimension of the extracted features is extremely reduced into a low-dimensional image representation using a modified pooling scheme (Section 3) of the compact bilinear pooling recommended by Gao et al. <ref type="bibr" target="#b18">[19]</ref>, which is crucial in terms of computation and memory usage. To the best of our knowledge, this is the first work employs the bilinear CNN-based architectures in the context of CBIR, which also performs transfer learning on several tasks, e.g. object, landmark retrieval, and large-scale image search. The experimental results achieved under different scenarios emphasize the efficiency of our deep learning model in the CBIR domain, which can further boost the retrieval performance in terms of accuracy, extraction and search speed, and memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Framework of Retrieval and Deep Learning</head><p>Our approach consists of three main steps: 1) Initialize the architecture by deep CNN networks pre-trained on millions of images; 2) fine-tune the bilinear CNN architecture on image retrieval datasets, i.e. transfer learning; and 3) extract features of query and dataset images. As shown in Figure <ref type="figure">1</ref>, the CNN architecture is based on two variants of recent neural networks <ref type="bibr" target="#b19">[20]</ref>: imagenet-vgg-m (VGG-m) and imagenet-vgg-verydeep-16 (VGG-16), and both are pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. These CNNs consists of convolutional layers, pooling layers, and fully connected layers. Both networks take images of size 224×224 pixels as input. To simplify the experiments, the fully connected layers are discarded from the fine-tuned bilinear CNN architecture, so that all image features are directly extracted from the activations of convolutional feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig.1. CRB-CNN architecture and retrieval framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Firstly, the original CNNs are truncated at the last convolutional layer in each model where the output size is 512. Specifically, the first 14 layers are taken from VGG-m and the first 30 layers from VGG-16. Secondly, three additional layers are added to the end of the resulting CNN architecture as follows: 1) root compact bilinear pooling to project the data into small size of N; 2) SQRT; and 3) L2 normalization. The resulting network is then fine-tuned on domain-specific (image retrieval) datasets using an end-to-end training. The lowdimensional bilinear features extracted from this network are formed into a single generic image vector by the inner product between the outputs of two extractors. Finally, as shown in the lower part of Figure <ref type="figure">1</ref>, the final architecture is used to extract features of queries and dataset images.</p><p>It is clear that all of images in our retrieval approach are indexed into one dataset of image vectors whose distance scores to each query image are computed. Several distance measures (e.g. Euclidean and Manhattan) are utilised to rank images then compute the retrieval accuracy and performance. The challenging task for the resulting network is the capability of preserving high discriminative image representations with a very compact size, which is a centric part of our architecture. An end-to-end training is conducted using unsupervised training, i.e. we group each dataset images into a set of classes according to their standard semantics but the network does not use any image labels, annotations or bounding boxes during the training and retrieval processes. Moreover, many critical issues in the context of image retrieval are considered; including the computation complexity, memory usage, and speed of feature extraction. Extensive experiments are carried out on the resulting architecture under several retrieval scenarios which is discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction and Retrieval</head><p>Given a pre-trained CNN network (VGG-m or VGG-16) with L layers, an input image I is warped into an 224x224 square to fit the size of training images then passed through the network in a forward pass of E epochs after applying the filters to the input image. In the last i-th convolutional layer, i.e. C 14 and C 30 in VGG-m and VGG-16 respectively, image features are accumulated at various locations and scales using the convolutional activations with a default output size of 512 for each of the two parallel CNNs. Specifically, a bilinear model B for CBIR task can be formulated as triple B = (f A , f B ,P). Both f A and f B are feature functions, and P is a pooling function. Given an image I and a location L, a feature function is a mapping</p><formula xml:id="formula_0">D a R I L f    :</formula><p>, where R a×D is the feature output of size a×D. Therefore, image features are combined by the bilinear feature combination of f A and f B at each image location l using the matrix outer product as follows:</p><formula xml:id="formula_1">Bilinear(l, I, f A ,f B ) = f A (l,I) T . f B (l,I)<label>(1)</label></formula><p>Hence, to construct an image vector the pooling function P aggregates the extracted bilinear features across all image locations.However, the bilinear pooling layer generates high dimensional features of size a×512 by each single CNN and accumulated by sum pooling, where a is the number of image local locations.Therefore, the dimension of pooled descriptors computed by the outer product between the resulting matrices is 512×512~= 262K. This high-dimensionality of image descriptor is unwieldy in the context of image retrieval where the indexing complexity and memory size are among of the most critical concerns. Therefore, a lowdimensional projection is applied to the extracted features using the root compact bilinear pooling.</p><p>The principal component analysis (PCA), which is one of the most commonly used approaches for dimensionality reduction, was initially applied to reduce the feature size at the testing level of image retrieval. The experimental results show that the retrieval accuracy is noticeably degraded after the PCA projection compared to the original size (262K). The reason is that because of the high dimensionality of resulting bilinear features (~262K) so that the PCA will be expensive and not suitable to get the principal components. Accordingly, another reduction approach is adopted in our pooling layer; specifically the compact pooling recommended in <ref type="bibr" target="#b18">[19]</ref>. As a result, our architecture replaces the bilinear pooling layer by its compact version but with efficient modification. We apply the square root on only one of the two bilinear descriptors generated after the reduction and before performing the inner product to form the image vector as detailed in the following:</p><p>Given two sets of local features X ={x 1 ,…,x |SP| , x SP ∈R a } from image I 1 and Y={y 1 ,…,y |SP| , y SP ∈R a } from image I 2 . The features are extracted using the last convolutional layer of the CNN network, where SP is a set of spatial locations. An image vector can be formed into (a×a) matrix using our modified root bilinear (RB) pooling as follows:</p><formula xml:id="formula_2">   SP sp T sp sp x x r X RB ) ( ) (<label>(2)</label></formula><p>where r(x SP ) = SQRT(|x SP |).By considering the kernelized version of bilinear pooling to compare X and Y of two images using the second order polynomial kernel:</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T      SP sp T sp sp SP sp T sp sp y y r x x r Y RB X RB ) ( , ) ( ) ( ), ( = 2 ), (     SP sp SP sp sp sp y x r (3)</formula><p>Then, any low-dimensional projection function applied to approximate image features into  (x)∈R dim and  (y)∈R dim , where dim&lt;&lt;c 2 , by calculating:</p><formula xml:id="formula_4">compact compact Y RB X RB Y RB X RB ) ( , ) ( ) ( ), (       SP sp SP sp sp sp y x ) ( , ) (   (4)</formula><p>In this modified version of compact pooling, the projection function random maclaurin (RM) <ref type="bibr" target="#b20">[21]</ref> is applied as recommended in <ref type="bibr" target="#b18">[19]</ref>. This rooted compact bilinear pooling has the advantages of breaking any symmetry property that may hold between the extracted features of the two CNN extractors; and largely increasing the retrieval accuracy due to the increment of descriptor's discrimination level, as presented in Section 4.2.</p><p>The resulting low-dimensional descriptor by the root compact pooling is then passed to the next layers, i.e. the SQRT and L2 normalization, as shown in Figure <ref type="figure">1</ref>. The process of feature extraction, pooling, and retrieval is simplified and summarized in Algorithm 1. The final fine-tuned architecture by backpropagation is used to extract the image vectors for both queries and dataset images in order to compute the distance scores then rank the retrieved images. Extensive experiments are carried out and discussed in Section 4 under different scenarios of network modelling, feature extraction, and projections. Holidays dataset <ref type="bibr" target="#b21">[22]</ref>. It is one of the standard benchmarking datasets commonly used in the CBIR to measure the robustness against image rotations, viewpoint and illumination changes, blurring, etc. The dataset consists of 1491 high resolution images with a large variety of scene types, e.g. natural, man-made, water and fire effects, etc., as shown in Figure <ref type="figure" target="#fig_1">2</ref> (top row). The dataset contains 500 image groups that represent distinct scenes. The first image of each image group is the query image and the correct relevant images are the other images of the group. For each query of 500 standard queries initiated in the retrieval system, the top relevant images are retrieved and ranked according to the similarity scores and then the standard mAP is computed to measure the retrieval accuracy based on the ground truth of Holidays dataset. The precision of ranked images is computed to measure the retrieval accuracy for any query as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Feature Extraction and Retrieval Procedures</head><formula xml:id="formula_5">images) (retrieved # images) retrieved images (relevant # ) (   k R P (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where the retrieved images are all of top images retrieved (R k ) and the relevant images are only images relevant to the query image according to its ground truth. For a single query image, the average precision (AP) is the average of the precision value obtained for the set of top k images existing after each relevant image is retrieved, and this value is then averaged over all queries in the image category. Therefore, if the set of relevant images for a query q j ∈ Q is {I 1 ,…., I m } where Q is the set of all queries, then the man average precision (mAP) is defined as:</p><formula xml:id="formula_7">mAP(Q)      m k k Q i R P m Q 1 1 ) ( 1 1 (6)</formula><p>Oxford buildings dataset <ref type="bibr" target="#b22">[23]</ref>. The Oxford Buildings dataset consists of 5062 images collected by searching for particular Oxford landmarks, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (middle row). The collection has a comprehensive ground truth for 11 different landmarks, each represented by 5 possible queries. This results in a set of 55 queries over which the retrieval system can be tested and evaluated using mAP as computed for Holidays dataset.</p><p>UK-bench dataset <ref type="bibr" target="#b23">[24]</ref>. It consists of 10200 images of 2250 different objects. Each object image is taken under four different viewpoints to get four visually similar images, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (bottom row). The first image of each object category is taken as query so 2250 queries are initiated in the experiments. The standard accuracy measure used for the UK-bench dataset is computing the precision at top 4 images then the results averages over all queries. The best accuracy can be achieved is 4, e.g. 1 indicates only one relevant image to the query is retrieved at top 4 images, and 4 indicates all relevant images are successfully retrieved and ranked. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments Setup</head><p>Two scenarios are examined in all experiments using the proposed model and denoted as compact root bilinear CNN (CRB-CNN).For simplicity, we refer the two models as: CRB-CNN-(M) and CRB-CNN-( <ref type="formula">16</ref>)for the VGG-M network and VGG-verydeep-16 network, respectively. The specifications of each scenario are listed in Table <ref type="table" target="#tab_0">1</ref>. In all experiments, the last layer is first trained using the logistic regression followed by fine-tuning the whole resulting model on the retrieval dataset using back-propagation training for a number of epochs (between 30 and 80). A small learning rate is set which is then changing gradually, and two scales are chosen in all experiments because it shows the best performance among a range of scales. Once the trained model is generated, it is then used to extract the features of all queries and images in the benchmarking datasets, i.e. Holidays, Oxford, and UK-bench. Basically, the following procedure is adopted to rank and show the top images for every query image: Let C be a collection of N images; let v n be the feature vector extracted from image n, where n ∈ 1,...,N; let d(v 1 , v 1 ) be a distance function defined between two vectors in the feature space; and let v q be the feature vector corresponding to a given query image. Accordingly, the images n sim in C most relevant and similar to the query image are the ones whose feature vectors minimize the distance to the query's feature vector:</p><p>) , ( min arg </p><formula xml:id="formula_8">1 n q N n sim v v d n    (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>All image descriptors are then indexed into a single dataset structure, i.e. database (DB) of image vectors. To record distance scores, every query image's vector is compared against all of DB vectors by using three common distance measures: Manhattan (L1), Euclidean (L2), and Cityblock. Images are then ranked according to their obtained scores. Finally, the most commonly used measure (mAP) is used to evaluate the retrieval accuracy, i.e. mAP, and computed based on the evaluation protocol of image dataset, i.e. considering only the top-k relevant images to each query.</p><p>For each model, the dimensionality of image features is reduced using the root compact bilinear pooling to a range of compact dimensions: (512), ( <ref type="formula">128</ref>), (64), <ref type="bibr" target="#b31">(32)</ref> and <ref type="bibr" target="#b15">(16)</ref>. Figure <ref type="figure" target="#fig_2">3</ref> shows sample results of the two models used to retrieve and rank the relevant images to each query image (top row). A query vector of small size and Euclidean distance are used to show the top 5 images. The ranked images to all queries are similar and belong to the same semantic group according to each dataset ground truth. In addition, different distances and positions are given to the ranked images by the two models, i.e. CRB-CNN-16 and CRB-CNN-M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Retrieval Accuracy on CBIR Tasks</head><p>In this section, the retrieval performance is evaluated in terms of accuracy, speed, and memory usage. The two models CRB-CNN-( <ref type="formula">16</ref>) and VGG-(M) are evaluated on the three image datasets that represent different CBIR tasks, i.e. Holidays for general retrieval, Oxford for landmark retrieval, and UK-bench for object-focused retrieval. The results of retrieval accuracy (mAP) of Holidays, Oxford, and UK-bench are shown in Table <ref type="table" target="#tab_1">2</ref>, Table <ref type="table" target="#tab_2">3</ref>, and Table <ref type="table" target="#tab_3">4</ref>, respectively. Firstly, the accuracy results reported in Table <ref type="table" target="#tab_1">2</ref> show that the very deep model CRB-CNN-( <ref type="formula">16</ref>)outperforms CRB-CNN-(M) by achieving better accuracy at most of vector lengths. Moreover, it is very clear that for both models the accuracy generally tends to increase when the vector length is decreased; emphasizing the astonishing capability of the proposed architectures on preserving high discrimination level while reducing the vector size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Additionally, the distance measures used for similarity matching have shown different performance for each architecture on Holidays. Both models perform better using Euclidean distance; especially using vectors of size 128, 64, and 16. The best accuracy achieved by the CRB-CNN-( <ref type="formula">16</ref>) on Holidays dataset is 95.1% using image vector of size 64, while the CRB-CNN-(M) is performing better using image vector of size 16 with 89.04% accuracy. The results listed in Table <ref type="table" target="#tab_1">2</ref> also show that the accuracy of both models is dramatically dropped using Manhattan distance on image vectors of size 32 and 16. For instance, the CRB-CNN-( <ref type="formula">16</ref>) performance is degraded by32% and 75% when the image vector is reduced from 64 to 32 and 16, respectively.</p><p>Secondly, the accuracy results obtained by the two models on Oxford dataset and shown in Table <ref type="table" target="#tab_2">3</ref> confirm the general trend of their performance. Specifically, they also perform better at the low image dimensions; especially at size 16 and the CRB-CNN-( <ref type="formula">16</ref>) and CRB-CNN-(M) achieve accuracy of 95.74% and 85.67%, respectively. These high accuracy values emphasize the efficiency of these models at retrieving very complex images even with many distractors exist in the most of images in Oxford dataset. Like in Holidays, the performance of both models is largely reduced at image when Manhattan distance is used at image vectors of size less than 64. As a result, high accuracy results obtained by the two models using the three distance measures except using Manhattan at vector of size 32 and 16.</p><p>Thirdly, Table <ref type="table" target="#tab_3">4</ref> shows the retrieval results achieved on UK-bench image dataset. First of all, the training procedure applied on this dataset is different from what has been applied on the other image datasets. Specifically, we have just applied only one training pass, i.e. one epoch, in the forward-propagation and backpropagation for the CRB-CNN-( <ref type="formula">16</ref>) and CRB-CNN-(M). Despite this shallow binary-like training, the accuracy results reported in Table <ref type="table" target="#tab_3">4</ref> are high using both models with the three distance measures. Unlike other datasets, the two models perform slightly better with image vectors of size between 128 and 512. These results affirm the effectiveness of the proposed architecture at learning visual features even with a large amount of image categories and variety of deformations, e.g. illumination, view-point, and rotation.</p><p>Finally, Figure <ref type="figure" target="#fig_3">4</ref> demonstrates the remarkable improvement in retrieval accuracy on Oxford dataset achieved by using the proposed root compact bilinear pooling (denoted as CRB with dotted lines in the figure) over the original compact pooling (denoted as CP with solid lines in the figure) and proposed in <ref type="bibr" target="#b18">[19]</ref>. The results obtained in Figure <ref type="figure" target="#fig_3">4</ref>(a) using the medium size CRB-CNN-(M) architecture show a noticeable increment in the retrieval accuracy on all vector lengths and distance measures, i.e. Euclidean, Manhattan, and CityBlock. Figure <ref type="figure" target="#fig_3">4</ref>(b) also confirms the superiority of the RCB layer in improving the retrieval accuracy using the very deep CRB-CNN-( <ref type="formula">16</ref>) architecture. Roughly speaking, both CRB architectures with root bilinear pooling are largely performing better than the compact pooling; especially with the small vector lengths . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Memory Usage and Search Time</head><p>As aforementioned, the compact size of image representations extracted by the CRB-CNN models is beneficial for the retrieval performance in terms of search time and memory size required to store the indexed images. In this section we provide more details on the retrieval performance the two models on all image datasets. All experiments in this work are reported using a CPU of 3.4GHz speed and 16GB RAM. The training process is carried out using the GPU NVIDIA Tesla K40.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> (left part) shows the average time that the CRB-VGG-( <ref type="formula">16</ref>) takes to extract the image features and formulate them into a single vector in each dataset. It is clear that even with this very deep model the time ranges between 200 and 750 milliseconds (ms), which depend on the image visual contents and the dimensions at query submission. This average time is about the half (80 to 400ms) needed by the CRB-CNN-(M). However, this time is almost the same over all dimensions of image vector, i.e. 16 to 512. For example in UK-bench, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>model takes about 490ms to extract a vector of size 512 while takes 480ms for a vector of size 16.Therefore, it spends extra time on data approximation from size 262K to 16 than the time spent to reduce the same size to 512. As a result, the time of image extraction is nearly the same over all image dimensions using both the CRB-CNN-( <ref type="formula">16</ref>) and CRB-CNN-(M).</p><p>On the other hand, Figure <ref type="figure" target="#fig_4">5</ref> (right part) presents how largely the memory storage is reduced in order to store every single image in the dataset. It is clear that the memory size is dropping linearly when the vector size goes from 512 to 16. The storage size required for every single image ranges from only 1.75 KB to 0.06 KB using image vectors of 512 to 16, respectively. Roughly speaking, one million images will approximately require 1.67 GB of memory using image vector of size 512 and only about 58.6 MB required using image vector of size 16. This massive save on memory usage is very important in the context of image retrieval and directly affects the performance of the CBIR applications.  For instance, Table <ref type="table" target="#tab_4">5</ref> provides more numerical details about the retrieval performance of our models CRB-CNN-( <ref type="formula">16</ref>) and CRB-CNN-(M)on Oxford dataset in terms of: 1) the average time required to extract every single image in milliseconds, 2) the average time needed to search the whole dataset including the similarity matching and image ranking, and 3) the average storage size required to store a single image on the disk. Foremost, both models (16 and M) have nearly the same average time required to search the whole dataset (5K images) and to sort the similarity scores for image ranking. This time takes 45ms in average over all vector dimensions, which is very fast due to the low-dimensionality of image vectors. In addition, the average time spent to extract the final image vector is 224ms in average for the very deep CRB-CNN-( <ref type="formula">16</ref>) and 84ms in average for the CRB-CNN-M model. Therefore, adding the time of image vectorization to the time of image search shows the efficiency of these models for the CBIR tasks. For example, the CRB-CNN-(M) takes only 129ms in average to extract the image vector and search the whole dataset with similarity matching using compact image lengths.</p><p>Finally, the average storage size required to store the indexed image is very small, e.g. less than 1KB for image of size &lt;256 and only 61Bytes (0.06KB) for the image of size 16. As a result, the medium and very deep models proposed in this work show high performance in all critical steps that should be considered in any CBIR application; including features extraction, image search and ranking, memory usage, and most critically the retrieval accuracy itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with the State-of-the-art</head><p>In this section, the accuracy results obtained by the CRB-CNN-( <ref type="formula">16</ref>) and CRN-CNN-(M) are compared on the three image datasets: Holidays, Oxford, and UK-bench. Table <ref type="table">6</ref> shows the retrieval accuracy (mAP) using highdimensional image vectors (upper part) and compact image vectors (lower part). It is clear that our models noticeably outperform the best results achieved by the local-based and CNN-based approaches even with highdimensional image vectors. Additionally, the CRB-CNN-( <ref type="formula">16</ref>) and CRB-CNN-(M) models have superiority of using smaller image vectors than other approaches including the compact CNN-based methods. For Holidays, an improvement of 10% achieved over the best accuracy of compact vectors and 6% over the high-dimensional vectors. For Oxford, the accuracy is largely increased by 35% over other large and compact image vectors. Finally, our models with only one training epoch achieved high and comparable accuracy, 3.56, compared to the other methods on the UK-bench dataset; emphasizing the high ability of the CRB-CNN models at learning image features for different types of CBIR tasks, i.e. general, landmarks, and object image retrieval. Table <ref type="table">6</ref>. Accuracy comparisons (mAP % and P@4) with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local and CNN-Based Approaches</head><p>Holidays Oxford UK-bench Spatial Pooling <ref type="bibr" target="#b17">[18]</ref> 89.7 84.4 -Neural codes <ref type="bibr" target="#b25">[26]</ref> 79.3 54.5 -OxfordNet <ref type="bibr" target="#b12">[13]</ref> 83.8 64.9 -VLAD <ref type="bibr" target="#b2">[3]</ref> 63.4 -3.47 Triangulation-Embedding <ref type="bibr" target="#b26">[27]</ref> 77.1 67.6 -Label-image Embedding <ref type="bibr" target="#b27">[28]</ref> 78.9 -3.36 Improved BOW <ref type="bibr" target="#b28">[29]</ref> 74.7 -3.55 SERVE <ref type="bibr" target="#b29">[30]</ref> 86.8 -3.69 HVLAD <ref type="bibr" target="#b30">[31]</ref> 72.1 63.8 3.56 Fine-residual VLAD <ref type="bibr" target="#b31">[32]</ref> 62.2 -3.43 S-sim <ref type="bibr" target="#b32">[33]</ref> 84.0 78.7  <ref type="formula">16</ref>), respectively. Foremost, the retrieval accuracy achieved by both models on this large-scale dataset is high and comparable with the results reported on Oxford5K under the same setting and using the same set of queries and ground truth. This emphasizes the robustness of the CRB-CNN models in searching and retrieving within images with numerous variations in contents and semantics. However, the accuracy results show that reducing the size of image vector eliminates most of unnecessary data while preserving the representative data with high discriminative level. This confirms the same conclusion reported on Oxford-5K, see Section 4.2. The best accuracy scores achieved by CRB-CNN-(M) is 73.30% using image representation of 16-dimension, while score of 88.63% is the best achieved by CRB-CNN-( <ref type="formula">16</ref>) also using 16-dimension. In consequence, the overall accuracy degradation compared to Oxford5K using an extreme dimensionality reduction (size-16) is about 11% using CRB-CNN-(M) and only 7% using CRB-CNN- <ref type="bibr" target="#b15">(16)</ref>. Again, the overall best scores of retrieval accuracy are obtained using Euclidean distance, followed by CityBlock then Manhattan, and the latter shows a noticeable degradation in the retrieval performance using images of size less than 32. Scaling up the deep architectures challenges their effectiveness in transferring the learned visual semantics on a small amount of training images in order to retrieve the most relevant images to the initiated query. Firstly, the CRB-CNN performance is compared with the deep models proposed by Babenko et al. <ref type="bibr" target="#b25">[26]</ref> on the full range of vector lengths, i.e. 16 to 512. Then, a comparison with other recent results reported on the large-scale image retrieval task is presented. Figure <ref type="figure" target="#fig_5">6</ref> shows the results obtained by: CRB-CNN-(M), CRB-CNN-( <ref type="formula">16</ref>), and neural codes approach <ref type="bibr" target="#b25">[26]</ref>. It is clear that both models (M) and ( <ref type="formula">16</ref>) using Euclidean measure outperforms the neural codes approach at all lengths of image representation; especially on the very compact ones, i.e. less than 64. In addition, the memory size required to store 105,000 images indexed by 16 dimensions is only about 6 MB, which is increasing linearly while the size of vector is increased. Finally, Table <ref type="table">9</ref> list the best results recently reported by state-of-the-art approaches on Oxford105K image dataset. The CRB-CNN outperforms the best recent scores of retrieval accuracy with minimum 9% and 24% improvement in accuracy using the CRB-CNN-(M) and CRB-CNN- <ref type="bibr" target="#b15">(16)</ref>, respectively. Table <ref type="table">9</ref>.Comparisons with state-of-the-art on Oxford105K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector Size mAP %</head><p>Razavian et al. <ref type="bibr" target="#b16">[17]</ref> 48.9 Babenko et al. <ref type="bibr" target="#b25">[26]</ref> 52.4 Yandex and Lempitsky <ref type="bibr" target="#b13">[14]</ref> 64.2 Jegou and Zisserman <ref type="bibr" target="#b26">[27]</ref> 61. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper introduces compact bilinear CNN-based architectures for several CBIR tasks using two parallel feature extractors without prior knowledge about the semantic meta-data of image contents. Image features are directly extracted from the activations of convolutional layers then largely reduced to very low-dimensional representations using the root bilinear compact pooling. The very deep architecture CRB-CNN-( <ref type="formula">16</ref>) and medium architecture CRB-CNN-(M) are fine-tuned for three CBIR tasks: general contents, landmarks, and object images. The extensive experiments conducted in this work provide several important conclusions. Firstly, the bilinear CRB-CNN models demonstrate high efficiency in learning even complex image contents belong to different semantic groups and include a lot of deformations and object distractors. Architectures initialization by deep models that are pre-trained on millions of images increases the level of image discrimination. Secondly, the CRB-CNN models reduce the image representations to very compact lengths, which remarkably boost the retrieval performance in terms of extraction and search time, and storage cost. Few tens of milliseconds (~ 130ms) are required to extract image features and search the database, and a small memory size (&lt; 1KB) is needed to store the image on disk. Thirdly, the results obtained show that in most cases the extracted representation becomes highly discriminative when reducing the initial size (262K) of convolutional features to a range of compact vector lengths (512 to 16). Fourthly, Euclidean distance measure shows the best retrieval accuracy over all vector dimensions on the three image dataset, followed by the CityBlock measure with close accuracy results and then followed by the Manhattan measure which shows a noticeable degradation in the performance at vector lengths less than 64. Finally, an end-to-end training is applied without using any annotations, content tags, or other meta-data, which confirms the high capability of the CRB-CNN models in CBIR tasks using only the features extracted from only the visual contents. The developed deep models are also evaluated on large-scale image retrieval and showed high retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>/</head><label></label><figDesc>Input: query images Q, dataset of N images, CL: last convolutional layer Output: mAP of all queries Q -Begin +For i = 1 to N do *ParFor cnn = 1 to 2 do // Two CNN feature extractor run in parallel 1. image = resize [image(i), 24,24] // Fit the size of input layer 2. F = extractCL(image) // F: convolutional features of size: w×512 *End 3. Fdim = rootBilinearPooling(F1,F2) // Generic image vector 4. V = sign(V) .* (absolute(V)) // Signed square root 5. V = V ./ norm(V+eps) // L2 normalization eps: floating-point relative accuracy +End ~For q = 1 to Q do // For all query images Q 6. Vq = repeat steps (1-6) 7. scores = distanceMeasure(Vq , VN) 8. topRanked = sort(scores) ~ End 9. mAP = computeMAP() // mean Average Precision -End Function: rootBilinearPooling Input: F1 from extractor CNN1, F2 from extractor CNN2 , where both have size of dim×1 Output: feature vector Fdim of size dim -Begin 1. [F1 , F2] = compactProjection(F1 , F2) // New F of size: dim×1 2. F1 = absolute(F1) 3. F1 = sqrt(F1) 4. Fdim = F1.*F2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Samples of image datasets: Holidays (top row), Oxford (middle row), and UK-bench (bottom row).</figDesc><graphic coords="7,72.00,538.18,426.97,161.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Sample queries and their top 5 similar images ranked from Holidays (a), Oxford (b), and UK-bench (c).</figDesc><graphic coords="8,65.47,382.40,440.13,363.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. The accuracy improvement achieved by our proposed root compact pooling over the compact pooling in [19].</figDesc><graphic coords="10,61.34,434.04,451.38,164.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. The vectorization time for each image in milliseconds (left), and the average memory size in Kilobytes to store a single image (right) using the CRB-CNN-16 model on a range of vector lengths.</figDesc><graphic coords="11,291.29,215.52,218.14,116.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Accuracy comparisons with compact vectors on Oxford105K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1 CRB-CNN-(M)73.3 CRB-CNN-<ref type="bibr" target="#b15">(16)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,59.84,463.93,451.38,273.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Specifications of the two scenarios performed in all experiments.</figDesc><table><row><cell>Model</cell><cell cols="2">Layers Truncated at</cell><cell>Initial length</cell><cell>Compact lengths</cell><cell>Distance Measure</cell></row><row><cell>CRB-CNN-(M)</cell><cell>19</cell><cell>Conv 14 +ReLU</cell><cell>512×512</cell><cell cols="2">16,32,64,128,256,512 L1, L2, CityBlock</cell></row><row><cell>CRB-CNN-(16)</cell><cell>35</cell><cell>Conv 30 +ReLU</cell><cell>512×512</cell><cell cols="2">16,32,64,128,256,512 L1, L2, CityBlock</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The retrieval accuracy mAP on Holidays dataset.</figDesc><table><row><cell>Image Vector Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The retrieval accuracy mAP on Oxford dataset.</figDesc><table><row><cell>Image Vector Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The precision at top4 on UK-bench dataset.</figDesc><table><row><cell>Image Vector Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The average performance results on Oxford using VGG-16 and VGG-M models.</figDesc><table><row><cell>Vector Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>. Large-Scale CBIR Using CRB-CNN Models This</head><label></label><figDesc>section presents the retrieval accuracy (mAP) of the CRB-CNN model on Oxford5K-Flickr100K image dataset. Table7and Table8list the results obtained by the CRB-CNN-(M) and CRB-CNN-(</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.24</cell></row><row><cell cols="5">Compact CNN-Based Approaches Length Holidays Oxford UK-bench</cell></row><row><cell>MOP-CNN [12]</cell><cell>512</cell><cell>78.4</cell><cell>-</cell><cell>-</cell></row><row><cell>NetVLAD-CNN [25]</cell><cell>256</cell><cell>86.0</cell><cell>63.5</cell><cell>-</cell></row><row><cell>Spatial Pooling [18]</cell><cell>256</cell><cell>74.2</cell><cell>53.3</cell><cell>-</cell></row><row><cell>Neural codes [26]</cell><cell>128</cell><cell>78.9</cell><cell>55.7</cell><cell>3.56</cell></row><row><cell>OxfordNet [13]</cell><cell>128</cell><cell>81.6</cell><cell>59.3</cell><cell>-</cell></row><row><cell>GoogLeNet [13]</cell><cell>128</cell><cell>83.6</cell><cell>55.8</cell><cell>-</cell></row><row><cell>CRB-CNN-16 (ours)</cell><cell>64</cell><cell>95.1</cell><cell>84.1</cell><cell>-</cell></row><row><cell>CRB-CNN-16 (ours)</cell><cell>16</cell><cell>81.85</cell><cell>95.7</cell><cell>-</cell></row><row><cell>CRB-CNN-M (ours)</cell><cell>64</cell><cell>86.3</cell><cell>72.1</cell><cell>-</cell></row><row><cell>CRB-CNN-M (ours)</cell><cell>16</cell><cell>89.0</cell><cell>85.7</cell><cell>-</cell></row><row><cell>CRB-CNN-16 (ours)</cell><cell>512</cell><cell>85.44</cell><cell>69.73</cell><cell>3.56</cell></row><row><cell>4.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The Retrieval Accuracy mAP(%) of CRB-CNN-(M) on Oxford105K.</figDesc><table><row><cell>Vector Size</cell><cell>Euclidean</cell><cell>Manhattan</cell><cell>CityBlock</cell><cell>DB Size on Memory (MB)</cell></row><row><cell>512</cell><cell>54.27</cell><cell>54.60</cell><cell>54.52</cell><cell>189.3</cell></row><row><cell>256</cell><cell>55.79</cell><cell>54.60</cell><cell>54.72</cell><cell>94.5</cell></row><row><cell>128</cell><cell>58.48</cell><cell>56.66</cell><cell>57.21</cell><cell>47.5</cell></row><row><cell>64</cell><cell>61.99</cell><cell>57.97</cell><cell>59.84</cell><cell>23.9</cell></row><row><cell>32</cell><cell>70.61</cell><cell>64.07</cell><cell>68.98</cell><cell>12.0</cell></row><row><cell>16</cell><cell>73.30</cell><cell>12.21</cell><cell>70.00</cell><cell>6.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The Retrieval Accuracy mAP(%)of CRB-CNN-(16) on Oxford105K.</figDesc><table><row><cell>Vector Size</cell><cell>Euclidean</cell><cell>Manhattan</cell><cell>CityBlock</cell><cell>DB Size on Memory (MB)</cell></row><row><cell>512</cell><cell>64.89</cell><cell>64.09</cell><cell>64.41</cell><cell>189.4</cell></row><row><cell>256</cell><cell>66.01</cell><cell>65.08</cell><cell>65.14</cell><cell>94.6</cell></row><row><cell>128</cell><cell>76.31</cell><cell>76.19</cell><cell>75.99</cell><cell>47.0</cell></row><row><cell>64</cell><cell>78.00</cell><cell>77.16</cell><cell>77.80</cell><cell>23.7</cell></row><row><cell>32</cell><cell>86.60</cell><cell>78.97</cell><cell>85.23</cell><cell>11.9</cell></row><row><cell>16</cell><cell>88.63</cell><cell>29.81</cell><cell>87.53</cell><cell>5.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We gratefully acknowledge the NVIDIA for their generous donation of the GPU Tesla K40 used in this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prof. Amira has been a PhD external examiner and member of advisory boards for many Universities worldwide and has participated as guest editor and member of the editorial board in many international journals. He has also been a regular referee for many national and international funding bodies, including (EPSRC-UK and QNRF-Qatar </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning and transferring midlevel image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning of binary hash codes for fast image retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: A comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting local features from deep networks for image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregating Local Deep Features for Image Retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yandex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6574v3</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local convolutional features with unsupervised training for image retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compact Bilinear Pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random feature maps for dot product kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hamming Embedding and Weak geometry consistency for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proceedings of the 10th European Conference on Computer vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Triangulation embedding and democratic aggregation for image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>J´egou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3310" to="3317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging category-level labels for instancelevel image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gordoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rodríguez-Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer IEEE Conference on Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3045" to="3052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SERVE: Soft and Equalized Residual VEctors for image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Uniforming residual vector distribution for distinctive image representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fine-residual VLAD for image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="1183" to="1191" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Database saliency for fast image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Biography</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
