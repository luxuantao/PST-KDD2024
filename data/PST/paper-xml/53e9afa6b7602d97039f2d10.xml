<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Expression Recognition Based on 3D Dynamic Range Model Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Binghamton</orgName>
								<address>
									<postCode>13902</postCode>
									<settlement>Binghamton, New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Binghamton</orgName>
								<address>
									<postCode>13902</postCode>
									<settlement>Binghamton, New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Expression Recognition Based on 3D Dynamic Range Model Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">870C0BCE1A72DF902C5A48CF68456790</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditionally, facial expression recognition (FER) issues have been studied mostly based on modalities of 2D images, 2D videos, and 3D static models. In this paper, we propose a spatio-temporal expression analysis approach based on a new modality, 3D dynamic geometric facial model sequences, to tackle the FER problems. Our approach integrates a 3D facial surface descriptor and Hidden Markov Models (HMM) to recognize facial expressions. To study the dynamics of 3D dynamic models for FER, we investigated three types of HMMs: temporal 1D-HMM, pseudo 2D-HMM (a combination of a spatial HMM and a temporal HMM), and real 2D-HMM. We also created a new dynamic 3D facial expression database for the research community. The results show that our approach achieves a 90.44% person-independent recognition rate for distinguishing six prototypic facial expressions. The advantage of our method is demonstrated as compared to methods based on 2D texture images, 2D/3D Motion Units, and 3D static range models. Further experimental evaluations also verify the benefits of our approach with respect to partial facial surface occlusion, expression intensity changes, and 3D model resolution variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research on FER has been based primarily on findings from Psychology and particularly on the Facial Action Coding System <ref type="bibr" target="#b0">[1]</ref>. Many successful approaches have utilized Action Units (AU) recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> or Motion Units (MU) detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Other well-developed approaches concentrate on facial region features, such as manifold features <ref type="bibr" target="#b11">[12]</ref> and facial texture features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Ultimately, however, all of above methods focus on most commonly used modality: 2D static images or 2D videos.</p><p>Recently, the use of 3D facial data for FER has attracted attention as the 3D data provides fine geometric information invariant to pose and illumination changes. There is some existing work for FER using 3D models created from 2D images <ref type="bibr" target="#b14">[15]</ref> or from 3D stereo range imaging systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. However, the 3D models that have been used are all static. The most recent technological advances in 3D imaging allow for real-time 3D facial shape acquisition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and analysis <ref type="bibr" target="#b19">[20]</ref>. Such 3D sequential data captures the dynamics of time-varying facial surfaces, thus allowing us to use 3D dynamic surface features or 3D motion units (rather than 2D motion units) to scrutinize facial behaviors at a detailed level. Wang et al <ref type="bibr" target="#b17">[18]</ref> have successfully developed a hierarchical framework for tracking high-density 3D facial sequences. The recent work in <ref type="bibr" target="#b19">[20]</ref> utilized dynamic 3D models of six subjects for facial analysis and editing based on the generalized facial manifold of a standard model.</p><p>Motivated by the recent work of 3D facial expression recognition reported by Yin et al <ref type="bibr" target="#b15">[16]</ref> based on a static 3D facial expression database <ref type="bibr" target="#b20">[21]</ref>, we extend the facial expression analysis to a dynamic 3D space. In this paper, we propose a spatio-temporal 3D facial expression analysis approach for FER using our newly-created 3D dynamic facial expression database. This database contains 606 3D facial video sequences with 101 subjects: each subject has six 3D sequences corresponding to six prototypic facial expressions. Our approach uses 3D labeled surface type to represent the human facial surface and transforms the feature to an optimal compact space using linear discriminative analysis. Such a 3D surface feature representation is relatively robust to changes of pose and expression intensities. To explore the dynamics of 3D facial surfaces, we investigated a 1D temporal HMM structure and extended it to a pseudo-2D HMM and a real 2D HMM. There have been existing HMM-based approaches for FER using 2D videos <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref>, by which either a 1D HMM or multi-stage 1D-HMM was developed. However, no true 2D-HMM structure was applied to address the FER problem. Our comparison study shows that the proposed real 2D-HMM structure is better than the 1D-HMM and pseudo 2D-HMM in describing the 3D spatio-temporal facial properties.</p><p>In this paper, we conducted comparative experiments using our spatio-temporal 3D model-based approach with approaches based on 2D/3D motion units, 2D textures, and 3D static models. The experimental results show that our approach achieves a 90.44% person-independent recognition rate in distinguishing the six prototypic expressions, which outperforms the other compared approaches. Finally, the performance of our proposed approach was evaluated on its robustness dealing with 1) partial facial surface occlusion, 2) expression intensity changes, and 3) 3D model resolution variations. The paper is organized as follows: we first introduce our new 3D dynamic facial expression database in Section 2. We then describe our 3D facial surface descriptor in Section 3 and the HMM classifiers in Section 4. The experimental results and analysis are reported in Section 5, followed by the conclusion in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dynamic 3D Face Database</head><p>There are some existing public 3D static face databases, such as FRGC 2.0 <ref type="bibr" target="#b22">[23]</ref>, BU-3DFE <ref type="bibr" target="#b20">[21]</ref>, etc. However, to the best of our knowledge, there is no 3D dynamic facial expression database publicly available. To investigate the usability and performance of the 3D dynamic facial models for FER, we created a dynamic 3D facial expression database <ref type="bibr" target="#b23">[24]</ref> using the Dimensional Imaging's 3D dynamic capturing system <ref type="bibr" target="#b18">[19]</ref>. The system captures a sequence of stereo images and produces the range models using a passive stereo-photogrammetry approach. At the same time, 2D texture videos of the dynamic 3D models are also recorded. Figure <ref type="figure" target="#fig_0">1</ref> shows the dynamic 3D face capture system with three cameras. Each subject was requested to perform the six prototypic expressions (i.e., anger, disgust, fear, smile, sad, and surprise) separately. Each 3D video sequence captured one expression at a rate of 25 frames per second and each 3D video clip lasts approximately 4 seconds with about 35,000 vertices per model. Our database currently consists of 101 subjects including 606 3D model sequences with 6 prototypic expressions and a variety of ethnic/racial ancestries. An example of a 3D facial sequence is shown in Figure <ref type="figure" target="#fig_0">1</ref>. More details can be found in <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D Dynamic Facial Surface Descriptor</head><p>The dynamic 3D face data provides both facial surface and motion information. Considering the representation of facial surface and the dynamic property of facial expressions, we propose to integrate a facial surface descriptor and Hidden Markov Models to analyze the spatio-temporal facial dynamics. It is worth noting that we aim at verifying the usefulness and merits of such 3D dynamic data for FER in contrast to the 2D static/dynamic data or 3D static data. Therefore, we do not focus on developing a fully automatic system for FER in this paper. Our system is outlined in Figure <ref type="figure" target="#fig_1">2</ref>, which consists of model pre-processing, HMM-based training, and recognition. In the first stage, we adapt a generic model (i.e., tracking model) to each range model of a 3D model sequence. The adaptation is controlled by a set of 83 pre-defined key points (colored points on the generic model in Figure <ref type="figure" target="#fig_1">2</ref>). After adaptation, the correspondence of the points across the 3D range model sequence is established. We apply a surface labeling approach <ref type="bibr" target="#b24">[25]</ref> to assign each vertex one of eight primitive shape types. Thus, each range model in the sequence is represented by a "label map", G, as shown in the 3D shape feature space of Figure <ref type="figure" target="#fig_1">2</ref>, where different colors represent different labeled shape types. We use Linear Discriminative Analysis (LDA) to transform the label map to an optimal compact space to better separate different expressions. Given the optimized features, the second stage is to learn one HMM for each expression. In recognition, the temporal/spatial dynamics of a test video is analyzed by the trained HMMs. As a result, the probability scores of the test video to each HMM are evaluated by the Bayesian decision rule to determine the expression type of the test video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Facial Model Tracking and Adaptation</head><p>As the high-resolution range models vary in the number of vertices across 3D video frames, we must establish the vertices' correspondences and construct a common feature vector. To do so, we applied a generic model adaptation approach to "sample" the range models. This process consists of two steps: control points tracking and generic model adaptation. A set of 83 pre-defined key points is tracked using an active appearance model based approach on 2D video sequences <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>, where the key points in the initial frame are manually picked. To reduce the tracking error, a post-processing procedure was applied by manually correcting some inaccurately tracked points. Since the 2D texture and the 3D range model of each frame are matched accurately from the system, the key points tracked in the 2D video can be exactly mapped to the 3D range surface. This semi-automatic approach allows us to obtain accurate control points on the sequential models. Figure <ref type="figure" target="#fig_0">1</ref> (bottom row) shows an example of a tracked sequence. The adaptation procedure is as follows: Given the N (=83) control points U i = (u i,x , u i,y , u i,z )</p><p>T ∈ R 3 on the generic model and the corresponding tracked points V i ∈ R 3 on each range model, we use the radial basis function (RBF) to adapt the generic model on the range face model. The interpolation function is formulated as:</p><formula xml:id="formula_0">f (p) = c1 + [c2c3c4] × p + N i=1 λiϕi (|p -Ui |) (1)</formula><p>where p i is a non-control vertex on the generic model and ϕ i is the RBF for U i . All coefficients c k (k=1,..,4) are determined by solving the equation:</p><formula xml:id="formula_1">f (Ui) = Vi, i = 1...N ,</formula><p>where N i=1 λ i = 0 and N i=1 U i λ i = (0, 0, 0) T . Then, the non-control vertex p i is mapped to f (p i ). The result of adaptation provides sufficient geometric information for subsequent labeling. Figure <ref type="figure" target="#fig_1">2(a)</ref> shows an example of an adapted model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geometric Surface Labeling</head><p>3D facial range models can be characterized by eight primitive surface features: convex peak, concave pit, convex cylinder, convex saddle, concave saddle, minimal surface, concave cylinder, and planar <ref type="bibr" target="#b24">[25]</ref>. After the tracking model is adapted to the range model, each vertex of the adapted model is labeled as one of the eight primitive features. This surface labeling algorithm is similar to the approach described in <ref type="bibr" target="#b15">[16]</ref>. The difference is that eight primitive features rather than twelve features are used for our expression representation because we apply a local coordinate system for feature calculation. Let p = (x, y, z) be a point on a surface S, N p be the unit normal to S at point p, and X uv be a local parameterization of surface S at p. A polynomial patch 2 + Gy 3 is used to approximate the local surface around p by using X u , X v and N p as a local orthogonal system. We then obtain the principal curvatures by computing the eigenvalues of the Weingarten matrix: W = (A, B; B, C). After obtaining the curvature values of each vertex, we apply the classification method described in <ref type="bibr" target="#b24">[25]</ref> to label each vertex of the adapted model. Thus, each range model is represented by a label map G = [g 1 , g 2 , ..., g n ], composed of all vertices' labels on the facial region. Here, g i is label types and n is the number of vertices in the facial region of the adapted model.</p><formula xml:id="formula_2">z (x, y) = 1 2 Ax 2 + Bxy + 1 2 Cy 2 + Dx 3 + Ex 2 y + F xy</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimal Feature Space Transformation</head><p>We now represent each face model by its label map G. We use LDA to project G to an optimal feature space O G that is relatively insensitive to different subjects while preserving the discriminative expression information. LDA defines the within-class matrix S w and the between-class matrix S b . It transforms a n-dimensional feature to an optimized</p><formula xml:id="formula_3">d-dimensional feature O G by OG = DO T • G, where d &lt; n , DO = arg maxD| D T S b D ¡ / D T SwD ¡¡</formula><p>and D, projection matrix. For our experiments, the discriminative classes are 6 expressions, thus the reduced dimension d is 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HMM Based Classifiers</head><p>Facial expression is a spatio-temporal behavior. To better characterize this property, we used Hidden Markov Models to learn the temporal dynamics and the spatial relationships of facial regions. In this section, we describe the Temporal-HMM (T-HMM), Pseudo Spatio-Temporal HMM (P2D-HMM), and real 2D HMM (R2D-HMM), progressively. P2D-HMM is extended from T-HMM, and in turn, R2D HMM is extended from P2D-HMM. As we will discuss, R2D-HMM is the most appropriate method for learning dynamic 3D face models to recognize expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporal HMM</head><p>Each prototypic expression is modeled as an HMM. Let λ = [A, B, π] denote an HMM to be trained and N be the number of hidden states in the model, we denote the states as S = {S 1 , S 2 , ..., S N } and the state at t is q t (see top row of Figure <ref type="figure" target="#fig_2">3</ref>). A = {a ij }is the state transition probability distribution, where </p><formula xml:id="formula_4">a ij = P [q t+1 = S j |q t = S i ], 1 ≤ i, j ≤ N . B = {b j (k)}is the observation probability distribution in state j, k is an observation . We use Gaussian distributions to estimate each B = {b j (k)} , where b j (k) = P [k|q t = S j ] ∼ N (μ j , Σ j ) , 1 ≤ j ≤ N . Let π = {π i } be the initial state distribution, where π i = P [q 0 = S i ] , 1 ≤ i ≤ N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial HMM (S-HMM):</head><p>Based on the feature points tracked on the facial surface (e.g., contours of eyebrows, eyes, nose, mouth, and chin), we subdivide each 3D frame model of a sequence into six regions, as shown in Figure <ref type="figure" target="#fig_1">2</ref>(b) (R 1 , R 2 , ..., R 6 ). We then build a 6-state 1D HMM, corresponding the six regions, as shown in each column of P2D-HMM in Figure <ref type="figure" target="#fig_2">3</ref>. Similar to the case of entire face regions in the previous section, we transform the labeled map of each sub-region of a frame to an optimized feature space using LDA, denoted as</p><formula xml:id="formula_5">O Gi = (O Gi,1 , O Gi,2 , O Gi,3 , O Gi,4 , O Gi,5 ) , (i = 1..6),</formula><p>where i is the region index of a frame model. We trained one HMM for each expression. Given a query face sequence with a length N , we compute the likelihood score of each frame and use the Bayesian decision rule to decide the frame's expression type. We make a final decision Decision S using majority voting. Thus, the query model sequence is recognized as an expression if this expression is the majority result among N frames. As this method tracks spatial dynamics of a facial surface, we call it a spatial HMM (S-HMM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of Spatial and Temporal HMMs:</head><p>To model both spatial and temporal information of 3D face sequences, we combine the S-HMM and the T-HMM to construct a pseudo 2D HMM (P2D-HMM) (see Figure <ref type="figure" target="#fig_2">3</ref>). The final decision Decision P 2D is based on both Decision S and Decision T . The decision rule of the P2D-HMM is also described Figure <ref type="figure" target="#fig_2">3</ref>. Here, we define Conf idence S as the ratio of the number of majority votes versus the total number of frames in the query model sequence. In our experiment, we took 6 frames as a sequence, and chose the threshold for this ratio as 0.67. As a consequence, if at least 4 frames of a query sequence are recognized as expression A by the S-HMM, we determine the query sequence is A. Otherwise, the result comes from the Decision T . Essentially, P2D-HMM uses the learned facial temporal characteristics to compensate for the learned facial spatial characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Real 2D Spatio-temporal HMM</head><p>The aforementioned HMM-based approaches are essentially 1-D or pseudo-2D approaches. However, the dynamic 3D facial models are four dimensional (i.e., 3D plus time). Considering the complexity of high-dimensional HMMs and motivated by the work of Othman et al <ref type="bibr" target="#b27">[28]</ref> for 2D face recognition, we developed a real 2D HMM (R2D-HMM) architecture to learn the 3D facial dynamics over time. As shown in Figure <ref type="figure" target="#fig_2">3</ref> (bottom-right), this architecture allows for both spatial (vertical) and temporal (horizontal) transition to each state simultaneously. The number of states along spatial (vertical) or temporal (horizontal) axes are all six. Simply put, each 3D sequence contains 6 temporal states, and each frame contains 6 spatial states from top to bottom. The transition from region R i of the previous frames to another region R j of the current frame can be learned from the R2D-HMM. In Figure <ref type="figure" target="#fig_2">3</ref>, H 4,2;4,3 and V 3,3;4,3 are the horizontal and vertical transition probabilities from the state S 4,2 and the state S 3,3 to the current state S 4,3 respectively, and a 4,3;4,3 is the self-transition probability of the state S 4,3 . Let O r,s be the observation vector of the r th region of the s th frame in a 3D video sequence, the corresponding set of feature vectors is defined as</p><formula xml:id="formula_6">O {m,n} = {O r,s : 1 ≤ r ≤ m, 1 ≤ s ≤ n}.</formula><p>The feature vector set of the past observation blocks O &lt;m,n&gt; is derived by excluding the current observation block O m,n , where O &lt;m,n&gt; = O {m,n} -O m,n . Note that the joint probability of the current state and the observations up to the current observation P q m,n = S a,b , O {m,n} can be predicted based on past observation blocks in a recursive form:</p><formula xml:id="formula_7">P q m,n = S a,b , O {m,n} = P (O m,n |q m,n = S a,b ) • | M,N i,j=1,1 P (q m,n = S a,b |q m-1,n = S i,j ) P q m-1,n = S i,j , O {m-1,n} • M,N k,l=1,1 P (q m,n = S a,b |q m,n-1 = S k,l ) P q m,n-1 = S k,l , O {m,n-1} | 1/2 (2)</formula><p>Similar to the standard 1-D HMM, approach, the state matrix is denoted as</p><formula xml:id="formula_8">δ m,n (a, b) = max qm-1,m,qm,n-1 P [q m,n = S a,b , O 1,1 , ...O m,n |λ]. The observation probability distribution B a,b (O m,n ) is given by B a,b (O m,n ) = 1 [2π] v/2 Σ 1/2 • e ( Om,n -μ a,b ) Σ -1 a,b ( Om,n -μ a,b ) T 2<label>(3)</label></formula><p>Using the Viterbi algorithm, we estimate the model parameter λ as</p><formula xml:id="formula_9">P (O, Q * |λ) is maximized, where P (O, Q * |λ) = max a,b [δ M,N (a, b)],</formula><p>and Q * is the optimal state sequence. This structure assumes the state transitions to be left-to-right horizontally and top-to-bottom structure vertically. We set the transition matrix in the diagonal direction to be zeros using the same calculation as described in <ref type="bibr" target="#b27">[28]</ref>. The expected complexity of the R2D-HMM method is only two times that of the 1D T-HMM structure with the same number of states. In our experiment, given a six-frame sequence, the observation vector is defined by a 6 × 6 matrix O, in which each cell is an observation block denoted as</p><formula xml:id="formula_10">O r,s = (O r,s,1 , O r,s,2 , O r,s,3 , O r,s,4 , O r,s,5 ) (r, s = 1...6)</formula><p>, where s is the frame index, r is the region index of the frame s, and O r,s (r, s = 1...6) is the optimized feature after the label map of the region r of the frame s is transformed using LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Analysis</head><p>We conducted person-independent experiments on 60 subjects selected from our database. To construct the training set and the testing set, we generated a set of 6-frame subsequences from each expression sequence. To do so, for each expression sequence of a subject, we chose the first six frames as the first subsequence. Then, we chose 6-consecutive frames starting from the second frame as the second subsequence. The process is repeated by shifting the starting index of the sequence every one frame till the end of the sequence. The rationale for this shifting is that a subject could come to the recognition system at any time, thus the recognition process could start from any frame. As a result, 30780 (= 95 × 6 × 54) subsequences of 54 subjects were derived for training, and 3420 (= 95 × 6 × 6) subsequences of the other 6 subjects were derived for testing. Following a ten-fold cross-validation, we report the average recognition rates of the ten trials as the final result. Our database contains not only the 3D dynamic model sequences but also the associated 2D texture videos. This allows us to compare the results using both 3D data and 2D data of same subjects simultaneously.</p><p>In the following section, we report the results of our proposed approaches using the 3D dynamic data and their comparative results of the existing approaches using 2D data and 3D static data. All the experiments were conducted in a person-independent fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic 3D region-based approaches:</head><p>We conducted experiments using the Temporal 1D-HMM (T-HMM), Pseudo-2D HMM (P2D-HMM), and Real 2D HMM (R2D-HMM) based on the 3D dynamic surface descriptor. As previously discussed, our facial feature descriptor is constructed from vertices' labels of either entire face region or local facial regions, and we dubbed these methods as "3D region-based" approaches. The experimental results are reported in the bottom three rows of the right of Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static 2D/3D region-based approaches:</head><p>(1) 2D static texture baseline: We used the Gabor-wavelet based approach <ref type="bibr" target="#b13">[14]</ref> as a 2D static baseline method. We used 40 Gabor kernels including 5 scales and 8 orientations and applied them to the 83 key points on the 2D texture frames of all video sequences. (2) 3D static models baseline: The LLE based <ref type="bibr" target="#b28">[29]</ref>, PCA-based, and LDA-based approaches <ref type="bibr" target="#b29">[30]</ref> were implemented as the 3D static baseline methods for comparison. The input vector for these three approaches is feature G as explained in section 3.2. For the LLE-based method, we first transform the label map G of each range model to the LLE space and select key frames using k-means clustering. Then, all selected key frame models are used as the gallery models for classification. We use majority voting to classify each 3D query model in the test set. The PCA-based approach and LDA-based approach take the labeled feature G as input vector and apply the PCA and LDA for the recognition.</p><p>(3) 3D static models using surface histograms: We implemented the algorithm reported in <ref type="bibr" target="#b15">[16]</ref> as the 3D static baseline method for comparison. We treat each frame of the 3D model sequences as a 3D static model. Based on <ref type="bibr" target="#b15">[16]</ref>, a so-called primitive surface feature distribution (PSFD) face descriptor is implemented and applied for six-expression classification using LDA. As seen from Table <ref type="table" target="#tab_0">1</ref>, our dynamic 3D model-based HMM approaches outperforms the above static 2D/3D-based approaches. The performance of the PSFD approach is relatively low when it is tested on our 3D dynamic database because its feature representation is based on the static model's surface feature distribution (i.e., histogram). Such a representation may not detect local surface changes in the presence of low-intensity expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic 2D/3D MU-based approaches:</head><p>To verify the usefulness of 3D motion units (MU) derived from our dynamic 3D facial models, and to compare it with the 2D MUbased approaches and our dynamic 3D region-based approaches, we implemented the approach reported by Cohen et al <ref type="bibr" target="#b8">[9]</ref> as the MU-2D baseline method.</p><p>(1) MU-2D based: According to <ref type="bibr" target="#b8">[9]</ref>, 12 motion units (MUs) are defined (as the 12 motion vectors) in areas of eyebrows, eye lids, lips, mouth corner and cheeks (see the left three images of Figure <ref type="figure" target="#fig_3">4</ref>). Since we have tracked 83 key points on both 2D videos and 3D models as described in Section 3.1, the 12 MU points can be obtained from the tracking result. Note that although more MU points can be used from the tracking (as studied by Pantic et al in <ref type="bibr" target="#b7">[8]</ref>), to be a fair comparison to the baseline approach, we only used the same 12 MU points as the ones defined in <ref type="bibr" target="#b8">[9]</ref>. To compensate for the global rigid motion, we align current frame with the first frame using the estimated head orientation and movement from our adapted 3D tracking model. As such, the displacement vector of a MU point in frame i is obtained by Disp (i) = F i -F ne , where F ne is the position of the MU point in the first frame (with a neutral expression) and F i is the position of the MU point in the frame i. Figure <ref type="figure" target="#fig_3">4</ref> (left three images) is an example of the 2D MUs derived from a video sequence. In our experiment, we used the 12 MUs, derived from the 2D videos, as the input to the baseline HMM <ref type="bibr" target="#b8">[9]</ref> to classify the six prototypic expressions. (2) MU-3D based: This is an extension of MU-2D method. It derives 3D displacement vectors of the 12 MUs from the dynamic 3D facial videos. Similarly, the 3D model of the current frame is also aligned to the 3D model of the first frame. The compensated 3D motion vectors are then used for HMM classification. Note that although the motion vectors of 2D and 3D models look alike in frontal view, they are actually different since 3D MUs also have motions perpendicular to the frontal view plane, as illustrated in the 2 nd image from right of Figure <ref type="figure" target="#fig_3">4</ref>. From Table <ref type="table" target="#tab_0">1</ref>, the MU-2D approach achieves a comparable result to that reported in <ref type="bibr" target="#b8">[9]</ref> in the case of person-independent recognition. The MU-3D approach outperforms the MU-2D approach because 3D models provides more motion information for FER. Nevertheless, it is not superior to our 3D label-based spatio-temporal approaches because the MU-based approaches do not take advantage of entire facial surface features and rely on very few feature points for classification, and thus are relatively sensitive to the influence of the inaccurate feature detection. The experiment also shows that our 3D label-based R2D-HMM method achieves the best recognition result (90.44%). However, the confusion matrix (Table <ref type="table" target="#tab_1">2</ref>) shows that sad, disgust, and fear expressions are likely to be mis-classified as anger. Our R2D-HMM based approach does not rely on a few features. On the contrary, it takes advantage of entire 3D facial features as well as their 3D dynamics, and thus is more closely matched to the 3D dynamic data and more tolerant to individual feature errors than other compared approaches are.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation Using R2D-HMM</head><p>To further evaluate our spatio-temporal based approaches for 3D dynamic facial expression recognition, we conducted experiments to test the robustness of our R2D-HMM method with respect to three aspects: partial facial surface occlusion, expression intensity variation, and 3D model resolution variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial facial surface occlusion:</head><p>Limited by views used in our current face imaging system, the facial surface may be partially missing due to the pose variation. To test the robustness of our proposed 3D facial descriptor and the dynamic HMM based classifier, we simulated the situation by changing the yaw and pitch angles of the facial models and generating a set of partially visible surfaces under different views. Ideally, we shall use the ground-true data collected systematically from a variety of views. However, it is hard (as well as expensive) to have such collection due to the difficulty to control the exact degree of pose during the subjects' motion. As such, in this paper we adopt the simulation approach for this study. Such a simulation allows us to study the performance of our proposed expression descriptor in the condition of partial surface invisible with a controllable degree of rotation. For the set of visible surfaces at different orientations, we report the recognition rate separately. Figure <ref type="figure" target="#fig_4">5</ref> shows the facial expression recognition rates with different yaw and pitch angles. The recognition results are based on our proposed dynamic-3D R2D-HMM based approach and the static-3D LDA-based approach. Generally, it shows that our dynamic approach outperforms the static approach in any situation since the motion information compensates for the loss of spatial information.</p><p>As shown in the the bottom row of Figure <ref type="figure" target="#fig_4">5</ref>, our approach achieves a relatively high recognition rate (over 80%) even when the yaw and pitch angles change to 60 degrees, which demonstrates its robustness to the data loss due to the partial data invisible. The first row of Figure <ref type="figure" target="#fig_4">5</ref> shows the FER rate when the pose changes in only one dimension (yaw/pitch). Out of the useful range (i.e., either pitch or yaw angle changes exceed 150 degrees from the frontal view), the FER rate degrades to zero dramatically because of the paucity of useful information for recognition. The recognition curve of yaw's rotation within the useful range (Top-Left of Figure <ref type="figure" target="#fig_4">5</ref>) is approximately symmetric with respect to the zero yaw angle. The recognition rate does not decrease too much even when the yaw angle is close to 90-degree (corresponds to half face visible). This is because either the left part or the right part of a face compensates for the other in the 3D space due to the approximate symmetric appearance of the face along the nose profile. However, the recognition curve of tilts rotation within the useful range is a little asymmetric as shown in the Top-Right of Figure <ref type="figure" target="#fig_4">5</ref>. When the face is tilted up, the recognition rate is degraded not as much as when the face is tilted down in the same degree. This asymmetric property implies that the lower part of the face may provide more useful information than the upper part for expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variation of expression intensity:</head><p>Our approach can also deal with variations of expression intensity since it not only includes different levels of intensities but also considers their dynamic changes. Based on our observation, we simply separated each 3D video sequence into two parts: a low intensity sequence (e.g., subsequences close to the starting or ending frames showing near-neutral expressions) and a high intensity sequence (subsequences excluding the low-intensity sequences). We performed the test on the low-intensity and high-intensity expressions individually using the R2D-HMM approach and the static PSFD approach <ref type="bibr" target="#b15">[16]</ref>. Our training set includes both levels of intensities. The results show that the R2D-HMM method can detect both weak and strong expressions well. It achieves a 88.26% recognition rate of low intensity expressions and 91.58% recognition rate of high intensity expressions. However, the PSFD method has 71.72% recognition rate of high intensity expressions. It has less than 50% recognition rate for low intensity expressions. The main reason is that the static surface histogram descriptor may not be able to capture small variations of facial features as our 3D surface label descriptor does. In addition, the high performance of our approach is also attributed to the applied R2D-HMM classifier, which learns temporal transitions of dynamic facial surfaces effectively for both low-intensity and high-intensity expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variation of facial model resolutions:</head><p>We down-sampled the test models to a lowresolution version with around 18,000 vertices, which is almost half the resolution of the original facial models (35,000 vertices) used for training. We then conducted the experiment to see whether the proposed approach works well for facial models with different resolutions. Based our R2D-HMM approach, the recognition rate for the low resolution models is 89.78%, which is comparable to the result of high resolution models (90.44%). This demonstrates that our approach has certain robustness to different resolutions, despite the fact that different resolutions could blur or sharpen the shape of facial surface. This result is supported by the psychological finding: blurring the shape information has little effect on the recognition performance as long as the motion channel is presented <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions and Conclusions</head><p>In this paper, we proposed a spatio-temporal approach to study the viability of using dynamic 3D facial range models for facial expression recognition. Integrating the 3D facial surface descriptor and the HMMs (R2D-HMM, or P2D-HMM, or T-HMM), our system is able to learn the dynamic 3D facial surface information and achieves 90.44% person-independent recognition rate with both low and high intensities. In general, the HMM has been widely used for 2D facial expression recognition and face recognition. However, the way that we applied the real 2D-HMM to address 3D dynamic facial expression recognition is novel. We have extended the work of FER from static 3D range data to 3D videos. Many previous studies showed that sequential images are better than static images for FER <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>. We have verified that this statement holds true for 3D geometric models. The advantage of our 3D dynamic model based approach has been demonstrated as compared to several existing 2D static/video based and 3D static model based approaches using our new 3D dynamic facial expression database. This database will be made public to the research community. Ultimately, however, our focus was to study the usefulness of the new dynamic 3D facial range models for facial expression recognition rather than develop a fully automatic FER system. Our current work requires a semi-automatic process to select feature points at the initial stage. A fully automatic system with a robust 3D feature tracking will be our next stage of the development. To investigate the recognition performance in terms of large pose variations, we will design a new approach to measure the exact pose degree during the capture of ground-true spontaneous expressions. In addition, we will also investigate an approach to detect 3D action units and integrate the motion vector information with our surface label descriptor in order to improve the current FER performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left:Dynamic 3D face capturing system setup. Right: sample videos of a subject with smile expression(from top to bottom: shaded models, textured models, and wire-frame models with 83 tracked control points).</figDesc><graphic coords="3,61.40,55.48,96.80,101.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left: Framework of the FER system. Right: sub-regions defined on an adapted model (a) and a labeled model (b).</figDesc><graphic coords="3,56.96,361.45,252.05,115.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Top:T-HMM; Bottom-left and middle:P2D-HMM and its decision rule; Bottomright:R2D-HMM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of MUs. Left three: 2D-MUs on the initial frame, motion vectors of MUs from the initial frame to the current frame, and MUs on the current frame of a 2D sequence. Right four: 3D-MUs on the initial frame, 3D motion vectors of MUs with two different views, and MUs on the current frame of a 3D sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. FER results with simulated partial data missing scenario. Top: FER rate curves with respect to yaw rotation only and pitch rotation only; Bottom: FER rates surface with both yaw and pitch rotations. The facial pictures in the bottom illustrate the visible parts of a face when the yaw and pitch angles change to +/-60 degrees. The recognition rates are also denoted besides the pictures.</figDesc><graphic coords="12,92.36,55.03,244.86,202.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Facial expression recognition results summary</figDesc><table><row><cell cols="2">Model property Method</cell><cell cols="3">Recognition rate Model property Method</cell><cell>Recognition rate</cell></row><row><cell>static 2D</cell><cell cols="2">Gabor-wavelet based 63.72%</cell><cell>dynamic 2D</cell><cell>MU-2D</cell><cell>66.95%</cell></row><row><cell>static 3D</cell><cell cols="2">LLE-based method 61.11%</cell><cell>dynamic 3D</cell><cell>MU-3D</cell><cell>70.31%</cell></row><row><cell>static 3D</cell><cell cols="2">PCA-based method 70.79%</cell><cell>dynamic 3D</cell><cell>T-HMM based</cell><cell>80.04%</cell></row><row><cell>static 3D</cell><cell cols="2">LDA-based method 77.04%</cell><cell>dynamic 3D</cell><cell cols="2">P2D-HMM based 82.19%</cell></row><row><cell>static 3D</cell><cell>PSFD method</cell><cell>53.24%</cell><cell>dynamic 3D</cell><cell cols="2">R2D-HMM based 90.44%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Confusion matrix using R2D-HMM method</figDesc><table><row><cell cols="2">In/out Anger Disgust Fear</cell><cell>Smile Sad</cell><cell>Surprise</cell></row><row><cell cols="4">Anger 92.44% 3.68% 1.94% 1.32% 0.00% 1.42%</cell></row><row><cell cols="4">Disgust 8.28% 87.58% 1.27% 1.27% 0.96% 0.64%</cell></row><row><cell>Fear</cell><cell cols="3">7.45% 3.42% 85.40% 0.62% 0.00% 3.11%</cell></row><row><cell cols="4">Smile 0.44% 0.22% 0.66% 97.81% 0.00% 0.87%</cell></row><row><cell>Sad</cell><cell cols="3">13.12% 1.56% 0.63% 4.06% 80.32% 0.31%</cell></row><row><cell cols="4">Surprise 0.33% 0.00% 0.00% 0.33% 0.00% 99.34%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This material is based upon the work supported in part by the National Science Foundation under grants IIS-0541044, IIS-0414029, and the NYSTAR's James D. Watson Investigator Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facial action unit recognition by exploiting their dynamic and semantic relationships</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1683" to="1699" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: the state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting coded dynamic features for facial action units and facial expression recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully automatic facial action recognition in spontaneous behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying facial actions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="974" to="989" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Subtly different facial expression recognition and expression intensity estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 1998</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting facial actions and their temporal segments in nearly frontalview face image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="3358" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences: temporal and static modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of CVIU</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Authentic facial expression analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1856" to="1863" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spontaneous emotional facial expression detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic expression analysis on manifolds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inter. Conf. on CVPR 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthesis and recognition of facial expressions in virtual 3d views</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGR</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on prmitive surface feature distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantifying facial expression abnormality in schizophrenia by combining 2d and 3d features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High resolution acquisition, learning and transfer of dynamic 3d facial expressions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROGRAPHICS 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Di3d</surname></persName>
		</author>
		<ptr target="http://www.di3d.com" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic 3d facial expression analysis in videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Velho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV 2005 Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE FGR</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From facial expression to level of interest: a spatio-temporal approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A high resolution 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE FGR</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d face recognition using two views face modeling and labeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR 2005 Workshop on A3DISS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A separable low complex 2d hmm with application to face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aoulnasr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: Unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pca versus lda</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Psychophysical evaluation of animated facial expressions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Symposium on Applied Perception in Graphics and Visualization</title>
		<meeting>of the 2nd Symposium on Applied Perception in Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
