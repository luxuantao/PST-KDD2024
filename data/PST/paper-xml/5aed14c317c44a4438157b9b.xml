<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STDP-based spiking deep convolutional neural networks for object recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-23">November 23, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reza</forename><surname>Saeed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Kheradpisheh</surname></persName>
							<email>kheradpisheh@ut.ac.ir</email>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">J</forename><surname>Ganjtabesh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothée</forename><surname>Thorpe</surname></persName>
						</author>
						<author>
							<persName><surname>Masquelier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neural</forename><surname>Networks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saeed</forename><forename type="middle">Reza</forename><surname>Kheradpisheh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Mathematics, Statistics, and Computer Science</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CERCO UMR 5549</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Toulouse 3</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Ganjtabesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Mathematics, Statistics, and Computer Science</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
							<email>simon.thorpe@cnrs.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CERCO UMR 5549</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Toulouse 3</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothée</forename><surname>Masquelier</surname></persName>
							<email>timothee.masquelier@cnrs.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CERCO UMR 5549</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Toulouse 3</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STDP-based spiking deep convolutional neural networks for object recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-23">November 23, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">3000C5FBBA07171AF0E3C8E9FA5BD061</idno>
					<idno type="DOI">10.1016/j.neunet.2017.12.005</idno>
					<note type="submission">Received date : 7 May 2017 Revised date : 23 November 2017 Accepted date : 8 December 2017 Preprint submitted to Neural Networks</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spiking Neural Network</term>
					<term>STDP</term>
					<term>Deep Learning</term>
					<term>Object Recognition</term>
					<term>Temporal Coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable.</p><p>Another line of research has demonstrated -using rate-based neural networks trained with backpropagation -that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Primate's visual system solves the object recognition task through hierarchical processing along the ventral pathway of the visual cortex <ref type="bibr" target="#b0">[1]</ref>. Through this hierarchy, the visual preference of neurons gradually increases from oriented bars in primary visual cortex (V1) to complex objects in inferotemporal cortex (IT), where neural activity provides a robust, invariant, and linearly-separable object representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Despite the extensive feedback connections in the visual cortex, the first feed-forward wave of spikes in IT (∼ 100 -150 ms post-stimulus presentation) appears to be sufficient for crude object recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>During the last decades, various computational models have been proposed to mimic this hier-archical feed-forward processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Despite the limited successes of the early models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, recent advances in deep convolutional neural networks (DCNN) led to high performing models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Beyond the high precision, DCNNs can tolerate object variations as humans do <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, use IT-like object representations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and match the spatio-temporal dynamics of the ventral visual pathway <ref type="bibr" target="#b19">[20]</ref>.</p><p>Although the architecture of DCNNs is somehow inspired by the primate's visual system <ref type="bibr" target="#b20">[21]</ref> (a hierarchy of computational layers with gradually increasing receptive fields), they totally neglect the actual neural processing and learning mechanisms in the cortex.</p><p>The computing units of DCNNs send floatingpoint values to each other which correspond to their activation level, while, biological neurons communicate to each other by sending electrical impulses (i.e., spikes). The amplitude and duration of all spikes are almost the same, so they are fully characterized by their emission time. Interestingly, mean spike rates are very low in the primate visual systems (perhaps only a few of hertz <ref type="bibr" target="#b21">[22]</ref>). Hence, neurons appear to fire a spike only when they have to send an important message, and some information can be encoded in their spike times. Such spike-time coding leads to a fast and extremely energy-efficient neural computation in the brain (the whole human brain consumes only about 10-20 Watts of energy <ref type="bibr" target="#b22">[23]</ref>).</p><p>The current top-performing DCNNs are trained with the supervised back-propagation algorithm which has no biological root. Although it works well in terms of accuracy, the convergence is rather slow because of the credit assignment problem <ref type="bibr" target="#b23">[24]</ref>. Furthermore, given that DCNNs typically have millions of free parameters, millions of labeled examples are needed to avoid over-fitting. However, primates, especially humans, can learn from far fewer examples while most of the time no label is available. They may be able to do so thanks to spike-timing-dependent plasticity (STDP), an unsupervised learning mechanism which occurs in mammalian visual cortex <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. According to STDP, synapses through which a presynaptic spike arrived before (respectively after) a postsy-naptic one are reinforced (respectively depressed).</p><p>To date, various spiking neural networks (SNN) have been proposed to solve object recognition tasks. A group of these networks are actually the converted versions of traditional DCNNs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. The main idea is to replace each DCNN computing unit with a spiking neuron whose firing rate is correlated with the output of that unit. The aim of these networks is to reduce the energy consumption in DCNNs. However, the inevitable drawbacks of such spike-rate coding are the need for many spikes per image and the long processing time. Besides, the use of back-propagation learning algorithm and having both positive (excitatory) and negative (inhibitory) output synapses in a neuron are not biologically plausible. On the other hand, there are SNNs which are originally spiking networks and learn spike patterns. First group of these networks exploit learning methods such as auto-encoder <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and back-propagation <ref type="bibr" target="#b32">[33]</ref> which are not biologically plausible. The second group consists of SNNs with bioinspired learning rules which have shallow architectures <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> or only one trainable layer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this paper we proposed a STDP-based spiking deep neural network (SDNN) with a spiketime neural coding. The network is comprised of a temporal-coding layer followed by a cascade of consecutive convolutional (feature extractor) and pooling layers. The first layer converts the input image into an asynchronous spike train, where the visual information is encoded in the temporal order of the spikes. Neurons in convolutional layers integrate input spikes, and emit a spike right after reaching their threshold. These layers are equipped with STDP to learn visual features. Pooling layers provide translation invariance and also compact the visual information <ref type="bibr" target="#b7">[8]</ref>. Through the network, visual features get larger and more complex, where neurons in the last convolutional layer learn and detect object prototypes. At the end, a classifier detects the category of the input image based on the activity of neurons in the last pooling layer with global receptive fields.</p><p>We evaluated the proposed SDNN on Caltech face/motorbike and ETH-80 datasets with largescale images of various objects taken form differ-ent viewpoints. The proposed SDNN reached the accuracies of 99.1% on face/motorbike task and 82.8% on ETH-80, which indicates its capability to recognize several natural objects even under severe variations. Based on our knowledge, there is no other spiking deep network which can recognize large-scale natural objects. We also examined the proposed SDNN on the MNIST dataset which is a benchmark for spiking neural networks, and interestingly, it reached 98.4% recognition accuracy. In addition to the high performance, the proposed SDNN is highly energy-efficient and works with a few number of spikes per image, which makes it suitable for neuromorphic hardware implementation. Although current state-of-the-art DCNNs achieved stunning results on various recognition tasks, continued work on brain-inspired models could end up in strong intelligent systems in future, which can even help us to improve our understanding of the brain itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Spiking Deep Neural Network</head><p>A sample architecture of the proposed SDNN with three convolutional and three pooling layers is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Note that the architectural properties (e.g., the number of layers and receptive field sizes) and learning parameters should be optimized for the desired recognition task.</p><p>The first layer of the network uses Difference of Gaussians (DoG) filters to detect contrasts in the input image. It encodes the strength of these contrasts in the latencies of its output spikes (the higher the contrast, the shorter the latency). Neurons in convolutional layers detect more complex features by integrating input spikes from the previous layer which detects simpler visual features. Convolutional neurons emit a spike as soon as they detect their preferred visual feature which depends on their input synaptic weights. Through the learning, neurons that fire earlier perform the STDP and prevent the others from firing via a winner-take-all mechanism. In this way, more salient and frequent features tend to be learned by the network. Pooling layers provide translation invariance using maximum operation, and also help the network to compress the flow of vi-sual data. Neurons in pooling layers propagate the first spike received from neighboring neurons in the previous layer which are selective to the same feature. Convolutional and pooling layers are arranged in a consecutive order. Receptive fields gradually increase through the network and neurons in higher layers become selective to complex objects or object parts.</p><p>It should be noted that the internal potentials of all neurons are reset to zero before processing the next image. Also, learning only happens in convolutional layers and it is done layer by layer. Since the calculations of each neuron is independent of other adjacent neurons, to speed-up the computations, each of the convolution, pooling, and STDP operations are performed in parallel on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DoG and temporal coding</head><p>The important role of the first stage in SNNs is to encode the input signal into discrete spike events in the temporal domain. This temporal coding determines the content and the amount of information carried by each spike, which deeply affects the neural computations in the network. Hence, using efficient coding scheme in SNNs can lead to fast and accurate responses. Various temporal coding schemes can be used in visual processing (see ref. <ref type="bibr" target="#b40">[41]</ref>). Among them, rank-order coding is shown to be efficient for rapid processing (even possibly in retinal ganglion cells) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Cells in the first layer of the network apply a DoG filter over their receptive fields to detect positive or negative contrasts in the input image. DoG well approximates the center-surround properties of the ganglion cells of the retina. When presented with an image, these DoG cells detect the contrasts and emit a spike; the more strongly a cell is activated (higher contrast), the earlier it fires. In other word, the order of the spikes depends on the order of the contrasts. This rankorder coding is shown to be efficient for obtaining V1 like edge detectors <ref type="bibr" target="#b43">[44]</ref> as well as complex visual features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>   trasts. A DoG cell is allowed to fire if its activation is above a certain threshold. Note that this scheme grantees that at most one of the two cells (positive or negative) can fire in each location. As mentioned above, the firing time of a DoG cell is inversely proportional to its activation value. In other words, if the output of the DoG filter at a certain location is r, the firing time of the corresponding cell is t = 1/r. For efficient GPU-based parallel computing, the input spikes are grouped into equal-size sequential packets. At each time step, spikes of one packet are propagated simultaneously. In this way, a packet of spikes with near ranks (carrying similar visual information) are propagated in parallel, while, the next spike packet will be processed in the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional layers</head><p>A convolutional layer contains several neuronal maps. Each neuron is selective to a visual feature determined by its input synaptic weights. Neurons in a specific map detect the same visual feature but at different locations. To this end, synaptic weights of neurons belonging to the same map should always be the same (i.e., weight sharing). Within a map, neurons are retinotopically arranged. Each neuron receives input spikes from the neurons located in a determined window in all neuronal maps of the previous layer. Hence, a visual feature in a convolutional layer is a combination of several simpler feature extracted in the previous layer. Note that the input windows of two adjacent neurons are highly overlapped. Hence, the network can detect the appearance of the visual features in any location.</p><p>Neurons in all convolutional layers are nonleaky integrate-and-fire neurons, which gather input spikes from presynaptic neurons and emit a spike when their internal potentials reach a prespecified threshold. Each presynaptic spike increases the neuron's potential by its synaptic weight. At each time step, the internal potential of the ith neuron is updated as follows:</p><formula xml:id="formula_0">V i (t) = V i (t -1) + ∑ j W j,i S j (t -1),<label>(1)</label></formula><p>where V i (t) is the internal potential of the ith convolutional neuron at time step t, W j,i is the synap-tic weight between the jth presynaptic neuron and the ith convolutional neuron, and S j is the spike train of the jth presynaptic neuron (S j (t -1) = 1 if the neuron has fired at time t -1, and S j (t -1) = 0 otherwise). If V i exceeds its threshold, V thr , then the neuron emits a spike and V i is reset:</p><formula xml:id="formula_1">V i (t) = 0 and S i (t) = 1, if V i (t) ≥ V thr . (2)</formula><p>Also, there is a lateral inhibition mechanism in all convolutional layers. When a neuron fires, in an specific location, it inhibits other neurons in that location belonging to other neuronal maps (i.e., resets their potentials to zero) and does not allow them to fire until the next image is shown. In addition, neurons are not allowed to fire more than once. These together provides an sparse but highly informative coding, because, there can be at most one spike at each location which indicates the existence of a particular visual feature in that location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local pooling layers</head><p>Pooling layers help the network to gain invariance by doing a nonlinear max pooling operation over a set of neighboring neurons with the same preferred feature. Some evidence suggests that such a max operation occurs in complex cells in visual cortex <ref type="bibr" target="#b7">[8]</ref>. Thanks to the rank-order coding used in the proposed network, the maximum operation of pooling layers simply consists of propagating the first spike emitted by the afferents <ref type="bibr" target="#b44">[45]</ref>.</p><p>A neuron in a neuronal map of a pooling layer performs the maximum operation over a window in the corresponding neuronal map of the previous layer. Pooling neurons are integrate-and-fire neurons whose input synaptic weights and threshold are all set to one. Hence, the first input spike activates them and leads to an output spike. Regarding to the rank-order coding, each pooling neuron is allowed to fire at most once. It should be noted that no learning occurs in pooling layers.</p><p>Another important role of pooling layers is to compress the visual information. Regarding to the maximum operation performed in pooling layers, adjacent neurons with overlapped inputs would carry redundant information (each spike is sent to many neighboring pooling neurons). Hence, in the proposed network, the overlap between the input windows of two adjacent pooling neurons (belonging to the same map) is set to be very small. It helps to compress the visual information by eliminating the redundancies, and also, to reduce the size of subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STDP-based learning</head><p>As mentioned above, learning occurs only in convolutional layers which should learn to detect visual features by combining simpler features extracted in the previous layer. The learning is done layer by layer, i.e., the learning in a convolutional layer starts when the learning in the previous convolutional layer is finalized. When a new image is presented, neurons of the convolutional layer compete with each other and those which fire earlier trigger STDP and learn the input pattern.</p><p>A simplified version of STDP <ref type="bibr" target="#b8">[9]</ref> is used:</p><formula xml:id="formula_2">∆w ij = { a + w ij (1 -w ij ), if t j -t i ≤ 0, a -w ij (1 -w ij ), if t j -t i &gt; 0,<label>(3)</label></formula><p>where i and j respectively refer to the index of post-and presynaptic neurons, t i and t j are the corresponding spike times, ∆w ij is the synaptic weight modification, and a + and a -are two parameters specifying the learning rate. Note that the exact time difference between two spikes does not affect the weight change, but only its sign is considered. Also, it is assumed that if a presynaptic neuron does not fire before the postsynaptic one, it will fire later. These simplifications are equivalent to assuming that the intensity-latency conversion of DoG cells compresses the whole spike wave in a relatively short time interval (say, 20 -30 ms), so that all presynaptic spikes necessarily fall close to the postsynaptic spike time, and the time lags are negligible. The multiplicative term w ij (1w ij ) ensures the weights remain in the range [0,1] and thus maintains all synapses in an excitatory mode in adding to implementing soft-bound effect. Note that choosing large values for the learning parameters (i.e., a + and a -) will decrease the learning memory, therefore, neurons would learn the last presented images and unlearn previously seen images. Also, choosing tiny values would slow down the learning process. At the beginning of the learning, when synaptic weights are random, neurons are not yet selective to any specific pattern and respond to many different patterns, therefore, the probability for a synapse to get depressed is higher than being potentiated. Hence, by setting a -to be greater than a + , synaptic weights gradually decay insofar as neurons can not reach their threshold to fire anymore. Therefore, a + is better to be greater than a -, however, by setting a + to be much greater than a -, neurons will tend to learn more than one pattern and respond to all of them. All in all, it is better to choose a + and a -not too big and not too small, and it is better to set a + a bit greater than a -.</p><p>During the learning of a convolutional layer, neurons in the same map, detecting the same feature in different locations, integrate input spikes and compete with each other to do the STDP. The first neuron which reaches the threshold and fires, if any, is the winner (global intra-map competition). The winner triggers the STDP and updates its synaptic weights. As mentioned before, neurons in different locations of the same map have the same input synaptic weights (i.e., weight sharing) to be selective to the same feature. Hence, the winner neuron prevents other neurons in its own map to do STDP and duplicates its updated synaptic weights into them. Also, there is a local inter-map competition for STDP. When a neuron is allowed to do the STDP, it prevents the neurons in other maps within a small neighborhood around its location from doing STDP. This competition is crucial to encourage neurons of different maps to learn different features.</p><p>Because of the discretized time variable in the proposed model, it is probable that some competitor neurons fire at the same time step. One possible scenario is to pick one randomly and allow it to do STDP. But a better alternative is to pick the one which has the highest potential indicating higher similarity between its learned feature and input pattern.</p><p>Synaptic weights of convolutional neurons initiate with random values drown from a normal distribution with the mean of µ = 0.8 and STD of σ = 0.05. Note that by choosing a small µ, neurons would not reach their threshold to fire and will not learn anything. Also, by choosing a large σ, some initial synaptic weights will be smaller (larger) than others and have less (more) contribution to the neuron activity, and regarding the STDP rule, they have a higher tendency to converge to zero (one). In other words, dependency on the initial weights will be higher for a large σ.</p><p>As the learning of a specific layer progresses, its neurons gradually converge to different visual features which are frequent in the input images. As mentioned before, learning in the subsequent convolutional layer statrs whenever the learning in the current convolutional layer is finalized. Here we measure the learning convergence of the lth convolutional layer as</p><formula xml:id="formula_3">C l = ∑ f ∑ i w f,i (1 -w f,i )/n w<label>(4)</label></formula><p>where, w f,i is the ith synaptic weight of the f th feature and n w is the total number of synaptic weights (independent of the features) in that layer. C l tends to zero if each of the synaptic weights converge towards zero or one. Therefore, we stop the learning of the lth convolutional layer, whenever C l was sufficiently close to zero (i.e. C l &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global pooling and classification</head><p>The global pooling layer is only used in the classification phase. Neurons of the last layer perform a global max pooling over their corresponding neuronal maps in the last convolutional layer. Such a pooling operation provides a global translation invariance for prototypical features extracted in the last convolutional layer. Hence, there is only one output value for each feature, which indicates the presence of that feature in the input image. The output of the global pooling layer over the training images is used to train a linear SVM classifier. In the testing phase, the test object image is processed by the network and the output of the global pooling layer is fed to the classifier to determine its category.</p><p>To compute the output of the global pooling layer, first, the threshold of neurons in the last convolutional layer were set to be infinite, and then, their final potentials (after propagating the whole spike train generated by the input image) were measured. These final potentials can be seen as the number of early spikes in common between the current input and the stored prototypes in the last convolutional layer. Finally, the global pooling neurons compute the maximum potential at their corresponding neuronal maps, as their output value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech face/motorbike dataset</head><p>We evaluated our SDNN on the face and motorbike categories of the Caltech 101 dataset available at http://www.vision.caltech.edu (see Fig. <ref type="figure" target="#fig_3">4</ref> for sample pictures). The training set contains 200 randomly selected images per category, and remaining images constitute the test set. The test images are not seen during the learning phase but used afterward to evaluate the performance on novel images. This standard cross-validation procedure allows measuring the system's ability to generalize, as opposed to learning the specific training examples. All images were converted to grayscale values and rescaled to be 160 pixels in height (preserving the aspect ratio). In all the experiments, we used linear SVM classifiers with penalty parameter C = 1.0 (optimized by a grid search in the range of (0, 10]).</p><p>Here, we used a network similar to Fig. <ref type="figure" target="#fig_0">1</ref>, with three convolutional layers each of which followed by a pooling layer. For the first layer, only ONcenter DoG filters of size 7 × 7 and standard deviations of 1 and 2 pixels are used. The first, second and third convolutional layers consists of 4, 20, and 10 neuronal maps with conv-window sizes of 5 × 5, 16 × 16 × 4, and 5 × 5 × 20 and firing thresholds of 10, 60, and 2, respectively. The pooling window sizes of the first and second pooling layers are 7 × 7 and 2 × 2 with the strides of 6 and 2, correspondingly. The third pooling layer performs a global max pooling operation. The learning rates of all convolutional layers are set to a + = 0.004 and a -= 0.003. In addition, each image is processed for 30 time steps. Fig. <ref type="figure" target="#fig_1">2</ref> shows the preferred visual features of some neuronal maps in the first, second and third convolutional layers through the learning process. To visualize the visual feature learned by a neuron, a backward reconstruction technique is used. Indeed, the visual features in the current layer can be reconstructed as the weighted combinations of the visual features in the previous layer. This backward process continues until the first layer, whose preferred visual features are computed by DoG functions. As shown in Fig. <ref type="figure" target="#fig_1">2A</ref>, interestingly, each of the four neuronal maps of the first convolutional layer converges to one of the four orientations: π/4, π/2, 3π/4, and π. This shows how efficiently the association of the proposed temporal coding in DoG cells and unsupervised learning method (the STDP and learning competition) led to highly diverse edge detectors which can represent the input image with edges in different orientations. These edge detectors are similar to the simple cells in primary visual cortex (i.e., V1 area) <ref type="bibr" target="#b43">[44]</ref>.</p><p>Fig. <ref type="figure" target="#fig_1">2B</ref> shows the learning progress for the neuronal maps of the second convolutional layer. As mentioned, the first convolutional layer detects edges with different orientations all over the image, and due to the used temporal coding, neurons corresponding to edges with higher contrasts (i.e., salient edges) will fire earlier. On the other hand, STDP naturally tends to learn those combination of edges that are consistently repeating in the training images (i.e., common features between the target objects). Besides, the learning competition tends to prevent the neuronal maps from learning similar visual features. Consequently, neurons in the second convolutional layer learn the most salient, common, and diverse visual features of the target objects, and do not learn the backgrounds that drastically change between images. As seen in Fig. <ref type="figure" target="#fig_1">2B</ref>, each of the maps gradually learns a different visual feature (combination of oriented edges) representing a face or motorbike feature.</p><p>The learning progress for two neuronal maps of the third convolutional layer are shown in Fig. <ref type="figure" target="#fig_1">2C</ref>. As seen, one of them gradually becomes selective to a complete motorbike prototype as a combi- nation of motorbike features such as back wheel, middle body, handle, and front wheel detected in the second layer. Also, the other map learns a whole face prototype as a combination of facial features. Indeed, the third convolutional layer learns the whole object prototypes using intermediate complexity features detected in the previous layer. Neurons in the second layer compete with each other and send spikes toward the third layer as they detect their preferred visual features. Since, different combinations of these features are detected for each object category, neuronal maps of the third layer will learn different prototypes of different categories. Therefore, the STDP and the learning competition mechanism direct neuronal maps of the third convolutional layer to learn highly category specific prototypes.</p><p>As mentioned in the previous section, learn- ing at the lth convolutional layer stops whenever the learning convergence index C l tends to zero.</p><p>Here, in Fig. <ref type="figure" target="#fig_2">3</ref>, we presented the evolution of C l during the learning of each convolutional layer. Also, we provided the weight histogram of the first convolutional layer at some critical points during the learning process. Since the initial synaptic weights are drown from a normal distribution with µ = 0.8 and σ = 0.05, at the beginning of the learning, the convergence index of the lth layer starts from C l ≃ 0.16. Since features are not formed at the early iterations, neurons respond to almost every pattern, therefore many of the synapses are depressed most of the times and gradually move towards 0.5, and C l peaks. As a consequence, neurons' activity decreases, and they start responding to a few of the patterns and not to others. From then, synapses contributed in these patterns are repeatedly potentiated and others are depressed. Due to the nature of the employed soft-bound STDP, learning is faster when the weights are around 0.5 and C l rapidly decreases. Finally, at the end of the learning, as features are formed and synaptic weights converge to zero or one, C l tends to zero. As seen in Fig. <ref type="figure" target="#fig_2">3</ref>, the weight changes in lower layers are faster than in higher layers. This is mainly due to the stronger competition among the feature maps in higher layers. For instance, in the first layer, there are only four feature maps with small receptive fields. If a neuron from one feature map loses the competition at one location, another neuron can still win the competition at some other location and do the STDP. While, in the third layer, there are more feature maps with larger receptive fields. If a neuron wins the competition, it prevents neurons of other feature maps from doing the STDP in a much wider area. Hence, for each image, only a few of the feature maps update their weights. Therefore, the competition is stronger in higher layers and learning takes a much longer time. Beside the competition factor, neurons in higher layers become selective to more complex and less frequent features so they do not reach their threshold as often as neurons in lower layers (e.g., all images contain edges).</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the accumulated spiking activity of the DoG and the following three convolutional layers over all time steps, for two face and motorbike sample images. For each layer, the preferred features of some neuronal maps with color coded borders are demonstrated on top, and their corresponding spiking activity are shown in panels below them. Each colored point inside a panel indicates the neuronal map of the neuron which has fired in that location at a time step. As seen, neurons of the DoG layer detect image contrasts, and edge detectors in the first convolutional layer detect the orientation of edges. Neurons in the second convolutional layer, which are selective to in-termediate complexity features, detect their preferred visual feature by combining input spikes from edge detector cells in the first layer. Finally, the coincidence of these features activates neurons in the third convolutional layer which are selective to object prototypes. As seen, when a face (motorbike) image is presented, neurons in the face (motorbike) maps fire. To better illustrate the learning progress of all the layers as well as their spiking activity in the temporal domain, we prepared a short video (see Video 1).</p><p>As mentioned in the previous section, the out- put of the global pooling layer is used by a linear SVM classifier to specify the object category of the input images. We trained the proposed SDNN on training images and evaluated it over the test images, where the model reached the categorization accuracy of 99.1 ± 0.2%. It shows how the object prototypes, learned in the highest layer, can well represent the object categories. Furthermore, we also calculated the single neuron accuracy. In more details, we separately computed the recognition accuracy of each neuron in the global pooling layer. Surprisingly, some single neurons reached an accuracy of 93%, and the mean accuracy was 89.8%. Hence, it can be said that single neurons in the highest layer are highly class specific, and different neurons carry complementary information which altogether provide robust object representations.</p><p>To better demonstrate the role of the STDP learning rule in the proposed SDNN, we used random features in different convolutional layers and assessed the final accuracy. To this end, we first trained all the three convolutional layers of the network (using STDP) until they all converged (synaptic weights had a bimodal distribution with 0 and 1 as centers). Then, for a certain convolutional layer, we counted the number of active synapses (i.e., close to one) of each of its feature maps. Corresponding to each learned feature in the first convolutional layer, we generated a random feature with the same number of active synapses. For the second and third convolutional layers, the number of active synapses in the random features was doubled. Note that with fewer active synapses, neurons in the second and third convolutional layers could not reach their threshold, and with more active synapses, many of the neurons tend to fire together. Anyways, we evaluated the network using these random features. Table <ref type="table" target="#tab_1">1</ref> presents the SDNN accuracy when we had random features in the third, or second and third, or in all the three convolutional layers. As seen, by replacing the learned features of the lower layers with random ones, the accuracy decreases more. Especially, using random features in the second layer, the accuracy severely drops. This shows the critical role of intermediate complexity features for object categorization.</p><p>We also evaluated how the proposed SDNN is robust to noise. To this end, we added a white noise to the neurons' threshold during both training and testing phases. Indeed, for each image, we added to the threshold of each neuron, V thr , a random value drawn from a uniform distribution in range ±α% of V thr . We evaluated the proposed DCNN for different amount of noise (from α = 5% to 50%) and the results are provided in Table <ref type="table" target="#tab_2">2</ref>. Up to the 20% of noise, the accuracy is still reasonable, but by increasing the noise level, the accuracy dramatically drops and reaches to the chance level in case of 50% of noise. In other words, the network is more or less able to tolerate the instability caused by the noise below 20%, but when it goes further, the neurons' behavior drastically change during the learning and STDP can not extract informatic features from the input images.</p><p>In another experiment, we changed the number of training samples and calculated the recognition accuracy of the proposed SDNN. For in-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETH-80 dataset</head><p>The ETH-80 dataset contains eight different object categories: apple, car, toy cow, cup, toy dog, toy horse, pear, and tomato (10 instances per category). Each object is photographed from 41 view-points with different view angles and different tilts. Some examples of objects in this dataset are shown in Fig. <ref type="figure" target="#fig_5">6</ref>. ETH-80 is a good bench-Table <ref type="table">3</ref>: Recognition accuracies of the proposed SDNN and some other methods over the ETH-80 dataset. Note that all the models are trained on 5 object instances of each category and tested on other 5 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) HMAX <ref type="bibr" target="#b39">[40]</ref> 69.0 Convolutional SNN <ref type="bibr" target="#b39">[40]</ref> 81. Five randomly chosen instances of each object category are selected for the training set used in the learning phase. The remaining instances constitute the testing set, and are not seen during the learning phase. All the object images were converted to grayscale values. To evaluate the proposed SDNN on ETH-80, we used a network architecturally similar to the one used for Caltech face/motorbike dataset. The other parameters are also similar, except for the number of neuronal maps in the second and third convolutional and pooling layers. Here we used 400 neuronal maps in each of these layers.</p><p>Similar to the caltech dataset, neuronal maps of the first convolutional layer converged to the four oriented edges. Neurons in the second and third convolutional layers also became selective to intermediate features and object prototypes, respectively. Fig. <ref type="figure" target="#fig_5">6</ref> shows sample images from the ETH-80 dataset and the preferred features of some neuronal maps in the third convolutional layer which are activated for those images. As seen, neurons in the highest layer respond to different views of different objects, and altogether, provide an invariant object representation. Thus, the network learns 2D and view-dependent proto-types of each object category to achieve 3D representations.</p><p>As mentioned before, we evaluated the proposed SDNN over the test instances of each object category which are not shown to the network during the training. The recognition accuracy of the proposed SDNN along with some other models on the ETH-80 dataset are presented in Table <ref type="table">3</ref>. HMAX <ref type="bibr" target="#b45">[46]</ref> is one of the classic computational models of the object recognition process in visual cortex. It has 5000 features and uses a linear SVM as the classifier (see <ref type="bibr" target="#b39">[40]</ref> for more details). Convolutional SNN <ref type="bibr" target="#b39">[40]</ref> is an extension of the Masquelier et al. 2007 <ref type="bibr" target="#b8">[9]</ref> which had one trainable layer with 1200 visual features and used linear SVM for classification. AlexNet is the first DCNN that significantly improved recognition accuracy on Imagenet Dataset. Here, we compared our proposed SDNN with both Imagenet pre-trained and ETH-80 fine-tuned versions of AlexNet. To obtain the accuracy of the pre-trained AlexNet, images were shown to the model and feature vectors of the last layer were used to train and test a linear SVM classifier. Also, to fine-tune the Imagenet pretrained AlexNet on ETH-80, its decision layer was replaced by an eight-neuron decision layer and trained by the stochastic gradient descent learning algorithm.</p><p>Regarding the fact that the pre-trained AlexNet has not seen ETH-80 images and fine-tuned AlexNet has already trained by millions of images from Imagenet dataset, the comparison may not be fair enough. Therefore, we compared the proposed SDNN to a supervised DCNN with the same structure but having two extra dense and decision layers on top with 70 and 8 neurons, respectively. The DCNN was trained on the gray-scaled images of ETH-80 using the backpropagation algorithm. The ReLU and soft-max activation functions were employed for the intermediate and decision layers, respectively. We used a cross-entropy loss function with L2 kernel regularization and L1 activity regularization terms. We also performed a 50% dropout regularization on the dense layer. The hyper-parameters such as learning rates, momentums, and regularization factors were optimized using a grid search. Also, we used an early stop-ping startegy to prevent the network from overfitting. Eventually, the supervised DCNN reached the average accuracy of 81.9% (see Table <ref type="table">3</ref>). Note that we tried to evaluate the DCNN with a dense layer of more than 70 neurons, but all the time the network got quickly overfitted on the training data with no accuracy improvement over the test set. It seems that the supervised DCNN suffers from the lack of sufficient training data.</p><p>In addition, we compared the proposed SDNN to a deep convolutional autoencoder (DCA) which is one of the best unsupervised learning algorithms in machine learning. We developed a DCA with an encoder network having the same architecture as our SDNN followed by a decoder network with reversed architecture. The ReLU activation function was used in the convolutional layers of both encoder and decoder networks. We used the crossentropy loss function and stochastic gradient descent learning algorithm to train the DCA. The learning parameters (i.e., learning rate and momentum) were optimized through a grid search. When the learning had converged, we eliminated the decoder part and used the encoder's output representations to train and test a linear SVM classifier. Note that DCA was trained on grayscaled images of ETH-80. The DCA reached the average accuracy of 80.7% (see Table <ref type="table">3</ref>) and the proposed SDNN could outperform the DCA network with the same structure.</p><p>We also evaluated the proposed SDNN that has two convolutional layers. Indeed, we removed the third convolutional layer, applied the global pooling over the second convolutional layer, and trained a new SVM classifier over its output. The model's accuracy dropped by 5% and reached to 77.4%. Although the features in the second layer represent object parts with intermediate complexity, it is the combination of these features in the last layer which culminates the object representations and makes the classification easier. In other words, the similarity between the smaller parts of objects of differen categories but with similar shapes (e.g., apple and potato) is higher than the whole objects, hence, the visual features in the higher layers can provide better object representations.  In a subsequent analysis, we computed the confusion matrix, to see which categories are mostly confused with each other. Fig. <ref type="figure" target="#fig_6">7</ref> illustrates the confusion matrix of the proposed SDNN over the ETH-80 dataset. As seen, most of the errors are due to the miscategorization of dogs, horses, and cows. We checked whether these errors belong to the some specific viewpoints or not. We found out that the errors are uniformly distributed between different viewpoints. Some misclassified samples are shown in Fig. <ref type="figure" target="#fig_7">8</ref>. Overall, it can be concluded that these categorization errors are due to the overall shape similarity between these object categories.</p><p>The other important aspect of the proposed SDNN is the computational efficiency of the network. For each ETH-80 image, on average, about 9100 spikes are emitted in all the layers, i.e., about 0.02 spike per neuron per image. Note that the number of inhibitory events is equal to the number of spikes. These together points to the fact that the proposed SDNN can recognize objects with high precision but low computational cost. This efficiency is caused by the association of the proposed temporal coding and STDP learning rule which led to a sparse but informative visual coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST dataset</head><p>MNIST <ref type="bibr" target="#b46">[47]</ref> is a benchmark dataset for SNNs which has been widely used <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>. We also evaluated our SDNN on the MNIST dataset which contains 60,000 training and 10,000 test handwritten single-digit images. Each image is of size 28 × 28 pixels and contains one of the digits 0-9. For the first layer, ON-and OFF-center DoG filters with standard deviations of 1 and 2 pixels are used. The first and second convolutional layers respectively consist of 30 and 100 neuronal maps with 5 × 5 convolutionwindow and firing thresholds of 15 and 10. The pooling-window of the first pooling layer was of size 2 × 2 with the stride of 2. The second pooling layer performs a global max operation. Note that the learning rates of all convolutional layers were set to a + = 0.004 and a -= 0.003. In all the experiments, we used linear SVM classifiers with penalty parameter C = 2.4 (optimized by a grid search in the range of (0, 10]).</p><p>Fig. <ref type="figure" target="#fig_8">9</ref> shows the preferred features of some neuronal maps in the first convolutional layer. The green and red colors correspond to ON-and OFFcenter DoG filters. Interestingly, this layer converged to Gabor-like edge detectors with different orientations, phase and polarity. These edge features are combined in the next layer and provide easily separable digit representation.</p><p>Recognition performance of the proposed method and some recent SNNs on the MNIST dataset are provided in Table <ref type="table" target="#tab_4">4</ref>. As seen, the proposed SDNN outperforms unsupervised SNNs by reaching 98.4% recognition accuracy. Besides, the accuracy of the proposed SDNN is close to the 99.1% accuracy of the totally supervised rate-based SDNN <ref type="bibr" target="#b49">[50]</ref> which is indeed the converted version of a traditional DCNN trained by back-propagation.</p><p>The important advantage of the proposed SDNN is the use of much fewer spikes. Our SDNN uses only about 600 spikes for each MNIST images in total for all the layers, while the supervised rate-based SDNN uses thousands of spikes per layer <ref type="bibr" target="#b49">[50]</ref>. Also, because of using rate-based neural coding in such networks, they need to process images for hundreds of time steps, while our network process the MNIST images in 30 time steps only. Notably, whenever a neuron in our network fires it inhibits other neurons at the same position, therefore, the total number of inhibitory events per each MNIST image is equal to the number of spikes(i.e., 600). As stated in Section 2, the proposed SDNN uses a temporal code which encodes the information of the input image in the spike times, and each neuron in all layers, is allowed to fire at most once. This temporal code associated with unsupervised STDP rule leads to a fast, accurate, and efficient processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Recent supervised DCNNs have reached high accuracies on the most challenging object recognition datasets such as Imagenet. Architecture of theses networks are largely inspired by the deep hierarchical processing in the visual cortex. For instance, DCNNs use retinotopically arranged neurons with restricted receptive fields, and the receptive field size and feature complexity of the neurons gradually increase through the layers. However, the learning and neural processing mechanisms applied in DCNNs are inconsistent with A popular approach in previous researches is to convert pre-trained supervised DCNNs into equivalent spiking network. To simulate the floatingpoint calculations in DCNNs, they have to use the firing rate as the neural code, which in result increases the number of required spikes and the processing time. For instance, the converted version of a simple two-layer DCNN for the MNIST dataset with images of 28 × 28 pixels requires thousands of spikes and hundreds of time steps per image. On the other hand, there are some SDNNs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> which are originally spiking network but employ firing rate coding and biologically implausible learning rules such as autoencoders and back-propagation.</p><p>Here, we proposed a STDP-based SDNN with a spike-time coding. Each neuron was allowed to fire at most once, where its spike-time indicates the significance of its visual input. Therefore, neurons that fire earlier are carrying more salient visual information, and hence, they were allowed to do the STDP and learn the input patterns. As the learning progresses, each layer converged to a set of diverse but informative features, and the feature complexity gradually increases through the layers from simple edge features to object prototypes. The proposed SDNN was evaluated on several image datasets and reached high recognition accuracies. This shows how the proposed temporal coding and learning mechanism (STDP and learning competition) lead to discriminative object representations.</p><p>The proposed SDNN has several advantages to its counterparts. First, our proposed SDNN is the first spiking neural network with more than one learnable layer which can process large-scale natural object images. Second, due to the use of an efficient temporal coding, which encodes the visual information in the time of the first spikes, it can process the input images with a low number of spikes and in a few processing time steps. Third, the proposed SDNN exploits the bio-inspired and totally unsupervised STDP learning rule which can learn the diagnostic object features and neglect the irrelevant backgrounds.</p><p>We compared the proposed SDNN to several other networks including unsupervised methods such as HMAX and convolutional autoencoder network, and supervised methods such as DC-NNs. The proposed SDNN could outperform the unsupervised methodes which shows its advantages in extracting more informatic features from training images. Also, it was better than the supervised deep network which largely suffered from overfitting and lack of sufficient training data. Although, the state-of-the-art supervised DCNNs have stunning performance on large datasets like Imagenet, but contrary to the proposed SDNN, they fall in trouble with small and mideum size datasets.</p><p>Our SDNN could be efficiently implemented in parallel hardware (e.g., FPGA <ref type="bibr" target="#b50">[51]</ref>) using address event representation (AER) <ref type="bibr" target="#b51">[52]</ref> protocol. With AER, spike events are represented by the addresses of sending and receiving neurons, and time is represented by the asynchronous occurrence of spike events. Since these hardware are much faster than biological hardware, simulations could run several order of magnitude faster than real time <ref type="bibr" target="#b52">[53]</ref>. The primate visual system extracts the rough content of an image in about 100 ms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b4">5]</ref>. We thus speculate that some dedicated hardware will be able to do the same in the order of a millisecond or less.</p><p>Also, the proposed SDNN can be modified to use spiking retinal models <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> as the input layer. These models mimic the spatiotemporal filtering of the retinal ganglion cells with center/surround receptive fields. Alternatively, we could use neuromorphic asynchronous event-based cameras such as dynamic vision sensor (DVS), which generate output events when they capture transients in the scene <ref type="bibr" target="#b56">[57]</ref>. Finally, due to the DoG filtering in the input layer of the proposed SDNN, some visual information such as texture and color are lost. Hence, future studies should focus on encoding these additional pieces of information in the input layer.</p><p>Biological evidence indicate that in addition to the unsupervised learning mechanisms (e.g., STDP), there are also dopamine-based reinforcement learning strategies in the brain <ref type="bibr" target="#b57">[58]</ref>. Besides, although it is still unclear how supervised learning is implemented in biological neural networks, it seems that for some tasks (e.g., motor control and sensory inputs prediction) the brain must constantly learn temporal dynamics based on error feedback <ref type="bibr" target="#b58">[59]</ref>. Employing such reinforcement and supervised learning strategies could improve the proposed SDNN in different aspects which are inevitable with unsupervised learning methods. Particularly, they can help to reduce the number of required features and to extract optimized task-dependent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information</head><p>Video 1. We prepared a video available at https: //youtu.be/u32Xnz2hDkE showing the learning progress and neural activity over the Caltech face and motorbike task. Here we presented the face and motorbike training examples, propagated the corresponding spike waves, and applied the STDP rule. The input image is presented at the top-left corner of the screen. The output spikes of the input layer (i.e., DoG layer) at each time step is presented in the top-middle panel, and the accumulation of theses spikes is shown in the top-right panel. For each of the subsequent convolutional layers, the preferred features, the output spikes at each time step, and the accumulation of the output spikes are presented in the corresponding panels. Note that 4, 8, and 2 features from the first, second and third convolutional layers are selected and shown, respectively. As mentioned, the learning occurs layer by layer, thus, the label of the layer which is currently doing the learning is specified by the red color. As seen, the first layer learns to detect edges, the second layer learns intermediate features, and finally the third layer learns face and motorbike prototype features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sample architecture of the proposed SDNN with three convolutional and three pooling layers. The first layer applies ON-and OFF-center DoG filters of size w D 1 × w D 2 on the input image and encode the image contrasts in the timing of the output spikes. The ith convolutional layer, Conv i, learns combinations of features extracted in the previous layer. The ith pooling layer, Pool i, provides translation invariance for features extracted in the previous layer and compress the visual information using a local maximum operation. Finally the classifier detects the object category based on the feature values computed by the global pooling layer. The window size of the ith convolutional and pooling layers are indicated by w ci 1,2 and w pi 1,2 , respectively. The number of the neuronal maps of the ith convolutional and pooling layer are also indicated by n i below each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The synaptic changes of some neuronal maps in different layers through the learning with the Caltech face/motorbike dataset. A) The first convolutional layer becomes selective to oriented edges. B) The second convolutional layer converges to object parts. C) The third convolutional layer learns the object prototype and respond to whole objects.</figDesc><graphic coords="9,71.44,82.21,452.37,474.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Each curve shows the variation of the convergence index through the learning of a convolutonal layer. The weight histogram of the first convolutional layer at some critical points during the learning are shown next to its convergence curve.</figDesc><graphic coords="10,101.22,82.20,392.80,255.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The spiking activity of the convolutional layers with the face and motorbike images. The preferred features of neuronal maps in each convolutional layer are shown on the right. Each feature is coded by a specific color border. The spiking activity of the convolutional layers, accumulated over all the time steps, is shown in the corresponding panels. Each point in a panel indicates that a neuron in that location has fired at a time step, and the color of the point indicates the preferred feature of the activated neuron.</figDesc><graphic coords="11,115.10,82.21,365.04,416.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recognition accuracies (mean ± std) of the proposed SDNN for different number of training images per category used in the STDP-based feature learning and/or training the classifier. The red curve presents the model's accuracy when the different number of images are used to train the network (by unsupervised STDP) and the classifier as well. The blue curve shows the model's accuracy when the layers of the network are trained using STDP with 400 images (200 from each category), and the classifier is trained with different number of labeled images per category.</figDesc><graphic coords="12,38.05,82.21,250.14,185.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Some sample images of different object categories of ETH-80 in different viewpoints. For each image, the preferred feature of an activated neuron in the third convolutional layer is shown in below.</figDesc><graphic coords="13,107.02,209.92,381.20,193.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The confusion matrix of the proposed SDNN over the ETH-80 dataset.</figDesc><graphic coords="15,340.05,83.20,201.79,169.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Some misclassified samples of the ETH-80 dataset by the proposed SDNN.</figDesc><graphic coords="15,306.58,317.09,253.48,51.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The Gabor-like features learned by the neuronal maps of the first convolutional layer from the MNIST images. The red and green colors receptively indicate the strength of input synapses from ON-and OFF-center DoG cells.</figDesc><graphic coords="16,333.58,82.21,197.09,162.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Recognition accuracies of the proposed SDNN with random features in different convolutional layers.</figDesc><table><row><cell cols="4">Conv layers with random features Non 3rd 2nd &amp; 3rd 1st &amp; 2nd &amp; 3rd</cell></row><row><cell>Accuracy (%)</cell><cell>99.1 80.2</cell><cell>67.8</cell><cell>66.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Recognition accuracies of the proposed SDNN for differen amounts of noise.</figDesc><table><row><cell>Noise level</cell><cell cols="7">α = 0% α = 5% α = 10% α = 20% α = 30% α = 40% α = 50%</cell></row><row><cell>Accuracy (%)</cell><cell>99.1</cell><cell>95.4</cell><cell>91.6</cell><cell>84.3</cell><cell>63.7</cell><cell>57.6</cell><cell>54.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Recognition accuracies of the proposed SDNN and some other SNNs over the MNIST dataset.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Neural coding Learning-type</cell><cell>Learning-rule</cell><cell>Accuracy (%)</cell></row><row><cell>Dendritic neurons [48]</cell><cell>Rate-based</cell><cell>Supervised</cell><cell>Morphology learning</cell><cell>90.3</cell></row><row><cell cols="2">Convolutional SNN [37] Spike-based</cell><cell>Supervised</cell><cell>Tempotron rule</cell><cell>91.3</cell></row><row><cell>Two layer network [36]</cell><cell>Spike-based</cell><cell>Unsupervised</cell><cell>STDP</cell><cell>93.5</cell></row><row><cell>Spiking RBM [49]</cell><cell>Rate-based</cell><cell>Supervised</cell><cell>Contrastive divergence</cell><cell>94.1</cell></row><row><cell>Two layer network [38]</cell><cell>Spike-based</cell><cell>Unsupervised</cell><cell>STDP</cell><cell>95.0</cell></row><row><cell>Convolutional SNN [50]</cell><cell>Rate-based</cell><cell>Supervised</cell><cell>Back-propagation</cell><cell>99.1</cell></row><row><cell>Proposed SDNN</cell><cell>Spike-based</cell><cell>Unsupervised</cell><cell>STDP</cell><cell>98.4</cell></row><row><cell cols="3">the visual cortex, where neurons communicate us-</cell><cell></cell><cell></cell></row><row><cell cols="3">ing spikes and learn the input spike patterns in</cell><cell></cell><cell></cell></row><row><cell cols="3">a mainly unsupervised manner. Employing such</cell><cell></cell><cell></cell></row><row><cell cols="3">mechanisms in DCNNs can improve their energy</cell><cell></cell><cell></cell></row><row><cell cols="3">consumption and decrease their need for an ex-</cell><cell></cell><cell></cell></row><row><cell cols="3">pensive supervised learning with millions of la-</cell><cell></cell><cell></cell></row><row><cell>beled images.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research received funding from the European Research Council under the European Unions 7th Framework Program (FP/2007-2013) / ERC Grant Agreement n.323711 (M4 project). The authors thank the NVIDIA Academic Programs team for donating a GPU hardware.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Untangling invariant object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="333" to="341" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speed of processing in the human visual system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="520" to="522" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast readout of object identity from macaque inferior temporal cortex</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="page" from="863" to="866" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neocognitron : a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Brain Theory and Neural Networks</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features through spike timing dependent plasticity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="1" to="8" />
			<pubPlace>New York, New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparing state-of-the-art visual features on invariant object recognition tasks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Barhomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE workshop on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Kona, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feedforward objectvision models only tolerate small image variations compared to human</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farzmahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Khaligh-Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks resemble human feedforward vision in invariant object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32672</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humans and deep networks largely agree on which kinds of variation make object recognition harder</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate it cortex for core visual object recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003963</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep supervised, but not unsupervised, models may explain it cortical representation</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Khaligh-Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003915</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How silent is the brain: is there a dark matter problem in neuroscience?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Physiology A</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="777" to="784" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computing with spikes, Special Issue on Foundations of Information Processing of TELEMATIK</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computational neuroscience of vision</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Oxford university press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Receptive-field modification in rat visual cortex induced by paired visual stimulation and single-cell spiking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Meliza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="183" to="189" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Associative hebbian synaptic plasticity in primate visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rozas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Treviño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirkwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7575" to="7579" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stimulus timingdependent plasticity in high-level vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Leopold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="332" to="337" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spiking deep convolutional neural networks for energy-efficient object recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08829</idno>
		<title level="m">Spiking deep networks with lif neurons</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zarrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Pedroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Rebooting Computing</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mirrored stdp implements autoencoder learning in a network of spiking neurons</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Burbank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1004566</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04156</idno>
		<title level="m">Towards biologically plausible deep learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning real-world stimuli in a neural network with spike-driven synaptic dynamics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2881" to="2912" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Homeostatic plasticity in bayesian spiking networks as expectation maximization with posterior constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Habenschuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="773" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Immunity to device variations in a spiking neural network with memristive nanodevices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="288" to="295" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feedforward categorization on aer motion events using cortex-like features in a spiking neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Categorization and decision-making in a neurobiologically plausible spiking network using a stdp-like learning rule</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beyeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Krichmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bio-inspired unsupervised learning of visual features leads to robust invariant object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="382" to="392" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spike-based strategies for rapid processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="715" to="725" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rate coding versus temporal order coding: what the retinal ganglion cells tell the visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1255" to="1283" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rank order coding: a retinal information decoding strategy revealed by largescale multielectrode array retinal recordings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Portelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hilgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maccione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berdondini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kornprobst</surname></persName>
		</author>
		<author>
			<persName><surname>Sernagor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eneuro</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Networks of integrate-and-fire neurons using rank order coding b: Spike timing dependent plasticity and emergence of orientation selectivity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perrinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="539" to="545" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Taking the max from neuronal responses</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rousselet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre-Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="99" to="102" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A feedforward architecture accounts for rapid categorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="6424" to="6429" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved margin multi-class classification using dendritic neurons with morphological learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting><address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2640" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time classification and sensor fusion with a spiking deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>Killarney, Ireland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast Pipeline 128??128 pixel spiking convolution core for event-driven vision processing in FP-GAs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Event-Based Control, Communication and Signal Processing</title>
		<meeting>1st International Conference on Event-Based Control, Communication and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wiring considerations in analog VLSI systems with application to field-programmable networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sivilotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Div., California Inst. Technol</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Pasadena, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">STDP and STDP variations with memristors for spiking neuromorphic learning systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Prodromakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1762" to="1776" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Virtual Retina: a biological retina model and simulator, with contrast gain control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="219" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Computational Framework for Realistic Retina Modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Martínez-Cañada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pelayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1650030</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An 128x128 120dB 15us-latency temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Role of dopamine neurons in reward and aversion: a synaptic plasticity perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pignatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1145" to="1157" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Complementary roles of basal ganglia and cerebellum in learning and motor control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="732" to="739" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
