<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random forest versus logistic regression: a large-scale benchmark experiment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Raphael</forename><surname>Couronn√©</surname></persName>
							<email>raphael.couronne@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Information Processing, Biometry and Epidemiology</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Probst</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Information Processing, Biometry and Epidemiology</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne-Laure</forename><surname>Boulesteix</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Information Processing, Biometry and Epidemiology</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Random forest versus logistic regression: a large-scale benchmark experiment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">032A3FBA462B185024F52476B06B5DC5</idno>
					<idno type="DOI">10.1186/s12859-018-2264-5</idno>
					<note type="submission">Received: 4 December 2017 Accepted: 27 June 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Logistic regression</term>
					<term>Classification</term>
					<term>Prediction</term>
					<term>Comparison study</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background and goal: The Random Forest (RF) algorithm for regression and classification has considerably gained popularity since its introduction in 2001. Meanwhile, it has grown to a standard classification approach competing with logistic regression in many innovation-friendly scientific fields. Results: In this context, we present a large scale benchmarking experiment based on 243 real datasets comparing the prediction performance of the original version of RF with default parameters and LR as binary classification tools. Most importantly, the design of our benchmark experiment is inspired from clinical trial methodology, thus avoiding common pitfalls and major sources of biases. Conclusion: RF performed better than LR according to the considered accuracy measured in approximately 69% of the datasets. The mean difference between RF and LR was 0.029 (95%-CI=[ 0.022, 0.038]) for the accuracy, 0.041 (95%-CI=[ 0.031, 0.053]) for the Area Under the Curve, and -0.027 (95%-CI=[ -0.034, -0.021]) for the Brier score, all measures thus suggesting a significantly better performance of RF. As a side-result of our benchmarking experiment, we observed that the results were noticeably dependent on the inclusion criteria used to select the example datasets, thus emphasizing the importance of clear statements regarding this dataset selection process. We also stress that neutral studies similar to ours, based on a high number of datasets and carefully designed, will be necessary in the future to evaluate further variants, implementations or parameters of random forests which may yield improved accuracy compared to the original version with default values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In the context of low-dimensional data (i.e. when the number of covariates is small compared to the sample size), logistic regression is considered a standard approach for binary classification. This is especially true in scientific fields such as medicine or psycho-social sciences where the focus is not only on prediction but also on explanation; see Shmueli <ref type="bibr" target="#b0">[1]</ref> for a discussion of this distinction. Since its invention 17 years ago, the random forest (RF) prediction algorithm <ref type="bibr" target="#b1">[2]</ref>, which focuses on prediction rather than explanation, has strongly gained popularity and is increasingly becoming a common "standard tool" also used by scientists without any strong background in statistics or machine learning. Our experience as authors, reviewers and readers is that random forest can now be default, a method should be simple to use and not require any complex human intervention (such as parameter tuning) demanding particular expertise. Our experience from statistical consulting is that applied research practitioners tend to apply methods in their simplest form for different reasons including lack of time, lack of expertise and the (critical) requirement of many applied journals to keep data analysis as simple as possible. Currently, the simplest approach consists of running RF with default parameter values, since no unified and easy-to-use tuning approach has yet established itself. It is not the goal of this paper to discuss how to improve RF's performance by appropriate tuning strategies and which level of expertise is ideally required to use RF. We simply acknowledge that the standard variant with default values is widely used and conjecture that things will probably not dramatically change in the short term. That is why we made the choice to consider RF with default values as implemented in the very widely used package randomForest-while admitting that, if time and competence are available, more sophisticated strategies may often be preferable. As an outlook, we also consider RF with parameters tuned using the recent package tuneRanger <ref type="bibr" target="#b3">[4]</ref> in a small additional study.</p><p>Comparison studies published in literature often include a large number of methods but a relatively small number of datasets <ref type="bibr" target="#b4">[5]</ref>, yielding an ill-posed problem as far as statistical interpretation of benchmarking results are concerned. In the present paper we take an opposite approach: we focus on only two methods for the reasons outlined above but design our benchmarking experiments in such a way that it yields solid evidence. A particular strength of our study is that we as authors are equally familiar with both methods. Moreover, we are "neutral" in the sense that we have no personal priori preference for one of the methods: ALB published a number of papers on RF, but also papers on regressionbased approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and papers pointing to critical problems of RF <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Neutrality and equal expertise would be much more difficult if not impossible to ensure if several variants of RF (including tuning strategies) and logistic regression were included in the study. Further discussions of the concept of authors' neutrality can be found elsewhere <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Most importantly, the design of our benchmark experiment is inspired by the methodology of clinical trials that has been developed with huge efforts for several decades. We follow the line taken in our recent paper <ref type="bibr" target="#b10">[11]</ref> and carefully define the design of our benchmark experiments including, beyond issues related to neutrality outlined above, considerations on sample size (i.e. number of datasets included in the experiment) and inclusion criteria for datasets. Moreover, as an analogue to subgroup analyses and the search for biomarkers of treatment effect in clinical trials, we also investigate the dependence of our conclusions on datasets' characteristics.</p><p>As an important by-product of our study, we provide empirical insights into the importance of inclusion criteria for datasets in benchmarking experiments and general critical discussions on design issues and scientific practice in this context. The goal of our paper is thus two-fold. Firstly we aim to present solid evidence on the performance of standard logistic regression and random forests with default values. Secondly, we demonstrate the design of a benchmark experiment inspired from clinical trial methodology.</p><p>The rest of this paper is structured as follows. After a short overview of LR and RF, the associated VIM, partial dependence plots <ref type="bibr" target="#b11">[12]</ref>, the cross-validation procedure and performance measures used to evaluate the methods ("Background" section), we present our benchmarking approach in "Methods" section, including the criteria for dataset selection. Results are presented in "Results" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>This section gives a short overview of the (existing) methods involved in our benchmarking experiments: logistic regression (LR), random forest (RF) including variable importance measures, partial dependence plots, and performance evaluation by cross-validation using different performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic regression (LR)</head><p>Let Y denote the binary response variable of interest and X 1 , . . . , X p the random variables considered as explaining variables, termed features in this paper. The logistic regression model links the conditional probability P(Y = 1|X 1 , ..., X p ) to X 1 , . . . , X p through</p><formula xml:id="formula_0">P(Y = 1|X 1 , ..., X p ) = exp Œ≤ 0 + Œ≤ 1 X 1 + ‚Ä¢ ‚Ä¢ ‚Ä¢ + Œ≤ p X p 1 + exp Œ≤ 0 + Œ≤ 1 X 1 + ‚Ä¢ ‚Ä¢ ‚Ä¢ + Œ≤ p X p ,<label>(1)</label></formula><p>where Œ≤ 0 , Œ≤ 1 , . . . , Œ≤ p are regression coefficients, which are estimated by maximum-likelihood from the considered dataset. The probability that Y = 1 for a new instance is then estimated by replacing the Œ≤'s by their estimated counterparts and the X's by their realizations for the considered new instance in Eq. <ref type="bibr" target="#b0">(1)</ref>. The new instance is then assigned to class Y = 1 if P(Y = 1) &gt; c, where c is a fixed threshold, and to class Y = 0 otherwise. The commonly used threshold c = 0.5, which is also used in our study, yields a so-called Bayes classifier. As for all model-based methods, the prediction performance of LR depends on whether the data follow the assumed model. In contrast, the RF method presented in the next section does not rely on any model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random forest (RF) Brief overview</head><p>The random forest (RF) is an "ensemble learning" technique consisting of the aggregation of a large number of decision trees, resulting in a reduction of variance compared to the single decision trees. In this paper we consider Leo Breiman's original version of RF <ref type="bibr" target="#b1">[2]</ref>, while acknowledging that other variants exist, for example RF based on conditional inference trees <ref type="bibr" target="#b12">[13]</ref> which address the problem of variable selection bias <ref type="bibr" target="#b13">[14]</ref> and perform better in some cases, or extremely randomized trees <ref type="bibr" target="#b14">[15]</ref>.</p><p>In the original version of RF <ref type="bibr" target="#b1">[2]</ref>, each tree of the RF is built based on a bootstrap sample drawn randomly from the original dataset using the CART method and the Decrease Gini Impuritiy (DGI) as the splitting criterion <ref type="bibr" target="#b1">[2]</ref>. When building each tree, at each split, only a given number mtry of randomly selected features are considered as candidates for splitting. RF is usually considered a black-box algorithm, as gaining insight on a RF prediction rule is hard due to the large number of trees. One of the most common approaches to extract from the random forest interpretable information on the contribution of different variables consists in the computation of the socalled variable importance measures outlined in "Variable importance measures" section. In this study we use the package randomForest <ref type="bibr" target="#b2">[3]</ref> (version 4. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> with default values, see the next paragraph for more details on tuning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>This section presents the most important parameters for RF and their common default values as implemented in the R package randomForest <ref type="bibr" target="#b2">[3]</ref> and considered in our study. Note, however, that alternative choices may yield better performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and that parameter tuning for RF has to be further addressed in future research. The parameter ntree denotes the number of trees in the forest. Strictly speaking, ntree is not a tuning parameter (see <ref type="bibr" target="#b17">[18]</ref> for more insight into this issue) and should be in principle as large as possible so that each candidate feature has enough opportunities to be selected. In practice, however, performance reaches a plateau with a few hundreds of trees for most datasets <ref type="bibr" target="#b17">[18]</ref>. The default value is ntree=500 in the package randomForest. The parameter mtry denotes the number of features randomly selected as candidate features at each split. A low value increases the chance of selection of features with small effects, which may contribute to improved prediction performance in cases where they would otherwise be masked by features with large effects. A high value of mtry reduces the risk of having only non-informative candidate features. In the package randomForest, the default value is ‚àö p for classification with p the number of features of the dataset. The parameter nodesize represents the minimum size of terminal nodes. Setting this number larger yields smaller trees. The default value is 1 for classification. The parameter replace refers to the resampling scheme used to randomly draw from the original dataset different samples on which the trees are grown. The default is replace=TRUE, yielding bootstrap samples, as opposed to replace=FALSE yielding subsamples-whose size is determined by the parameter sampsize.</p><p>The performance of RF is known to be relatively robust against parameter specifications: performance generally depends less on parameter values than for other machine learning algorithms <ref type="bibr" target="#b18">[19]</ref>. However, noticeable improvements may be achieved in some cases <ref type="bibr" target="#b19">[20]</ref>. The recent R package tuneRanger <ref type="bibr" target="#b3">[4]</ref> allows to automatically tune RF's parameters simultaneously using an efficient modelbased optimization procedure. In additional analyses presented in "Additional analysis: tuned RF" section, we compare the performance of RF and LR with the performance of RF tuned with this procedure (denoted as TRF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable importance measures</head><p>As a byproduct of random forests, the built-in variable importance measures (VIM) rank the variables (i.e. the features) with respect to their relevance for prediction <ref type="bibr" target="#b1">[2]</ref>. The so-called Gini VIM has shown to be strongly biased <ref type="bibr" target="#b13">[14]</ref>. The second common VIM, called permutation-based VIM, is directly based on the accuracy of RF: it is computed as the mean difference (over the ntree trees) between the OOB errors before and after randomly permuting the values of the considered variable. The underlying idea is that the permutation of an important feature is expected to decrease accuracy more strongly than the permutation of an unimportant variable.</p><p>VIMs are not sufficient in capturing the patterns of dependency between features and response. They only reflect-in the form of a single number-the strength of this dependency. Partial dependence plots can be used to address this shortcoming. They can essentially be applied to any prediction method but are particularly useful for black-box methods which (in contrast to, say, generalized linear models) yield less interpretable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial dependence plots</head><p>Partial dependence plots (PDPs) offer insight of any black box machine learning model, visualizing how each feature influences the prediction while averaging with respect to all the other features. The PDP method was first developed for gradient boosting <ref type="bibr" target="#b11">[12]</ref>. Let F denote the function associated with the classification rule: for classification, F X 1 , . . . , X p ‚àà [0, 1] is the predicted probability of the observation belonging to class 1. Let j be the index of the chosen feature X j and X j its complement, such that X j = X 1 , ..., X j-1 , X j+1 , ..., X p . The partial dependence of F on feature X j is the expectation</p><formula xml:id="formula_1">F X j = E X j F X j , X j (2)</formula><p>which can be estimated from the data using the empirical distribution</p><formula xml:id="formula_2">pX j (x) = 1 N N i=1 F x i,1 , ...x i,j-1 , x, x i,j+1 , ..., x i,p ,<label>(3)</label></formula><p>where x i,1 , . . . , x i,p stand for the observed values of X 1 , . . . , X p for the ith observation. As an illustration, we display in Fig. <ref type="figure">1</ref> the partial dependence plots obtained by logistic regression and random forest for three simulated datasets representing classification problems, each including n = 1000 independent observations. For each dataset the variable Y is simulated according to the formula log(P(Y = 1)/P(Y = 0))</p><formula xml:id="formula_3">= Œ≤ 0 + Œ≤ 1 X 1 + Œ≤ 2 X 2 + Œ≤ 3 X 1 X 2 + Œ≤ 4 X 2 1 .</formula><p>The first dataset (top) represents the linear scenario (Œ≤ 1 = 0, Œ≤ 2 = 0, Œ≤ 3 = Œ≤ 4 = 0), the second dataset (middle) an interaction (Œ≤ 1 = 0, Œ≤ 2 = 0, Œ≤ 3 = 0, Œ≤ 4 = 0) and the third (bottom) a case of non-linearity (Œ≤ 1 = Œ≤ 2 = Œ≤ 3 = 0, Œ≤ 4 = 0). For all three datasets the random vector (X 1 , X 2 ) follows distribution N 2 (0, I), with I representing the identity matrix. The data points are represented in the left column, while the PDPs are displayed Fig. <ref type="figure">1</ref> Example of partial dependence plots. Plot of the PDP for the three simulated datasets. Each line is related to a dataset. On the left, visualization of the dataset. On the right, the partial dependence for the variable X 1 . First dataset:</p><formula xml:id="formula_4">Œ≤ 0 = 1, Œ≤ 1 = 5, Œ≤ 2 = -2 (linear), second dataset: Œ≤ 0 = 1, Œ≤ 1 = 1, Œ≤ 2 = -1, Œ≤ 3 = 3 (interaction), third dataset Œ≤ 0 = -2, Œ≤ 4 = 5 (non-linear)</formula><p>in the right column for RF, logistic regression as well as the true logistic regression model (i.e. with the true coefficient values instead of fitted values). We see that RF captures the dependence and non-linearity structures in cases 2 and 3, while logistic regression, as expected, is not able to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance assessment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation</head><p>In a k-fold cross-validation (CV), the original dataset is randomly partitioned into k subsets of approximately equal sizes. At each of the k CV iterations, one of the folds is chosen as the test set, while the k -1 others are used for training. The considered performance measure is computed based on the test set. After the k iterations, the performances are finally averaged over the iterations. In our study, we perform 10 repetitions of stratified 5-fold CV, as commonly recommended <ref type="bibr" target="#b20">[21]</ref>. In the stratified version of the CV, the folds are chosen such that the class frequencies are approximately the same in all folds. The stratified version is chosen mainly to avoid problems with strongly imbalanced datasets occurring when all observations of a rare class are included in the same fold. By "10 repetitions", we mean that the whole CV procedure is repeated for 10 random partitions into k folds with the aim to provide more stable estimates.</p><p>In our study, this procedure is applied to different performance measures outlined in the next subsection, for LR and RF successively and for M real datasets successively. For each performance measure, the results are stored in form of an M √ó 2 matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance measures</head><p>Given a classifier and a test dataset of size n test , let pi , i = 1, . . . , n denote the estimated probability of the ith observation (i = 1, . . . , n test ) to belong to class Y = 1, while the true class membership of observation i is simply denoted as y i . Following the Bayes rule implicitly adopted in LR and RF, the predicted class ≈∑i is simply defined as ≈∑i = 1 if pi &gt; 0.5 and 0 otherwise.</p><p>The accuracy, or proportion of correct predictions is estimated as</p><formula xml:id="formula_5">acc = 1 n test n t est i=1 I y i = ≈∑i ,</formula><p>where I(.) denotes the indicator function (I(A) = 1 if A holds, I(A) = 0 otherwise). The Area Under Curve (AUC), or probability that the classifier ranks a randomly chosen observation with Y = 1 higher than a randomly chosen observation with Y = 0 is estimated as</p><formula xml:id="formula_6">auc = 1 n 0,test n 1,test i:y i =1 j:y j =0 I pi &gt; pj ,</formula><p>where n 0,test and n 1,test are the numbers of observations in the test set with y i = 0 and y i = 1, respectively.</p><p>The Brier score is a commonly and increasingly used performance measure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. It measures the deviation between true class and predicted probability and is estimated as</p><formula xml:id="formula_7">brier = 1 n test n test i=1 pi -y i 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The OpenML database So far we have stated that the benchmarking experiment uses a collection of M real datasets without further specifications. In practice, one often uses already formatted datasets from public databases. Some of these databases offer a user-friendly interface and good documentation which facilitate to some extent the preliminary steps of the benchmarking experiment (search for datasets, data download, preprocessing). One of the most well-known database is the UCI repository <ref type="bibr" target="#b23">[24]</ref>. Specific scientific areas may have their own databases, such as ArrayExpress for molecular data from high-throughput experiments <ref type="bibr" target="#b24">[25]</ref>. More recently, the OpenML database <ref type="bibr" target="#b25">[26]</ref> has been initiated as an exchange platform allowing machine learning scientists to share their data and results. This database included as many as 19660 datasets in October 2016 when we selected datasets to initiate our study, a non-negligible proportion of which are relevant as example datasets for benchmarking classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inclusion criteria and subgroup analyses</head><p>When using a huge database of datasets, it becomes obvious that one has to define criteria for inclusion in the benchmarking experiment. Inclusion criteria in this context do not have any long tradition in computational science. The criteria used by researchers-including ourselves before the present study-to select datasets are most often completely non-transparent. It is often the fact that they select a number of datasets which were found to somehow fit the scope of the investigated methods, but without clear definition of this scope. We conjecture that, from published studies, datasets are occasionally removed from the experiment a posteriori because the results do not meet the expectations/hopes of the researchers. While the vast majority of researchers certainly do not cheat consciously, such practices may substantially introduce bias to the conclusion of a benchmarking experiment; see previous literature <ref type="bibr" target="#b26">[27]</ref> for theoretical and empirical investigation of this problem. Therefore, "fishing for datasets" after completion of the benchmark experiment should be prohibited, see Rule 4 of the "ten simple rules for reducing over-optimistic reporting" <ref type="bibr" target="#b27">[28]</ref>.</p><p>Independent of the problem of fishing for significance, it is important that the criteria for inclusion in the benchmarking experiment are clearly stated as recently discussed <ref type="bibr" target="#b10">[11]</ref>. In our study, we consider simple datasets' characteristics, also termed "meta-features". They are presented in Table <ref type="table" target="#tab_0">1</ref>. Based on these datasets' characteristics, we define subgroups and repeat the benchmark study within these subgroups, following the principle of subgroup analyses in clinical research. For example, one could analyse the results for "large" datasets (n &gt; 1000) and "small datasets" (n ‚â§ 1000) separately. Moreover, we also examine the subgroup of datasets related to biosciences/medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-learning</head><p>Taking another perspective on the problem of benchmarking results being dependent on dataset's meta-features, we also consider modelling the difference between the methods' performances (considered as response variable) based on the datasets' meta-features (considered as features). Such a modelling approach can be seen as a simple form of meta-learning-a well-known task in machine learning <ref type="bibr" target="#b28">[29]</ref>. A similar approach using linear mixed models has been recently applied to the selection of an appropriate classification method in the context of high-dimensional gene expression data analysis <ref type="bibr" target="#b29">[30]</ref>. Considering the potentially complex dependency patterns between response and features, we use RF as a prediction tool for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power calculation</head><p>Considering the M √ó 2 matrix, collecting the performance measures for the two investigated methods (LR and RF) on the M considered datasets, one can perform a test for paired samples to compare the performances of the two methods <ref type="bibr" target="#b30">[31]</ref>. We refer to the previously published statistical framework <ref type="bibr" target="#b30">[31]</ref> for a precise mathematical definition of the tested null-hypothesis in the case of the t-test for paired samples. In this framework, the datasets play the </p><formula xml:id="formula_8">M req ‚âà z 1-Œ±/2 + z 1-Œ≤ 2 œÉ 2 Œ¥ 2 (4)</formula><p>where z q is the q-quantile of the normal distribution and œÉ 2 is the variance of the difference between the two methods' performances over the datasets, which may be roughly estimated through a pilot study or previous literature.</p><p>For example, the required number of datasets to detect a difference in performances of Œ¥ = 0.05 with Œ± = 0.05 and 1 -Œ≤ = 0.8 is M req = 32 if we assume a variance of œÉ 2 = 0.01 and M req = 8 for œÉ 2 = 0.0025. It increases to M req = 197 and M req = 50, respectively, for differences of Œ¥ = 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>Several R packages are used to implement the benchmarking study: mlr (version 2.10) for higher abstraction and a simpler way to conduct benchmark studies <ref type="bibr" target="#b31">[32]</ref>, OpenML (version 1.2) for loading the datasets <ref type="bibr" target="#b32">[33]</ref>, and batchtools (version 0.9.2) for parallel computing <ref type="bibr" target="#b33">[34]</ref>. Note that the LR and RF learners called via mlr are wrappers on the functions glm and randomForest, respectively.</p><p>The datasets supporting the conclusions of this article are freely available in OpenML as described in "The OpenML database" section.</p><p>Emphasis is placed on the reproducibility of our results. Firstly, the code implementing all our analyses is fully available from GitHub <ref type="bibr" target="#b34">[35]</ref>. For visualization-only purposes, the benchmarking results are available from this link, so that our graphics can be quickly generated by mouse-click. However, the code to re-compute these results, i.e. to conduct the benchmarking study, is also available from GitHub. Secondly, since we use specific versions of R and add-on packages and our results may thus be difficult to reproduce in the future due to software updates, we also provide a docker image <ref type="bibr" target="#b35">[36]</ref>. Docker automates the deployment of applications inside a so called "Docker container" <ref type="bibr" target="#b36">[37]</ref>. We use it to create an R environment with all the packages we need in their correct version. Note that docker is not necessary here (since all our codes are available from GitHub), but very practical for a reproducible environment and thus for reproducible research in the long term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In our study we consider a set of M datasets (see "Included datasets" section for more details) and compute for each of them the performance of random forest and logistic regression according to the three performance measures outlined in "Performance assessment" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Included datasets</head><p>From approximately 20000 datasets currently available from OpenML <ref type="bibr" target="#b25">[26]</ref>, we select those featuring binary classification problems. Further, we remove the datasets that include missing values, the obviously simulated datasets as well as duplicated datasets. We also remove datasets with more features than observations (p &gt; n), and datasets with loading errors. This leaves us with a total of 273 datasets. See Fig. <ref type="figure" target="#fig_0">2</ref> for an overview. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing values due to errors</head><p>Out of the 273 selected datasets, 8 require too much computing time when parallelized using the package batchtools and expired or failed. These-extremely largedatasets are discarded in the rest of the study, leaving us with 265 datasets.</p><p>Both LR and RF fail in the presence of categorical features with too many categories. More precisely, RF fails when more than 53 categories are detected in at least one of the features, while LR fails when levels undetected during the training phase occur in the test data. We could admittedly have prevented these errors through basic preprocessing of the data such as the removal or recoding of the features that induce errors. However, we decide to just remove the datasets resulting in NAs because we do not want to address preprocessing steps, which would be a topic on their own and cannot be adequately treated along the way for such a high number of datasets. Since 22 datasets yield NAs, our study finally includes 265-22=243 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main results</head><p>Overall performances are presented in a synthesized form in Table <ref type="table" target="#tab_1">2</ref> for all three measures in form of average performances along with standard deviations and confidence intervals computed using the adjusted bootstrap percentile (BCa) method <ref type="bibr" target="#b37">[38]</ref>. The boxplots of performances of Random Forest (RF) and Logistic Regression (LR) for the three considered performance measures are depicted in Fig. <ref type="figure">3</ref>, which also includes the boxplot of the difference in performances (bottom row). It can be seen from Fig. <ref type="figure">3</ref> that RF performs better for the majority of datasets (69.0% of the datasets for acc, 72.3% for auc and 71.5% for brier). Furthermore, when LR outperforms RF the difference is small. It can also be noted that the differences in performance tend to be larger for auc than for acc and brier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explaining differences: datasets' meta-features</head><p>In this section, we now perform different types of additional analyses with the aim to investigate the relation between the datasets' meta-features and the performance difference between LR and RF. In "Preliminary analysis" section, we first consider an example dataset in detail to examine whether changing the sample size n and the number p of features for this given dataset changes the difference between performances of LR and RF (focusing on a specific dataset, we are sure that confounding is not an issue). In "Subgroup analyses: meta-features" to "Meta-learning" sections, we then assess the association between dataset's meta-features and performance difference over all datasets included in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary analysis</head><p>While it is obvious to any computational scientist that the performance of methods may depend on meta-features, this issue is not easy to investigate in real data settings because i) it requires a large number of datasets-a condition that is often not fulfilled in practice; ii) this problem is enhanced by the correlations between meta-features. In our benchmarking experiment, however, we consider such a huge number of datasets that an investigation of the relationship between methods' performances and datasets' characteristic becomes possible to some extent. As a preliminary, let us illustrate this idea using only one (large) biomedical dataset, the OpenML dataset with ID = 310 including n 0 = 11183 observations and p 0 = 7 features. A total of N = 50 sub-datasets are extracted from this dataset by randomly picking a number n &lt; n 0 of observations or a number p &lt; p 0 of features. Thereby we successively set n to n = 5.10 2 , 10 3 , 5.10 3 , 10 4 and p to p = 1, 2, 3, 4, 5, 6. Figure <ref type="figure">4</ref> displays the boxplots of the accuracy of RF (white) and LR (dark) for varying n (top-left) and varying p (top-right). Each boxplot represents N = 50 data points. It can be seen from Fig. <ref type="figure">4</ref> that the accuracy increases with p for both LR and RF. This reflects the fact that relevant features may be missing from the considered random subsets of p features. Interestingly, it can also be seen that the increase of accuracy with p is more pronounced for RF than for LR. This supports the commonly formulated assumption that RF copes better with large numbers of features. As a consequence, the difference between RF and LR (bottom-right) increases with p from negative values (LR better than RF) to positive values (RF better than LR). In contrast, as n increases the performances of RF and LR increase slightly but quite similarly (yielding a relatively stable difference), whileas expected-their variances decrease; see the left column of Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgroup analyses: meta-features</head><p>To further explore this issue over all 243 investigated datasets, we compute Spearman's correlation coefficient between the difference in accuracy between random forest and logistic regression ( acc) and various datasets' meta-features. The results of Spearman's correlation test are shown in Table <ref type="table" target="#tab_2">3</ref>. These analyses again point to the importance of the number p of features (and related meta-features), while the dataset size n is not significantly correlated with acc. The percentage C max of observations in the majority class, which was identified as influencing the relative performance of RF and LR in a previous study <ref type="bibr" target="#b38">[39]</ref> conducted on a dataset from the field of political science is also not significantly correlated with acc in our study. Note that our results are averaged over a large number of different datasets: they are not incompatible with the existence of an effect in some cases.</p><p>To investigate these dependencies more deeply, we examine the performances of RF and LR within subgroups of datasets defined based on datasets' meta-features (called meta-features from now on), following the principle of subgroup analyses well-known in clinical research. As some of the meta-features displayed in Table <ref type="table" target="#tab_2">3</ref> are mutually (highly) correlated, we cluster them using a hierarchical clustering algorithm (data not shown). From the resulting dendogram we decide to select the meta-features p, n, p n , C max , while other meta-features are considered redundant and ignored in further analyses.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> displays the boxplots of the differences in accuracy for different subgroups based on the four selected meta-features p, n, p n and C max . For each of the four metafeatures, subgroups are defined based on different cut-off values, denoted as t, successively. The histograms of the four meta-features for the 243 datasets are depicted in the It can be observed from Fig. <ref type="figure" target="#fig_2">5</ref> that RF tends to yield better results than LR for a low n, and that the difference decreases with increasing n. In contrast, RF performs comparatively poorly for datasets with p &lt; 5, but better than LR for datasets with p ‚â• 5. This is due to low performances of RF on a high proportion of the datasets with p &lt; 5. For p n , the difference between RF and LR is negligible in low dimension p n &lt; 0.01 , but increases with the dimension. The contrast is particularly striking between the subgroups p n &lt; 0.1 (yielding a small acc) and p n ‚â• 0.1 (yielding a high acc), again confirming the hypothesis that the superiority of RF over LR is more pronounced for larger dimensions.</p><p>Note, however, that all these results should be interpreted with caution, since confounding may be an issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgroup analyses: substantive context</head><p>Furthermore, we conduct additional subgroup analyses focusing on the subgroup of datasets from the field of biosciences/medicine. Out of the 243 datasets considered so far, 67 are related to this field. The modified versions of Figs. <ref type="figure">3</ref> and<ref type="figure" target="#fig_2">5</ref> and Table <ref type="table" target="#tab_1">2</ref> (as well as Fig. <ref type="figure">6</ref> discussed in "Meta-learning" section) obtained based on the subgroup formed by datasets from biosciences/medicine are displayed in Additional file 2. The outperformance of RF over LR is only slightly lower for datasets from biosciences/medicine than for the other datasets: the difference between datasets from biosciences/medicine and datasets from other fields is not significantly different from 0. Note that one may expect bigger differences between specific subfields of biosciences/medicine (depending on the considered prediction task). Such investigations, however, would require subject matter knowledge on each of these tasks. They could be conducted in future studies by experts of the respective tasks; see also the "Discussion" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-learning</head><p>The previous section showed that benchmarking results in subgroups may be considerably different from that of the entire datasets collection. Going one step further, one can extend the analysis of meta-features towards metalearning to gain insight on their influence. More precisely, taking the datasets as observations we build a regression RF that predicts the difference in performance between RF and LR based on the four meta-features considered in the previous subsection p, n, p n and C max . Figure <ref type="figure">6</ref> depicts partial dependence plots for visualization of the influence of each meta-feature. Again, we notice a dependency on p and p n as outlined in "Subgroup analyses: Although these results should be considered with caution, since they are possibly highly dependent on the particular distribution of the meta-features over the 243 datasets and confounding may be an issue, we conclude from "Explaining differences: datasets' meta-features" section that meta-features substantially affect acc. This points out the importance of the definition of clear inclusion criteria for datasets in a benchmark experiment and of the consideration of the meta-features' distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explaining differences: partial dependence plots</head><p>In the previous section we investigated the impact of datasets' meta-features on the results of benchmarking and modeled the difference between methods' performance based on these meta-features. In this section, we Fig. <ref type="figure">6</ref> Plot of the partial dependence for the 4 considered meta-features : log(n), log(p), log p n , C max . The log scale was chosen for 3 of the 4 features to obtain more uniform distribution (see Fig. <ref type="figure" target="#fig_2">5</ref> where the distribution is plotted in log scale). For each plot, the black line denotes the median of the individual partial dependences, and the lower and upper curves of the grey regions represent respectively the 25%-und 75%-quantiles. Estimated mse is 0.00382 via a 5-CV repeated 4 times take a different approach for the explanation of differences. We use partial dependence plots as a technique to assess the dependency pattern between response and features underlying the prediction rule. More precisely, the aim of these additional analyses is to assess whether differences in performances (between LR and RF) are related to differences in partial dependence plots. After getting a global picture for all datasets included in our study, we inspect three interesting "extreme cases" more closely. In a nutshell, we observe no strong correlation between the difference in performances and the difference in partial dependences over the 243 considered datasets. More details are given in Additional file 3: in particular, we see in the third example dataset that, as expected from the theory, RF performs better than LR in the presence of a non-linear dependence pattern between features and response.</p><p>expect too much from tuning RF in general (note, however, that tuning may improve performance in other cases, as indicated by our large-scale benchmark study).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>We presented a large-scale benchmark experiment for comparing the performance of logistic regression and random forest in binary classification settings. The overall results on our collection of 243 datasets showed better accuracy for random forest than for logistic regression for 69.0% of the datasets. On the whole, our results support the increasing use of RF with default parameter values as a standard method-which of course neither means that it performs better on all datasets nor that other parameter values/variants than the default are useless!</p><p>We devoted particular attention to the inclusion criteria applied when selecting datasets for our study. We investigated how the conclusions of our benchmark experiment change in different subgroups of datasets. Our analyses reveal a noticeable influence of the number of features p and the ratio p n . The superiority of RF tends to be more pronounced for increasing p and p n . More generally, our study outlines the importance of inclusion criteria and the necessity to include a large number of datasets in benchmark studies as outlined in previous literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Firstly, as previously discussed <ref type="bibr" target="#b10">[11]</ref>, results of benchmarking experiments should be considered as conditional on the set of included datasets. As demonstrated by our analyses on the influence of inclusion criteria for datasets, different sets of datasets yield different results. While the set of datasets considered in our study has the major advantages of being large and including datasets from various scientific fields, it is not strictly speaking representative of a "population of datasets", hence essentially yielding conditional conclusions.</p><p>Secondly, as all real data studies, our study considers datasets following different unknown distributions. It is not possible to control the various datasets' characteristics that may be relevant with respect to the performance of RF and LR. Simulations fill this gap and often yield some valuable insights into the performance of methods in various settings that a real data study cannot give.</p><p>Thirdly, other aspects of classification methods are important but have not been considered in our study, for example issues related to the transportability of the constructed prediction rules. By transportability, we mean the possibility for interested researchers to apply a prediction rule presented in the literature to their own data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. With respect to transportability, LR is clearly superior to RF, since it is sufficient to know the fitted values of the regression coefficient to apply a LR-based prediction rule. LR also has the major advantage that it yields interpretable prediction rules: it does not only aim at predicting but also at explaining, an important distinction that is extensively discussed elsewhere <ref type="bibr" target="#b0">[1]</ref> and related to the "two cultures" of statistical modelling described by Leo Breiman <ref type="bibr" target="#b40">[41]</ref>. These important aspects are not taken into account in our study, which deliberately focuses on prediction accuracy.</p><p>Fourthly, our main study was intentionally restricted to RF with default values. The superiority of RF may be more pronounced if used together with an appropriate tuning strategy, as suggested by our additional analyses with TRF. Moreover, the version of RF considered in our study has been shown to be (sometimes strongly) biased in variable selection <ref type="bibr" target="#b13">[14]</ref>. More precisely, variables of certain types (e.g., categorical variables with a large number of categories) are systematically preferred by the algorithm for inclusion in the trees irrespectively of their relevance for prediction. Variants of RF addressing this issue <ref type="bibr" target="#b12">[13]</ref> may perform better, at least in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outlook</head><p>In this paper, we mainly focus on RF with default parameters as implemented in the widely used package randomForest and only briefly consider parameter tuning using a tuning procedure implemented in the package tuneRanger as an outlook. The rationale for this choice was to provide evidence for default values and thereby the analysis strategy most researchers currently apply in practice. The development of reliable and practical parameter tuning strategies, however, is crucial and more attention should be devoted in the future. Tuning strategies should be themselves compared in benchmark studies. Beyond the special case of RF, particular attention should be given to the development of user-friendly tools such as tuneRanger <ref type="bibr" target="#b3">[4]</ref>, considering that one of the main reasons for using default values is probably the ease-of-use-an important aspect in the hectic academic context. By presenting the results on the average superiority with default values over LR, we by no means want to definitively establish these default values. Instead, our study is intended as a fundamental first step towards welldesigned studies providing solid well-delimited evidence on the performance.</p><p>Before further studies are performed on tuning strategies, we insist that, whenever performed in applications of RF, parameter tuning should ideally always be reported clearly including all technical details either in the main or in its supplementary materials. Furthermore, the uncertainty regarding the "best tuning strategy" should in no circumstances be exploited for conscious or subconscious "fishing for significance".</p><p>Moreover, our study could also be extended to yield differentiated results for specific prediction tasks, e.g., prediction of disease outcome based on different types of omics data, or prediction of protein structure and function. In the present study, we intentionally considered a broad spectrum of data types to achieve a high number of datasets. Obviously, performance may depend on the particular prediction task, which should be addressed in more focused benchmark studies conducted by experts of the corresponding prediction task with good knowledge of the considered substantive context. However, the more specific the considered prediction task and data type, the more difficult it will be to collect the needed number of datasets to achieve the desired power. In real data studies, there is a trade-off between the homogeneity and the number of available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Our systematic large-scale comparison study performed using 243 real datasets on different prediction tasks shows the good average prediction performance of random forest (compared to logistic regression) even with the standard implementation and default parameters, which are in some respects suboptimal. This study should in our view be seen both as (i) an illustration of the application of principles borrowed from clinical trial methodology to benchmarking in computational sciences-an approach that could be more widely adopted in this field and (ii) a motivation to pursue research (and comparison studies!) on random forests, not only on possibly better variants and parameter choices but also on strategies to improve their transportability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Selection of datasets. Flowchart representing the criteria for selection of the datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Fig. 3 Main results of the benchmark experiment. Boxplots of the performance for the three considered measures on the 243 considered datasets. Top: boxplot of the performance of LR (dark) and RF (white) for each performance measure. Bottom: boxplot of the difference of performances perf = perf RFperf LR</figDesc><graphic coords="8,126.82,94.02,340.36,219.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Subgroup analyses. Top: for each of the four selected meta-features n, p, p/n and C max , boxplots of acc for different thresholds as criteria for dataset selection. Bottom: distribution of the four meta-features (log scale), where the chosen thresholds are displayed as vertical lines. Note that outliers are not shown here for a more convenient visualization. For a corresponding figure including the outliers as well as the results for auc and brier, see Additional file 1</figDesc><graphic coords="10,63.82,94.02,467.08,231.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,65.80,422.07,214.60,241.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Considered meta-features role of the i.i.d. observations used for the t-test. Sample size calculations for the t-test for paired samples can give an indication of the rough number of datasets required to detect a given difference Œ¥ in performances considered as relevant for a given significance level (e.g., Œ± = 0.05) and a given power (e.g., 1 -Œ≤ = 0.8). For large numbers and a two-sided test, the required number of datasets can be approximated as</figDesc><table><row><cell cols="2">Meta-feature Description</cell></row><row><cell>n</cell><cell>Number of observations</cell></row><row><cell>p</cell><cell>Number of features</cell></row><row><cell>p n</cell><cell>Dimensionality</cell></row><row><cell>d</cell><cell>Number of features of the associated design matrix for LR</cell></row><row><cell>d n</cell><cell>Dimensionality of the design matrix</cell></row><row><cell>p numeric</cell><cell>Number of numeric features</cell></row><row><cell>p categorical</cell><cell>Number of categorical features</cell></row><row><cell>p numeric,rate</cell><cell>Proportion of numeric features</cell></row><row><cell>C</cell><cell></cell></row></table><note><p>max Percentage of observation of the majority class time Duration for the run a 5-fold CV with a default Random Forest</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Performances of LR and RF (top: accuracy, middle: AUC, bottom: Brier score): (top: accuracy, middle: AUC, bottom: Brier score): mean performance Œº, standard deviation œÉ and confidence interval for the mean (estimated via the bootstrap BCa method<ref type="bibr" target="#b37">[38]</ref>) on the 243 datasets</figDesc><table><row><cell>Acc</cell><cell>Œº</cell><cell>œÉ</cell><cell>BCa confidence interval</cell></row><row><cell>Logistic regression</cell><cell>0.826</cell><cell>0.135</cell><cell>[0.808, 0.842]</cell></row><row><cell>Random forest</cell><cell>0.854</cell><cell>0.134</cell><cell>[0.837, 0.870]</cell></row><row><cell>Difference</cell><cell>0.029</cell><cell>0.067</cell><cell>[0.021, 0.038]</cell></row><row><cell>Auc</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression</cell><cell>0.826</cell><cell>0.149</cell><cell>[0.807, 0.844]</cell></row><row><cell>Random forest</cell><cell>0.867</cell><cell>0.147</cell><cell>[0.847, 0.884]</cell></row><row><cell>Difference</cell><cell>0.041</cell><cell>0.088</cell><cell>[0.031, 0.054]</cell></row><row><cell>Brier</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression</cell><cell>0.129</cell><cell>0.091</cell><cell>[0.117, 0.140]</cell></row><row><cell>Random forest</cell><cell>0.102</cell><cell>0.080</cell><cell>[0.092, 0.112]</cell></row><row><cell>Difference</cell><cell>-0.0269</cell><cell>0.054</cell><cell>[-0.034, -0.021]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Correlation between acc and dataset's features bottom row of the figure, where the considered cutoff values are materialized as vertical lines. Similar pictures are obtained for the two alternative performance measures auc and brier; See Additional file 1.</figDesc><table><row><cell></cell><cell>Spearman's œÅ</cell><cell>Spearman's œÅ p-value</cell></row><row><cell>n</cell><cell>-0.0338</cell><cell>6.00 ‚Ä¢ 10 -1</cell></row><row><cell>p</cell><cell>0.331</cell><cell>1.32 ‚Ä¢ 10 -7</cell></row><row><cell>p n</cell><cell>0.254</cell><cell>6.39 ‚Ä¢ 10 -5</cell></row><row><cell>d</cell><cell>0.258</cell><cell>4.55 ‚Ä¢ 10 -5</cell></row><row><cell>d n</cell><cell>0.246</cell><cell>1.04 ‚Ä¢ 10 -4</cell></row><row><cell>p numeric</cell><cell>0.254</cell><cell>6.09 ‚Ä¢ 10 -5</cell></row><row><cell>p categorical</cell><cell>-0.076</cell><cell>2.37 ‚Ä¢ 10 -1</cell></row><row><cell>p numeric,rate</cell><cell>0.240</cell><cell>1.54 ‚Ä¢ 10 -4</cell></row><row><cell>C max</cell><cell>0.00735</cell><cell>9.10 ‚Ä¢ 10 -1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Bernd Bischl for valuable comments and Jenny Lee for language corrections.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>See "Availability of data and materials" section.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This project was supported by the Deutsche Forschungsgemeinschaft (DFG), grants BO3139/6-1 and BO3139/2-3 to ALB.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional analysis: tuned RF</head><p>As an outlook, a third method is compared to RF and LR: RF tuned using the package tuneRanger <ref type="bibr" target="#b3">[4]</ref> with all arguments set to the defaults (in particular, tuning is performed by optimizing the Brier score by using the out-of-bag observations). To keep computational time reasonable, in this additional study CV is performed only once (and not repeated 10 times as in the main study), and we focus on the 67 datasets from biosciences/medicine. The results are displayed in Additional file 4 in the same format as the previously described figures.</p><p>Tuned RF (TRF) has a slightly better performance than RF: both acc and auc are on average by 0.01 better for TRF than for RF. Apart from this slight average difference, the performances of RF and TRF appear to be similar with respect to subgroup analyses and partial dependence plots. The most noticeable, but not very surprising result is that improvement through tuning tends to be more pronounced in cases where RF performs poorly (compared to LR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to C-to-U conversion data</head><p>As an illustration, we apply LR, RF and TRF to the C-to-U conversion data previously investigated in relation to random forest in the bioinformatics literature <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref>. In summary, RNA editing is the process whereby RNA is modified from the sequence of the corresponding DNA template <ref type="bibr" target="#b39">[40]</ref>. For instance, cytidine-to-uridine conversion (abbreviated C-to-U conversion) is common in plant mitochondria. Cummings and Myers <ref type="bibr" target="#b39">[40]</ref> suggest to use information from neighboring sequence regions flanking the sites of interest to predict editing status, among others in Arabidopsis thaliana. For each of the 876 complete observations included in the dataset (available at https://static-content.springer.com/esm/art%3A10.1186 %2F1471-2105-5-132/MediaObjects/12859_2004_248_ MOESM1_ESM.txt), the following features are available:</p><p>‚Ä¢ the binary response at the site of interest (edited versus not edited) ‚Ä¢ the 40 nucleotides at positions -20 to 20, relative to the edited site (4 categories: A, C, T, G), whereby we consider only the nucleotides at positions -5 to 5 as candidates in the present study, ‚Ä¢ the codon position cp (4 categories: P0, P1, P2, PX),</p><p>‚Ä¢ the (continuous) estimated folding energy (fe)</p><p>‚Ä¢ the (continuous) difference dfe in estimated folding energy between pre-edited and edited sequences.</p><p>When evaluating LR and RF on this dataset using the same evaluation procedure as for the OpenML datasets, we see that LR and RF perform very similarly for all three considered measures: 0.722 for LR versus 0.729 for RF for the accuracy (acc), 0.792 for LR versus 0.785 for RF for the Area Under the Curve (auc) and 0.185 for LR versus 0.187 for RF for the Brier score. When looking at permutation variable importances (for RF) and p-values of the Wald test (for LR), we see that the 13 candidate features are assessed similarly by both methods. In particular, the two closest neighbor nucleotides are by far the strongest predictors for both methods.</p><p>Using the package 'tuneRanger' (corresponding to method TRF in our benchmark), the results are extremely similar for all three measures (acc: 0.722, auc: 0.7989, brier: 0.184), indicating that, for this dataset, the default values are adequate. Using the package 'glmnet' to fit a ridge logistic regression model (with the penalty parameter chosen by internal cross-validation, as done by default in 'glmnet'), the results are also similar: 0.728 for acc, 0.795 for auc and 0.189 for brier.</p><p>To gain further insight into the impact of specific tuning parameters, we proceed by running RF with its default parameters except for one parameter, which is set to several candidate values successively. The parameters mtry, nodesize and sampsize are considered successively as varying parameter (while the other two are fixed to the default values). More precisely, mtry is set 1, 3, 5, 10 and 13 successively; nodesize is set to 2, 5, 10, 20 successively; and sampsize is set to 0.5n and 0.75n successively. The result is that all three performance measures are remarkably robust to changes of the parameters: all accuracy values are between 0.713 and 0.729, all AUC values are between 0.779 and 0.792, and all Brier score values are between 0.183 and 0.197. Large nodesize values seem to perform slightly better (this is in line with the output of tuneRanger, which selects 17 as the optimal nodesize value), while there is no noticeable trend for mtry and sampsize. In conclusion, the analysis of the C-to-U conversion dataset illustrates that one should not</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional files</head><p>Additional file 1: Additional results of subgroup analyses. Additional file 1 extends Fig. <ref type="figure">5</ref>  Ethics approval and consent to participate Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To explain or to predict?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="289" to="310" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification and regression by randomforest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R News</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tune Random Forest of the &apos;ranger&apos; Package</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><surname>Tuneranger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R package version 0.1</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A plea for neutral comparison studies in computational sciences</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eugster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">61562</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subsampling versus bootstrapping in resampling-based model selection for multivariable regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sauerbrei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="272" to="280" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IPF-LASSO: integrative L1-penalized regression with penalty factors for prediction based on multi-omics data</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1155/2017/7691937</idno>
		<ptr target="https://doi.org/10.1155/2017/7691937" />
	</analytic>
	<monogr>
		<title level="j">Comput Math Models Med</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random forest gini importance favours snps with large minor allele frequency: impact, sources and recommendations</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bermejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief Bioinform</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="292" to="304" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine learning versus statistical modeling</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biom J</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="588" to="593" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making complex prediction rules applicable for readers: Current practice in random forest literature and recommendations</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Busen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hapfelmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrical J</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards evidence-based computational statistics: lessons from clinical research on the role and design of real-data benchmark studies</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hapfelmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med Res Methodol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unbiased recursive partitioning: A conditional inference framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Graph Stat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="651" to="674" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bias in random forest variable importance measures: Illustrations, sources and a solution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>K√∂nig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="493" to="507" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The parameter sensitivity of random forests</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Boutros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">331</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">To tune or not to tune the number of trees in random forest</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">181</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Importance of hyperparameters of machine learning algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><surname>Tunability</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.09596" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hyperparameters and Tuning Strategies for Random Forest</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804.03515" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resampling methods for meta-model validation with recommendations for evolutionary computation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weihs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="275" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing the performance of prediction models: a framework for some traditional and novel measures</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Steyerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Obuchowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pencina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Use of brier score to assess binary predictions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rufibach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Epidemiol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="938" to="939" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013-07-04">2013. 4 July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Arrayexpress-a public repository for microarray gene expression data at the EBI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brazma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sarkans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shojatalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abeygunawardena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kapushesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kemmeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="68" to="71" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OpenML: networked science in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reporting bias when using real data sets to analyze classification performance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ten simple rules for reducing overoptimistic reporting in methodological computational research</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1004191</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on meta-learning</title>
		<author>
			<persName><forename type="first">Giraud-Carrier C</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="193" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selecting a classification function for class prediction with gene expression data</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Novianti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Roes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eijkemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1814" to="1822" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A statistical framework for hypothesis testing in real data comparison studies</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eugster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am Stat</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="212" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schiffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><surname>Mlr</surname></persName>
		</author>
		<ptr target="https://github.com/mlr-org/mlr" />
		<title level="m">Machine Learning in R. 2016. R package version 2.10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring Machine Learning Better, Together</title>
		<author>
			<persName><forename type="first">G</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hofner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bossek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kerschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><surname>Openml</surname></persName>
		</author>
		<ptr target="https://github.com/openml/openml-r" />
	</analytic>
	<monogr>
		<title level="m">R package version 1.0</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">batchtools: Tools for R to work on batch systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Surmann</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00135</idno>
		<ptr target="https://doi.org/10.21105/joss.00135" />
	</analytic>
	<monogr>
		<title level="j">J Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Couronn√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.439090</idno>
		<ptr target="https://doi.org/10.5281/zenodo.439090" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Docker image: Benchmarking random forest: a large-scale experiment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Couronn√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.804427</idno>
		<ptr target="https://doi.org/10.5281/zenodo.804427" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An introduction to docker for reproducible research</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boettiger</surname></persName>
		</author>
		<idno type="DOI">10.1145/2723872.2723882</idno>
		<ptr target="https://doi.org/10.1145/2723872.2723882" />
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper Syst Rev</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="79" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Hinkley</surname></persName>
		</author>
		<title level="m">Bootstrap Methods and Their Application</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparing random forest with logistic regression for predicting class-imbalanced civil war onset data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Muchlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siroky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polit Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="103" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple statistical models predict C-to-U edited sites in plant mitochondrial RNA</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Central</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>BMC Bioinform</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Statistical modeling: The two cultures (with comments and a rejoinder by the author)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="231" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
