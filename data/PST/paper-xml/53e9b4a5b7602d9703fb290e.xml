<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gesture spotting with body-worn inertial sensors to detect user activities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Holger</forename><surname>Junker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Wearable Computing Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Gloriastrasse 35</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Oliver</forename><surname>Amft</surname></persName>
							<email>amft@ife.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Wearable Computing Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Gloriastrasse 35</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Lukowicz</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Embedded Systems Group</orgName>
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstrasse 43</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerhard</forename><surname>Tr√∂ster</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Wearable Computing Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Gloriastrasse 35</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gesture spotting with body-worn inertial sensors to detect user activities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB391DA2428EC3DA3BC82A6DFEAA1188</idno>
					<idno type="DOI">10.1016/j.patcog.2007.11.016</idno>
					<note type="submission">Received 10 January 2007; received in revised form 15 November 2007; accepted 19 November 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural gesture segmentation</term>
					<term>Gesture spotting</term>
					<term>Activity recognition</term>
					<term>Automatic dietary monitoring</term>
					<term>Event detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for spotting sporadically occurring gestures in a continuous data stream from body-worn inertial sensors. Our method is based on a natural partitioning of continuous sensor signals and uses a two-stage approach for the spotting task. In a first stage, signal sections likely to contain specific motion events are preselected using a simple similarity search. Those preselected sections are then further classified in a second stage, exploiting the recognition capabilities of hidden Markov models. Based on two case studies, we discuss implementation details of our approach and show that it is a feasible strategy for the spotting of various types of motion events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monitoring and classification of human activity using simple body-worn sensors is emerging as an important research area in machine learning. Activity monitoring itself is motivated by a variety of mobile and ubiquitous computing applications, such as personalisation of the user interface, behavioural monitoring in medicine, medication assessment, assistive systems for the elderly and cognitively disabled or intelligent information delivery and recording systems for industrial assembly and maintenance.</p><p>The choice of simple sensors, such as accelerometers instead of computer vision, stems from the limited computational resources of mobile and ubiquitous systems and the very diversified, dynamic environment in which such systems need to operate. The latter often implies varying light conditions, changing backgrounds and a large clutter. This makes extracting relevant information from visual signals difficult and computationally intensive. Body-mounted motion sensors, on the other hand, are influenced by user activity only. The problem with activity recognition using such sensors lies less in the extraction of relevant features than in the fact that the information is often ambiguous and incomplete. Thus, once a vision system has managed to track, for example, the user's arm, relatively exact trajectories could be obtained for activity recognition. In contrast, arm worn accelerometers react to a combination of earth gravity and arm speed changes. Gyroscopes describe rotational motions of the arm. However, none of the above provides exact trajectory information.</p><p>Despite the disadvantages listed above, body-worn motion sensors have been successfully used for a variety of tasks (see related work). One area where little progress has been made so far, is the spotting of sporadically occurring activities in a continuous data stream. This is known to be difficult, even if complete trajectory information is available from a vision system. It is even more difficult in a wearable sensor-based environment.</p><p>This paper describes a novel method for tackling this problem based on appropriately adapted machine learning techniques. Focusing on activities associated with distinct arm gestures, the performance of the proposed method is evaluated in two elaborate case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Paper scope and background</head><p>Depending on the specific application, very different types of activity recognition are needed. As an example, consider a system designed to monitor the overall physical activity level of a person. The idea behind such systems is to provide general information about the effect of certain behavioural recommendations or to estimate energy expenditure without having the patient admitted to stationary care or a laboratory for observation. A wearable system deploying appropriate body-worn sensors can be used to collect this data. Obviously, the type of information that such systems need to deliver is not about single, specific actions, but more about the overall level of activity. Often, the activity level can be assessed by averaging parameters, such as mean acceleration of specific body parts. In a way, this is a very simple form of activity recognition.</p><p>On the other side of the spectrum are applications, where reliable recognition on a more fine-grained level is needed. Such applications may include, e.g. the monitoring of specific tasks and/or movements in a rehabilitation scenario, the spotting of specific gestures for novel, more natural human-computer interface or the classification of dietary intake gestures for an automated nutrition monitoring system. Such recognition tasks are particularly difficult, because the relevant activities occur sporadically in between a large variety of other activities. For example, in between the actual activity a user might fetch tools, drink, chat with another person or just scratch the head. As a consequence, the task at hand can be described as activity spotting. It is widely recognised as a particularly complex domain of activity recognition and is still an open problem.</p><p>The work described in this paper is part of a larger effort of our groups, directed at this problem, e.g. Refs. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. It focuses on activities that are associated with a characteristic arm gestures. For such activities, the paper presents a novel gesture spotting method based on arm-worn motion sensors. The method uses a natural partitioning of human motions. In order to achieve a balance between precision and recall with reasonable computational effort, the task is partitioned into a fast highly sensitive stage to pick up potentially interesting signal segments and a more complex, highly selective second stage to narrow down the selection and get rid of false positives.</p><p>Our method is primarily intended as part of a large activity spotting system that uses additional information such as location, modes of locomotion, e.g. sitting standing, walking <ref type="bibr" target="#b3">[4]</ref>, supplementary location sensors <ref type="bibr" target="#b4">[5]</ref> or information on objects involved in the activity <ref type="bibr" target="#b5">[6]</ref>. Nonetheless, we present experiments on activities from two different everyday life domains indicating that even on its own our method achieves reasonable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related work</head><p>In contrast to isolated motion recognition that has been shown in various areas, the spotting task is much more challenging. The difficulty of spotting specific human motion events stems from a number of sources. These include, among others, co-articulation, where consecutive gestures influence each other <ref type="bibr" target="#b6">[7]</ref>, as well as intra-and inter-person variability. Another challenge, the system has to deal with, is the fact that the motion events to be spotted may only occur sporadically, in a continuous data stream, while at the same time being embedded into other, partly arbitrary movements (called zeroclass). These movements, however, are inherently difficult to model, due to their complexity and unpredictability. As a consequence, conventional recognition schemes for continuous classification, such as hidden Markov models (HMMs), are not directly applicable for our recognition task, since they rely on appropriate zero-class models. Consequently, we cannot take advantage of the implicit data segmentation capabilities that HMMs provide. Moreover, we have to deal with the fact that motion events are typically very short. This means that for any explicit segmentation-based recognition, exact localisation of event boundaries is important.</p><p>The recognition of gestures has been studied extensively over years and many approaches have been proposed to tackle the diverse problems. In general, these approaches can be broadly categorised in either of the two following categories: gesture recognition, requiring external infrastructure and gesture recognition, focusing on wearable instrumentation.</p><p>The first category is dominated by vision-based motion recognition, using a single or multiple cameras. While an exhaustive review of literature is beyond the scope of this work, we exemplary indicate related works. Starner <ref type="bibr" target="#b7">[8]</ref> proposed an approach for American Sign Language recognition, Campbell and Bobick <ref type="bibr" target="#b8">[9]</ref> developed a system for recognising classical ballet steps, Yamato et al. <ref type="bibr" target="#b9">[10]</ref> worked on the recognition of different tennis strokes, Brand et al. <ref type="bibr" target="#b10">[11]</ref> targeted T'ai Chi movements, Lee and Kim <ref type="bibr" target="#b11">[12]</ref> dealt with typical gestures for interacting with a computer and Rao and Shah <ref type="bibr" target="#b12">[13]</ref> aimed at manipulative gestures. Further literature on vision-based motion capture and recognition can be found in Refs. <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>More recently the use of wearable instrumentation for gesture recognition has gained much attention mainly due to the success in sensor miniaturisation. Various approaches dealing with the recognition of activities or events have been presented. Chambers et al. <ref type="bibr" target="#b16">[17]</ref> targeted Kung Fu moves and Benbasat <ref type="bibr" target="#b17">[18]</ref> focused on the recognition of "atomic" gestures. Kern et al. <ref type="bibr" target="#b18">[19]</ref> looked at activities, such as keyboard typing, writing on a white-board and shaking hands. Cakmakci et al. <ref type="bibr" target="#b19">[20]</ref> tried to identify when a person was looking at the watch. Bao <ref type="bibr" target="#b20">[21]</ref> aimed at typical household activities including vacuuming, folding laundry, watching TV or brushing teeth. Lukowicz et al. <ref type="bibr" target="#b21">[22]</ref> concentrated on workshop activities including sawing, hammering, drilling and filing. Brashear et al. <ref type="bibr" target="#b22">[23]</ref> dealt with gestures for American Sign Language and Lementec and Bajcsy <ref type="bibr" target="#b23">[24]</ref> worked on the recognition of gestures used to instruct pilots after landing.</p><p>Although many motion recognition approaches exist, few are dealing with the spotting task itself. Deng and Tsui <ref type="bibr" target="#b24">[25]</ref> proposed a method for spotting gestures in continuous data. Their approach makes use of an HMM-based accumulation score that supports endpoint detection of a particular gesture in a continuous data stream. Based on a potential endpoint their algorithm searches for a corresponding start point using the viterbi algorithm. While this approach seems promising, it has been evaluated solely for the recognition of two-dimensional trajectories (Arabic numbers). Lee and Kim <ref type="bibr" target="#b11">[12]</ref> developed a method deploying HMMs directly, to spot gestures in a continuous stream of sensor data. They introduced the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures and is constructed from all existing gesture models. Lukowicz et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated continuous, online motion recognition by partitioning the incoming data using an intensity analysis based on the signals of two microphones exploiting the fact that the movements to be recognised are accompanied with a particular sound.</p><p>While the first two approaches made use of the implicit segmentation capabilities of HMMs, the third approach used an explicit segmentation step to facilitate spotting. We believe that explicit gesture segmentation can be very helpful and efficient to facilitate the spotting task. Lee and Yangsheng <ref type="bibr" target="#b25">[26]</ref> developed a system for online gesture recognition using HMMs. They were among the first researchers to use segmentation as a pre-processing step to gesture recognition and were able to recognise 14 different gestures online. While they proposed acceleration thresholds for segmentation, they used a simple velocity-based segmentation relying on the fact that there must be short pauses between two consecutive gestures. They successfully demonstrated good recognition performance for the trained gestures; however, they did not deal with the rejection of non-relevant movements. Kahol et al. <ref type="bibr" target="#b26">[27]</ref> proposed a gesture segmentation algorithm which employs a hierarchical layered structure to represent the human anatomy. The algorithm used low-level motion parameters to characterise motion in the various layers of this hierarchy and was able to predict segmentation boundaries based on profiles, generated from segmentation results. The segmentation, in turn, was provided by observers, who manually segmented training data. In a recent work, Kahol et al. <ref type="bibr" target="#b27">[28]</ref> used the concept to fully document every motion in dance activities using a Vicon camera system. Wang et al. <ref type="bibr" target="#b28">[29]</ref> presented an approach for automatically segmenting sequences of natural activities into atomic sections and clustering them. The segmentation was based on finding the local minimum of velocity and local maximum of change in direction. The minimum below and the maximum above the certain threshold were selected as segment points. The limitation of their approach is that it can only segment and label continuous human gestures, but not spot them. Liang and Ouhyoung <ref type="bibr" target="#b29">[30]</ref> used a temporal segmentation based on the discontinuity of the movements according to four gesture parameters and HMMs to perform realtime continuous gesture recognition of sign language. Their approach allows the recognition of gestures that were defined in vocabularies only; thus rejection of non-gesture patterns is not considered. Morguet <ref type="bibr" target="#b30">[31]</ref> proposed a two-step approach to the continuous recognition of gestures in video sequences. In a first step, a simple segmentation algorithm was used to identify start and endpoints of potentially meaningful segments. This segmentation process used a threshold on a specific motion parameter in conjunction with simple rules to obtain valid segments. These segments were then classified in isolation. However, this approach cannot reject non-gesture patterns that are falsely retrieved in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Paper contributions and organisation</head><p>As stated in the introduction, the work presented in this paper is part of a large effort towards reliable spotting of complex activities using simple on-body sensors. It focuses on the recognition of gestures that build the basis for the inference of more abstract activities. The primary aim is to support complex activity spotting systems rather than to develop an activity spotting system based solely on arm gestures. Nonetheless, we show how, for suitable domains, good performance can be achieved without any additional information.</p><p>Within this scope the paper makes the following contributions:</p><p>(1) It presents a novel, two-stage gesture spotting method based on body-worn motion sensors. The method is specifically designed towards the needs and constraints of activity recognition in wearable and pervasive systems. This includes a large null class, lack of appropriate models for the null class, large variability in the way gestures are performed and a variable gesture length. It also refrains from excessively computationally intensive operations such as correlations over large data sets or complex searches. Instead, it uses a natural partitioning of human motions, combined with a simple parametrisation scheme as a computationally cheap preselection stage (PS) that identifies potentially interesting data sections. These sections are then reevaluated using HMMs to reduce the number of classification errors. This combination of a cheap, highly sensitive initial stage with a highly selective second stage is what makes our approach unique and well suited to the intended domain.</p><p>(2) The paper describes the verification of the proposed method on two scenarios that together comprise nearly a thousand relevant gestures. The first one, interaction with different everyday objects, is part of a wide range of wearable systems applications. The second one, nutrition intake, is a highly specialised application motivated by the needs of a large industry dominated health monitoring project. In both cases studies we arrive at recall values between 80% and 90% and a precision of over 70%. The significance of these case studies and results are twofold. First, they confirm the soundness of our approach. Second they are a strong indication for the feasibility of reliable activity spotting using wearable sensors, in particular, since the approach presented in this paper is meant to be used as part of a large system that uses other information to further improve the results.</p><p>As indicated in the related work section, two-stage activity spotting approaches have been tried before. However, to our knowledge, the specific approach described in this paper, with its focus on the peculiarities of activity spotting using simple sensors and wearable systems is novel. Taking into account the results achieved in our case studies it represents a significant contribution to the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1.">Paper organisation</head><p>In Section 2 of the paper, we introduce our two-stage spotting approach. In Section 3, we describe the case studies used to validate our approach. In Section 4, we focus on implementation details of the spotting algorithm and in Section 5, we detail the experimental setup to acquire sensor data for the case studies. In Section 6, we finally present our evaluation results. In Sections 7 and 8, we discuss the results and highlight future work, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spotting approach</head><p>Our two-stage spotting approach consists of a PS stage (first stage) and a classification stage (CS) (second stage) as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The task of the PS stage is to localise and preselect sections in the continuous signal stream, likely to contain relevant motion events. These candidate sections are then passed on to the CS and are classified in isolation using appropriate classifiers.</p><p>The preselection of sections in a continuous signal stream can be considered as a search problem. In a naive approach, the search may be performed on all possible sections in the data stream. The major problem of this exhaustive approach is its computational effort. To reliably capture human motions with inertial sensors, the sensors are usually sampled with up to 100 Hz. Considering that a relevant motion event may take several seconds, the above-mentioned search strategy would require to check a large number of sections.</p><p>Obviously, one solution to reduce the complexity is to apply a coarse search, where not all but only certain sections in the continuous signal stream are considered for the search. One way to implement such a coarse search is to partition the signal stream into segments which are significantly longer than a single sampling interval and to consider the segment boundaries as possible start/endpoints of the sections to be searched. However, an artificial partitioning is likely to miss the exact boundaries of the relevant motion events contained in the data stream. This makes the recognition more complicated, since sections may contain only parts of the relevant motion event as well as other motions.</p><p>We propose to use a natural partitioning of the data into "motion segments". Inspired by the taxonomy of Bobick <ref type="bibr" target="#b31">[32]</ref> these motion segments are described as non-overlapping, atomic units of human movement, characterised by their spatio-temporal trajectories. Assuming that a motion event can be subdivided into a sequence of motion segments, we can obtain a natural, nonambiguous partitioning of the overall motion with the start and end of the motion events corresponding to the start/end of a specific motion segment. Thus, the search can be constrained to those sections, whose boundaries coincide with the boundaries  For the partitioning task, motion parameter(s) were used that represent the motion event closely. The number and types of motion parameters to be used is specific to the motion event to be recognised. For arm-related motion events, they include, e.g. relative orientation information, such as joint angles between the lower and the upper arm, absolute orientation information of the arm segments to an earth-fixed reference frame or simply the raw signals from the sensors attached to the arm segments.</p><p>While the PS stage identifies potential candidate sections, the classification stage is used to eliminate those sections that have been falsely retrieved in the PS stage. This is achieved by individually classifying the candidate sections using HMMs and comparing the classification result to the result of the PS stage.</p><p>The main motivation behind this two-stage approach is to reduce the complexity of the spotting task, by constraining the search space within the continuous data stream and by applying a simple similarity analysis to preselect potential sections. The subsequent CS is used to make the recognition more robust and retain only relevant sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Case studies</head><p>In order to discuss the implementation of our approach, we considered the spotting of typical, everyday life gestures in a continuous data stream from body-worn inertial sensors. Specifically, we investigated two different case studies.</p><p>Case study 1 deals with the spotting of diverse object interaction gestures, reflecting common activities of daily living. The detection of such gestures is considered as key component in a context recognition system, to monitor complex human activities. Furthermore, such gestures may facilitate more natural human-computer interfaces.</p><p>Case study 2 focuses on dietary intake gestures. The spotting of body motions related to food intake is expected to become one sensing domain of an automated dietary monitoring system <ref type="bibr" target="#b32">[33]</ref>. Although the automatic determination of exact type and amount of all foods is rather visionary, we believe that an assistive system based on different sensors is conceivable. Hence, the gestures included in this study refer to frequently used human feeding motions. Detecting such gestures reveals information about the timing of nutrition events, e.g. breakfast or lunch and on the category of the food item, e.g. a soup is fed with a spoon, a glass, cup or bottle is usually used for drinking.</p><p>Figs. 2 and 3 illustrate the gestures that we aimed to recognise (relevant gestures) in each case study (see Table <ref type="table" target="#tab_1">2</ref> for a brief description). All relevant gestures are characterised by distinctive movements of the left or right arm. While in case study 1 only movements from the right arm and trunk were used to detect the gestures; case study 2 uses information from both arms as well as from the trunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spotting implementation</head><p>The implementation of our two-stage spotting approach is detailed in this section. The first stage preselects candidate sections and the second stage refines the preselection (see Fig. <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preselection stage</head><p>This section details the segmentation scheme used for the initial partitioning of the continuous signal stream into motion segments, the search strategy and similarity measure applied to identify potential sections and, finally, the selection of candidate sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Motion segment partitioning</head><p>The task of the segmentation algorithm is to partition a motion parameter into non-overlapping, meaningful segments. This task can be considered as a time-series segmentation problem, which has been extensively studied in many application domains. An excellent review of time-series segmentation approaches was provided by Keogh et al. <ref type="bibr" target="#b33">[34]</ref>.</p><p>As motion parameter, we used the pitch and the roll of the lower arm, which are the angle of the lower arm segment to the horizontal plane and the rotation angle with the rotation axis along the limb of the lower arm (see Fig. <ref type="figure">5</ref>).</p><p>These angles have been chosen mainly for the following reasons: Many movements of the entire arm typically involve movements of the lower arm as well. Furthermore, the signals of the lower arm orientation (and in particular pitch and roll) correlated well with our visual perception of the gestures. Despite good initial results by using the pitch in case study 1, the roll was additionally investigated in case study 2. For certain gestures the segmentation based on the roll matched the gesture boundaries better. This can be explained by the typical feeding motion (moving the hand with a tool to the mouth) involved in the gestures of case study 2.</p><p>Although relative orientation information between the lower arm and the upper arm segment, such as joint angles, would generally be well suited for the partitioning of the signal streams, we found that the estimation of those angles using inertial sensors attached to the arm segments can be prone to large errors. The two major sources of errors were inaccurate orientation estimation of the involved sensors (mainly due to magnetic disturbances) and the loose attachment of the sensors to the arm segments. Attachment issues make the sensors susceptible to displacement while moving the arm. Conversely the pitch and roll of the lower arm could be derived very robustly. The estimation of these angles from raw sensor data was less prone to magnetic disturbances than other orientation angles, specifically the orientation in the horizontal plane.</p><p>For the segmentation task, we used the sliding-window and bottom-up algorithm (SWAB) introduced by Keogh et al. <ref type="bibr" target="#b33">[34]</ref>. Based on the evaluation of typical test data, we found the algorithm to be well suited for our application. SWAB combines the advantages of a precise bottom-up segmentation scheme with those of a sliding-window algorithm. This allows the algorithm to be used online while keeping a global view on the data.</p><p>The algorithm kept a small buffer of the signal data. A bottom-up segmentation was applied to the data in the buffer. From the resulting signal segmentation, the segment with the oldest data was extracted from the buffer and new data were added using the sliding-window approach. This procedure   was repeated as long as new data were available, potentially forever.</p><p>The bottom-up partitioning of each buffer of length n started from the arbitrary segmentation of the signal into n/2 segments. Next, the cost of merging each pair of adjacent segments was calculated and the lowest cost pair in the buffer was iteratively merged. As the algorithm iterates, more signal segments were merged until all adjacent segments in the buffer exceeded a cost threshold when merged. Figs. <ref type="figure">6(a</ref>) and (b) depicts the segmentation process of the buffered signal for different segmentation steps (iterations). At iteration 0 the fine-grained initial partitioning can be seen. The final state is depicted in Fig. <ref type="figure">6(b)</ref>. The sliding-window algorithm of SWAB reported the left-most segment from the bottom-up buffer and added new data accordingly. The procedure was restarted with this new data in the bottom-up buffer.</p><p>The cost metric for merging two segments was based on the error of approximating the signal with its linear regression (residuals) in the bounds defined by the merged segment. This method can be explained as follows: When the pair of segments differ strongly in its signal shape, the approximation of the merged segments incurs large residuals. Hence it is less likely that the segments belong to the same motion segment. We used the squared sum of the residuals in the bounds of the merged segment as cost function.</p><p>To ensure that the algorithm provided a good approximation of the signal, a small cost threshold was required, typically leading to a large number of segments for any of the relevant gestures. These segments did not correspond well to the small number of visually perceived sub-movements of the gesture. As a solution to this problem, we merged adjacent segments, as created by the SWAB algorithm, if their linear regressions had similar slopes. As a result of this extension we obtained motion segment boundaries. Fig. <ref type="figure">7</ref>   a Pitch, roll and yaw are Euler angles representing rotations of an object in three-dimensional Euclidean space. The orientation of pitch and roll angles are described in Section 4.1 and Fig. <ref type="figure">5</ref>. The yaw angle corresponds to absolute orientation in the horizontal plane. segmentation steps, based on the "DK" gesture that uses the pitch angle as segmentation signal.</p><p>For each gesture an individual segmentation parameter could be chosen. Person-specific training was used to accommodate for the dominant body side. In the investigated case studies, the body side was fixed. Table <ref type="table" target="#tab_3">3</ref> summarises the final choices made in our implementation.</p><p>The mean number of segmentation points per gesture for the data sets of both case studies are shown in Table <ref type="table" target="#tab_4">4</ref>. The ratio The SWAB segmentation points correspond to the total number of segmentation points for the entire data sets. The ratio of segmentation points by total recorded samples indicates the reduction in search effort achieved by the preselection stage.</p><p>of segmentation points to the total recorded samples indicates the achieved reduction in search effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Section similarity search</head><p>A coarse search based on the motion segment boundaries was used to find sections thatt contain relevant gestures. The search was performed by considering each motion segment endpoint as potential end of a gesture. For each endpoint, potential start points were derived from preceding motion segment boundaries. The search was performed for each gesture separately. To confine the search space, we introduced two constraints on the sections to be searched. These constraints were adapted to the gesture by training data:</p><p>1. For the actual length T of the section we constrained T min T T max , where T min and T max denote the minimum and maximum length of the section to be considered. 2. For the number of motion segments n MS in the actual section, we selected N MS,min n MS N MS,max , where N MS,min and N MS,max , correspond to the minimum and maximum number of motion segments to be contained in the section, respectively.</p><p>As search criterion, we used the normalised Euclidean distance <ref type="foot" target="#foot_0">1</ref> given in Eq. ( <ref type="formula" target="#formula_0">1</ref>), where f P S denotes the N Fdimensional feature vector of the PS, derived from the section under consideration.</p><p>We used simple single-value features, such as minimum and maximum signal values of the lower and upper arm pitch and roll, sum of signal samples, the duration of the gesture and the number of motion segments in the section under consideration.</p><p>In case study 2, the minimal distance between the hand and estimated head position was additionally used.</p><p>The parameters ik and ik represent the mean and the standard deviation of the ith element of the feature vector of gesture G k . These were computed from training data:</p><formula xml:id="formula_0">d(f P S ; G k ) = N F i=1 f P S i -ik ik 2 , f P S = [f P S 1 , . . . , f P S N F ].<label>(1)</label></formula><p>The normalised Euclidean distance provided a measure of how similar the motion pattern given in the section were to a specific gesture. During the evaluation of all possible start points for one endpoint, only the section with the minimal distance as retained.</p><p>If the distance d(f P S , G k ) was smaller than a gesture-specific threshold value d min (G k ), the section under investigation was considered to contain gesture G k . If the condition was satisfied for more than one gesture, the section was considered to contain either one of the corresponding gestures. Depending on the application such collisions need to be checked and handled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Selection of candidate sections</head><p>Fig. <ref type="figure">8</ref> schematically shows the collision of two sections obtained by the section search procedure. These overlapping candidate sections were resolved by selecting sections with the smallest similarity values for every occurring collision. In this way non-overlapping candidate sections were obtained for a particular gesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification stage</head><p>In the CS, we used HMMs, which have long been used in speech recognition, due to their ability to cope with temporal and spatial variations of input patterns <ref type="bibr" target="#b34">[35]</ref>.</p><p>For our evaluation, we considered left-right models with eight continuous features. The features used for the classification differ from the features used in the PS. While in the PS, data sections were characterised by single-valued features, such as the minimum and maximum signal value and the duration of the section, the HMMs were fed with time-series features derived from the candidate sections. Moreover, a separate definition of the feature set was useful to address the classification goal.</p><p>The following features were used for the HMM-based CS:</p><p>‚Ä¢ Pitch and roll angles from the lower arm sensors.</p><p>‚Ä¢ Pitch and roll angles from the upper arm sensors.</p><p>‚Ä¢ Derivative of the acceleration signal from the lower arm sensor, with the measurement orientation along the pitch angle measurement.</p><p>‚Ä¢ The cumulative sum of the acceleration from the lower arm (orientation as before). ‚Ä¢ Derivative of the rate of turn signal from the lower arm sensor, with the measurement orientation along the roll angle measurement.</p><p>‚Ä¢ The cumulative sum of the rate of turn from the lower arm (orientation as before).</p><p>We found that all gestures could be modelled using single Gaussian models. Our gesture models consisted of 4-10 states. The choice of the states for each gesture model reflects a tradeoff between the complexity of the gesture on the one hand, and available training data, which is necessary to estimate the model parameters properly, on the other. Although some gestures may require more states, we achieved good recognition results with our models, as shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>For the experimental evaluation of our approach, we recorded a variety of different data sets using a commercially available measurement system<ref type="foot" target="#foot_1">2</ref> with five inertial sensors placed on the body (see Fig. <ref type="figure" target="#fig_5">9</ref>). Sensors were attached to the wrists, upper arms and on the upper torso.</p><p>Using this setup, we independently recorded continuous data sets from one female and three male right-handed subjects, aged 25-35 years in both case studies. In case study 2 food intake was recorded in two sessions on different days. The subject data sets (S1.1-S1.4 for case study 1 and S2.1-S2.4 for case study 2) were used for testing of our spotting approach. Additional person-specific data were used for training purposes. The purpose of the studies was explained to the subjects. However, the subjects were asked to perform the movements as natural as possible while wearing the sensors.</p><p>In order to obtain data sets with a realistic zero-class, we did not set constraints to the movements of the subjects, except that we asked the subject to perform the relevant gestures according to the descriptions given in Table <ref type="table" target="#tab_1">2</ref>. Moreover, to enrich the diversity of movements and to avoid wide intervals constituting  no motion, we defined eight additional gestures to be carried out during the recording which were similar to those gestures we intended to spot. In total 2 h of motion data were recorded for case study 1, and 4.7 h for case study 2, with only 25.4% and 34.7% of the data sets containing relevant gestures for case studies 1 and 2, respectively (see Table <ref type="table" target="#tab_6">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>For the evaluation of our approach, the evaluation metrics Precision and Recall were used. These metrics were derived as follows:</p><formula xml:id="formula_1">Recall =</formula><p>Recognised gestures Relevant gestures , P recision = Recognised gestures Retrieved gestures .</p><p>Relevant gestures are those gestures that have been conducted by the subject, while retrieved gestures represent the sections that have been reported in either PS or CS. A recognised gesture is a relevant gesture that has been retrieved. Furthermore, we derived the number of insertions (sections that have been retrieved but do not contain a relevant gesture), and the number of deletions (relevant gestures that have not been reported). Fig. <ref type="figure" target="#fig_3">10</ref> illustrates the different evaluation metrics schematically. Set A corresponds to the relevant gestures, set B to the retrieved gestures after the PS and set C to gestures retained after the CS. The depicted subsets (1-5) reflect the metrics used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Preselection stage</head><p>For the spotting of sections likely to contain motion events, appropriate threshold values d min (G k ) were identified for each gesture G k , by evaluating the performance of the PS on the training data. In general, we observed that the larger the threshold value, the more relevant gestures were retrieved. However, at the same time, the total number of falsely retrieved gestures increased. The precision-recall curves given in Fig. <ref type="figure" target="#fig_3">11</ref> for the gesture "HS" from case study 1 and Fig. <ref type="figure" target="#fig_3">12</ref> for the gesture "SP" from case study 2 illustrate this trade-off for the test data sets (S1.1-S2.4), respectively. Moreover, the individual curves in figures indicate the variation of the detection performance among the subjects.</p><p>The vertical lines towards the maximum recall in Fig. <ref type="figure" target="#fig_3">12</ref> can be seen as limitation of the similarity search. For these gestures, some instances were not successfully detected due to variation between training and testing gestures.</p><p>Based on such precision-recall curves derived from training data, appropriate threshold values can be chosen considering application-specific requirements. For further evaluation of our approach, we set the thresholds for the individual gestures such that at least 90% of the relevant gestures (gestures that have been conducted) were retrieved in case study 1 and 70% of the relevant in case study 2. This corresponds to a recall value of 0.90 and 0.70, respectively.</p><p>Table <ref type="table" target="#tab_7">6</ref> finally summarises the results of the PS stage for both case studies. For an overall recall value larger than 0.90, we obtained an overall precision value of 0.47 in case study 1. In case study 2, with a recall larger than 0.70, precision dropped to 0.57. The low precision indicated many falsely retrieved sections that did not contain a relevant gesture (insertions). As can be seen, the spotting of simple gestures such as "HS" and "HH" tend to cause more insertions (smaller precision values) than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Classification stage</head><p>We used HMMs to refine the spotting results from the PS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Model training and initial testing</head><p>To accommodate for varying quality in the training process that is due to random initialisation of certain HMM parameters, we trained 10 instantiations of each model and kept the one with the highest score.</p><p>For initial model validation, isolated recognition was performed on the test data based on manually added labelling information. From 258 gestures in case study 1, 254 were classified correctly, leading to a recognition rate of 98.4%. For case study 2, a recognition rate of 97.4% was reached from 784 gestures. The results indicate that the models represented the gestures well and were able to recognise the different gestures in the test set accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Classification of candidate sections</head><p>The trained models were used to classify the candidate sections that have been retrieved in the PS. Only those sections were retained, for which the recognition of PS and CS agreed. Table <ref type="table">7</ref> presents the final results of this stage for both case studies and all subjects.</p><p>The CS correctly recognised most of the relevant gestures that have been retrieved in the PS (the average recall value was slightly reduced from 0.96 to 0.93 for case study 1 and from 0.80 to 0.79 for case study 2). The CS discarded many sections that have been falsely retrieved, leading to much higher precision values, especially in case of the "HH", "HS" and "LB" gestures. Finally, Fig. <ref type="figure" target="#fig_3">13</ref> depicts the summarised spotting results for all gestures of the case studies 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Extensions of the CS</head><p>Several options exist in which our spotting approach can be extended. One possibility is to include a zero-class model in the CS. The modelling of the zero-class is a challenging and yet unsolved problem. We evaluated the use of two different zeroclass models as extension of the CS. These extensions propose no viable elements of our spotting approach, but rather indicate directions of further research. The preliminary results of this investigation are shown in this section.</p><p>In case study 1 we evaluated the performance of a zeroclass model that is extracted from all relevant gesture models, following the approach presented by Lee and Kim <ref type="bibr" target="#b11">[12]</ref>. This modified CS yields a total recall performance of 0.81 (without threshold model: 0.93) and a total precision of 0.82 (without threshold model: 0.74). In direct comparison to the classification without the threshold model, a further increase of the precision was achieved, however, at the cost of decreased recall. Fig. <ref type="figure" target="#fig_3">13</ref> shows the results graphically.</p><p>In case study 2, we evaluated the spotting performance using a zero-class model that was constructed on the basis of additional gestures that were carried out by the subjects. An equal number of the gestures was used to build one additional HMM. This garbage model was included in the CS. The modified CS yielded a total recall performance of 0.78 (without garbage model: 0.79) and a total precision of 0.77 (without garbage model: 0.73). Compared to the results of the classification without the garbage model this indicates an improvement of precision at almost constant recall.</p><p>Both concepts indicate that classification improvements with zero-class models can be achieved; however, further work in these area is needed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>HMMs have proven to be applicable for recognition tasks in a variety of application domains, including gesture classification from inertial body-worn sensors. However, the spotting of gestures in a continuous data stream with HMMs is problematic due to their complexity and requirement for a zero-class. The similarity-based search in the PS of our approach presents an elegant way to avoid the explicit modelling of a zero-class. In the HMM-based classification we exploit the competition of all trained models to select the most probable one. This requires that more than one gesture needs to be included in the CS, which can be seen as a limitation of our approach. However, for most applications, the spotting of several different motion events is aimed. Moreover, an explicit zero-class model can be added, when available, to improve the recognition. Initial results for two different zero-class extensions have been presented in this work.</p><p>The similarity-based search procedure used in the PS permits different feature sets for individual gestures. Thus, the search can be tailored to the individual characteristic of a gesture. For example, consider game control gestures, as in Ref. <ref type="bibr" target="#b35">[36]</ref>, that are conducted in the horizontal or vertical plane only. Such gestures could be described more precisely by specific feature sets. This is an advantage over many established classification procedures, such as k-nearest neighbour classifiers or HMMs, which use the same features for all gestures to be recognised.</p><p>The section similarity search can be regarded as a natural extension of the frequently used sliding-window approach for motion and activity detection, as, e.g. in Ref. <ref type="bibr" target="#b1">[2]</ref>. We introduced a size-variable search window to accommodate for the variability in the length of gestures and used a dynamic step size given by the segmentation points. While the trivial sliding window was mainly used for the detection of repetitive motions, such as hammering, the approach presented in this work was successfully evaluated for non-cyclic motion events in the two case studies.</p><p>The problem of human gesture recognition depends largely on the application domain: In contrast to artificial gestures used for human-computer control or repetitive motions in very specific activities, natural motions in activities of daily living are more challenging to spot. This is due to the fact that control gestures can be constructed to provide strong discrimination, that is typically not the case for gestures being part of activities of daily living. Hence, such gestures contain more intra-and inter-person variability, making the spotting task more challenging. However, the presented results indicate that our spotting procedure performs well for these types of gestures.</p><p>In a related work of the authors, one-hand gestures, specifically constructed for game control, were investigated <ref type="bibr" target="#b35">[36]</ref>. The approach in that work differed from the current work: firstly, raw inertial sensor signals were used from a sensor attached to a glove at the hand and secondly, the gestures were designed to aim at discrimination and detection in a gaming scenario. In contrast, the current work aimed at recognising natural everyday life gestures with large fluctuations in length and execution from using sensor data from the lower and upper arm. Consequently, with the use of HMMs, a more complex approach was deployed in the current work to achieve the recognition.</p><p>The focus of the current work was to analyse the recognition performance using person-specific training. The case studies were designed to incorporate additional motions and gestures and maintain a low share of relevant gestures: 25.4% in case study 1 and 34.7% in case study 2. Both case studies evaluated four subjects each. An initial insight into the subject-specific variability was obtained from reviewing the precision-recall curves. However, a larger number of users should be evaluated in future works to study the fluctuation in recognition performance and investigate non-personalised detection models in more depth.</p><p>The temporal phases of a gestures are onset, core and conclusion. Typically, onset and conclusion are variable transfer states between consecutive gestures. However, the core part is specific for a gesture. In the evaluated case studies most of the gestures were acquired with a defined start and ending position, but all contained a core part. For example, in the phone pick up gesture, the user's hand moved towards the receiver, picked it up and moved the receiver to the ear. While the movement may commence with the hand at an arbitrary position, the core is preserved in order to successfully complete the activity. The motion segments in the core phase and during the transitions involve direction changes in the segmentation signal. In our approach, segmentation points were created at these positions. Based on the preselection feature set, the section similarity search was used to test for gestures cores at every segmentation point. Hence, we expect that by using the segmentation and search procedure, gestures embedded in arbitrary transitions can be detected.</p><p>Looking at the individual results of case studies 1 and 2, we observe lower spotting performance for those gestures included in case study 2. We assume that this is due to higher intra-person variability of those gestures. More specifically, we observed the following additional challenges for the spotting of gestures: (1) differences in the size and consistency of food pieces, (2) additional degrees of freedom produced by the tools used for the food intake and (3) temporal aspects, such as the temperature change of the food and the natural satiety of the subject developed during the intake session. To overcome potential weaknesses in the spotting of gestures related to food intake, we argue that the recognition of such gestures can be enhanced by combining different sensing modalities to develop a dietary monitoring system <ref type="bibr" target="#b36">[37]</ref>.</p><p>We expect that the presented spotting approach can be applied to other types of motion events. At the implementation level an appropriate motion parameter must be selected. This motion parameter shall describe the major properties of the motion event and lead to a reproducible and distinctive motion segmentation. We believe that this can be achieved for many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and outlook</head><p>We conclude that our spotting scheme based on the concept of motion segments is a feasible strategy for the identification of motion events in a continuous signal stream. We demonstrated that our approach works well for arm-based motions that are particularly difficult to recognise due to the inherent complexity of arm motions. Moreover, we have shown that our approach simplifies the rejection of non-relevant gestures. We argue that our method is likely to facilitate a wide range of real-life applications of context and activity recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sensor data flow through the two-stage recognition framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualisation of the relevant gestures (acted) as performed in case study 1: (a) Handshake, (b) light button, (c) door, (d) phone and (e) coin.</figDesc><graphic coords="5,112.54,80.31,360.06,463.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualisation of the relevant gestures (acted) as performed in case study 2: (a) cutlery, (b) drink, (c) spoon and (d) handheld.</figDesc><graphic coords="6,128.33,71.10,360.06,454.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Case study 1</head><label>1</label><figDesc>Light button (LB) Press light button to turn lights on Handshake (HS) Greet person by shaking hands Phone up (PU) Pick up receiver. Start position: arm resting on leg, end position: hold receiver to ear Phone down (PD) Put down telephone receiver. End position: arm resting on leg Door (DR) Turn door knob and open door of cabinet Coin (CN) Take out purse from right back pocket of trousers-open purse with right hand-take coin and insert it into slot of vending machine-close purse with right hand-put purse back into pocket</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Fig. 5. Orientation angle "pitch" and "roll" of the lower arm segment.</figDesc><graphic coords="7,344.04,491.32,73.14,137.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Sensor placement for gesture recording.</figDesc><graphic coords="10,217.79,71.44,55.08,166.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 Fig. 10 .Fig. 11 .Fig. 12 .</head><label>5101112</label><figDesc>Fig. 10. Visualisation of the applied evaluation metrics for the preselection (PS) and classification (CS) stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Applied terminology of human motion in this work</figDesc><table><row><cell>Term</cell><cell>Description</cell></row><row><cell>Motion segment</cell><cell>Represents atomic, non-overlapping unit of hu-</cell></row><row><cell></cell><cell>man motion that can be characterised by their</cell></row><row><cell></cell><cell>spatio-temporal trajectory</cell></row><row><cell>Motion event</cell><cell>Span a sequence of motion segments. A gesture</cell></row><row><cell></cell><cell>can be considered as a particular class of motion</cell></row><row><cell></cell><cell>events, mainly involving movements of the arms</cell></row><row><cell></cell><cell>and trunk</cell></row><row><cell>Activity</cell><cell>Describes a situation that may consist of various</cell></row><row><cell></cell><cell>motion events. Thus, it refers to higher-level</cell></row><row><cell></cell><cell>context</cell></row><row><cell>Signal segment</cell><cell>A slice of sensor data that corresponds to a</cell></row><row><cell></cell><cell>motion segment</cell></row><row><cell>Candidate section</cell><cell>A slice of sensor data that may contain a gesture</cell></row></table><note><p><p><p>of the motion segments. Table</p>1</p>summarises the terminology used in this work.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Description of the relevant gestures in case studies 1 and 2</figDesc><table><row><cell>Gesture</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Unless otherwise noted, all gestures were conducted with the right arm pointing downwards at start/end in case study 1 and with both arms at rest on table at start/end in case study 2.</figDesc><table><row><cell>Case study 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cutlery (CL)</cell><cell cols="3">Meal intake of Lasagne using fork and knife.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Fork tap, loading and manoeuvring to mouth</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">and back with left hand</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Drink (DK)</cell><cell cols="3">Pick up cup from table-take a sip-put cup</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">back on the table</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spoon (SP)</cell><cell cols="3">Meal intake of cereals or soup using a spoon.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Spoon loading and manoeuvring to mouth and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>back</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Handheld (HH)</cell><cell cols="3">Meal intake of bread slice or chocolate bar using</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">the hand only: moving the left hand to mouth</cell><cell></cell><cell></cell></row><row><cell></cell><cell>and back.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Inertial</cell><cell>Motion</cell><cell>Section</cell><cell>Selection of</cell><cell>HMM-based</cell><cell>Identified</cell></row><row><cell></cell><cell>sensor</cell><cell>segment</cell><cell>similarity</cell><cell>candidate</cell><cell>section</cell><cell>gesture</cell></row><row><cell></cell><cell>data</cell><cell>partitioning</cell><cell>search</cell><cell>sections</cell><cell>classification</cell><cell>sections</cell></row></table><note><p>Fig. 4. Detailed structure of the two-stage recognition framework.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Motion parameter selection for the SWAB algorithm</cell><cell></cell></row><row><cell></cell><cell>SWAB motion</cell><cell>Body side used</cell></row><row><cell>Gesture</cell><cell>parameter</cell><cell>in studies</cell></row><row><cell>Study 1</cell><cell></cell><cell></cell></row><row><cell>Light button (LB), handshake (HS),</cell><cell>pitch LA (t) a</cell><cell>Right</cell></row><row><cell>phone up (PU), phone down (PD),</cell><cell></cell><cell></cell></row><row><cell>door (DR), coin (CN)</cell><cell></cell><cell></cell></row><row><cell>Study 2</cell><cell></cell><cell></cell></row><row><cell>Cutlery (CL)</cell><cell>roll LA (t) a</cell><cell>Left</cell></row><row><cell>Drink (DK), spoon (SP)</cell><cell>pitch LA (t)</cell><cell>Right</cell></row><row><cell>Handheld (HH)</cell><cell>pitch LA (t)</cell><cell>Left</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>SWAB segmentation results</cell><cell></cell><cell></cell></row><row><cell>Segmentation</cell><cell>Case study 1</cell><cell>Case study 2</cell></row><row><cell>category</cell><cell></cell><cell></cell></row><row><cell>Mean number of SWAB</cell><cell>15 506</cell><cell>13 020</cell></row><row><cell>segmentation points per</cell><cell></cell><cell></cell></row><row><cell>gesture</cell><cell></cell><cell></cell></row><row><cell>Ratio of segmentation</cell><cell>2.2</cell><cell>0.77</cell></row><row><cell>points per gesture by total</cell><cell></cell><cell></cell></row><row><cell>recorded samples (%)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Statistics of the recorded data sets</figDesc><table><row><cell>Feature</cell><cell>Case study 1</cell><cell>Case study 2</cell></row><row><cell>Total duration of all data sets</cell><cell>7185 s (2.00 h)</cell><cell>16 848 s (4.68 h)</cell></row><row><cell cols="3">Share of relevant gestures in data sets 25.4% (1826 s) 34.7% (5846 s)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Evaluation results of preselection stage</figDesc><table><row><cell>Case study 1</cell></row></table><note><p><p><p>a See Fig.</p>10</p>for corresponding description of evaluation metrics.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The normalised Euclidean distance corresponds to the Mahalanobis distance where the covariance matrix is a diagonal matrix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.xsens.com.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partly supported by the Swiss State Secretariat for Education and Research (SER).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>About the Author-HOLGER JUNKER received the Dipl.-Ing. (M.Sc.) degree in electrical engineering from the Technical University at Brunswick, Germany, in 2000 and the Dr. sc. ETH Zurich (Ph.D.) degree in information technology and electrical engineering from ETH Zurich, Switzerland, in 2005. He joined the Electronics Laboratory at ETH Zurich in 2000 as a research and teaching assistant in the Wearable Computing Group. His research interests included wearable computing, context modelling and recognition, and hardware design of context-aware system architectures.</p><p>About the Author-OLIVER AMFT is a Ph.D. candidate at the Wearable Computing Lab., ETH Zurich, planning to complete his thesis before March 2008. He received his Dipl.-Ing. (M.S.) in electrical engineering from Technical University Chemnitz, Germany, in 1999. Before joining ETH in 2004, he was five years with ABB Inc., Switzerland. Oliver led the development of embedded communication systems in different ranks, including the Senior Development Engineer and the R&amp;D Project Manager. He continues to support early development stages at ABB as external consultant. Oliver's research focuses on pervasive healthcare and assistive systems. This includes embedded systems as well as pervasive sensing and pattern recognition for physiology, activity and behavioural analyses. His Ph.D. thesis investigates solutions for on-body automatic dietary monitoring.</p><p>About the Author-PAUL LUKOWICZ received the M.Sc. degree in computer science, the M.Sc. degree in physics and the Ph.D. degree in computer science from the University of Karlsruhe, Germany, in 1992, 1993 and 1999, respectively. From 1999 to 2004 he led the Wearable Computing and Computer Architecture Groups at the Electronics Laboratory, ETH Zurich. In 2003, he was appointed Professor of Computer Science at the Institute for Computer Systems and Networks, University of Health Informatics and Technology Tirol, Innsbruck, Austria. Since 2006 he is a professor for Computer Science at the University of Passau, Germany. His research interests include wearable and mobile computer architecture, context and activity recognition.</p><p>About the Author-GERHARD TR√ñSTER received the M.Sc. degree from the Technical University of Karlsruhe, Germany, in 1978, and the Ph.D degree from the Technical University of Darmstadt, Germany, in 1984, both in electrical engineering. He is a Professor and head of the Electronics Laboratory, ETH Zurich, Switzerland. During the eight years he spent at Telefunken Corporation, Germany; he was responsible for various national and international research projects focused on key components for ISDN and digital mobile phones. His field of research includes wearable computing, reconfigurable systems, signal processing, mechatronics and electronic packaging. He authored and coauthored more than 100 articles and holds five patents.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gesture spotting using wrist worn microphone and 3-axis accelerometer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Joint Conference on Smart Objects and Ambient Intelligence: Innovative Context-Aware Services: Usages and Technologies</title>
		<meeting>the 2005 Joint Conference on Smart Objects and Ambient Intelligence: Innovative Context-Aware Services: Usages and Technologies</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activity recognition of assembly tasks using body-worn microphones and accelerometers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1553" to="1567" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soundbutton: design of a low power wearable audio classification system</title>
		<author>
			<persName><forename type="first">M</forename><surname>St√§ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von B√ºren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC 2003: Proceedings of the Seventh IEEE International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locomotion analysis using a simple feature derived from force sensitive resistors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Biomedical Engineering</title>
		<meeting>the Second International Conference on Biomedical Engineering</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using ultrasonic hand tracking to augment motion analysis based recognition of manipulative gestures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ogris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stiefmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC 2005: Proceedings of the Ninth IEEE International Symposium on Wearable Computers</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rhodes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Philipose, Fine-grained activity recognition by aggregating abstract object usage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC 2005: Proceedings of the Ninth IEEE International Symposium on Wearable Computers</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rhodes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sign language recognition based on HMM/ANN/DP</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Pattern Recognition Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="587" to="602" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visual recognition of American sign language using hidden markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Boston, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition of human body motion using phase space constraints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Computer Vision</title>
		<meeting>the Fifth International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="624" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognizing human action in time-sequential images using hidden Markov model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 1992: Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="379" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coupled hidden Markov models for complex action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 1997: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="994" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An HMM-based threshold model approach for gesture recognition</title>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="961" to="973" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">View-invariance in action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2001: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Motion-Based Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of computer vision-based human motion capture</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="268" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision-based gesture recognition: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Gesture Workshop</title>
		<meeting>the International Gesture Workshop<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical recognition of intentional human gestures for sports video annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Pattern Recognition</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Laurendeau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Suen</surname></persName>
		</editor>
		<meeting>the 16th International Conference on Pattern Recognition<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1082" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An inertial measurement unit for user interfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Benbasat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Boston, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-sensor activity context detection for wearable computing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Ambient Intelligence</title>
		<meeting>the European Symposium on Ambient Intelligence<address><addrLine>Eindhoven, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="220" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context awareness in systems with limited resources</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cakmakci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coutaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2002: Proceedings of the Third Workshop on Artificial Intelligence in Mobile Systems (AIMS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Physical activity recognition from acceleration data under semi-naturalistic conditions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Boston, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>St√§ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atrash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recognizing workshop activity using body worn microphones and accelerometers</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Pervasive Computing</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using multiple sensors for mobile sign language recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Brashear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC2003: Proceedings of the Seventh IEEE International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognition of arm gestures using multiple orientation sensors: gesture classsification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lementec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International IEEE Conference on Intelligent Transportation Systems</title>
		<meeting>the Seventh International IEEE Conference on Intelligent Transportation Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="965" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An HMM-based approach for gesture segmentation and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="679" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">interactive learning of gestures for human/robot interfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yangsheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Online</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA 1996: Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Caplan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2982" to="2987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gesture segmentation in complex motion sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kahol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP2003: Proceedings of the International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Documenting motion sequences with a personalized annotation system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kahol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised analysis of human gestures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Rim Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A real-time continuous gesture recognition system for sign language</title>
		<author>
			<persName><forename type="first">R.-H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic modeling of image sequences for the segmentation and recognition of dynamic gestures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morguet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universit√§t M√ºnchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Movement, activity, and action: the role of knowledge in the perception of motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. London Ser. B</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="1257" to="1265" />
			<date type="published" when="1358">1358. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detection of eating and drinking arm gestures using inertial body-worn sensors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Amft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC 2005: IEEE Proceedings of the Ninth International Symposium on Wearable Computers</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rhodes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An online algorithm for segmenting time series</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining</title>
		<meeting>the IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Waving real hand gestures recorded by wearable motion sensors to a virtual car and driver in a mixed-reality parking game</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Amft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIG 2007: Proceedings of the 2nd IEEE Symposium on Computational Intelligence and Games</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analysis of chewing sounds for dietary monitoring</title>
		<author>
			<persName><forename type="first">O</forename><surname>Amft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>St√§ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tr√∂ster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp 2005: Proceedings of the Seventh International Conference on Ubiquitous Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Tokuda</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3660</biblScope>
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
