<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GIANT: Globally Improved Approximate Newton Method for Distributed Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shusen</forename><surname>Wang</surname></persName>
							<email>shusen.wang@stevens.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farbod</forename><surname>Roosta-Khorasani</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
							<email>pengxu@stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
							<email>mmahoney@stat.berkeley.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of California at Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GIANT: Globally Improved Approximate Newton Method for Distributed Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AACA9000A12F5991E09504B39A619DFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The large-scale nature of many modern "big-data" problems, arising routinely in science, engineering, financial markets, Internet and social media, etc., poses significant computational as well as storage challenges for machine learning procedures. For example, the scale of data gathered in many applications nowadays typically exceeds the memory capacity of a single machine, which, in turn, makes learning from data ever more challenging. In this light, several modern parallel (or distributed) computing architectures, e.g., MapReduce <ref type="bibr" target="#b3">[4]</ref>, Apache Spark <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19]</ref>, GraphLab <ref type="bibr" target="#b13">[14]</ref>, and Parameter Server <ref type="bibr" target="#b10">[11]</ref>, have been designed to operate on and learn from data at massive scales. Despite the fact that, when compared to a single machine, distributed systems tremendously reduce the storage and (local) computational costs, the inevitable cost of communications across the network can often be the bottleneck of distributed computations. As a result, designing methods which can strike an appropriate balance between the cost of computations and that of communications are increasingly desired.</p><p>The desire to reduce communication costs is even more pronounced in the federated learning framework <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>. Similarly to typical settings of distributed computing, federated learning assumes data are distributed over a network across nodes that enjoy reasonable computational resources, e.g., mobile phones, wearable devices, and smart homes. However, the network has severely limited bandwidth and high latency. As a result, it is imperative to reduce the communications between the center and a node or between two nodes. In such settings, the preferred methods are those which can perform expensive local computations with the aim of reducing the overall communications across the network.</p><p>Optimization algorithms designed for distributed setting are abundantly found in the literature. Firstorder methods, i.e, those that rely solely on gradient information, are often embarrassingly parallel and easy to implement. Examples of such methods include distributed variants of stochastic gradient descent (SGD) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref>, accelerated SGD <ref type="bibr" target="#b34">[35]</ref>, variance reduction SGD <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>, stochastic coordinate descent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref> and dual coordinate ascent algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. The common denominator in all of these methods is that they significantly reduce the amount of local computation. But this blessing comes with an inevitable curse that they, in turn, may require a far greater number of iterations and hence, incur more communications overall. Indeed, as a result of their highly iterative nature, many of these first-order methods require several rounds of communications and, potentially, synchronizations in every iteration, and they must do so for many iterations. In a computer cluster, due to limitations on the network's bandwidth and latency and software system overhead, communications across the nodes can oftentimes be the critical bottleneck for the distributed optimization. Such overheads are increasingly exacerbated by the growing number of compute nodes in the network, limiting the scalability of any distributed optimization method that requires many communication-intensive iterations. To remedy such drawbacks of high number of iterations for distributed optimization, communicationefficient second-order methods, i.e., those that, in addition to the gradient, incorporate curvature information, have also been recently considered <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>; see also Section 1.1.</p><p>The common feature in all of these methods is that they intend to increase the local computations with the aim of reducing the overall iterations, and hence, lowering the communications. In other words, these methods are designed to perform as much local computation as possible before making any communications across the network. Pursuing similar objectives, in this paper, we propose a Globally Improved Approximate NewTon (GIANT) method and establish its improved theoretical convergence properties as compared with other similar second-order methods. We also showcase the superior empirical performance of GIANT through several numerical experiments.</p><p>The rest of this paper is organized as follows. Section 1.1 briefly reviews prior works most closely related to this paper. Section 1.2 gives a summary of our main contributions. The formal description of the distributed empirical risk minimization problem is given in Section 2, followed by the derivation of various steps of GIANT in Section 3. Section 4 presents the theoretical guarantees. The most commonly used notation is listed in Table <ref type="table" target="#tab_0">1</ref>. Due to the page limit, the readers can refer to the long version <ref type="bibr" target="#b40">[41]</ref>; Section 5 provides a summary of our experiments. The proofs can be found in the long version <ref type="bibr" target="#b40">[41]</ref>.</p><p>Table <ref type="table">2</ref>: The number of communications (proportional to the number of iterations) required for the ridge regression problem. Here κ is the condition number of the Hessian matrix, µ is the matrix coherence, and Õ conceals constants (analogous to µ) and logarithmic factors.</p><formula xml:id="formula_0">Method #Iterations Metric GIANT [this work] t = O log(dκ/E) log(n/µdm) wt -w 2 ≤ E DiSCO [45] t = Õ dκ 1/2 m 3/4 n 3/4 + κ 1/2 m 1/4 n 1/4 log 1 E f (wt) -f (w ) ≤ E DANE [36] t = Õ κ 2 m n log 1 E f (wt) -f (w ) ≤ E AIDE [29] t = Õ κ 1/2 m 1/4 n 1/4 log 1 E f (wt) -f (w ) ≤ E CoCoA [38] t = O n + 1 γ log n E f (wt) -f (w ) ≤ E AGD t = O κ 1/2 log d E wt -w 2 ≤ E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Among the existing distributed second-order optimization methods, the most notably are DANE <ref type="bibr" target="#b35">[36]</ref>, AIDE <ref type="bibr" target="#b28">[29]</ref>, and DiSCO <ref type="bibr" target="#b44">[45]</ref>. Another similar method is CoCoA <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>, which is analogous to second-order methods in that it involves sub-problems which are local quadratic approximations to the dual objective function. However, despite the fact that CoCoA makes use of the smoothness condition, it does not exploit any explicit second-order information.</p><p>We can evaluate the theoretical properties the above-mentioned methods in light of comparison with optimal first-order methods, i.e., accelerated gradient descent (AGD) methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. It is because AGD methods are mostly embarrassingly parallel and can be regarded as the baseline for distributed optimization. Recall that AGD methods, being optimal in worst-case analysis sense <ref type="bibr" target="#b20">[21]</ref>, are guaranteed to convergence to E-precision in O( √ κ log<ref type="foot" target="#foot_0">1</ref> E ) iterations <ref type="bibr" target="#b22">[23]</ref>, where κ can be thought of as the condition number of the problem. Each iteration of AGD has two rounds of communicationsbroadcast or aggregation of a vector.</p><p>In Table <ref type="table">2</ref>, we compare the communication costs with other methods for the ridge regression problem: min w 1 The communication cost of GIANT has a mere logarithmic dependence on the condition number κ; in contrast, the other methods have at least a square root dependence on κ. Even if κ is assumed to be small, say κ = O( √ n), which was made by <ref type="bibr" target="#b44">[45]</ref>,</p><formula xml:id="formula_1">1 n Xw -y 2 2 + γ w 2 2 .</formula><p>GIANT's bound is better than the compared methods regarding the dependence on the number of partitions, m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Newton (ANT) Direction</head><p>Globally Improved Approximate Newton (GIANT) Direction</p><formula xml:id="formula_2">! " # " $ %," ⋯ ! ( # ( $ %,( ⋯ ! ) # ) $ %,) ! " # " * + %," ⋯ ! ( # ( * + %,( ⋯ ! ) # ) * + %,) g t = Mw t + m X i=1 g t,i w t+1 = w t 1 m m X i=1 pt,i pt = 1 m m X i=1 pt,i w t+1 = w t pt pt = 1 m m X i=1 pt,i w t+1 = w t pt g t = w t + m X i=1 g t,i g t = m X i=1 g t,i</formula><p>Figure <ref type="figure">1</ref>: One iteration of GIANT. Here X and y are respectively the features and lables; X i and y i denotes the blocks of X and y, respectively. Each one-to-all operation is a Broadcast and each all-to-one operation is a Reduce.</p><p>Our GIANT method is motivated by the subsampled Newton method <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25]</ref>. Later on, we realized that a similar idea has been proposed by DANE <ref type="bibr" target="#b35">[36]</ref>; GIANT and DANE are identical for quadratic programming; they are different for the general convex problems. Nevertheless, we show better convergence bounds than DANE, even for quadratic programming. Our improvement over DANE is obtained by better bounds the Hessian matrix approximation and better analysis of convex optimization.</p><p>GIANT also bears a resemblance to FADL <ref type="bibr" target="#b15">[16]</ref>, but we show better convergence bounds. Mahajan et al. <ref type="bibr" target="#b15">[16]</ref> has conducted comprehensive empirical comparisons among many distributed computing methods and concluded that the local quadratic approximation, which is very similar to GIANT, is the final method which they recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>In this paper, we consider the problem of empirical risk minimization involving smooth and strongly convex objective function (which is the same setting considered in prior works of DANE, AIDE, and DiSCO). In this context, we propose a Globally Improved Approximate NewTon (GIANT) method and establish its theoretical and empirical properties as follows.</p><p>• For quadratic objectives, we establish global convergence of GIANT. To attain a fixed precision, the number of iterations of GIANT (which is proportional to the communication complexity) has a mere logarithmic dependence on the condition number. In contrast, the prior works have at least square root dependence. In fact, for quadratic problems, GIANT and DANE <ref type="bibr" target="#b35">[36]</ref> can be shown to be identical. In this light, for such problems, our work improves upon the convergence of DANE.</p><p>• For more general problems, GIANT has linear-quadratic convergence in the vicinity of the optimal solution, which we refer to as "local convergence". <ref type="foot" target="#foot_1">2</ref> The advantage of GIANT mainly manifests in big-data regimes where there are many data points available. In other words, when the number of data points is much larger than the number of features, the theoretical convergence of GIANT enjoys significant improvement over other similar methods.</p><p>• In addition to theoretical features, GIANT also exhibits desirable practical advantages. For example, in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, GIANT only involves one tuning parameter, i.e., the maximal iterations of its sub-problem solvers, which makes GIANT easy to implement in practice. Furthermore, our experiments on a computer cluster show that GIANT consistently outperforms AGD, L-BFGS, and DANE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>In this paper, we consider the distributed variant of empirical risk minimization, a supervisedlearning problem arising very often in machine learning and data analysis <ref type="bibr" target="#b33">[34]</ref>. More specifically, let </p><formula xml:id="formula_3">x 1 , • • • , x n ∈ R d be</formula><formula xml:id="formula_4">w∈R d f (w) 1 n n j=1 j (w T x j ) + γ 2 w 2 2 ,<label>(1)</label></formula><p>where j : R → R is convex, twice differentiable, and smooth. We further assume that f is strongly convex, which in turn, implies the uniqueness of the minimizer of (1), denoted throughout the text by w . Note that y j is implicitly captured by j . Examples of the loss function, j , appearing in (1) include linear regression: j (z j ) = 1 2 (z jy j ) 2 , logistic regression: j (z j ) = log(1 + e -zj yj ).</p><p>Suppose the n feature vectors and loss functions (x 1 , 1 ), • • • , (x n , n ) are partitioned among m worker machines. Let s n/m be the local sample size. Our theories require s &gt; d; nevertheless, GIANT empirically works well for s &lt; d.</p><p>We consider solving (1) in the regimes where n d. We assume that the data points, {x i } n i=1 are partitioned among m machines, with possible overlaps, such that the number of local data is larger than d. Otherwise, if n d, we can consider the dual problem and partition features. If the dual problem is also decomposable, smooth, strongly convex, and unconstrained, e.g., ridge regression, then our approach directly applies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm Description</head><p>In this section, we present the algorithm derivation and complexity analysis. GIANT is a centralized and synchronous method; one iteration of GIANT is depicted in Figure <ref type="figure">1</ref>. The key idea of GIANT is avoiding forming of the exact Hessian matrices H t ∈ R d×d in order to avoid expensive communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gradient and Hessian</head><p>GIANT iterations require the exact gradient, which in the t-th iteration, can be written as</p><formula xml:id="formula_5">g t = ∇f (w t ) = 1 n n j=1 j (w T t x j ) x j + γw t ∈ R d .<label>(2)</label></formula><p>The gradient, g t can be computed, embarrassingly, in parallel. The driver Broadcasts w t to all the worker machines. Each machine then uses its own {(x j , j )} to compute its local gradient. Subsequently, the driver performs a Reduce operation to sum up the local gradients and get g t . The per-iteration communication complexity is Õ(d) words, where Õ hides the dependence on m (which can be m or log m, depending on the network structure).</p><p>More specifically, in the t-th iteration, the Hessian matrix at w t ∈ R d can be written as</p><formula xml:id="formula_6">H t = ∇ 2 f (w t ) = 1 n n j=1 j (w T t x j ) • x j x T j + γI d .<label>(3)</label></formula><p>To compute the exact Hessian, the driver must aggregate the m local Hessian matrices (each of size d × d) by one Reduce operation, which has Õ(d 2 ) communication complexity and is obviously impractical when d is thousands. The Hessian approximation developed in this paper has a mere Õ(d) communication complexity which is the same to the first-order methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximate NewTon (ANT) Directions</head><p>Assume each worker machine locally holds s random samples drawn from {(x j , j )} n j=1 . <ref type="foot" target="#foot_2">3</ref> Let J i be the set containing the indices of the samples held by the i-th machine, and s = |J i | denote its size. Each worker machine can use its local samples to form a local Hessian matrix</p><formula xml:id="formula_7">H t,i = 1 s j∈Ji j (w T t x j ) • x j x T j + γI d .</formula><p>Clearly, E[ H t,i ] = H t . We define the Approximate NewTon (ANT) direction by pt,i = H -1 t,i g t . The cost of computing the ANT direction pt,i in this way, involves O(sd 2 ) time to form the d × d dense matrix H t,i and O(d 3 ) to invert it.</p><p>To reduce the computational cost, we opt to compute the ANT direction by the conjugate gradient (CG) method <ref type="bibr" target="#b23">[24]</ref>. Let</p><formula xml:id="formula_8">a j = j (w T t x j ) • x j ∈ R d , A t = [a T 1 ; • • • ; a T n ] ∈ R n×d ,<label>(4)</label></formula><p>and A t,i ∈ R s×d contain the rows of A t indexed by the set J i . Using the matrix notation, we can write the local Hessian matrix as</p><formula xml:id="formula_9">H t,i = 1 s A T t,i A t,i + γI d .<label>(5)</label></formula><p>Employing CG, it is unnecessary to explicitly form H t,i . Indeed, one can simply approximately solve</p><formula xml:id="formula_10">1 s A T t,i A t,i + γI d p = g t<label>(6)</label></formula><p>in a "Hessian-free" manner, i.e., by employing only Hessian-vector products in CG iterations. In each round of GIANT, the local computational cost of a worker machine is O q • nnz(A t,i ) , where q is the number of CG iterations specified by the users and typically set to tens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Globally Improved ANT (GIANT) Direction</head><p>Using random matrix concentration, we can show that for sufficiently large s, the local Hessian matrix H t,i is a spectral approximation to H t . Now let pt,i be an ANT direction. The Globally Improved ANT (GIANT) direction is defined as</p><formula xml:id="formula_11">pt = 1 m m i=1 pt,i = 1 m m i=1 H -1 t,i g t = H -1 t g t .<label>(7)</label></formula><p>Interestingly, here H t is the harmonic mean defined as</p><formula xml:id="formula_12">H t ( 1 m m i=1 H -1 t,i ) -1</formula><p>, whereas the true Hessian H t is the arithmetic mean defined as H t 1 m m i=1 H t,i . If the data is incoherent, that is, the "information" is spread-out rather than concentrated to a small fraction of samples, then the harmonic mean and the arithmetic mean are very close to each other, and thereby the GIANT direction pt = H -1 g t very well approximates the true Newton direction H -1 g t . This is the intuition of our global improvement.</p><p>The motivation of using the harmonic mean, H t , to approximate the arithmetic mean (the true Hessian matrix), H t , is the communication cost. Computing the arithmetic mean H t 1 m m i=1 H t,i would require the communication of d × d matrices which is very expensive. In contrast, computing pt merely requires the communication of d-dimensional vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Time and Communication Complexities</head><p>For each worker machine, the per-iteration time complexity is O(sdq), where s is the local sample size and q is the number of CG iterations for (approximately) solving ( <ref type="formula" target="#formula_10">6</ref>). (See Proposition 5 for the setting of q.) If the feature matrix X ∈ R n×d has a sparsity of = nnz(X)/(nd) &lt; 1, the expected per-iteration time complexity is then O( sdq).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each iteration of GIANT has four rounds of communications: two Broadcast for sending and two</head><p>Reduce for aggregating some d-dimensional vector. If the communication is in a tree fashion, the per-iteration communication complexity is then Õ(d) words, where Õ hides the factor involving m which can be m or log m. In contrast, the naive Newton's method has Õ(d 2 ) communication complexity, because the system sends and receives d × d Hessian matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we formally present the convergence guarantees of GIANT. Section 4.1 focuses on quadratic loss and treats the global convergence of GIANT. This is then followed by local convergence properties of GIANT for more general non-quadratic loss in Section 4.2. For the results of Sections 4.1 and 4.2, we require that the local linear system to obtain the local Newton direction is solved exactly. Section 4.3 then relaxes this requirement to allow for inexactness in the solution, and establishes similar convergence rates as those of exact variants.</p><p>For our analysis here, we frequently make use of the notion of matrix row coherence, defined as follows. Such a notation has been used in compressed sensing <ref type="bibr" target="#b2">[3]</ref>, matrix completion <ref type="bibr" target="#b1">[2]</ref>, and randomized linear algebra <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Coherence).</head><p>Let A ∈ R n×d be any matrix and U ∈ R n×d be its column orthonormal bases. The row coherence of A is µ</p><formula xml:id="formula_13">(A) = n d max j u j 2 2 ∈ [1, n d ]. Remark 1.</formula><p>Our work assumes A t ∈ R n×d , which is defined in (4), is incoherent, namely µ(A t ) is small. The prior works, DANE, AIDE, and DiSCO, did not use the notation of incoherence; instead, they assume ∇ 2 w l j (w T x j ) | w=wt = a j a T j is upper bounded for all j ∈ [n] and w t ∈ R d , where a j ∈ R d is the j-th row of A t . Such an assumption is different from but has similar implication as our incoherence assumption; under either of the two assumptions, it can be shown that the Hessian matrix can be approximated using a subset of samples selected uniformly at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quadratic Loss</head><p>In this section, we consider a special case of (1) with i (z) = (zy i )<ref type="foot" target="#foot_3">2</ref> /2, i.e., the quadratic optimization problems:</p><formula xml:id="formula_14">f (w) = 1 2n Xw -y 2 2 + γ 2 w 2 2 .<label>(8)</label></formula><p>The Hessian matrix is given as ∇ 2 f (w) = 1 n X T X + γI d , which does not depend on w. Theorem 1 describes the convergence of the error in the iterates, i.e., ∆ t w tw . Theorem 1. Let µ be the row coherence of X ∈ R n×d and m be the number of partitions. Assume the local sample size satisfies s ≥ 3µd η 2 log md δ for some η, δ ∈ (0, 1). It holds with probability</p><formula xml:id="formula_15">1 -δ that ∆ t 2 ≤ α t √ κ ∆ 0 2 , where α = η √ m + η 2 and κ is the condition number of ∇ 2 f (w) = 1 n X T X + γI d . Remark 2.</formula><p>The theorem can be interpreted in the this way. Assume the total number of samples, n, is at least 3µdm log(md). Then</p><formula xml:id="formula_16">∆ t 2 ≤ 3µdm log(md/δ) n + 3µd log(md/δ) n t √ κ ∆ 0 2</formula><p>holds with probability at least 1δ.</p><p>If the total number of samples, n, is substantially bigger than µdm, then GIANT converges in a very small number of iterations. Furthermore, to reach a fixed precision, say ∆ t 2 ≤ E, the number of iterations, t, has a mere logarithmic dependence on the condition number, κ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Smooth Loss</head><p>For more general (not necessarily quadratic) but smooth loss, GIANT has linear-quadratic local convergence, which is formally stated in Theorem 2 and Corollary 3. Let H = ∇ 2 f (w ) and</p><formula xml:id="formula_17">H t = ∇ 2 f (w t ).</formula><p>For this general case, we assume the Hessian is L-Lipschitz, which is a standard assumption in analyzing second-order methods. Assumption 1. The Hessian matrix is L-Lipschitz continuous, i.e., ∇ 2 f (w) -∇ 2 f (w ) 2 ≤ L ww 2 , for all w and w .</p><p>Theorem 2 establishes the linear-quadratic convergence of ∆ t w tw . We remind that A t ∈ R n×d is defined in (4) (thus A T t A t + γI d = H t ). Note that, unlike Section 4.1, the coherence of A t , denote µ t , changes with iterations. Theorem 2. Let µ t ∈ [1, n/d] be the coherence of A t and m be the number of partitions. Assume the local sample size satisfies s t ≥ 3µtd η 2 log md δ for some η, δ ∈ (0, 1). Under Assumption 1, it holds with probability 1δ that</p><formula xml:id="formula_18">∆ t+1 2 ≤ max α σmax(Ht) σmin(Ht) ∆ t 2 , 2L σmin(Ht) ∆ t 2 2 , where α = η √ m + η 2 .</formula><p>Remark 3. The standard Newton's method is well known to have local quadratic convergence; the quadratic term in Theorem 2 is the same as Newton's method. The quadratic term is caused by the non-quadritic objective function. The linear term arises from the Hessian approximation. For large sample size, s, equivalently, small η, the linear term is small. Note that in Theorem 2 the convergence depends on the condition numbers of the Hessian at every point. Due to the Lipschitz assumption on the Hessian, it is easy to see that the condition number of the Hessian in a neighborhood of w is close to κ(H ). This simple observation implies <ref type="bibr">Corollary 3,</ref><ref type="bibr">in</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inexact Solutions to Local Sub-Problems</head><p>In the t-th iteration, the i-th worker locally computes pt,i by solving H t,i p = g t , where H t,i is the i-th local Hessian matrix defined in <ref type="bibr" target="#b4">(5)</ref>. In high-dimensional problems, say d ≥ 10 4 , the exact formation of H t,i ∈ R d×d and its inversion are impractical. Instead, we could employ iterative linear system solvers, such as CG, to inexactly solve the arising linear system in <ref type="bibr" target="#b5">(6)</ref>. Let p t,i be an inexact solution which is close to pt,i H -1 t,i g t , in the sense that</p><formula xml:id="formula_19">H 1/2 t,i p t,i -pt,i 2 ≤ 0 2 H 1/2 t,i pt,i 2 ,<label>(9)</label></formula><p>for some 0 ∈ (0, 1). GIANT then takes p t = 1 m m i=1 p t,i as the approximate Newton direction in lieu of pt . In this case, as long as 0 is of the same order as η √ m + η 2 , the convergence rate of such inexact variant of GIANT remains similar to the exact algorithm in which the local linear system is solved exactly. Theorem 4 makes convergence properties of inexact GIANT more explicit. Theorem 4. Suppose inexact local solution to (6), denote p t,i , satisfies <ref type="bibr" target="#b8">(9)</ref>. Then Theorems 1 and 2 and Corollary 3 all continue to hold with α = η √ m + η 2 + 0 .</p><p>Proposition 5 gives conditions to guarantee (9), which is, in turn, required for Theorem 4. Proposition 5. To compute an inexact local Newton direction from the sub-problem (6), suppose each worker performs q = log 8</p><formula xml:id="formula_20">2 0 log √ κt+1 √ κt-1 ≈ √ κt-1 2 log 8 2 0</formula><p>iterations of CG, initialized at zero, where κt and κ t are, respectiely, the condition number of H t,i and H t . Then requirement (9) is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Summary of the Empirical Study</head><p>Due to the page limit, the experiments are not included in this paper; please refer to the long version <ref type="bibr" target="#b40">[41]</ref> for the experiments. The Apache Spark code is available at https://github.com/ wangshusen/SparkGiant.git. Here we briefly describe our results.</p><p>We implement GIANT, Accelerated Gradient Descent (AGD) <ref type="bibr" target="#b22">[23]</ref>, Limited memory BFGS (L-BFGS) <ref type="bibr" target="#b11">[12]</ref>, and Distributed Approximate NEwton (DANE) <ref type="bibr" target="#b35">[36]</ref> in Scala and Apache Spark <ref type="bibr" target="#b43">[44]</ref>.</p><p>We empirically study the 2 -regularized logistic regression problem (which satisfies our assumptions):</p><formula xml:id="formula_21">min w 1 n n j=1 log 1 + exp(-y j x T j w) + γ 2 w 2 2 ,<label>(10)</label></formula><p>We conduct large-scale experiments on the Cori Supercomputer maintained by NERSC, a Cray XC40 system with 1632 compute nodes, each of which has two 2.3GHz 16-core Haswell processors and 128GB of DRAM. We use up to 375 nodes (12,000 CPU cores).</p><p>To apply logistic regression, we use three binary classification datasets: MNIST8M (digit "4" versus "9", thus n = 2M and d = 784), Covtype (n = 581K and d = 54), and Epsilon (n = 500K and d = 2K), which are available at the LIBSVM website. We randomly hold 80% for training and the rest for test. To increase the size of the data, we generate 10 4 random Fourier features <ref type="bibr" target="#b25">[26]</ref> and use them in lieu of the original features in the logistic regression problem.</p><p>For the four methods, we use different settings of the parameters and report the best convergence curve; we do not count the cost of parameter tuning. (This actually favors AGD and DANE because they have more tuning parameters than GIANT and L-BFGS.) Using the same amount of wall-clock time, GIANT consistently converges faster than AGD, DANE, and L-BFGS in terms of both training objective value and test classification error (see the figures in <ref type="bibr" target="#b40">[41]</ref>).</p><p>Our theory requires the local sample size s = n m to be larger than d. But in practice, GIANT converges even if s is smaller than d. In this set of experiments, we set m = 89, and thus s is about half of d. Nevetheless, GIANT converges in all of our experiments. Our empirical may imply that the theoretical sample complexity can be potentially improved.</p><p>We further use data augmentation (i.e., adding random noise to the feature vectors) to increase n to 5 and 25 times larger. In this way, the feature matrices are all dense, and the largest feature matrix we use is about 1TB. As we increase both n and the number of compute nodes, the advantage of GIANT further increases, which means GIANT is more scalable than the compared methods. It is because as we increase the number of samples and the number of nodes by the same factor, the local computation remains the same, but the communication and synchronization costs increase, which favors the communication-efficient methods; see the figures and explanations in <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have proposed GIANT, a practical Newton-type method, for empirical risk minimization in distributed computing environments. In comparison to similar methods, GIANT has three desirable advantages. First, GIANT is guaranteed to converge to high precision in a small number of iterations, provided that the number of training samples, n, is sufficiently large, relative to dm, where d is the number of features and m is the number of partitions. Second, GIANT is very communication efficient in that each iteration requires four or six rounds of communications, each with a complexity of merely Õ(d). Third, in contrast to all other alternates, GIANT is easy to use, as it involves tuning one parameter. Empirical studies also showed the superior performance of GIANT as compared several other methods.</p><p>GIANT has been developed only for unconstrained problems with smooth and strongly convex objective function. However, we believe that similar ideas can be naturally extended to projected Newton for constrained problems, proximal Newton for non-smooth regularization, and trust-region method for nonconvex problems. However, strong convergence bounds of the extensions appear nontrivial and will be left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Commonly used notation.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>n</cell><cell>total number of samples</cell></row><row><cell>d</cell><cell>number of features (attributes)</cell></row><row><cell>m</cell><cell>number of partitions</cell></row><row><cell>f</cell><cell>objective function</cell></row><row><cell>γ</cell><cell>regularization parameter</cell></row><row><cell>wt</cell><cell>the variable at iteration t</cell></row><row><cell>w</cell><cell>the variable that minimizes f</cell></row><row><cell>κ</cell><cell>some condition number</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>which the dependence of the local convergence of GIANT on iterations via H t is removed. Assumption 2. Assume w t is close to w in that ∆ t 2 ≤ 3 L • σ min (H ), where L is defined in Assumption 1. Corollary 3. Under the same setting as Theorem 2 and Assumption 2, it holds with probability 1δ</figDesc><table><row><cell>that</cell><cell>∆ t+1 2 ≤ max 2α</cell><cell>√</cell><cell>κ ∆ t 2 ,</cell></row></table><note><p><p>3L</p>σmin(H ) ∆ t</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As for general convex problems, it is very hard to present the comparison in an easily understanding way. This is why we do not compare the convergence for the general convex optimization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The second-order methods typically have the local convergence issue. Global convergence of GIANT can be trivially established by following<ref type="bibr" target="#b31">[32]</ref>, however, the convergence rate is not very interesting, as it is worse than the first-order methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>If the samples themselves are i.i.d. drawn from some distribution, then a data-independent partition is equivalent to uniform sampling. Otherwise, the system can Shuffle the data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>, where κ is the condition number of the Hessian matrix at w .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Kimon Fountoulakis, Alex Gittens, Jey Kottalam, Zirui Liu, Hao Ren, Sathiya Selvaraj, Zebang Shen, and Haishan Ye for their helpful suggestions. The four authors would like to acknowledge ARO, DARPA, Cray, and NSF for providing partial support of this work. Farbod Roosta-Khorasani was partially supported by the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical secure aggregation for privacy preserving machine learning</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karn</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACR Cryptology ePrint Archive</title>
		<imprint>
			<biblScope unit="page">281</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-optimal signal recovery from random projections: Universal encoding strategies?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MapReduce: simplified data processing on large clusters. Communications of the ACM</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimization in high dimensions via accelerated, parallel, and proximal coordinate descent</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="739" to="771" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting the Nyström method for improved large-scale machine learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gittens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3977" to="4041" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed dual coordinate ascent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Terhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Federated optimization: distributed machine learning for on-device intelligence</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02527</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Federated learning: strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity</title>
		<author>
			<persName><forename type="first">Jason D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07595</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Yiing</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An asynchronous parallel stochastic coordinate descent algorithm</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikrishna</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed GraphLab: A framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adding vs. averaging in distributed primal-dual optimization</title>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An efficient distributed learning algorithm based on effective local functional approximations</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sathiya Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.8418</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sathiya Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0636</idno>
		<title level="m">A parallel SGD method with strong convergence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MLlib: machine learning in Apache Spark</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davies</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Amde</surname></persName>
		</author>
		<author>
			<persName><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallel random coordinate descent method for composite minimization: Convergence analysis and error bounds</title>
		<author>
			<persName><forename type="first">Ion</forename><surname>Necoara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragos</forename><surname>Clipici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="197" to="226" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience publication</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Pilanci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="245" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hogwild: a lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On variance reduction in stochastic gradient descent and its asynchronous variants</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">AIDE: fast and communication efficient distributed optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Póczós</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06879</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed coordinate descent method for learning with big data</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2657" to="2681" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parallel coordinate descent methods for big data optimization</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takávc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="433" to="484" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Farbod</forename><surname>Roosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04737</idno>
		<title level="m">Sub-sampled Newton methods I: globally convergent algorithms</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Farbod</forename><surname>Roosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04738</idno>
		<title level="m">Sub-sampled Newton methods II: Local convergence rates</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Understanding machine learning: from theory to algorithms</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Shwartz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed stochastic optimization and learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed optimization using an approximate Newton-type method</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Federated multi-task learning</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Kai</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10467</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">CoCoA: A general framework for communication-efficient distributed optimization</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02189</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging</title>
		<author>
			<persName><forename type="first">Shusen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gittens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SPSD matrix approximation vis column selection: Theories, algorithms, and extensions</title>
		<author>
			<persName><forename type="first">Shusen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">GIANT: Globally improved approximate Newton method for distributed optimization</title>
		<author>
			<persName><forename type="first">Shusen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farbod</forename><surname>Roosta-Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03528</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sub-sampled Newton methods with non-uniform sampling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farbod</forename><surname>Roosta-Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trading computation for communication: distributed stochastic dual coordinate ascent</title>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HotCloud</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10-10</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DiSCO: distributed optimization for self-concordant empirical loss</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A general distributed dual coordinate optimization framework for regularized loss minimization</title>
		<author>
			<persName><forename type="first">Shun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03763</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
