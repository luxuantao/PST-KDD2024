<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PGA-SiamNet: Pyramid Feature-Based Attention-Guided Siamese Network for Remote Sensing Orthoimagery Building Change Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-03">3 February 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huiwei</forename><surname>Jiang</surname></persName>
							<email>huiwei_jiang@whu.edu.cn</email>
							<idno type="ORCID">0000-0002-1560-5577</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyun</forename><surname>Hu</surname></persName>
							<idno type="ORCID">0000-0002-1560-5577</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center of Geospatial Technology</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinqi</forename><surname>Gong</surname></persName>
							<email>jinqigong@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
							<email>mizhang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PGA-SiamNet: Pyramid Feature-Based Attention-Guided Siamese Network for Remote Sensing Orthoimagery Building Change Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-03">3 February 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">899D54DDB646EC4A8443A95D52ADD023</idno>
					<idno type="DOI">10.3390/rs12030484</idno>
					<note type="submission">Received: 10 January 2020; Accepted: 1 February 2020;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>building change detection</term>
					<term>remote sensing orthoimagery</term>
					<term>attention mechanism</term>
					<term>Siamese convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, building change detection has made remarkable progress through using deep learning. The core problems of this technique are the need for additional data (e.g., Lidar or semantic labels) and the difficulty in extracting sufficient features. In this paper, we propose an end-to-end network, called the pyramid feature-based attention-guided Siamese network (PGA-SiamNet), to solve these problems. The network is trained to capture possible changes using a convolutional neural network in a pyramid. It emphasizes the importance of correlation among the input feature pairs by introducing a global co-attention mechanism. Furthermore, we effectively improved the long-range dependencies of the features by utilizing various attention mechanisms and then aggregating the features of the low-level and co-attention level; this helps to obtain richer object information. Finally, we evaluated our method with a publicly available dataset (WHU) building dataset and a new dataset (EV-CD) building dataset. The experiments demonstrate that the proposed method is effective for building change detection and outperforms the existing state-of-the-art methods on high-resolution remote sensing orthoimages in various metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background</head><p>Remote sensing imagery has found a wide range of applications because it can obtain change information occurring around the world, both in densely populated cities and in hard-to-reach areas. Meanwhile, change detection (CD), as a hot topic in the field of remote sensing analysis, has been studied for several decades. Because of its unique characteristics, many CD studies have been dedicated to solving large-scale and complicated problems using remote sensing images, for example, for the monitoring of forests and urban sprawl, and earthquake assessment over long periods. A lot of research institutions have made many intensive studies on CD, such as the seasonal and annual change monitoring (SATChMo) project <ref type="bibr" target="#b0">[1]</ref> of Poland, earth watching <ref type="bibr" target="#b1">[2]</ref> of European space agency (ESA) and Onera satellite change detection <ref type="bibr" target="#b2">[3]</ref> of the IEEE geosciences and remote sensing association (IEEE GRSS).</p><p>Recently, the high-resolution (HR) or very-high-resolution (VHR) images have received a lot of attention because they can reveal more detailed information about the land surface, thereby increasing the possibility of monitoring small but important objects such as buildings. Driven by this, building change detection (BCD) has attracted substantial attention in applications such as urbanization monitoring, illegal or unauthorized building identification, and disaster evaluation. In addition, automatic building change detection for remote sensing images has become a topical issue because carrying out the task manually is time consuming and tedious <ref type="bibr" target="#b3">[4]</ref>. Therefore, there is a crucial need to investigate efficient building change detection algorithms for remote sensing images.</p><p>In general, the traditional change detection process consists of three major steps: preprocessing, change detection technique selection, and accuracy assessment. Moreover, these methods typically follow one of two approaches <ref type="bibr" target="#b4">[5]</ref>: pixel-based or object-based methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Pixel-based methods are mainly used in large-scale change detection with low or medium resolution images (e.g., MODIS), while object-based methods are more popular for HR or VHR images (e.g., QuickBird, GeoEye-1, WorldView-1/2, Aerial imagery), because the high-frequency components in the VR/VHR images cannot be fully represented by pixel-based methods. These methods have been fully developed in various scenarios in the past years; however, the features used by change detection algorithms are almost hand crafted and so are weak in image representation <ref type="bibr" target="#b10">[11]</ref>. The features are also susceptible to the process in the preprocessing stage, such as those involved in radiometric correction, geometric correction, and image registration. Besides this, the effects of changes in the appearance of objects caused by different photographic angles can be alleviated by orthorectification, but this also brings new problems affecting accurate detection. In high-resolution orthorectified images, the displacement of buildings (especially high-rise buildings) mainly caused by rectifications with a digital elevation model (DEM) and the poor alignment caused by the displacement can lead to many false positive changes. In addition, as the spatial resolution of satellite images increases, the accuracy of image registration tends to worsen <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Generally, the overall framework of the change detection technique can be summarized as feature extractions and change decisions, which is depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Remote Sens. 2020, 12, 484 2 of 21</p><p>Recently, the high-resolution (HR) or very-high-resolution (VHR) images have received a lot of attention because they can reveal more detailed information about the land surface, thereby increasing the possibility of monitoring small but important objects such as buildings. Driven by this, building change detection (BCD) has attracted substantial attention in applications such as urbanization monitoring, illegal or unauthorized building identification, and disaster evaluation. In addition, automatic building change detection for remote sensing images has become a topical issue because carrying out the task manually is time consuming and tedious <ref type="bibr" target="#b3">[4]</ref>. Therefore, there is a crucial need to investigate efficient building change detection algorithms for remote sensing images.</p><p>In general, the traditional change detection process consists of three major steps: preprocessing, change detection technique selection, and accuracy assessment. Moreover, these methods typically follow one of two approaches <ref type="bibr" target="#b4">[5]</ref>: pixel-based or object-based methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Pixel-based methods are mainly used in large-scale change detection with low or medium resolution images (e.g., MODIS), while object-based methods are more popular for HR or VHR images (e.g., QuickBird, GeoEye-1, WorldView-1/2, Aerial imagery), because the high-frequency components in the VR/VHR images cannot be fully represented by pixel-based methods. These methods have been fully developed in various scenarios in the past years; however, the features used by change detection algorithms are almost hand crafted and so are weak in image representation <ref type="bibr" target="#b10">[11]</ref>. The features are also susceptible to the process in the preprocessing stage, such as those involved in radiometric correction, geometric correction, and image registration. Besides this, the effects of changes in the appearance of objects caused by different photographic angles can be alleviated by orthorectification, but this also brings new problems affecting accurate detection. In high-resolution orthorectified images, the displacement of buildings (especially high-rise buildings) mainly caused by rectifications with a digital elevation model (DEM) and the poor alignment caused by the displacement can lead to many false positive changes. In addition, as the spatial resolution of satellite images increases, the accuracy of image registration tends to worsen <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Generally, the overall framework of the change detection technique can be summarized as feature extractions and change decisions, which is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Considering the facts described above, it is not sufficient to obtain the building changes with the 2D information delivered by satellite images, in which many irrelevant changes are mixed with the desired changes. Recently, a lot of research has focused on improving the precision of building change detection. For example, when taking the features extended into 3D space, i.e., the height information, which is free of illumination variations and perspective distortions <ref type="bibr" target="#b11">[12]</ref>, building extraction and change detection accuracy is improved <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, with the Lidar system expansion, some approaches using laser scanning data have been making headway <ref type="bibr" target="#b14">[15]</ref>. However, when it comes to large and remote areas, the data is either low frequency, or hard or even impossible to acquire. In addition, owing to breakthroughs in dense image matching (DIM) techniques, the availability of image-based 3D information <ref type="bibr" target="#b15">[16]</ref> has greatly increased; this field being known as DSM (digital surface model)-assisted building change detection <ref type="bibr" target="#b16">[17]</ref>. However, the relatively low quality Considering the facts described above, it is not sufficient to obtain the building changes with the 2D information delivered by satellite images, in which many irrelevant changes are mixed with the desired changes. Recently, a lot of research has focused on improving the precision of building change detection. For example, when taking the features extended into 3D space, i.e., the height information, which is free of illumination variations and perspective distortions <ref type="bibr" target="#b11">[12]</ref>, building extraction and change detection accuracy is improved <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, with the Lidar system expansion, some approaches using laser scanning data have been making headway <ref type="bibr" target="#b14">[15]</ref>. However, when it comes to large and remote areas, the data is either low frequency, or hard or even impossible to acquire. In addition, owing to breakthroughs in dense image matching (DIM) techniques, the availability of image-based 3D information <ref type="bibr" target="#b15">[16]</ref> has greatly increased; this field being known as DSM (digital surface model)-assisted building change detection <ref type="bibr" target="#b16">[17]</ref>. However, the relatively low quality of the DSMs from satellite data, which is strongly dependent on image matching technology, is still a major obstacle for detection accuracy.</p><p>Recently, the excitement around deep convolutional neural networks (CNN) has been tremendous, with successful applications in many real-world vision tasks, such as object detection <ref type="bibr" target="#b17">[18]</ref>, image classification <ref type="bibr" target="#b18">[19]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20]</ref>, as well as change detection <ref type="bibr" target="#b20">[21]</ref>. However, remote sensing applications present some new challenges for deep learning regarding multimodal and multisource data <ref type="bibr" target="#b21">[22]</ref>. Thanks to learned feature representations, which are more robust to appearance and shape variations, significant performance improvements have been achieved. The researches in the field of building change detection can be divided into two: (1) post-classification-based methods; and (2) direct-detection-based methods. The first methods involve first extracting the buildings from the images at a different time in the same area, and then obtaining the changed buildings by comparing the extracted building maps. Furthermore, if applying the post-classification-based methods, the displacement of the objects becomes less important, and changes are found by verifying whether the two images contain the same object or not <ref type="bibr" target="#b22">[23]</ref>. However, a high accuracy of building extraction is required, which is a hard task in itself and can lead to accumulated errors. The second set of methods are based on an end-to-end framework and have been successfully used to identify building changes. They avoid some of the weaknesses of classic methods, especially in dense urban areas. Rodrigo has trained an end-to-end Siamese architecture from scratch using a fully convolutional network, and the result surpassed the state-of-the-art methods in change detection, both in accuracy and in inference speed without any post-processing <ref type="bibr" target="#b23">[24]</ref>. As mentioned above, the displacement of the buildings in the orthorectified images is a major challenge for most end-to-end building change detection methods. However, most current methods do not take the building displacement into account and ignore the correlation between the image pairs. Lebedev proposed a specially modified generative adversarial network (GAN) architecture based on pix2pix for automatic change detection in season-varying remote sensing images. In this research, object shift was considered, which is crucial for the buildings in orthorectified images <ref type="bibr" target="#b24">[25]</ref>.</p><p>Despite the wide availability of CNN, it lacks large amounts of available corresponding change annotations to provide training data, which is necessary to train a reliable change detector in a supervised approach. Focusing on this issue, many recent researches explore alternatives, such as training a weak supervised network <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, applying an unsupervised approach <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, or even focusing on noisy data <ref type="bibr" target="#b27">[28]</ref>. However, studies on building change detection mainly concentrate on either two-stage detection accompanied with building detection or one-stage without taking the displacement of the buildings into account.</p><p>Recently, weakly supervised approaches have been proposed in an attempt to compensate for the dependence on the large annotated data, such as training with synthetic data obtained by a given geometric transformation <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. However, detecting building changes with these methods has some restrictions: either an accurate position of the change cannot be given, or information of an independent building is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.">Attention Mechanism</head><p>The so-called attention is a way to observe the world and imitate the mechanism of humans <ref type="bibr" target="#b34">[35]</ref>. Recently, it has been demonstrated to be a simple but effective tool for improving the representation ability of CNN through reweighting of the feature maps; this is achieved using spatial attention and channel attention to scale the features which are meaningful or useless <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>.</p><p>Channelwise attention. In order to improve the discriminant of the abstract feature, the channels of high-level features may number in the thousands (in which there inevitably exists redundant information), and each of them can be regarded as a class-specific response <ref type="bibr" target="#b36">[37]</ref>. To exploit the interdependencies between these maps, channelwise attention was created to emphasize the channels that are relatively informative and focus on the meaningful input. Through obtaining the channel relationship matrix, the self-attention channel weight of the original feature map can be further calculated, helping to boost feature discriminability <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Spatialwise attention. In view of the fact that the low-level features usually contain a large number of details and the receptive field of the convolution layer in traditional FCN (full convolutional network) is limited, if the network only establishes a pixel relationship in the local neighborhood, it can easily lead to unsatisfactory results. In addition, the studies on how effectively obtained long-range dependencies without using deeply stacking convolution layers are more attractive. Instead of considering all positions, spatial attention is introduced to find the relationships of the position and highlight the meaningful region. To take full advantage of the information of the features, more and more researchers are preferring to combine the spatialwise attention and channelwise attention <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Co-Attention mechanism. More recently, in order to understand the fine-grained relationship and mine the underlying correlations between different modalities, co-attention mechanisms have been widely studied in vision-and-language tasks, such as in visual question answering (VQA) <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>. In the computer vision area, Lu was inspired by the above-mentioned works and built the co-attention module to capture the coherence between video frames and effectively suppress the current alternatives <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.">Semantic Correspondence Mechanism</head><p>In general, the issue of change detection can be attributed to finding and matching pairs of images <ref type="bibr" target="#b49">[50]</ref><ref type="bibr">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. The rule of the method is based on the idea that by determining whether a similar semantic object in the second image exists or not to express unchanged and changed. Therefore, finding the correspondence of the point has long been one of the fundamental problems in the field of computer vision and photogrammetry. Traditional methods have been quite successful; they usually employed hand-crafted descriptors (e.g., Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b52">[53]</ref>, Histogram of Oriented Gradients (HOG) [54], Oriented FAST and Rotated BRIEF (ORB) <ref type="bibr">[55]</ref>) to find the key point correspondences based on minimizing an empirical matching strategy and then rejecting the outliers with a geometric match model. Afterwards, several studies began to consider trainable descriptors <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> with CNN. However, these methods heavily depended on predefined features of sparse points and none of the models could be directly used for semantic alignment <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b57">58]</ref>. Given the recent success of using end-to-end CNN in various tasks, many approaches for semantic matching have been proposed with promising results <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b58">59]</ref>. However, these methods also suffer from the same limitations as many other machine learning tasks. Therefore, to achieve satisfactory results, large-scale and diverse training data are required, which is labor intensive and time consuming.</p><p>In this paper, we propose a novel framework for building change detection, using satellite orthorectified images to model the complex change relationships of the buildings with displacement in the scene. The main contributions of this paper can be summarized as follows:</p><p>(1) We introduce a co-attention module which can deal with the displacement of buildings in orthoimages to enhance the feature representations and further mine the correlations therein. Meanwhile, we fuse the semantic and context information of the feature using a context fusion strategy; (2) We provide a new satellite dataset for building change detection frameworks covering various sensors, and verify its effectiveness by conducting extensive experiments; (3) We propose an effective Siamese building change detection framework and make some improvements. Moreover, we train our model using two different datasets. The proposed method shows superior performance: it can directly obtain pixel-level predictions without any other post-processing techniques.</p><p>The structure of this paper is organized as follows: Section 2 describes the datasets and the proposed method of this paper. The experimental results and accuracy assessments are presented in Section 3. Section 4 presents the discussion. Finally, the conclusion of this paper is summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>In this section, we provide the formulation of our method to detect building changes. Firstly, we introduce the datasets used in our study in Section 2.1. The description of our network, pyramid feature-based attention-guided Siamese network (PGA-SiamNet), is presented in Section 2.2. Finally, in Section 2.3, the implementation of the experiment is described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>In this paper, in order to train the proposed network and evaluate its performance, we adopted two different building change detection datasets, namely dataset I (DI) and dataset II (DII). The first dataset (DI) is the Wuhan University (WHU) building change detection dataset <ref type="bibr" target="#b59">[60]</ref> which covers Christchurch, New Zealand, and contains two scenes acquired at the same location in 2012 and 2016, with the semantic labels of the buildings and the change detection labels. The dataset is made up of aerial imagery data with a 0.075 m spatial resolution.</p><p>We named the DII dataset the earth vision-change detection (EV-CD building) dataset. The dataset was labeled by us and is extremely challenging. Moreover, soon it will be available to the public. The dataset is much more complex than the DI dataset and is made up of satellite imagery data instead of aerial imagery data. The dataset consists of data from a variety of sensors with different spatial resolution ranges from 0.2 to 2 m and contains several cities in the south of China. In addition, there are many high-rise buildings with large displacements. Figure <ref type="figure" target="#fig_2">2</ref> shows part of the DI and DII datasets with the building changes labeled by vectorized polygons. Figure <ref type="figure" target="#fig_4">2a</ref> is the DI dataset, Figure <ref type="figure" target="#fig_4">2b</ref>,c belong to the DII dataset. The zoomed-in images in the right of Figure <ref type="figure" target="#fig_2">2</ref> show that the buildings in dataset DII are more diverse than those in dataset DI. In addition, dataset DII has more high-rise buildings with unavoidable displacement, which is what we were focusing on all along.</p><p>Remote Sens. 2020, 12, 484 5 of 21 Section 3. Section 4 presents the discussion. Finally, the conclusion of this paper is summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>In this section, we provide the formulation of our method to detect building changes. Firstly, we introduce the datasets used in our study in Section 2.1. The description of our network, pyramid feature-based attention-guided Siamese network (PGA-SiamNet), is presented in Section 2.2. Finally, in Section 2.3, the implementation of the experiment is described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>In this paper, in order to train the proposed network and evaluate its performance, we adopted two different building change detection datasets, namely dataset I (DI) and dataset II (DII). The first dataset (DI) is the Wuhan University (WHU) building change detection dataset <ref type="bibr" target="#b59">[60]</ref> which covers Christchurch, New Zealand, and contains two scenes acquired at the same location in 2012 and 2016, with the semantic labels of the buildings and the change detection labels. The dataset is made up of aerial imagery data with a 0.075 m spatial resolution.</p><p>We named the DII dataset the earth vision-change detection (EV-CD building) dataset. The dataset was labeled by us and is extremely challenging. Moreover, soon it will be available to the public. The dataset is much more complex than the DI dataset and is made up of satellite imagery data instead of aerial imagery data. The dataset consists of data from a variety of sensors with different spatial resolution ranges from 0.2 to 2 m and contains several cities in the south of China. In addition, there are many high-rise buildings with large displacements. Figure <ref type="figure" target="#fig_2">2</ref> shows part of the DI and DII datasets with the building changes labeled by vectorized polygons. Figure <ref type="figure" target="#fig_4">2a</ref> is the DI dataset, Figure <ref type="figure" target="#fig_4">2b</ref>,c belong to the DII dataset. The zoomed-in images in the right of Figure <ref type="figure" target="#fig_2">2</ref> show that the buildings in dataset DII are more diverse than those in dataset DI. In addition, dataset DII has more high-rise buildings with unavoidable displacement, which is what we were focusing on all along.</p><p>(a)  For the two datasets, we divided all the images into tiles of 512 × 512 pixels, with both the overlay of the width and height being 200 pixels, and finally split each of the two datasets into training sets, validation sets, and test sets randomly in a ratio of 7:1:2. Table <ref type="table" target="#tab_0">1</ref> shows the general information of the two datasets used in our experiments, including the ground sample distance (GSD), source, pixel size of the tile, and the image number to training, validation, and test sets.  For the two datasets, we divided all the images into tiles of 512 × 512 pixels, with both the overlay of the width and height being 200 pixels, and finally split each of the two datasets into training sets, validation sets, and test sets randomly in a ratio of 7:1:2. Table <ref type="table" target="#tab_0">1</ref> shows the general information of the two datasets used in our experiments, including the ground sample distance (GSD), source, pixel size of the tile, and the image number to training, validation, and test sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Problem Description</head><p>PGA-SiamNet was constructed as a Siamese network, with an encoder-decoder structure. The co-attention module setting at the end of the encoder learns the correlation between the deep features of the input image pair; this enables the PGA-SiamNet to find the objects with displacement in other images, which is vital to building change detection. The pyramid change module helps the network to discover the object changes of various sizes and so give better results. Specifically, context information is important for the object in complex scenes, thus aggregating long-range contextual information is useful to improve the feature representation for building change detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Architecture Overview</head><p>The proposed building change detection network is a Siamese network, following the encoder-decoder architecture as shown in Figure <ref type="figure" target="#fig_5">3</ref>. In particular, we employed the well-known VGG16 as the backbone to encode the features of the image pairs to be detected, with each branch sharing the weight. We built a network with the change residual (CR) module for the two-input feature but without any attention mechanism as the baseline; this can be seen in the blue dashed box; the yellow box is the change residual (CR) module.</p><p>Remote Sens. 2020, 12, 484 7 of 21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Problem Description</head><p>PGA-SiamNet was constructed as a Siamese network, with an encoder-decoder structure. The co-attention module setting at the end of the encoder learns the correlation between the deep features of the input image pair; this enables the PGA-SiamNet to find the objects with displacement in other images, which is vital to building change detection. The pyramid change module helps the network to discover the object changes of various sizes and so give better results. Specifically, context information is important for the object in complex scenes, thus aggregating long-range contextual information is useful to improve the feature representation for building change detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Architecture Overview</head><p>The proposed building change detection network is a Siamese network, following the encoderdecoder architecture as shown in Figure <ref type="figure" target="#fig_5">3</ref>. In particular, we employed the well-known VGG16 as the backbone to encode the features of the image pairs to be detected, with each branch sharing the weight. We built a network with the change residual (CR) module for the two-input feature but without any attention mechanism as the baseline; this can be seen in the blue dashed box; the yellow box is the change residual (CR) module. Thereafter, we introduced attention modules to enrich the CR module. For example, by increasing the receptive field to extract a different scale of feature information, we applied an atrous spatial pyramid pooling (ASPP) module for the deepest level feature of the encoder. We conducted ablation studies for comparison by modifying our network with proposed modules, as is discussed in Section 3.1. To emphasize the useful information of the deep features with 512 channels, the channelwise attention is used for layer 4, 5, and 6. Similarly, the shallow features with rich position information are optimized with spatialwise attention. Finally, a co-layer aggregation (CLA) module was used to aggregate the low-level and high-level features, thus fusing the semantic and the context information. Thereafter, we introduced attention modules to enrich the CR module. For example, by increasing the receptive field to extract a different scale of feature information, we applied an atrous spatial pyramid pooling (ASPP) module for the deepest level feature of the encoder. We conducted ablation studies for comparison by modifying our network with proposed modules, as is discussed in Section 3.1. To emphasize the useful information of the deep features with 512 channels, the channelwise attention is used for layer 4, 5, and 6. Similarly, the shallow features with rich position information are optimized with spatialwise attention. Finally, a co-layer aggregation (CLA) module was used to aggregate the low-level and high-level features, thus fusing the semantic and the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Co-Attention Module</head><p>The first module of PGA-SiamNet is a co-attention block with an elegant differentiable attention based on a correlation network, which takes in deep feature representations from an image pair as inputs and outputs for a correlation map. If the image pairs contain common objects and therefore belong to the unchanged category, the features at the locations of the shared objects exhibit similar characteristics. Therefore, inspired by the co-attention mechanism, which discriminates objects from video, the co-attention block was added to the proposed network to identify the changes.</p><p>The neighborhood consensus module <ref type="bibr" target="#b60">[61]</ref> is used to obtain correlations of the two given features f a and f b , because it has already been applied and achieved superior performance in previous research <ref type="bibr" target="#b58">[59]</ref>. COSNet <ref type="bibr" target="#b61">[62]</ref> proposed another method which uses an affinity matrix to denote co-attention, and so to mine the correlations through adding a weight matrix and verifying the proposed three matrix styles by experiments. In the paper, the co-attention style is exploited to obtain the correlation map like COSNet, which is showed in Figure <ref type="figure" target="#fig_7">4</ref> with the blue dashed box. The correlation map, referred to as affinity matrix S R (h×w)×(h×w) between f a and f b , is derived from</p><formula xml:id="formula_0">S = f T b W f a<label>(1)</label></formula><formula xml:id="formula_1">W = P -1 DP (2)</formula><p>where f a R C × (h × w) and f b R C × (h × w) are the features of the input image pair, the two features are obtained by the encoder of the network, W R C × C is a weight matrix, P is an invertible matrix, and D is a diagonal matrix, and h and w indicate the height and width of the input features, respectively. Then, a softmax function was used to normalize S by column-wise and row-wise.</p><p>Remote Sens. 2020, 12, 484 8 of 21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Co-Attention Module</head><p>The first module of PGA-SiamNet is a co-attention block with an elegant differentiable attention based on a correlation network, which takes in deep feature representations from an image pair as inputs and outputs for a correlation map. If the image pairs contain common objects and therefore belong to the unchanged category, the features at the locations of the shared objects exhibit similar characteristics. Therefore, inspired by the co-attention mechanism, which discriminates objects from video, the co-attention block was added to the proposed network to identify the changes.</p><p>The neighborhood consensus module <ref type="bibr" target="#b60">[61]</ref> is used to obtain correlations of the two given features 𝑓 𝑎 and 𝑓 𝑏 , because it has already been applied and achieved superior performance in previous research <ref type="bibr" target="#b58">[59]</ref>. COSNet <ref type="bibr" target="#b61">[62]</ref> proposed another method which uses an affinity matrix to denote coattention, and so to mine the correlations through adding a weight matrix and verifying the proposed three matrix styles by experiments. In the paper, the co-attention style is exploited to obtain the correlation map like COSNet, which is showed in Figure <ref type="figure" target="#fig_7">4</ref> with the blue dashed box. The correlation map, referred to as affinity matrix 𝑆𝜖𝑅 (ℎ×𝑤)×(ℎ×𝑤) between 𝑓 𝑎 and 𝑓 𝑏 , is derived from</p><formula xml:id="formula_2">𝑆 = 𝑓 𝑏 𝑇 𝑊𝑓 𝑎<label>(1)</label></formula><formula xml:id="formula_3">𝑊 = 𝑃 -1 𝐷𝑃<label>(2)</label></formula><p>where 𝑓 𝑎 𝜖𝑅 𝐶 × (ℎ × 𝑤) and 𝑓 𝑏 𝜖𝑅 𝐶 × (ℎ × 𝑤) are the features of the input image pair, the two features are obtained by the encoder of the network, 𝑊𝜖𝑅 𝐶 × C is a weight matrix, P is an invertible matrix, and 𝐷 is a diagonal matrix, and h and w indicate the height and width of the input features, respectively. Then, a softmax function was used to normalize S by column-wise and row-wise. 𝑆 𝑐 = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑆), 𝑆 𝑟 = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑆 𝑇 ) (3)</p><formula xml:id="formula_4">𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑆 𝑖 ) = 𝑒 𝑆 𝑖 ∑ 𝑒 𝑆 𝑖 ℎ×𝑤 𝑖<label>(4)</label></formula><p>where 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(•) is to normalize the correlation map S ; 𝑆 𝑐 𝜖𝑅 ℎ×𝑤 and 𝑆 𝑟 𝜖𝑅 ℎ×𝑤 are the normalization of S by column-wise and row-wise, respectively. 𝑆 𝑐 represents the relevance of each feature in 𝑓 𝑎 to the feature in 𝑓 𝑏 ; Similarly, 𝑆 𝑟 is the relevance of each feature in 𝑓 𝑏 to the feature in 𝑓 𝑎 . 𝑆 𝑖 is the 𝑖-th feature of the 𝑆.</p><p>Then the input feature 𝑓 𝑎 and 𝑓 𝑏 can be computed as follows:</p><formula xml:id="formula_5">𝑓 𝑎 ′ = 𝑓 𝑎 ⊗ 𝑆 𝑐 = [𝑓 𝑎 1 , 𝑓 𝑎 2 , … 𝑓 𝑎 𝑖 , 𝑓 𝑎 ℎ × 𝑤 ] ∈ 𝑅 𝑐 × (ℎ × 𝑤)<label>(5)</label></formula><formula xml:id="formula_6">𝑓 𝑏 ′ = 𝑓 𝑏 ⊗ 𝑆 𝑟 = [𝑓 𝑏 1 , 𝑓 𝑏 2 , … 𝑓 𝑏 𝑖 , 𝑓 𝑏 ℎ × 𝑤 ] ∈ 𝑅 𝑐 × (ℎ × 𝑤)<label>(6)</label></formula><p>where 𝑓 𝑎 𝑖 , 𝑓 𝑏 ′ denote the 𝑖 -th column of 𝑓 𝑎 ′ and 𝑓 𝑏 ′ , and the operator ⊗ represents elementwise multiplication. S c = so f tmax(S), S r = so f tmax S T (3)</p><formula xml:id="formula_7">so f tmax S i = e S i h×w i e S i<label>(4)</label></formula><p>where so f tmax(•) is to normalize the correlation map S; S c R h×w and S r R h×w are the normalization of S by column-wise and row-wise, respectively. S c represents the relevance of each feature in f a to the feature in f b ; Similarly, S r is the relevance of each feature in f b to the feature in f a . S i is the i-th feature of the S.</p><p>Then the input feature f a and f b can be computed as follows:</p><formula xml:id="formula_8">f a = f a ⊗ S c = f 1 a , f 2 a , . . . f i a , f h × w a ∈ R c × (h × w)<label>(5)</label></formula><formula xml:id="formula_9">f b = f b ⊗ S r = f 1 b , f 2 b , . . . f i b , f h × w b ∈ R c × (h × w)<label>(6)</label></formula><p>where f i a , f b denote the i-th column of f a and f b , and the operator ⊗ represents elementwise multiplication. Furthermore, an attention gate was followed to weight the information of the pair features. The gate is composed of one convolution layer with the kernel size being 1.</p><p>In the end, the features are concatenated and fed to a multi-layer perceptron (MLP) to obtain a new representation about the correlation map. To avoid parameter excessive, the MLP is composed of three convolution layers, and the kernel size of each layer is 1, 3, and 1, respectively. After obtaining the common object from the two inputs of the correlation map by MLP, a linear transformation is used to compute the change information. In short, the changed feature f d is calculated as follows:</p><formula xml:id="formula_10">f = concat f a , f b<label>(7)</label></formula><formula xml:id="formula_11">A( f ) = σ(MLP( f )) = σ(W 2 (W 1 (W 0 ( f ))))<label>(8)</label></formula><formula xml:id="formula_12">f d = (1 + A( f )) × f d (9) σ(x) = 1 1 + e -x<label>(10)</label></formula><p>where σ denotes the sigmoid function,</p><formula xml:id="formula_13">W 0 R 1 × 1 × C × C/r , W 1 R 3 × 3 × C/r × C/r</formula><p>, and</p><formula xml:id="formula_14">W 2 R 1 × 1 × C/r × C/r</formula><p>, r is the reduction ratio of the channel and equals two in the paper, f d represents the output of the change residual (CR) module. Note that, before input to the MLP, the feature f should be normalized to [0,1] by a sigmoid function. Figure <ref type="figure" target="#fig_7">4</ref> depicts the computation process of the changed feature with co-attention module and the module is showed in the blue dashed box. Detailed experiments were conducted to compare the effects of the module during the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Co-Layer Aggregation Module</head><p>Recent studies have shown that the high-level layers encode abundant contextual and global information but lost fine spatial information, while the low-level layers are the opposite. Therefore, by adopting layer aggregation to merge the different level features with various details, a good performance may be obtained <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. In this paper, we added a co-layer aggregation (CLA) module to the proposed network to weight the low-level features in order to enhance the change information.</p><p>The encoder of our model contains six layers, we chose the first three layers: f l f 1 , f 2 , f 3 as the low-level features, and the last layer weighted by co-attention: f h f 6 as the high-level feature to perform the operation. As show in Figure <ref type="figure" target="#fig_9">5</ref>, to merge both spatial and channelwise information, the SE block <ref type="bibr" target="#b37">[38]</ref> was firstly applied to both the shallow and the deep features. After being given the transformed features, we forwarded the transformed high-level feature to a global pooling and two convolutions to get a global attention, which can be used to enhance the context representation of the low-level feature.</p><p>Remote Sens. 2020, 12, 484 9 of 21 Furthermore, an attention gate was followed to weight the information of the pair features. The gate is composed of one convolution layer with the kernel size being 1.</p><p>In the end, the features are concatenated and fed to a multi-layer perceptron (MLP) to obtain a new representation about the correlation map. To avoid parameter excessive, the MLP is composed of three convolution layers, and the kernel size of each layer is 1, 3, and 1, respectively. After obtaining the common object from the two inputs of the correlation map by MLP, a linear transformation is used to compute the change information. In short, the changed feature 𝑓 𝑑 ′ is calculated as follows:</p><formula xml:id="formula_15">𝑓 = 𝑐𝑜𝑛𝑐𝑎𝑡(𝑓 𝑎 ′ , 𝑓 𝑏 ′ )<label>(7)</label></formula><formula xml:id="formula_16">𝐴(𝑓) = 𝜎(𝑀𝐿𝑃(𝑓)) = 𝜎(𝑊 2 (𝑊 1 (𝑊 0 (𝑓))))<label>(8)</label></formula><formula xml:id="formula_17">𝑓 𝑑 ′ = (1 + 𝐴(𝑓)) × 𝑓 𝑑<label>(9)</label></formula><formula xml:id="formula_18">𝜎(𝑥) = 1 1 + 𝑒 -𝑥<label>(10)</label></formula><p>where 𝜎 denotes the sigmoid function,</p><formula xml:id="formula_19">𝑊 0 𝜖𝑅 1 × 1 × 𝐶 × 𝐶 𝑟 ⁄ , 𝑊 1 𝜖𝑅 3 × 3 × 𝐶 𝑟 ⁄ × 𝐶 𝑟 ⁄ , and 𝑊 2 𝜖𝑅 1 × 1 × 𝐶 𝑟 ⁄ × 𝐶 𝑟 ⁄ ,</formula><p>r is the reduction ratio of the channel and equals two in the paper, 𝑓 𝑑 represents the output of the change residual (CR) module. Note that, before input to the MLP, the feature 𝑓 should be normalized to [0,1] by a sigmoid function. Figure <ref type="figure" target="#fig_7">4</ref> depicts the computation process of the changed feature with co-attention module and the module is showed in the blue dashed box. Detailed experiments were conducted to compare the effects of the module during the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Co-Layer Aggregation Module</head><p>Recent studies have shown that the high-level layers encode abundant contextual and global information but lost fine spatial information, while the low-level layers are the opposite. Therefore, by adopting layer aggregation to merge the different level features with various details, a good performance may be obtained <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. In this paper, we added a co-layer aggregation (CLA) module to the proposed network to weight the low-level features in order to enhance the change information.</p><p>The encoder of our model contains six layers, we chose the first three layers: 𝑓 𝑙 𝜖{𝑓 1 , 𝑓 2 , 𝑓 3 } as the low-level features, and the last layer weighted by co-attention : 𝑓 ℎ 𝜖{𝑓 6 } as the high-level feature to perform the operation. As show in Figure <ref type="figure" target="#fig_9">5</ref>, to merge both spatial and channelwise information, the SE block <ref type="bibr" target="#b37">[38]</ref> was firstly applied to both the shallow and the deep features. After being given the transformed features, we forwarded the transformed high-level feature to a global pooling and two convolutions to get a global attention, which can be used to enhance the context representation of the low-level feature.  Finally, the origin low feature was added into the enhanced one as a residual block. In this way, the shallow features are refined by correlation if they merge with the changed feature produced by co-attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5.">Pyramid Change Module</head><p>To make full use of the effective receptive field at each level, the decoder consists of a pyramid of features f 1 c , f 2 c , . . . f N c , as show in Figure <ref type="figure" target="#fig_11">6</ref>, which is designed to find the building changes at different scales in the images. At each scale, the feature from the previous scale is unsampled and added to the changed feature f d generated from change residual (CR) module. Then, the result is fed into a convolution layer, with the kernel size being 1. After performing the same steps for all the scales, the results from each scale were concatenated together, and then fed into a convolution layer; the output is the change map. Referring to the classic feature pyramid method, FPN (feature pyramid network) <ref type="bibr" target="#b64">[65]</ref> iteratively merges features with a top-down mechanism until the resolution of the last layer recover to the original input. We fused the changed feature in a top-down pathway in order to catch the change information with different sizes.</p><p>Remote Sens. 2020, 12, 484 10 of 21</p><p>Finally, the origin low feature was added into the enhanced one as a residual block. In this way, the shallow features are refined by correlation if they merge with the changed feature produced by co-attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5.">Pyramid Change Module</head><p>To make full use of the effective receptive field at each level, the decoder consists of a pyramid of features {𝑓 𝑐 1 , 𝑓 𝑐 2 , … 𝑓 𝑐 𝑁 }, as show in Figure <ref type="figure" target="#fig_11">6</ref>, which is designed to find the building changes at different scales in the images. At each scale, the feature from the previous scale is unsampled and added to the changed feature 𝑓 𝑑 generated from change residual (CR) module. Then, the result is fed into a convolution layer, with the kernel size being 1. After performing the same steps for all the scales, the results from each scale were concatenated together, and then fed into a convolution layer; the output is the change map. Referring to the classic feature pyramid method, FPN (feature pyramid network) <ref type="bibr" target="#b64">[65]</ref> iteratively merges features with a top-down mechanism until the resolution of the last layer recover to the original input. We fused the changed feature in a top-down pathway in order to catch the change information with different sizes. The CR module is shown in Figure <ref type="figure" target="#fig_14">7</ref>. The objective of the module is to obtain the distinctive and discriminative features for the two inputs. As shown in Figure <ref type="figure" target="#fig_15">7c</ref>, the generation starts with the input of the two image features 𝑓 𝑎 and 𝑓 𝑏 , and learns to produce a difference map for the input features. The module merges with two kinds of fusion strategy: elementwise difference and elementwise addition. The elementwise difference is to get the absolute value of their difference (see Figure <ref type="figure" target="#fig_15">7a</ref>) while the elementwise addition is to add the two input features (see Figure <ref type="figure" target="#fig_15">7b</ref>). The CR module learns the addition features (see Figure <ref type="figure" target="#fig_15">7b</ref>) as the residual counterparts, which are added by the difference feature (see Figure <ref type="figure" target="#fig_15">7a</ref>), making the information refinement task easier. The CR module is shown in Figure <ref type="figure" target="#fig_14">7</ref>. The objective of the module is to obtain the distinctive and discriminative features for the two inputs. As shown in Figure <ref type="figure" target="#fig_15">7c</ref>, the generation starts with the input of the two image features f a and f b , and learns to produce a difference map for the input features. The module merges with two kinds of fusion strategy: elementwise difference and elementwise addition. The elementwise difference is to get the absolute value of their difference (see Figure <ref type="figure" target="#fig_15">7a</ref>) while the elementwise addition is to add the two input features (see Figure <ref type="figure" target="#fig_15">7b</ref>). The CR module learns the addition features (see Figure <ref type="figure" target="#fig_15">7b</ref>) as the residual counterparts, which are added by the difference feature (see Figure <ref type="figure" target="#fig_15">7a</ref>), making the information refinement task easier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Implementation Details</head><p>The proposed PGA-SiamNet was implemented using Pytorch; the training procedure used a single NVIDIA RTX 2080 Ti GPU with 11 GB memory. We used a mini-batch of two and the initial learning rate was 10 -4 for the two datasets and decreased linearly according to the number of iteration times. The optimization algorithm to train the network was the adaptive moment estimation (Adam) algorithm <ref type="bibr" target="#b65">[66]</ref>. We regarded the task as a binary segmentation for the final output of the network, which is change or no-change. To measure the performance of the proposed network, the metrics intersection over union (IoU), F1 score, precision, recall, and overall accuracy (OA) were used. Generally, the most meaningful metric was IoU in our research. The imbalance in the two classes, changed and un-changed, resulted in a large value of OA. A large number of unchanged pixels have severely made the calculated results obviously too high. Just as OA shows in Section 3, the value is too high when regarding the unchanged pixels, so it is not a good metric to reflect the accuracy of the results. Taking our focus into consideration, the precision, recall, and F1 were only calculated on the changed pixels. The metrics are defined as follows: </p><formula xml:id="formula_20">𝐼𝑜𝑈 = 𝑇𝑃 𝑇𝑃 + 𝐹𝑃 + 𝐹𝑁<label>(11)</label></formula><formula xml:id="formula_21">𝑂𝐴 = 𝑇𝑃 + 𝑇𝑁 𝑇𝑃 + 𝐹𝑃 + 𝑇𝑁 + 𝐹𝑁<label>(12)</label></formula><formula xml:id="formula_22">𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃 𝑇𝑃 + 𝐹𝑃<label>(13)</label></formula><formula xml:id="formula_23">𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑇𝑃 𝑇𝑃 + 𝐹𝑁<label>(14)</label></formula><formula xml:id="formula_24">𝐹1 = 2 × 𝑃 × 𝑅 𝑃 + 𝑅<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Implementation Details</head><p>The proposed PGA-SiamNet was implemented using Pytorch; the training procedure used a single NVIDIA RTX 2080 Ti GPU with 11 GB memory. We used a mini-batch of two and the initial learning rate was 10 -4 for the two datasets and decreased linearly according to the number of iteration times. The optimization algorithm to train the network was the adaptive moment estimation (Adam) algorithm <ref type="bibr" target="#b65">[66]</ref>. We regarded the task as a binary segmentation for the final output of the network, which is change or no-change. To measure the performance of the proposed network, the metrics intersection over union (IoU), F1 score, precision, recall, and overall accuracy (OA) were used. Generally, the most meaningful metric was IoU in our research. The imbalance in the two classes, changed and un-changed, resulted in a large value of OA. A large number of unchanged pixels have severely made the calculated results obviously too high. Just as OA shows in Section 3, the value is too high when regarding the unchanged pixels, so it is not a good metric to reflect the accuracy of the results. Taking our focus into consideration, the precision, recall, and F1 were only calculated on the changed pixels. The metrics are defined as follows:</p><formula xml:id="formula_25">IoU = TP TP + FP + FN<label>(11)</label></formula><formula xml:id="formula_26">OA = TP + TN TP + FP + TN + FN<label>(12)</label></formula><formula xml:id="formula_27">Precision = TP TP + FP<label>(13)</label></formula><formula xml:id="formula_28">Recall = TP TP + FN<label>(14)</label></formula><formula xml:id="formula_29">F1 = 2 × P × R P + R<label>(15)</label></formula><formula xml:id="formula_30">Kappa = p 0 -p c 1 -p c<label>(16)</label></formula><formula xml:id="formula_31">p 0 = OA<label>(17)</label></formula><formula xml:id="formula_32">p C = (TP + FP)(TP + FN) + (FN + TN)(FP + TN) (TP + FP + TN + FN) 2<label>(18)</label></formula><p>where true positive (TP) indicates the number of pixels correctly classified as changed buildings, true negative (TN) denotes the number of pixels correctly classified as unchanged buildings, false positive (FP) represents the number of pixels misclassified as changed buildings, and false negative (FN) is the number of pixels misclassified as unchanged buildings. In Equation ( <ref type="formula" target="#formula_24">15</ref>), P and R denotes precision and recall, respectively. During the training period, Z-score standardization was firstly used for the multitemporal image pairs. In addition, we trained a convolution neural network to obtain better results by multiple iterations. The binary cross-entropy loss function is a popular and effective solution, so we minimized it to optimize the network. The loss function is calculated as follows:</p><formula xml:id="formula_33">l = -ylogy -(1 -y) log(1 -y ) = -logy , y = 1 -log(1 -y ), y = 0 (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>where y refers to the ground truth and y refers to the predicted result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In this section, some ablation studies are provided in Section 3.1. Then, we compared the results of the proposed method with other methods in Section 3.2. Furthermore, the robustness of the proposed algorithm is proved in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation Study</head><p>In this part, we focus on exploration studies to assess the different components of the network with the two datasets, DI and DII. We trained the proposed baseline network and obtained a satisfactory result, which confirms that our proposed base network is effective. Furthermore, we introduced some attention modules to improve the performance. However, it is difficult to balance the performance on two datasets because of the completely different sensors involved. After a lot of experiments, we verified that the described modules can improve the performance of the network; the results of the metrics are showed in Table <ref type="table" target="#tab_2">2</ref>. Line 2 of Table <ref type="table" target="#tab_2">2</ref> shows that, by adding channelwise and spatialwise attention (CS) and a co-layer aggregation (CLA) module to the base network, we obtained a slight increase in accuracy for the two datasets. After adding the ASPP module to the network, there was a slight decline regarding dataset DI, but a great improvement regarding dataset DII in all areas. The reason for this is that dataset DII consists of results from various sensors and more variable information with multi-scales, while dataset DI is relatively unvaried as regards scales information. Finally, we introduced a co-attention (CoA) module, which is important for the performance when using orthoimages with building displacement. In addition, Line 4 in Table <ref type="table" target="#tab_2">2</ref> further demonstrates the efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparisons with Other Methods</head><p>To evaluate the performance of the proposed architecture, we further compared our method with other recent change detection methods: a deep architecture for detecting changes (ChangeNet) <ref type="bibr" target="#b66">[67]</ref>, a correlated Siamese change detection network (CSCDNet) <ref type="bibr" target="#b67">[68]</ref>, multiple side-output fusion (MSOF) <ref type="bibr" target="#b68">[69]</ref>, dual task constrained deep Siamese convolutional network (DTCDSCN) <ref type="bibr" target="#b69">[70]</ref>, multi-scale fully convolutional early fusion (MSFC-EF) <ref type="bibr" target="#b30">[31]</ref>, deep Siamese multi-scale fully convolutional network (DSMS-FCN) <ref type="bibr" target="#b30">[31]</ref>, fully convolutional early fusion (FC-EF) <ref type="bibr" target="#b23">[24]</ref>, fully convolutional Siamese-difference (FC-Siam-Diff) <ref type="bibr" target="#b23">[24]</ref>, and fully convolutional Siamese-concatenation (FC-Siam-Conc) <ref type="bibr" target="#b23">[24]</ref>. CSCDNet was proposed to train a semantic change detection network with a Streetview dataset; by inserting correlation layers into the network, it can overcome the limitation caused by camera viewpoints, which is a major problem to the end-to-end building change detection task. In this paper, to validate the effect of the layer, we give the comparison of the network with and without the correlation layer, denoted CSCDNet/w and CSCDNet/wo, respectively. 'FC-' is a series of full convolution network and the performance is improved through extracting multiscale features in the decoder, such as MSFC-EF and DSMS-FCN. For a fair comparison, we trained and tested our PGA-SiamNet and other methods with the two available datasets mentioned above and the same parameter settings. The results are shown in Table <ref type="table" target="#tab_3">3</ref>. One thing to be mentioned is that some of the comparison methods are completed with the semantic task, we only took the change detection network as the comparison due to a lack of semantic labels. The results show that PGA-SiamNet easily outperforms other approaches. Simultaneously, the visualized results of the proposed method and the other methods were also compared, as shown in Figure <ref type="figure" target="#fig_17">8</ref>, which contains the first three image pairs are from the DI dataset and the last four pairs are from DII dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Robustness of the Method</head><p>To prove the robustness of the proposed algorithm, we tested on other orthoimages with the model trained on EV-CD building dataset. The image pairs are different with our training samples because they are located in the north of China. As shown in Figure <ref type="figure" target="#fig_19">9</ref>, the acceptable result shows that the proposed method has a great potential for the high-resolution remote sensing orthoimages from various sensors with more training samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Robustness of the Method</head><p>To prove the robustness of the proposed algorithm, we tested on other orthoimages with the model trained on EV-CD building dataset. The image pairs are different with our training samples because they are located in the north of China. As shown in Figure <ref type="figure" target="#fig_19">9</ref>, the acceptable result shows that the proposed method has a great potential for the high-resolution remote sensing orthoimages from various sensors with more training samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>The main goal of the study was to find changes of the buildings on high-resolution remote sensing orthoimages automatically. We first trained an end-to-end framework on the two available datasets. Then, representation of the features was enhanced by attention modules by fusing global context features and local semantic features. Meanwhile, the correlation of the image pairs was considered in the network by co-attention module and co-layer aggregation. Finally, the proposed method obtained better results in our studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Importance of the Proposed Dataset</head><p>Change detection, as a hot topic in the field of remote sensing, has attracted extensive attention and emerged many related datasets. Buildings, as an important man-made object, are often in the spotlight. However, only the public WHU building datasets provide changes for buildings at present. As for satellite imagery, there are almost no available datasets for building change detection. The difficulty of building change detection is the displacement of high-rise buildings. Therefore, we built a building change detection dataset (EV-CD building datasets) with the existing satellite images. In WHU building datasets, the buildings are low with little displacement. Besides this, the buildings are mostly independent in WHU building datasets. However, in the complex cities, the buildings are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>The main goal of the study was to find changes of the buildings on high-resolution remote sensing orthoimages automatically. We first trained an end-to-end framework on the two available datasets. Then, representation of the features was enhanced by attention modules by fusing global context features and local semantic features. Meanwhile, the correlation of the image pairs was considered in the network by co-attention module and co-layer aggregation. Finally, the proposed method obtained better results in our studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Importance of the Proposed Dataset</head><p>Change detection, as a hot topic in the field of remote sensing, has attracted extensive attention and emerged many related datasets. Buildings, as an important man-made object, are often in the spotlight. However, only the public WHU building datasets provide changes for buildings at present. As for satellite imagery, there are almost no available datasets for building change detection. The difficulty of building change detection is the displacement of high-rise buildings. Therefore, we built a building change detection dataset (EV-CD building datasets) with the existing satellite images.</p><p>In WHU building datasets, the buildings are low with little displacement. Besides this, the buildings are mostly independent in WHU building datasets. However, in the complex cities, the buildings are often distributed densely. Since the focus of our study is on the complex cities, it is necessary to build a relevant dataset to promote the research. The experiments show that the dataset is effective. In addition, we will publish the dataset and enlarge it in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Advantages of the Proposed Baseline</head><p>The results of the experiments show that the proposed baseline network is effective and surpasses other methods with the two datasets. There are several advantages of the proposed baseline. We found that adjusting the learning rate according to the number of iteration times is a better way after performing many experiments, and the networks obtain a better performance with a pretrained weight. Given that there are a variety of buildings with different size in the datasets, we took the pyramid feature into consideration to discover the changes with various sizes in the proposed baseline network. Inspired by ResNet <ref type="bibr" target="#b70">[71]</ref>, the proposed change (CR) module is also a key part to get a changed feature using a residual structure. The module can fuse the features from different sources without degradation. In the decoder, the features from each scale are concatenated together. In this way, our network detects the building area accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results Compared with Other Methods</head><p>Some of the networks mentioned above are proposed for street view change detection, such as ChangeNet and CSCDNet. In ChangeNet, the branch of the Siamese contains convolutional neural networks and deconvolutional neural networks. The weights of the two branches are shared and fixed in convolutional neural networks. In the deconvolutional neural networks, the weights of the layers are not fixed. The ChangeNet only concatenates three changed features produced by the Siamese feature extraction network to incorporate both coarse and finer details. However, only combining some outputs of the decoder, the relevance of the two channels is not enough to detect changes with remote sensing images. CSCDNet achieved a better result with a slightly lower accuracy than our baseline, especially with the correlation layer. The correlation layer was utilized to deal with difference in camera viewpoints like the displacement on remote sensing images, but the correlation layer is very time consuming. The architecture of our model is somewhat similar to CSCDNet. We got more change information by CR module and applied a co-attention module to obtain the correlation of the two input features instead of the time-consuming correlation layer. Then a co-layer aggregation module was used to fuse changed features extracted from co-attention module to the shallow features. The aggregated features further improved the representation of the features in the pyramid. DTCDSCN was proposed to complete both change detection and semantic segmentation at the same time, while we only employed the subnetwork of change detection. The model has shallower convolution layers than other models, and it may ignore the multi-scale change. In addition, the proposed improved network increases the receptive field to extract a different scale changed feature using the ASPP module, and it is helpful for building change detection in complex cities such as the EV-CD building dataset. Overall, the models trained with pretrained weight are superior to those without pretrained weight, such as FC-Siam-Diff and DSMS-FCN, in our studies. Finally, an addition experiment demonstrated that the proposed method has a strong robustness by testing on other orthoimages with the model trained on EV-CD building dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed an end-to-end attention-guided Siamese network based on a pyramid feature (PGA-SiamNet) network. It performed excellently on remote sensing orthoimagery for building change detection and yielded better results on complex urban environments when compared with other methods. By using a co-attention mechanism, the method learns to discriminate feature change by capturing the correlation between image pairs. To obtain long-range dependencies effectively, we adopted an attention-guided method. Our experiments on the two available datasets show that our method gives comparable results to other state-of-the-art techniques. Therefore, the modules added to this framework are both independent and can be adopted for building change detection in a convenient way. Meanwhile, the experimental results with the WHU datasets show a better performance than the results with the satellite imagery EV-CD dataset; the complexity and diversity of the contribute to this result, while this type of data is more in our research. Owing to the machine-learning boom, building extraction, which used to be the central problem of traditional building change detection, has become unnecessary. However, the need for large and accurate sample data is still the main concern for deep learning, thus data-independent research is increasingly important, since most data at hand is inadequate and noisy. As regards future studies, on the one hand, we may place more attention on noisy data and one-shot/few-shot learning; on the other hand, it is possible to imagine involving more diverse information, such as using auxiliary DSM information as an object guide, as well as mining more information from the current data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Patents</head><p>At present, we are applying for the patent based on the research results of this paper and the application material has been submitted to China National Intellectual Property Administration (the patent application number is 2020100445918). Moreover, we are waiting for the examination and grant of this patent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The basic framework for change detection.</figDesc><graphic coords="2,145.02,427.01,306.00,148.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The basic framework for change detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Cont.</figDesc><graphic coords="5,81.29,581.86,277.71,111.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The images in dataset DI and dataset DII. In (a), the first two rows show the two scenes in the same area. In (b,c), the first two columns show the two scenes in the same area. The changed buildings are marked with a red polygon in the right images. The last column is the zoomed-in view of area selected by the white box.</figDesc><graphic coords="6,384.13,396.01,124.67,85.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The images in dataset DI and dataset DII. In (a), the first two rows show the two scenes in the same area. In (b,c), the first two columns show the two scenes in the same area. The changed buildings are marked with a red polygon in the right images. The last column is the zoomed-in view of area selected by the white box.</figDesc><graphic coords="6,228.20,299.67,136.05,145.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overview of pyramid feature-based attention-guided Siamese network (PGA-SiamNet Network). CoA indicates the co-attention module.</figDesc><graphic coords="7,100.47,348.67,397.98,204.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overview of pyramid feature-based attention-guided Siamese network (PGA-SiamNet Network). CoA indicates the co-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overview of changed feature map produced by co-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overview of changed feature map produced by co-attention module.</figDesc><graphic coords="8,100.95,348.52,397.88,153.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Co-layer aggregation module.</figDesc><graphic coords="9,208.10,574.87,176.00,181.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Co-layer aggregation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Pyramid change feature decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Pyramid change feature decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) Elementwise difference (b) Elementwise addition (c) Change residual (CR) module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Single scale changed feature generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Single scale changed feature generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results by our proposed method, and comparisons with others. (a) Before images; (b) after images; (c) ground truth; (d) PGA-SiamNet; (e) CSCDNet; (f) MSOF; (g) FC-Siam-Diff.</figDesc><graphic coords="14,90.37,474.02,54.10,54.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results by our proposed method, and comparisons with others. (a) Before images; (b) after images; (c) ground truth; (d) PGA-SiamNet; (e) CSCDNet; (f) MSOF; (g) FC-Siam-Diff.</figDesc><graphic coords="14,150.35,474.02,54.10,54.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Results on other image pairs. (a) Before images; (b) after images; (c) results of the proposed method.</figDesc><graphic coords="15,235.11,376.45,127.55,127.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Results on other image pairs. (a) Before images; (b) after images; (c) results of the proposed method.</figDesc><graphic coords="15,87.70,376.45,127.55,127.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Information of our datasets for experiments.</figDesc><table><row><cell>Datasets</cell><cell>GSD (Meters)</cell><cell>Source</cell><cell>Size (Pixels)</cell><cell>Number (Tiles) (Training/Validation/Test)</cell></row><row><cell>DI(WHU)</cell><cell>0.075</cell><cell>Aerial</cell><cell>512 × 512</cell><cell>691/97/199</cell></row><row><cell>DII(EV-CD)</cell><cell>0.2-2</cell><cell>Satellite</cell><cell>512 × 512</cell><cell>1225/175/350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Information of our datasets for experiments.</figDesc><table><row><cell>Datasets</cell><cell>GSD (Meters)</cell><cell>Source</cell><cell>Size (Pixels)</cell><cell>Number (Tiles) (Training/Validation/Test)</cell></row><row><cell>DI(WHU)</cell><cell>0.075</cell><cell>Aerial</cell><cell>512 × 512</cell><cell>691/97/199</cell></row><row><cell>DII(EV-CD)</cell><cell>0.2-2</cell><cell>Satellite</cell><cell>512 × 512</cell><cell>1225/175/350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation experiments of the methods with different modules (the shadow represents the best results of the basic network with all the proposed modules).</figDesc><table><row><cell></cell><cell cols="2">IoU (%)</cell><cell cols="2">OA (%)</cell><cell cols="2">Recall (%)</cell><cell cols="2">Precision (%)</cell><cell cols="2">F1(%)</cell><cell cols="2">Kappa (%)</cell></row><row><cell>Network</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell></row><row><cell>Baseline</cell><cell>96.52</cell><cell>92.02</cell><cell>99.75</cell><cell>99.63</cell><cell>96.24</cell><cell>90.23</cell><cell>96.89</cell><cell>92.65</cell><cell>96.25</cell><cell>90.67</cell><cell>96.12</cell><cell>90.48</cell></row><row><cell cols="2">+CS+CLA 97.15</cell><cell>92.13</cell><cell>99.77</cell><cell>99.65</cell><cell>96.62</cell><cell>89.42</cell><cell>97.79</cell><cell>93.55</cell><cell>97.09</cell><cell>90.83</cell><cell>96.97</cell><cell>90.65</cell></row><row><cell>+ASPP</cell><cell>97.11</cell><cell>92.52</cell><cell>99.78</cell><cell>99.66</cell><cell>96.91</cell><cell>90.25</cell><cell>97.38</cell><cell>93.83</cell><cell>97.0</cell><cell>91.38</cell><cell>96.88</cell><cell>91.21</cell></row><row><cell>+CoA</cell><cell>97.38</cell><cell>92.73</cell><cell>99.79</cell><cell>99.68</cell><cell>97.01</cell><cell>90.59</cell><cell>97.84</cell><cell>94.01</cell><cell>97.29</cell><cell>91.74</cell><cell>97.17</cell><cell>91.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with other related methods (the lighter shadow represents the best results of other related methods; the darker shadow represents the results of our proposed method).</figDesc><table><row><cell></cell><cell cols="2">IoU (%)</cell><cell cols="2">OA (%)</cell><cell cols="2">Recall (%)</cell><cell cols="2">Precision (%)</cell><cell cols="2">F1(%)</cell><cell cols="2">Kappa (%)</cell></row><row><cell>Network</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell><cell>DI</cell><cell>DII</cell></row><row><cell>ChangeNet</cell><cell>70.80</cell><cell>56.19</cell><cell>96.88</cell><cell>97.44</cell><cell>52.97</cell><cell>21.39</cell><cell>66.99</cell><cell>32.48</cell><cell>57.48</cell><cell>23.46</cell><cell>55.79</cell><cell>22.41</cell></row><row><cell>MSOF</cell><cell>90.84</cell><cell>82.66</cell><cell>99.08</cell><cell>99.20</cell><cell>88.68</cell><cell>71.55</cell><cell>92.45</cell><cell>89.12</cell><cell>89.40</cell><cell>78.20</cell><cell>88.92</cell><cell>77.81</cell></row><row><cell>DTCDSCN</cell><cell>83.55</cell><cell>78.67</cell><cell>98.61</cell><cell>99.11</cell><cell>80.44</cell><cell>64.74</cell><cell>78.28</cell><cell>84.55</cell><cell>78.22</cell><cell>71.2</cell><cell>77.45</cell><cell>70.77</cell></row><row><cell>CSCDNet/w</cell><cell>95.04</cell><cell>87.91</cell><cell>99.63</cell><cell>99.49</cell><cell>94.03</cell><cell>83.15</cell><cell>95.96</cell><cell>90.19</cell><cell>94.66</cell><cell>85.69</cell><cell>94.45</cell><cell>85.43</cell></row><row><cell>CSCDNet/wo</cell><cell>94.68</cell><cell>87.53</cell><cell>99.63</cell><cell>99.45</cell><cell>93.74</cell><cell>81.38</cell><cell>95.63</cell><cell>91.19</cell><cell>94.09</cell><cell>85.13</cell><cell>93.89</cell><cell>84.85</cell></row><row><cell>FC-EF</cell><cell>78.70</cell><cell>67.24</cell><cell>97.98</cell><cell>98.36</cell><cell>71.24</cell><cell>47.71</cell><cell>74.81</cell><cell>57.67</cell><cell>71.43</cell><cell>50.03</cell><cell>70.33</cell><cell>49.26</cell></row><row><cell>FC-Siam-Diff</cell><cell>88.66</cell><cell>80.5</cell><cell>99.0</cell><cell>99.1</cell><cell>85.67</cell><cell>71.73</cell><cell>88.89</cell><cell>79.95</cell><cell>86.11</cell><cell>74.15</cell><cell>85.58</cell><cell>73.71</cell></row><row><cell>FC-Siam-Con</cell><cell>82.08</cell><cell>68.02</cell><cell>98.4</cell><cell>98.43</cell><cell>74.67</cell><cell>47.76</cell><cell>88.72</cell><cell>60.85</cell><cell>76.57</cell><cell>51.47</cell><cell>75.69</cell><cell>50.73</cell></row><row><cell>MSFC-EF</cell><cell>90.72</cell><cell>83.65</cell><cell>99.26</cell><cell>99.29</cell><cell>88.54</cell><cell>79.97</cell><cell>90.31</cell><cell>87.51</cell><cell>88.72</cell><cell>79.69</cell><cell>88.30</cell><cell>79.33</cell></row><row><cell>DSMS-FCN</cell><cell>88.61</cell><cell>83.37</cell><cell>99.12</cell><cell>99.25</cell><cell>88.29</cell><cell>73.01</cell><cell>86.32</cell><cell>89.35</cell><cell>86.09</cell><cell>79.18</cell><cell>85.62</cell><cell>78.81</cell></row><row><cell>Baseline(ours)</cell><cell>96.52</cell><cell>92.02</cell><cell>99.75</cell><cell>99.63</cell><cell>96.24</cell><cell>90.23</cell><cell>96.89</cell><cell>92.65</cell><cell>96.25</cell><cell>90.67</cell><cell>96.12</cell><cell>90.48</cell></row><row><cell>PGA-SiamNet</cell><cell>97.38</cell><cell>92.73</cell><cell>99.79</cell><cell>99.68</cell><cell>97.01</cell><cell>90.59</cell><cell>97.84</cell><cell>94.01</cell><cell>97.29</cell><cell>91.74</cell><cell>97.17</cell><cell>91.57</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The authors sincerely appreciate that academic editors and reviewers give their helpful comments and constructive suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This research was partially supported by National Natural Science Foundation of China, grant number 41771363; the research funding was from Guangzhou Science, Technology and Innovation Commission (201802030008).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: H.J. conceived, conducted, and improved the experiments, and they also wrote the manuscript. X.H. directed and revised this manuscript. K.L. assisted in the experimental verification and revised the manuscript. J.Z. and J.G. labeled the datasets and reviewed the manuscript. M.Z. revised the manuscript. All authors have read and agreed to publish the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Change Detection Algorithm for the Production of Land Cover Change Maps over the European Union Countries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aleksandrowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Turlej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lewi Ński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bochenek</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs6075976</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5976" to="5994" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://earth.esa.int/web/earth-watching/change-detection" />
		<title level="m">Earth Watching</title>
		<imprint>
			<date type="published" when="2019-01-25">25 January 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Onera Satellite Change Detection</title>
		<ptr target="http://dase.grss-ieee.org" />
		<imprint>
			<date type="published" when="2019-05-10">10 May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">D Building Change Detection from High Resolution Aerial Images and Correlation Digital Surface Models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Champion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="197" to="202" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification of the wildland-urban interface: A comparison of pixel-and object-based classifications using high-resolution aerial photography</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cleve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moritz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compenvurbsys.2007.10.001</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Environ. Urban Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="317" to="326" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building Change Detection from Multitemporal High-Resolution Remotely Sensed Images Based on a Morphological Building Index</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2013.2252423</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object-based change detection using correlation image analysis and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tullis</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160601075582</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="399" to="423" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic change detection of buildings in urban environment from very high spatial resolution images using existing geodatabase and prior knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bouziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goïta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2009.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="143" to="153" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geographic Object-Based Image Analysis-Towards a new paradigm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Addink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Queiroz Feitosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Der Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Werff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Coillie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.09.014</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Change Detection Based on Deep Siamese Convolutional Network for Optical Aerial Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2738149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">D change detection-Approaches and applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.09.013</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building Change Detection in Very High Resolution Satellite Stereo Image Time Series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprsannals-III-7-149-2016</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="155" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Change detection of buildings from satellite imagery and lidar data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Malpica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arozarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martínez De Agirre</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2012.725483</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1652" to="1675" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building Change Detection by Combining Lidar data and Ortho Image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLI-B3-669-2016</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">State of the art in high density image matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Spera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nocerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<idno type="DOI">10.1111/phor.12063</idno>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Rec</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="144" to="166" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building Change Detection Based on Satellite Stereo Imagery and Digital Surface Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2013.2240692</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="406" to="417" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual Attention Network for Image Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Change Detection between Multimodal Remote Sensing Data Using Siamese CNN</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2017.2762307</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Change Detection in High Resolution Satellite Images Using an Ensemble of Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<meeting>the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="12" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully Convolutional Siamese Networks for Change Detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE International Conference on Image Processing</title>
		<meeting>the 25th IEEE International Conference on Image Processing<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Change Detection in Remote Sensing Images Using Conditional Adversarial Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">V</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-565-2018</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly Supervised Change Detection in a Pair of Images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep structured network for weakly supervised change detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Guided Anisotropic Diffusion and Iterative Learning for Weakly Supervised Change Detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caye Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised Change Detection in Satellite Images Using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L D</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bosman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1812.05815?context=cs.NE" />
		<imprint>
			<date type="published" when="2019-02-22">22 February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transferred Deep Learning-Based Change Detection in Remote Sensing Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2909781</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="6960" to="6973" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep Siamese Multi-scale Convolutional Network for Change Detection in Multi-temporal VHR Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.11479" />
		<imprint>
			<date type="published" when="2019-07-01">1 July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">WarpNet: Weakly Supervised Matching for Single-View Reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1604.05592" />
		<imprint>
			<date type="published" when="2019-09-18">18 September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic Context Correspondence Network for Semantic Alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02">27 October-2 November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1601.06823" />
		<title level="m">Survey on the Attention Based RNN Model and Its Applications in Computer Vision. Available online</title>
		<imprint>
			<date type="published" when="2019-10-18">18 October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">August 2018</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision (ECCV))</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual Attention Network for Scene Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise Spatial Attention Network for Scene Parsing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">August 2018</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pyramid Attention Network for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.10180" />
		<imprint>
			<date type="published" when="2019-04-18">18 April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><surname>See More</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploiting Feature Context in Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Gather-Excite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference and Workshop on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Conference and Workshop on Neural Information Processing Systems (NeurIPS)<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02">27 October-2 November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Context Encoding for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><surname>Sca-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Coattention Networks for Question Answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Modular Co-Attention Networks for Visual Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</title>
		<author>
			<persName><forename type="first">D.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NeurIPS)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cosegmentation for Object-Based Building Change Detection from High-Resolution Remotely Sensed Images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1587" to="1603" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Siamese Network with Multi-Level Features for Patch-Based Change Detection in Satellite Imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Cor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerekes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</title>
		<meeting>the 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)<address><addrLine>Anaheim, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Urban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGARSS.2018.8518015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the IEEE International Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="2115" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Universal Correspondence Network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference and Workshop on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Conference and Workshop on Neural Information Processing Systems (NeurIPS)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to Find Good Correspondences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moo Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep Semantic Matching with Foreground Detection and Cycle-Consistency</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the 14th Asian Conference on Computer Vision (ACCV)<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-06">2-6 December 2018</date>
			<biblScope unit="page" from="347" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Multisource Building Extraction from an Open Aerial and Satellite Imagery Data Set</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2858817</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neighbourhood Consensus Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NeurIPS)<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Match and Segment: Joint Learning of Semantic Matching and Object Co-Segmentation</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Show</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.05857?context=cs.CV" />
		<imprint>
			<date type="published" when="2019-09-15">15 September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Salient Object Detection via Deep Hierarchical Context Aggregation and Multi-Layer Supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Taiwan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. September 2019</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Salient Object Detection via High-to-Low Hierarchical Context Aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1812.10956" />
		<imprint>
			<date type="published" when="2019-05-25">25 May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>Banff, AL, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ChangeNet: A Deep Learning Architecture for Visual Change Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balamuralidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Weakly Supervised Silhouette-based Semantic Change Detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.11985v1" />
		<imprint>
			<date type="published" when="2019-06-25">25 June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">End-to-End Change Detection for High Resolution Satellite Images Using Improved UNet++</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wanbing</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111382</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1382">2019. 1382</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Building Change Detection for Remote Sensing Images Using a Dual Task Constrained Deep Siamese Convolutional Network Model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.07726?context=cs.CV" />
		<imprint>
			<date type="published" when="2019-10-18">18 October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
