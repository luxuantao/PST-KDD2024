<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolutionary Multiobjective Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julia</forename><surname>Handl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemistry</orgName>
								<orgName type="institution">UMIST</orgName>
								<address>
									<addrLine>Sackville Street</addrLine>
									<postBox>PO Box 88</postBox>
									<postCode>M60 1QD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Knowles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemistry</orgName>
								<orgName type="institution">UMIST</orgName>
								<address>
									<addrLine>Sackville Street</addrLine>
									<postBox>PO Box 88</postBox>
									<postCode>M60 1QD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolutionary Multiobjective Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D459F32692053318DFE2D2F4E877ECF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new approach to data clustering is proposed, in which two or more measures of cluster quality are simultaneously optimized using a multiobjective evolutionary algorithm (EA). For this purpose, the PESA-II EA is adapted for the clustering problem by the incorporation of specialized mutation and initialization procedures, described herein. Two conceptually orthogonal measures of cluster quality are selected for optimization, enabling, for the first time, a clustering algorithm to explore and improve different compromise solutions during the clustering process. Our results, on a diverse suite of 15 real and synthetic data sets -where the correct classes are known -demonstrate a clear advantage to the multiobjective approach: solutions in the discovered Pareto set are objectively better than those obtained when the same EA is applied to optimize just one measure. Moreover, the multiobjective EA exhibits a far more robust level of performance than both the classic k-means and average-link agglomerative clustering algorithms, outperforming them substantially on aggregate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automation of the human ability to recognize patterns in data, and to induce useful hypotheses from them, is the key goal of data-mining. A major branch of this project is the development of methods for unsupervised classification of multi-dimensional data, namely the clustering of data into homogeneous groups: by now a classic AI problem with algorithms dating back to the 60s <ref type="bibr" target="#b14">[15]</ref>. In a broad definition, clustering of data might include the recognition and removal of outliers, the recognition and focusing on key dimensions of the data (i.e. feature selection) and the estimation of the correct number of clusters inherent to the data. In a far more restricted definition, the k-clustering problem (on which we focus here) simply requires us to find a partitioning of a set of data into k disjoint sets such that some objective function operating on this partitioning, and employing a notion of distance in the data space, is optimized. This restricted (but still very broad) problem is NP-complete when stated as a question, and remains NP-complete for many restrictions on the distance functions used and the nature of the objective function, even when k = 2 <ref type="bibr" target="#b2">[3]</ref>.</p><p>Both classic and a vast array of new algorithms for k-clustering exist <ref type="bibr" target="#b11">[12]</ref>. Common to almost all of them is the fact that they optimize either implicitly or explicitly just one measure on the partitioning of the data. For example, kmeans <ref type="bibr" target="#b14">[15]</ref> attempts to minimize the summed variance of points within each cluster from their centroid. Although such a method is very effective on certain sets of data, it is also clear that it will fail to find even very obvious cluster structure in other data sets. This is because variance is only a proxy for (i.e. one aspect of) a more fuzzy 'concept' of true cluster structure. Thus, by focusing on just one aspect of cluster quality, most clustering algorithms can fail catastrophically on certain data-sets: they are not robust to variations in cluster shape, size, dimensionality and other characteristics.</p><p>To combat this problem, practitioners in some fields (where time constraints are secondary) are used to applying several different algorithms to their data, in the hope or expectation that one of them will deliver a good solution. Subsequently, these different partitionings can be tested in the real world: e.g. a biologist with several hypothetical groupings of functionally-related genes can devise experiments that test these alternatives. The idea central to our work is that in such a situation, it may be better to generate alternative solutions using a single algorithm, but one that explicitly optimizes multiple proxy measures of cluster quality: namely, a Pareto multiobjective EA <ref type="bibr" target="#b6">[7]</ref>. This approach may offer greater flexibility and variety in the measures that are used to optimize the clustering, affording higher quality solutions, and, in the process, facilitate greater understanding of the data's structure. In future work we may incorporate feature selection, outlier-removal and determination of k, all within a multiobjective EA framework. However, in this our first paper on multiobjective clustering, we focus on the pivotal question whether this approach can generate objectively high quality solutions.</p><p>Readers familiar with clustering research may notice similarities between our proposed approach and other recent methods. Certainly, several EAs for clustering have been proposed <ref type="bibr">([16, 10, 14, 8]</ref>), though none to our knowledge have used a Pareto multiobjective EA. Other recent work has also used the term 'multiobjective' with regard to clustering <ref type="bibr" target="#b12">[13]</ref>, but the approach was based on using an ensemble of clustering algorithms <ref type="bibr" target="#b17">[18]</ref> and then obtaining a consensus clustering from these, similarly to the EA proposed in <ref type="bibr" target="#b9">[10]</ref>. Our proposed approach, on the other hand, optimizes different objectives explicitly in one clustering algorithm, enabling different tradeoffs to be explored during the clustering process. Its originality derives from this.</p><p>The remainder of the paper is organized as follows. Section 2 describes our multiobjective EA, including our selected representation and operators. The objective functions are discussed in Section 3. Section 4 briefly introduces the test suite and points to supporting material where more information on this can be found. Section 5 details our experimental set-up including comparison of our multiobjective EA to two single-objective versions as well as k-means and average-link agglomerative clustering. Section 6 presents results and Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VIENNA: An EA for Clustering</head><p>A multiobjective evolutionary algorithm (MOEA) for clustering was developed through extensive preliminary experimentation on a diverse set of clustering problems. This algorithm, employing specialized initialization and mutation operators, is called VIENNA (for Voronoi Initialized Evolutionary Nearest-Neighbour Algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PESA-II</head><p>We based VIENNA on the elitist MOEA, PESA-II, described in detail in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Briefly, PESA-II updates, at each generation, a current set of nondominated solutions stored in an external population (of non-fixed but limited size), and uses this to build an internal population of fixed size to undergo reproduction and variation. PESA-II uses a selection policy designed to give equal reproduction opportunities to all regions of the current nondominated front; thus in the clustering application, it should provide a diverse set of solutions trading off different clustering measures. No critical parameters are associated with this 'niched' selection policy, as it uses an adaptive range equalization and normalization of the objectives. PESA-II may be used to optimize any number of objective functions, allowing us to simultaneously optimize several clustering measures, but in this paper we will use just two (conceptually distant) measures as objectives, described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representation Issues</head><p>PESA-II can be applied without changes to the clustering problem, given a suitable representation of a partitioning, and related operators. A number of GA clustering representations have been tried and compared in the literature, with seemingly no clear overall winner <ref type="bibr" target="#b3">[4]</ref>. In the end, we have chosen to use a straightforward representation in which each gene represents a data item, and its allele value represents the label of the cluster to which it is assigned. This means that for any partition, multiple genotypes code for it, i.e. it is a non-injective encoding -normally thought to be undesirable <ref type="bibr" target="#b16">[17]</ref>. This drawback is not serious, however, provided there is not a significant bias or over-representation of certain solutions, and/or we can design operators that work effectively and quickly with this coding. Regarding undesirable bias, the inherent frequency of solutions is free from bias: for every solution that correctly partitions the data into k clusters, there are exactly k! genotypes coding for it. Regarding operators, we have discovered an initialization and mutation operator that work well with this coding, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Initialization Based on Random Voronoi Cells</head><p>In preliminary work not reported here, we investigated an alternative representation for our EA to use, based on optimizing Voronoi cells. This representation was inspired by <ref type="bibr" target="#b15">[16]</ref>, where an EA was used to optimize the location of k cluster 'centres', to minimize overall variance when the data points were assigned to the nearest centre. This GA achieves results similar to (but slightly better than) the k-means algorithm. Our idea was to extend this representation by allowing the EA to use j &gt; k cluster 'centres' (for a partitioning of k clusters) to enable it to cope better with non-hyperspherical, and especially elongated and intertwined, clusters. In our representation, in addition to the location of the j centres, each centre's label is also evolved on the genotype. The kind of clustering solution that this representation affords is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Although this representation performs well with PESA-II correctly configured, we have found it slightly inflexible compared with the direct encoding we have opted for, as well as adding an unwelcome parameter j, to be chosen. However, we found that the Voronoi coding is very effective at generating diverse and high-quality clustering solutions that can be used to 'seed' our direct-coded EA. The initialization that we have found effective, and which we use in all our experiments, is to set j = 2k, and to place the cluster centres uniformly at random in a rectangular polytope centred on the data, and of side-length twice the range of the data, in each objective. The labels associated with each of the j centres is also assigned uniformly at random, from which it is possible to label all of the data items. We then decode this partitioning into our direct coding, and the Voronoi representation is no longer used. This initialization is used for all members of the initial population in VIENNA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Directed Mutation Based on Nearest Neighbours</head><p>We have explored numerous recombination and mutation operators in preliminary investigations not reported here, including Grouping GA-based methods <ref type="bibr" target="#b8">[9]</ref>, as well as multi-parent recombinations based on expectation maximization of an ensemble <ref type="bibr" target="#b17">[18]</ref>. Overall, we have found it very difficult to design operators that enable a GA to explore broadly enough to escape the very strong local attractors found in some problems when optimizing certain objectives (e.g. variance on non-spherical clusters), without descending into a fruitless random search of what is a very large search space, and whilst also enabling small clustering differences to be explored. However, in the end, we have found that a single, deceptively simple, directed mutation operator (and no crossover) is sufficient to drive the search. This mutation operator is applied to every gene with probability p m , which we set to 1/N in all experiments, where N is the size of the data set. When a gene undergoes mutation to a different allele value (i.e. cluster), a number g of other genes are simultaneously 'moved' with it into the same target cluster (and the genotype is updated accordingly). The particular data items that undergo this move are the g nearest neighbours to the data item coded for by the initially mutated gene. The integer g itself is chosen, independently at each mutation event, uniformly at random in 0..N/k. This operator enables very large changes to result from a single mutation, yet constrains them to be 'reasonable' moves that respect local distance relations. On the other hand, very small changes in the clustering solution are also possible. The operator works in linear time since the nearest neighbours of every data item can be pre-computed once at the beginning of the EA's run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Objective Functions for Clustering</head><p>Given a candidate partitioning of the data, numerous 'internal' measures for estimating its quality exist <ref type="bibr" target="#b10">[11]</ref>. These measures are based on intuitive notions of the properties of a desirable partitioning -such as the compactness of clusters and their clear separation.</p><p>In the EA we present in this paper, we optimize two such internal measures, described next, though we have tried several others in our preliminary testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Deviation</head><p>The overall deviation of a clustering solution reflects the overall intra-cluster 'spread' of the data. It is computed as</p><formula xml:id="formula_0">Dev (C ) = C k ∈C i∈C k δ(i, µ k ),</formula><p>where C is the set of all clusters, µ k is the centroid of cluster C k and δ(., .) is the chosen distance function (see Section 4). As an objective, overall deviation should be minimized. Note the relationship to variance (e.g. as used in k-means), which squares the δ(., .) in the sum. In preliminary experiments, we found overall deviation to be preferable to variance for use in our EA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connectivity</head><p>As a second objective function, we propose a new measure, connectivity, which evaluates the degree to which neighbouring datapoints have been placed in the same cluster. It is computed as</p><formula xml:id="formula_1">Conn(C ) = 1 N N i=1 h j=1 x i,nni(j) h , where x r,s = 1 if ∃C k : r, s ∈ C k 0 otherwise, nn i (j)</formula><p>is the jth nearest neighbour of datum i, and h is a parameter determining the number of neighbours that contribute to the connectivity measure. The value of connectivity lies in the interval [0,1], and as an objective, it should be maximized. Connectivity, unlike overall deviation, is relatively indifferent to the shape of clusters, and we have found it robust to the chosen value of h, independently of the data set. It is also fast to compute as the nearest neighbour list can be pre-computed. One drawback of this measure is that trivial attractors, with all, or nearly all, data items placed in the same cluster, exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Sets</head><p>Clustering problems vary greatly along a number of important dimensions. For this reason, it is incumbent on the researcher developing new clustering algorithms to test them on a range of problems that exhibit this variety as much as possible. We use eight synthetic and seven real data sets; the former allow us to control several characteristics in isolation, while the latter help to verify that our results are ultimately meaningful in real applications.</p><p>For the real data, we first normalize each dimension to have a mean of zero and a standard deviation of one, and use the Cosine similarity as distance function. For the synthetic data, the Euclidean distance is used with no prior normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Data</head><p>All eight of our synthetic data sets consist of 500 two-dimensional data items, enabling us to easily visualize the results of a clustering. Pictures and explanations for all these sets are available at <ref type="bibr" target="#b0">[1]</ref> but we briefly describe them below. Note: the synthetic data sets are defined in terms of distributions, and the actual data points are sampled from these, independently, in each successive algorithm run.</p><p>Three of the data sets (Square1, Square3 and Square5) consist of a square arrangement of four clusters of equal size and spread, each cluster being a Gaussian distribution about a central point. The difference between the sets is the degree of overlap of the four clusters. In Square1, the clusters touch but hardly overlap, whereas for Square5 the overlap is so much that there is little density difference moving from one cluster to the next.</p><p>The next three data sets (Sizes1, Sizes3 and Sizes5) are based on Square1, but change the relative cluster sizes (in terms of the number of constituent data items) such that the ratio of the three smaller to the one larger cluster is respectively 2, 6, and 10. Note: the spread of the clusters is unchanged.</p><p>The last two of our synthetic data sets (Smile and Long1) contain different, non-spherically shaped clusters, making it more difficult for methods based on minimizing variance. For pictures of these demanding problems, see <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real Data</head><p>For the real data sets we chose seven from the UCI Machine Learning Repository <ref type="bibr" target="#b1">[2]</ref> to obtain a good variety in data dimensionality, size of the data set, number of clusters and evenness/unevenness of the cluster sizes. The sets we chose are Dermatology, Digits, Iris, Wine, Wisconsin, Zoo and Yeast. They range up to size 3498 and dimension 34, with up to 10 clusters. For complete details also see <ref type="bibr" target="#b0">[1]</ref>. Note: we permute the data in these sets on each algorithm run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In our experiments, we compare VIENNA running with two objectives -overall deviation and connectivity -against two classical clustering algorithms with proven performance across a wide range of data sets: k-means and average-link agglomerative clustering. Moreover, to establish the usefulness of a multiobjective approach we also compare against two single-objective versions of VIENNA, using identical operators and parameter settings, but optimizing only one objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">VIENNA Configuration</head><p>The parameter settings for VIENNA, held constant over all problems, are given in Table <ref type="table" target="#tab_0">1</ref>. There are three versions of the algorithm: a multiobjective one, which we label, VIENNA-moo; and two single-objective versions, named VIENNA-dev and VIENNA-conn. In addition to the objective(s) to optimize, all VIENNA algorithms use a constraint that no cluster should be empty, and enforce this constraint by lethally penalizing the value(s) of each objective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Average-Link Agglomerative Clustering and k-Means</head><p>We use a standard implementation of average-link agglomerative clustering, <ref type="bibr" target="#b19">[20]</ref>, which is deterministic for a given data order. The implementation of the kmeans <ref type="bibr" target="#b14">[15]</ref> algorithm is based on the batch version, that is, cluster centres are only recomputed after the reassignment of all data items. As k-means can sometimes generate empty clusters, empty clusters are identified in each iteration and are randomly reinitialized. Obviously, this enforcement of the correct number of clusters can prevent convergence, and we therefore set the maximum number of iterations to 100. To avoid suboptimal solutions k-means is run repeatedly (100 times per 'run') using random initialization, and only the best result in terms of minimum variance is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data Collection and Processing</head><p>For each problem, we perform 50 independent runs of the VIENNA algorithms and collect data on the entire evolution of the population. In particular, for VI-ENNA-moo we collect the final external population of nondominated points. For k-means and average-link agglomerative clustering we also perform 50 independent runs and collect the final output solution obtained in each run. We evaluate solutions objectively using the F -measure <ref type="bibr" target="#b18">[19]</ref>, which combines information on the purity and the completeness of the generated clusters with respect to the known, real class memberships. This measure is limited to the range [0, 1], where 1 reflects a perfect agreement with the correct partitioning.</p><p>Importantly, the F measure we quote for VIENNA-moo will be for the solution in the final external population with the best F measure value. This is in-line with our philosophy that in many applications we would be able to test a small number of alternative clustering solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results of the F -measure are presented graphically as boxplots <ref type="bibr" target="#b20">[21]</ref> in Figure 2. On several datasets the solutions generated by VIENNA-moo are better than those of the other algorithms by a large margin. On Iris, Yeast, Zoo, Long1, and Smile, its superiority is clear. Equally impressive, however, is the fact that it exhibits a far more robust performance than any other algorithm: indeed, the sample median F measure of the VIENNA-moo solutions is unbeaten across all data sets (not significance tested), and is even slightly better than that of the multiple-restarted k-means algorithm on its 'home territory' of the Sizes and Square series of data-sets, with all-spherical clusters. Tabulated results are available at <ref type="bibr" target="#b0">[1]</ref>.</p><p>Further graphical results are also available at <ref type="bibr" target="#b0">[1]</ref>, including figures displaying the Pareto fronts obtained on some problems, and plots of the time-evolution of the F measure and the two objectives. A number of these results indicate that the global optima on overall deviation and/or connectivity alone do not correspond with the global optimum on the F measure. It is only by exploring other nondominated local optima (trading off these two objectives) that the EA is able to find good F measure solutions. This exploration is possible as a direct consequence of optimizing multiple objectives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Most clustering algorithms operate by optimizing (either implicitly or explicitly) a single measure of cluster solution quality. Such methods may perform well on certain data-sets but lack robustness with respect to variations in cluster shape, proximity, evenness and so forth. In this paper, we have proposed an alterna-tive approach: to optimize simultaneously over a number of objectives using a multiobjective EA. We demonstrated that with this approach a greater robustness may be achieved -solutions selected from the generated nondominated front were never worse than those generated by either of two classic algorithms, across all 15 of our data sets, and were substantially better on a number of them, including three of seven real data sets from the UCI Machine Learning Repository. Much further work is needed to investigate using different and more objectives, and to test the approach still more extensively. However, we will first concentrate on the important issue of developing methods for identifying the best candidate solution(s) from the Pareto front, or reducing the number of solutions that must be assayed. We have already begun with this, and have found it possible to cluster the nondominated front to just a handful of significantly different solutions on the data sets used here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The kind of complex partitioning boundary enabled by a Voronoi cell genotype coding. Here there are two clusters (k = 2) but j = 6 centres (squares) have been used to cluster the data. The label of each centre (here visualized by its colour) takes a value in 1..k. Both the label and location of each centre are coded by the genotype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ZooFig. 2 .</head><label>2</label><figDesc>Fig. 2. Boxplots<ref type="bibr" target="#b20">[21]</ref> giving the distribution of F measure values achieved for 50 runs of each algorithm on the 15 data sets. Key: A=average-link agglomerative clustering, K = k-means, Vc=VIENNA-conn, Vd=VIENNA-dev, Vmo=VIENNA-moo. Median and IQR values have also been tabulated and can be found at<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameter settings for all VIENNA algorithms.</figDesc><table><row><cell>Parameter</cell><cell cols="2">setting</cell></row><row><cell>Number of generations (synthetic data) Number of generations (real data)</cell><cell>40</cell><cell>500 √ N</cell></row><row><cell>External population size</cell><cell cols="2">100 (VIENNA-moo only)</cell></row><row><cell>Internal population size</cell><cell></cell><cell>200</cell></row><row><cell>Initialization</cell><cell cols="2">random Voronoi cells (see section 2.3)</cell></row><row><cell>Mutation type</cell><cell cols="2">g nearest neighbours (see section 2.4)</cell></row><row><cell>Mutation rate pm</cell><cell cols="2">1/N</cell></row><row><cell>Recombination</cell><cell cols="2">none</cell></row><row><cell>VIENNA-moo objective functions</cell><cell cols="2">deviation and connectivity (h = 10)</cell></row><row><cell>VIENNA-dev objective function</cell><cell cols="2">overall deviation only</cell></row><row><cell>VIENNA-conn objective function</cell><cell cols="2">connectivity (h = 10) only</cell></row><row><cell>Constraints</cell><cell cols="2">empty clusters lethally penalized</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Yao et al. (Eds.): PPSN VIII, LNCS 3242, pp. 1081-1091, 2004. c Springer-Verlag Berlin Heidelberg 2004</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>VIENNA is built from David Corne's original PESA-II code. JH gratefully acknowledges support of a scholarship from the Gottlieb-Daimler-and Karl Benz-Foundation, Germany. JK is supported by a David Phillips Fellowship from the Biotechnology and Biological Sciences Research Council (BBSRC), UK.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://dbk.ch.umist.ac.uk/handl/vienna/" />
		<title level="m">Supporting material</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http:://www.ics.uci.edu/∼mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Sciences, University of California, Irvine</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization and Operations Research, chapter On the complexity of clustering problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="45" to="54" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Clustering with genetic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">6907</biblScope>
			<pubPlace>Nedlands; Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Western Australia</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PESA-II: regionbased selection in evolutionary multiobjective optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Corne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jerram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Pareto envelope-based selection algorithm for multiobjective optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Corne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Parallel Problem Solving from Nature VI Conference</title>
		<meeting>the Parallel Problem Solving from Nature VI Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-objective optimization using evolutionary algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Chichester, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised clustering using genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Embrechts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Rensselaer Polytechnic Institute</publisher>
			<pubPlace>Troy, New York</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Genetic Algorithms and Grouping Problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Falkenauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust clustering by evolutionary computation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gablentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Köppen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dimitriadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Online World Conference on Soft Computing in Industrial Applications (WSC5)</title>
		<imprint>
			<publisher>The Internet</publisher>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quality scheme assessment in the clustering process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Halkidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Batistakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth European Conference on Principles of Data Mining and Knowledge Discovery, volume 1910 of LNCS</title>
		<meeting>the Fourth European Conference on Principles of Data Mining and Knowledge Discovery, volume 1910 of LNCS<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="265" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiobjective data clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applying genetic algorithms to search for the best hierarchical clustering of a dataset</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="911" to="918" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Genetic algorithm-based clustering technique</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1455" to="1465" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Equivalence class analysis of genetic algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Radcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="183" to="205" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A mixture model for clustering ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIAM Conf. on Data Mining</title>
		<meeting>SIAM Conf. on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Information Retrieval, 2nd edition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Butterworths, London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The effectiveness and efficiency of agglomerative hierarchical clustering in document retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Box-and-whisker plot</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="http://mathworld.wolfram.com/Box-and-WhiskerPlot.html" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
