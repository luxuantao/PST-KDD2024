<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Margin Multi-View Information Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
							<email>xuchao@cis.pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">D</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Sci-ence</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Min-istry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Quantum Computation and Intelligent Systems</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<addrLine>235 Jones Street</addrLine>
									<postCode>2007</postCode>
									<settlement>Sydney, Ultimo</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Margin Multi-View Information Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C323921F11C54CD8672144753FB78144</idno>
					<idno type="DOI">10.1109/TPAMI.2013.2296528</idno>
					<note type="submission">received 19 Apr. 2013; revised 15 Nov. 2013; accepted 6 Dec. 2013. Date of publication 1 Jan. 2014; date of current version 10 July 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-view learning</term>
					<term>large-margin learning</term>
					<term>information bottleneck</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we extend the theory of the information bottleneck (IB) to learning from examples represented by multi-view features. We formulate the problem as one of encoding a communication system with multiple senders, each of which represents one view of the data. Based on the precise components filtered out from multiple information sources through a "bottleneck", a margin maximization approach is then used to strengthen the discrimination of the encoder by improving the code distance within the frame of coding theory. The resulting algorithm therefore inherits all the merits of the IB principle and coding theory. It has two distinct advantages over existing algorithms, namely, that our method finds a tradeoff between the accuracy and complexity of the multi-view model, and that the encoded multi-view data retains sufficient discrimination for classification. We also derive the robustness and generalization error bound of the proposed algorithm, and reveal the specific properties of multi-view learning. First, the complementarity of multi-view features guarantees the robustness of the algorithm. Second, the consensus of multi-view features reduces the empirical Rademacher complexity of the objective function, enhances the accuracy of the solution, and improves the generalization error bound of the algorithm. The resulting objective function is solved efficiently using the alternating direction method. Experimental results on annotation, classification and recognition tasks demonstrate that the proposed algorithm is promising for practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç 1 INTRODUCTION</head><p>I N natural applications, data are usually collected from diverse domains or obtained from various feature extractors. They exhibit heterogeneous properties, because the variables of each example can be naturally partitioned into groups. Each variable group is referred to as a particular view, and multiple views for a particular problem can take different forms.</p><p>In web image classification, for example, an image can be distinguished by the surrounding text or by visual features such as color and text. In the category analysis of scientific publications, the information in the title and abstract of papers can be regarded as the text view, while the inbound and outbound references are the link views. In human motion recognition, features characterizing motion and appearance are extracted from the space-time volumes of videos, such as the histogram of oriented gradient (HOG) and the histogram of optic flow (HOF).</p><p>The information provided by a particular single view cannot comprehensively describe all examples. It has therefore become popular to develop multi-view learning algorithms by using various features from diverse views (or simply multi-view features) and integrating the complementary advantages of multi-view features to accomplish a specific prediction task.</p><p>The first successful multi-view learning algorithm is Blum's co-training algorithm <ref type="bibr" target="#b0">[1]</ref>, following which various algorithms such as co-EM <ref type="bibr" target="#b1">[2]</ref> and co-regularization <ref type="bibr" target="#b2">[3]</ref> have been proposed. In multiple kernel learning (MKL), different kernels can handle different views and can be elegantly integrated by algorithms such as semi-definite programming (SDP) <ref type="bibr" target="#b3">[4]</ref>, semi-infinite linear programming (SILP) <ref type="bibr" target="#b4">[5]</ref>, and simple MKL <ref type="bibr" target="#b5">[6]</ref>. Canonical correlation analysis (CCA) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, its kernel extension <ref type="bibr" target="#b8">[9]</ref>, its probabilistic interpretation <ref type="bibr" target="#b9">[10]</ref>, and its sparse formulation <ref type="bibr" target="#b10">[11]</ref> have been used to discover the low-dimensional representation of multi-view data. Recently, other methodologies <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> have been proposed for this task: <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> exploited different techniques to factorize the latent space of multi-view data into shared and private parts; <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> found the joint embedding for multi-view data by maximizing mutual information; while <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> discovered the subspace shared by multi-view data based on the Markov network.</p><p>Existing multi-view learning algorithms <ref type="bibr" target="#b20">[21]</ref> have shown promising performance in diverse applications <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, but they have one or more of the following limitations.</p><p>First, examples are usually represented by several high dimensional multi-view features, so the curse of dimensionality needs to be overcome. Second, a learned prediction model used for the subsequent classification needs to retain discriminative ability in processing data described by multiple features. Third, the generalization error of an algorithm must be theoretically bounded to guarantee its performance. In addition, it is preferable to control the accuracy and complexity of the learned multi-view model.</p><p>Inspired by the theory of the information bottleneck (IB), we propose the large-margin multi-view information bottleneck (LMIB) algorithm, which models the multi-view learning problem using a communication system with multiple senders. Each sender represents one view, thus the function of this system is to compress the data from multiple information sources and filter out the precise components through a "bottleneck". From knowledge of the coding theory, we know that the minimum code distance of the encoded signal determines the error detection performance of an encoder. Therefore we introduce the margin maximization approach to improve the code distance, which leads to a discriminative model based on the encoded examples. The resulting optimization problem finds a tradeoff between the accuracy and complexity of multi-view learning, and it can be decomposed into two convex sub-problems, which is theoretically guaranteed to converge. We derive the robustness and generalization error bound of the proposed algorithm, and analyze the influences of the consensus and the complementary properties of multi-view features on them. Finally, we evaluate the proposed algorithm using real-world examples of web image annotation, publication classification, and human motion recognition. Our experimental results show the effectiveness of the new method for compressing multiview data and learning a discriminative model for the subsequent classification.</p><p>The rest of the paper is organized as follows. Section 2 reviews related work. In Section 3, we formulate the multiview learning problem and propose the LMIB algorithm. The optimization method is presented in Section 4 and theoretical analysis is given in Section 5. Section 6 presents the experimental results, Section 7 provides detailed proofs of the theoretical results, and Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>According to the different approaches used to integrate multi-view features, we group multi-view learning algorithms <ref type="bibr" target="#b20">[21]</ref> into three categories: 1) co-training style algorithms, 2) multiple kernel learning, and 3) subspace-based approaches.</p><p>Co-training style algorithms usually train separate but correlated learners on each view. The outputs of learners are then forced to be similar on the same validation points in a semi-supervised way. Stemming from the co-training algorithm <ref type="bibr" target="#b0">[1]</ref>, Nigam et al. <ref type="bibr" target="#b1">[2]</ref> brought EM into the multiview setting and proposed co-EM, and Sindhwani et al. <ref type="bibr" target="#b2">[3]</ref> derived a co-regularization algorithm by reformulating cotraining into a regularized formulation.</p><p>In multiple kernel learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, it is preferable to have a set of kernels and to find proper criteria to linearly combine those kernels, instead of choosing a single kernel. Since different kernels may correspond to different notions of similarity or inputs from different views, optimally combining kernels can effectively integrate multiple information sources.</p><p>Most of the subspace-based approaches assume that the input views are generated from a latent view and attempt to obtain a latent subspace shared by multiple views. This kind of algorithm is often optimized using the two-step strategy: 1) learning to discriminate each view independently given the shared variable and then 2) updating the parameters for the shared space. However, they have completely different objectives in interpreting the multiview learning problem from different starting points. Classically, multi-view subspace has been achieved by the application of CCA, e.g., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> which attempts to compute a low-dimensional shared embedding of two sets of variables such that the correlation between them is maximized in the embedded space. Recently, this work has been extended to probabilistic <ref type="bibr" target="#b9">[10]</ref> and sparse formulations <ref type="bibr" target="#b10">[11]</ref>. The idea of factorizing the latent space to represent shared and private information from multiple views of the data has been well exploited using sparse coding <ref type="bibr" target="#b13">[14]</ref>, conditional random fields <ref type="bibr" target="#b14">[15]</ref> and Gaussian process techniques <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Diethe et al. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> successfully applied Fisher's discriminant analysis to multiview problems by maximizing the between-class variations and minimizing the within-class variations of the lowdimensional embeddings from both the intra-view and the inter-view in the common space. Memisevic et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> found the joint embedding for multi-view data by maximizing the mutual information between the embedding space and multiple feature spaces.</p><p>The recent work of max-margin Harmonium (MMH) <ref type="bibr" target="#b19">[20]</ref> shows that applying the large-margin principle to learn subspace shared by multi-view data is more suitable for prediction. However, the most important differences between MMH and our approach are the completely different objectives in formulating the subspace learning problem. MMH is an excellent method for multi-view learning through characterizing the latent subspace learning as the state transition in the Markov network. By contrast, we naturally integrate the latent subspace learning problem into the frame of information bottleneck by compressing the input data into the latent subspace while balancing its accuracy with regard to the output space. We suggest that the proposed LMIB algorithm brings a novel interpretation of the latent subspace learning problem, and achieves satisfactory performance.</p><p>Other related works inspired by information theory for dimension reduction include sufficient dimension reduction for co-occurrence data <ref type="bibr" target="#b31">[32]</ref>, kernel information embedding <ref type="bibr" target="#b16">[17]</ref>, and sufficient component analysis <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Take n examples fX 1 ; . . . ; X n g for training, and each example is sent to the system by m different senders as diverse views of the data. Therefore we seek m different channel transition matrices Q ¼ fQ 1 ; . . . ; Q m g which encode the information coming from different senders to low-dimensional representations</p><formula xml:id="formula_0">T ¼ fðX 1 ; . . . ; X m ; Q 1 ; . . . ; Q m Þ;<label>(1)</label></formula><p>where X i ¼ fX i 1 ; . . . ; X i n g represents the features of examples on the view-i. We assume that the side information Y ¼ fY 1 ; . . . ; Y n g regarding examples is already known, and this relevance relationship can be passed to the encoded multi-view data and measured by Y ¼ fðT ; wÞ:</p><p>(2)</p><p>Given Eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula">2</ref>), our objective is to learn m different transition matrices fQ 1 ; . . . ; Q m g, which encode multiview examples into low-dimensional representations, and a weight vector w to discriminate the encoded examples.</p><p>The theory of Information Bottleneck, originating from rate-distortion theory, is an important branch of information theory. IB aims to find the best trade-off between accuracy and compression (complexity) when summarizing a random variable X, given its relevant variable Y . Since the objective of IB coincides exactly with the needs of latent subspace learning, and it is natural to consider multiple views as different information sources for input, we are motivated to treat the multi-view learning problem as a practical communication system and apply the IB principle to optimize it. Therefore the resulting algorithm inherits all the merits of the IB principle. First, compared with other probabilistic methods, such as LDA <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> and MMH <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> based on general Markov networks, the IB principle has a novel interpretation of the procedure of latent subspace learning from the aspect of communication, and it is natural to illustrate the multi-view learning problem by considering each view as an information source for input. Second, the IB principle provides a unified frame to simultaneously consider the accuracy and compression influences on latent subspace learning, and optimally finds the tradeoff between these two important factors. Furthermore, from the knowledge of coding theory, we apply the large-margin approach to improve the code distance of encoded signals, which strengthens the discriminative capability of the IB principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Information Bottleneck</head><p>Let X be a finite set and X a random variable taking values x in X with the distribution pðxÞ ¼ Pr½X ¼ x. Similarly, let Y be a random variable taking values y in Y. Through a probability transition matrix (composed of conditional probabilities) characterizing the information channel X ! Y , the mutual information measuring the shared information between X and Y is defined by</p><formula xml:id="formula_1">IðX; Y Þ ¼ X x2X X y2Y pðxÞpðy j xÞlog pðy j xÞ pðyÞ :<label>(3)</label></formula><p>The IB method <ref type="bibr" target="#b35">[36]</ref> attempts to find a compact representation of the variable X through an auxiliary variable T , which preserves as much information as possible about the side information Y . By employing the mutual information between random variables as the basic quantity, the minimality of the representation of X is achieved by minimizing IðX; T Þ, and the sufficiency of information on Y is achieved by constraining the value of IðY; T Þ. Thus the objective function of IB is given by</p><formula xml:id="formula_2">min pðt j xÞ IðX; T Þ s:t: IðY; T Þ ! const: (4)</formula><p>By introducing a positive parameter b, we obtain the Lagrangian function</p><formula xml:id="formula_3">F ¼ min pðtjxÞ IðX; T Þ À bðIðY; T Þ À constÞ;</formula><p>(5) which can then be simplified as</p><formula xml:id="formula_4">F ¼ min pðt j xÞ IðX; T Þ À bIðY; T Þ; (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>where the objective variable pðtjxÞ is subject to the normalization constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-View Information Bottleneck</head><p>Although the IB principle provides us with an approach to obtain transition matrix learning with the side information, it cannot be straightforwardly applied to the multi-view learning problem. Therefore we improve the original IB problem and propose large-margin multi-view IB (LMIB) from two aspects. 1) Multi-view. The original theory of IB only considers one information source as input. If we naively concatenate multiple views into one view to adapt IB to the multi-view setting, the specific property of each view will be neglected, and the "curse of dimensionality" problem will be introduced, especially when the number of training examples is small. 2) Large-margin. The theory of IB attempts to maximize the correlation between encoded examples and the supervised information (e.g., the corresponding labels) in the measurement of mutual information. The large-margin principle is more suitable than mutual information with regard to classification, because the large-margin principle emphasizes the separation property between samples from different classes and demonstrates <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> an excellent capability for classification tasks. Specifically, we exploit the communication system with multiple senders to model the multi-view learning problem, in which multiple senders represent diverse views of the examples, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This system can extract the precise components of examples from different senders and encode them into low-dimensional representations with the help of an implicit "bottleneck". Since the minimum code distance of the encoded signals determines the error detection performance of an encoder, we introduce the margin maximization approach to improve the code distance and strengthen the discrimination of the encoded examples.</p><p>In a real world communication system, the signals from different senders are precisely summed for transmission in a single channel (see <ref type="bibr" target="#b38">[39]</ref>, p. 514]). Hence, we choose to use this summation strategy to integrate multiply views in the multi-view learning problem. Formally, given the i-th view X i ¼ fX i 1 ; . . . ; X i n g, associated with the label Y ¼ fy 1 ; . . . ; y n g, we define the low-dimensional representation for the example X j as</p><formula xml:id="formula_6">T j ¼ fðX j ; QÞ ¼ 1 m X m i Q i X i j ;<label>(7)</label></formula><p>where X i j is the feature of X j on view-i, and Q i is the transition matrix on view-i, which is composed of the conditional probabilities between X i and T . Then the discriminant function is given by y j ¼ signðhw; fðX j ; QÞiÞ:</p><p>The transition matrix Q and discriminant weight vector w can be learned in the frame of LMIB formulated in a margin maximization approach as</p><formula xml:id="formula_8">F ¼ min Q;w X m i IðX i ; T Þ þ C 1 2 kwk 2 þ C 2 n X n j h j s.t. y j w T 1 m X m i Q i X i j ! 1 À h j h j ! 0; all for 1 j n;<label>(9)</label></formula><p>where C 1 and C 2 are non-negative constants, which can be selected via cross-validation. Since the problem ( <ref type="formula" target="#formula_8">9</ref>) jointly maximizes the compaction of multiple views and minimizes the training loss between the encoded examples and the given labels, it can be expected that by solving this problem we will find a compact low-dimensional subspace and a discriminant model. This on the one hand will compact the data from multiple senders effectively, and on the other hand make accurate predictions on these encoded examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION METHODS</head><p>Problem ( <ref type="formula" target="#formula_8">9</ref>) can be decomposed into two convex sub-problems in an alternating optimization procedure. By fixing transition matrices fQ i g m 1 on all views, we obtain a standard SVM problem, and by fixing weight vector w, the original problem is reduced to an extension of the IB problem with hinge loss as the distortion function. Since both of these sub-problems can be seen as combinations of two convex functions, we can utilize an alternating direction method <ref type="bibr" target="#b39">[40]</ref> to conduct optimization and obtain the final solution efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transition Matrix Optimization</head><p>When the weight vector w and all the transition matrices but the matrix on ith view are fixed, Eq. ( <ref type="formula" target="#formula_8">9</ref>) can be minimized over Q i as</p><formula xml:id="formula_9">F 1 ¼ min Q i IðX i ; T Þ þ C 2 n X n j h j s.t. h j ¼ max 0; H i À 1 m y j w T Q i X i j &amp; ' ;<label>(10)</label></formula><p>where H i is the constant existing in the hinge loss when all the views except the ith view are fixed.</p><p>Although the problem <ref type="bibr" target="#b9">(10)</ref> can be regarded as an IB problem with one single view, it cannot be solved straightforwardly by the classical Blahut-Arimoto algorithm <ref type="bibr" target="#b38">[39]</ref> often utilized for solving IB problems, due to the discontinuity of the hinge loss. We regard F 1 as a composition of two convex functions: fðMÞ ¼ IðX i ; T Þ and gðMÞ ¼ C 2 n P n j h j , where M ¼ Q i is brought in for expression simplicity. Therefore the alternating direction method <ref type="bibr" target="#b39">[40]</ref> can be used to minimize the sum of these two convex functions. Since the alternating direction method is usually based on variable splitting combined with the augmented Lagrangian method, we initially split the variable M into two variables, and transform the primal problem <ref type="bibr" target="#b9">(10)</ref> as</p><formula xml:id="formula_10">min ffðMÞ þ gðZÞ : M À Z ¼ 0g; (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>then the corresponding augmented Lagrangian function is given by</p><formula xml:id="formula_12">L m ðM; Z; Þ ¼ fðMÞ þ gðZÞ À h; M À Zi þ 1 m D KL ðM j ZÞ;</formula><p>where is the Lagrange multiplier, which will be updated at each iteration for fast convergence, D KL ðMjZÞ is the KL divergence to regularize M and Z, and 1 m is the penalty parameter.</p><p>Since minimizing L m ðM; Z; Þ w.r.t. M and Z jointly is not easy, decreasing the complexity by minimizing L m ðM; Z; Þ w.r.t. M and Z alternately is an appropriate solution. At each iteration, we alternately minimize two different approximations to the original objective function, which is obtained by keeping one function unchanged and linearizing the other. Two new functions Q g ðM; ZÞ and Q f ðZ; MÞ are given by</p><formula xml:id="formula_13">Q g ðM; ZÞ ¼fðMÞ þ gðZÞ þ hrgðZÞ; M À Zi þ 1 m D KL ðMjZÞ; Q f ðZ; MÞ ¼gðZÞ þ fðMÞ þ hrfðMÞ; Z À Mi þ 1 m D KL ðZjMÞ:</formula><p>Hence M and Z can be alternately updated by the accelerated alternating direction method given in Algorithm 1. However, considering that gðMÞ is nonsmooth, we apply the smoothing technique introduced by <ref type="bibr" target="#b40">[41]</ref> to approximate the hinge loss with smoothness parameter s &gt; 0 as</p><formula xml:id="formula_14">h j ¼ max u2Q u j H i À 1 m y j w T Q i X i j À s 2m w À X i j Á T 1 u 2 j ;<label>(12)</label></formula><formula xml:id="formula_15">Q ¼ fu : 0 u j 1; u 2 R n g:</formula><p>The remaining problems are to optimize Q g ðM; ZÞ w.r.t. M and Q f ðZ; MÞ w.r.t. Z. We first show how to minimize Q g ðM; ZÞ w.r.t. M. Taking the derivative of Q g ðM; ZÞ w.r.t. M t;x , for given t and x on specific view-i, we obtain</p><formula xml:id="formula_16">@Q g ðM; ZÞ @M t;x ¼ pðxÞlog M t;x pðtÞ þ rgðZ t;x Þ þ 1 m log M t;x Z t;x þ 1 ;<label>(13)</label></formula><p>where M t;x and Z t;x represent the corresponding entries given t and x in transition matrices M and Z respectively. Setting the derivation Eq. ( <ref type="formula" target="#formula_16">13</ref>) to zero, we have</p><formula xml:id="formula_17">M t;x ¼ exp mpðxÞlogðpðtÞÞ þ logðZ t;x Þ À mrgðZ t;x Þ À 1 mpðxÞ þ 1 &amp; ' :<label>(14)</label></formula><p>Since each column in transition matrix M contains the conditional probabilities between each t and the corresponding x, the obtained M should then be normalized with respect to each column. Moreover, we have the constraint over pðtÞ</p><formula xml:id="formula_18">P ðT Þ ¼ 1 m X v¼m;v6 ¼i v¼1 Q v P ðX v Þ þ MP ðX i Þ ! ; (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>where P ðÁÞ is the vector of the distribution probability pðÁÞ, and i is the index of the current optimized-view. Alternating iterations of Eqs. ( <ref type="formula" target="#formula_17">14</ref>) and ( <ref type="formula" target="#formula_18">15</ref>) lead to the optimal M of Q g ðM; ZÞ on view-i.</p><p>Taking the derivative of Q f ðZ; MÞ w.r.t. Z t;x , for given t and x on specific view-i, we obtain</p><formula xml:id="formula_20">@Q f ðZ; MÞ @Z t;x ¼ rgðZ t;x Þ þ rfðM t;x Þ þ 1 m log Z t;x M t;x þ 1 :</formula><p>Since the gradient is continuous, we expect that gradientdescent type optimization algorithms will be able to efficiently optimize the objective, and derive the optimal Z of Q f ðZ; MÞ on view-i. Finally the transition matrix on each view can be sequentially optimized using the alternating direction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weight Vector Optimization</head><p>When all transition matrices fQ i g m 1 are fixed, Eq. ( <ref type="formula" target="#formula_8">9</ref>) can be reduced to an SVM problem</p><formula xml:id="formula_21">F 2 ¼ min w C 1 2 kwk 2 þ C 2 n X n j h j s.t. y j w T T j ! 1 À h j h j ! 0; all for 1 j n;<label>(16)</label></formula><p>where T j is the dimensional reduced data with respect to input example X j , and it is given by T j ¼ 1 m P m i Q i X i j , given the fixed transition matrices fQ i g for 1 i m.</p><p>Many efficient SVM solvers can be deployed to solve this problem, such as SVM-Light <ref type="bibr" target="#b41">[42]</ref>, SVM-Perf <ref type="bibr" target="#b42">[43]</ref>, and LibSVM <ref type="bibr" target="#b43">[44]</ref>, all of which exploit decomposition methods or reformulate the problem into a structural form. The alternating direction method (ADM) <ref type="bibr" target="#b39">[40]</ref> has undergone impressive development in the area of convex programming. As explained in <ref type="bibr" target="#b44">[45]</ref>, ADM is "at least comparable to very specialized algorithms, and in most cases, the simple ADM algorithm will be efficient enough to be useful". Furthermore, there are indeed many theoretical and experimental results <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> that illustrate the excellent performance of ADM in solving quadratic problems. Hence instead of employing conventional SVM solvers, we rely on ADM to solve this SVM sub-problem to make our optimization procedures for different sub-problems consistent. Our empirical studies also suggest that ADM demonstrates comparable performance to conventional SVM solvers. Given the limited page length, we do not include these results in this paper.</p><p>Similar to the transition matrix optimization method, the main procedure for optimizing the weight vector can be described as in Algorithm 1. Therefore we do not formulate this subproblem in detail, and suggest readers refer the existing works <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> on applying ADM for SVM problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL ANALYSIS</head><p>In this section, we first prove that the objective value F converges to a local minimum. We then study the robustness of the LMIB algorithm, following which we derive the generalization error bound of the algorithm and analyze the influence of the transition matrix on the bound. The detailed proofs can be found in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Convergence Analysis</head><p>We can solve problem <ref type="bibr" target="#b8">(9)</ref> in an iterative manner by alternately optimizing (10) w.r.t. Q given the fixed w, and optimizing (16) w.r.t. w, with fixed Q until the decrement of the objective function is zero. The convergence analysis of the above procedure is given in Proposition 1.</p><p>Proposition 1. Given multiple input features, the alternating optimization of (9) between the parameters w and Q converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robustness Analysis</head><p>If a test sample and a training sample are close to each other, their associated losses are also close. This property is formalized as "robustness" in <ref type="bibr" target="#b48">[49]</ref>, and the precise definition is:</p><p>Definition 1. An algorithm A is ðK; ðÁÞÞ robust, for K 2 N and ðÁÞ : Z ! R, if the sample Z ¼ Y Â X can be partitioned into K disjoint sets, denoted by fC i g K i¼1 , such that the following holds for all s &amp; Z, given the loss function lðA s ; zÞ of the algorithm A s trained on s: 8s 2 s; 8z 2 Z; 8i ¼ 1; . . . ; K : if s; z 2 C i ; then jlðA s ; sÞ À lðA s ; zÞj ðsÞ: Thus for the LMIB algorithm, we have the following theorem to measure robustness. Theorem 1. Let fx 1 ; . . . ; x m g be the feature spaces on different views associated with the fixed margins fg 1 ; . . . ; g m g, and denote the dimensions corresponding to different feature spaces as fD 1 ; . . . ; D m g. Given the covering number of X as N ðg; X ; j Á j 2 Þ, if for any X i 1 ; X i 2 on view-i and</p><formula xml:id="formula_22">X i 1 2 s, kX i 1 À X i 2 k g i , A then satisfies jlðA s ; X 1 Þ À lðA s ; X 2 Þj 1 m ffiffiffiffiffiffiffiffi 2C 2 C 1 s X m i¼1 ffiffiffiffiffi ffi D i p g i : Hence A is ðN ðg; X ; j Á j 2 Þ; 1 m ffiffiffiffiffiffi 2C 2 C 1 q P m i¼1 ffiffiffiffiffi ffi D i p g i Þ-robust.</formula><p>Robustness is a fundamental property, which ensures that a learning algorithm works well. In particular, it conveys the geometric intuition that if a test sample is "similar" to a training sample, then the test error is close to the training error. That is to say, a robust algorithm has the ability to handle the perturbations in the data, and it has been shown that weak robustness is sufficient and necessary for generalizability. More importantly, from the perspective of multiview learning, even if some views have unexpected noise, i. e., the corresponding margins fg i g are large, the overall performance of the multi-view learning algorithm will be preserved as long as there are still other accurate views with tiny margins. Therefore, multi-view features can complement one another and improve the overall robustness of the multi-view learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generalization Error Analysis</head><p>In this section, we derive the generalization error bound for the LMIB algorithm, estimate the parameters of the transition matrix, and assess its influence on the generalization error bound. We begin by stating the definition of Rademacher complexity <ref type="bibr" target="#b49">[50]</ref>.</p><p>Definition 2. Given a sample S ¼ fX 1 ; . . . ; X n g 2 X n and a real-valued function class F defined on a space X , the empirical Rademacher complexity of F is defined as</p><formula xml:id="formula_23">Rn ðF Þ ¼ E s sup f2F 2 n X n j¼1 s j fðX j Þ X 1 ; . . . ; X n ! ;</formula><p>where s ¼ ðs 1 ; . . . ; s n Þ are independent uniform fAE1g-valued Rademacher random variables. The Rademacher complexity of F is</p><formula xml:id="formula_24">R n ðF Þ ¼ E s ð Rn ðF ÞÞ ¼ E Ss sup f2F 2 n X n j¼1 s j fðX j Þ ! :</formula><p>For LMIB, we first define the function class for each view-i,</p><formula xml:id="formula_25">F i ¼ f i f i : X i ! w T 1 m Q i X i &amp; ' :<label>(17)</label></formula><p>Considering the convex combination of the function class on each view,</p><formula xml:id="formula_26">conðF Þ ¼ &amp; X i a i f i jf i 2 F i ; a i &gt; 0; X i a i 1 ' ;<label>(18)</label></formula><p>we have</p><formula xml:id="formula_27">conðF Þ ¼ X ! 1 m w T X m i Q i X i ( ) ¼ X ! X m i 1 m w T Q i X i ( ) ¼ X ! X m i 1 m f i ( ) ¼ con [ m i F i :<label>(19)</label></formula><p>Hence, we are interested in the empirical Rademacher complexity of a convex hull as given by conðF Þ, which is well known to be bounded by</p><formula xml:id="formula_28">Rn ðconðF ÞÞ Rn ðF Þ:<label>(20)</label></formula><p>Lemma 1 <ref type="bibr" target="#b50">[51]</ref> proves a high probability upper bound for the empirical Rademacher complexity of a joint function class</p><formula xml:id="formula_29">F ¼ S m i F i . Lemma 1. Let X ¼ fX 1 ; . . . ; X n g be a sample of n examples from</formula><p>X associated with m views, then the empirical Rademacher complexity Rn ðF Þ of the class F ¼ [ X i 2X F i ; i 2 f1; . . . ; mg, satisfies</p><formula xml:id="formula_30">Rn ðF Þ max 1 i m Rn ðF i Þ þ 8 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ln ð mþ1 d Þ 2n s :</formula><p>The generalization error bound for LMIB can therefore be approximated by the following theorem. Theorem 2. Fix g &gt; 0 and d 2 ð0; 1Þ. Let F be the class of functions described above, and let fX j g n j¼1 be a sample of examples drawn independently according to a probability distribution D. Assume that each example is represented by m views, whose dimensions are denoted as fD 1 ; . . . ; D m g. Then with probability at least 1 À d over random draws of samples of size n, every</p><formula xml:id="formula_31">f 2 F satisfies RðfÞ 1 n X n j¼1 j þ 2B ng max 1 i m ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi D i X n j¼1 À X i j Á T X i j v u u t þ 11 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ln À mþ3 d Á 2n s :</formula><p>Furthermore, if for any example on view-i, ðX i j Þ T X i j is bounded by R 2 , and assuming the maximum empirical Rademacher complexity is obtained on the view-k, we have</p><formula xml:id="formula_32">RðfÞ 1 n X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi D k R 2 =g 2 n r þ<label>11</label></formula><formula xml:id="formula_33">ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ln ð mþ3 d Þ 2n s :</formula><p>According to Theorem 2, the generalization error bound of any function in F (19) depends on the sum of all the slack variables associated with training examples and the maximum empirical Rademacher complexity of the hypothesis class on each view. Therefore, constructing the large-margin classifier helps to improve the generalization error bound by constraining the slack variables. More importantly, from a multi-view learning perspective, by leveraging the view consistencies among different views, we effectively limit the union of hypothesis classes and thus reduce the empirical Rademacher complexity.</p><p>This generalization error bound improves some existing theoretical results in multi-view learning. For example, Sindhwani et al. <ref type="bibr" target="#b2">[3]</ref> gave the Rademacher complexity for the co-regularization algorithm and Farquhar et al. <ref type="bibr" target="#b51">[52]</ref> proved the generalization error bound for SVM-2K. However, these bounds were specifically deduced for the twoview learning algorithms and are difficult to adapt to a multi-view setting. In contrast with the Rademacher bound for MKL in <ref type="bibr" target="#b50">[51]</ref>, we introduce transition matrices fQ i g to explore the intrinsic subspaces of multiple views.</p><p>Considering that the transition matrix Q i has a large influence on the approximation of the empirical Rademacher complexity and the generalization error bound, and thus we proceed with the study of the accuracy of the estimated transition matrix Q i .</p><p>Without loss of generality, we consider the transition matrix Q k for the view-k, which corresponds to the maximum empirical Rademacher complexity of the function class, compared with those of other views. Observing that all the examples on any view will influence the estimation of Q k , we define an auxiliary function c i ðÁÞ to pass on the influences of examples on view-i on the estimation of the transition matrix Q k ,</p><formula xml:id="formula_34">Qk ¼ 1 mn X n j X m i c i À X i j Á :<label>(21)</label></formula><p>Moreover for each entry of c i ðX i j Þ, we assume there exist A and B &gt; 0 such that ½c i ðX i j Þ p 2 ½A; A þ B, where p is the index value of the matrix.</p><p>We therefore have the following proposition to bound the difference between Q k and Qk . </p><formula xml:id="formula_35">that ½c i ðX i j Þ p 2 ½A; A þ B,</formula><p>for some A and B &gt; 0, and kQ k X k À Q i X i k 1 i for each view-i. Then with probability at least 1 À d, we have </p><formula xml:id="formula_36">kQ k À Qk k 1 1 mkX k k 1 X m i i þ D T</formula><formula xml:id="formula_37">k is D T Â D k dimen- sional. Given ðX i j Þ T X i j R 2 and kQ k X k À Q i X i k 1 i</formula><p>for each view-i, then with probability at least 1 À d over random draws of samples of size n, every f 2 F satisfies</p><formula xml:id="formula_38">RðfÞ 1 n X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi D k R 2 =g 2 n r 1 þ 1 mkX k k 1 X m i i þ D T ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi B 2 ln ð2D T D k =dÞ 2mn r ! þ 11 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ln ð mþ3 d Þ 2n s :</formula><p>According to Corollary 1, we observe that the value of kQ k X k À Q i X i k 1 i on each view influences the accuracy of the estimated transition matrix Q k , and the generalization error bound. Hence for the multi-view learning problem, pursuing the consistency of different views will lead to a more accurate solution of the objective function and will improve the generalization error bound accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>Experiments were conducted on image annotation, human motion recognition, and publication classification tasks. The proposed LMIB algorithm was compared with conventional SVM <ref type="bibr" target="#b43">[44]</ref> (on original feature spaces), sufficient component analysis (SCA) <ref type="bibr" target="#b32">[33]</ref>, CCA <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, shared kernel information embedding (sKIE) <ref type="bibr" target="#b17">[18]</ref>, and max-margin Harmonium (MMH) <ref type="bibr" target="#b19">[20]</ref>. In particular, for singleview algorithms (SVM and SCA), multi-view features were concatenated into long feature vectors to exploit the low-dimensional representations of examples. Furthermore, SVMs were built on the low-dimensional representations of examples generated by SCA, CCA and sKIE. The experiments are described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets and Features</head><p>The data sets used in our experiments are PASCAL VOC'07, MIR Flickr <ref type="bibr" target="#b52">[53]</ref>, HMDB <ref type="bibr" target="#b53">[54]</ref> and CiteSeer <ref type="bibr" target="#b54">[55]</ref>. These data sets have been collected from different sources and have diverse feature types, as detailed below.</p><p>There are 9,963 images of 20 different object categories in the PASCAL VOC'07 set and 25,000 images of 38 categories in the MIR Flickr set. To simulate a multi-view setting, we chose to represent each image by one visual word histogram, which was generated by quantizing the local SIFT descriptors using k-means, and another feature vector of user tags. 1 For the PASCAL VOC'07 data set, we used the standard train/test split. For the MIR Flickr data set, images were randomly split into equally sized training and test sets.</p><p>The HMDB data set is a recently released large video database for human motion recognition. It has been collected from various sources, mostly movies, and contains 6,849 clips divided into 51 action categories, each containing a minimum of 101 clips. For every clip a set of 3D  For evaluation, we selected 70 training and 30 test clips in each action category as suggested by <ref type="bibr" target="#b53">[54]</ref> to ensure that the clips in the training set and the test set did not come from the same video file.</p><p>The CiteSeer data set is a collection of scientific publications which contains 3,312 documents belonging to six classes, i.e., Agents, AI. IR, ML and HCI. There are three natural views for each document: the text view consists of the title and abstract of the paper, and the two link views are inbound and outbound references. We used a 5-fold cross validation for evaluation.</p><p>The LMIB approach has a number of parameters to set: the cost terms C 1 and C 2 , the penalty parameter m and the smoothness parameter s. In all the experiments, we fixed C 1 at 1, and s at 0:5. We adopted the continuation strategy used in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b55">[56]</ref> to update m in the iterations. Specifically, we set m kþ1 ¼ maxð10 À6 ; hm k Þ, where m 0 ¼ kZk=1:25 and h ¼ 2=3. C 2 in LMIB and the cost terms C in training downstream SVMs using the representations by different subspace learning algorithms were optimized using crossvalidation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Annotation</head><p>Since images in PASCAL VOC'07 and MIR Flickr data sets <ref type="bibr" target="#b52">[53]</ref> may belong to more than one class, we performed image annotation experiments on these two data sets by considering all the categories as the dictionary of keywords, and we report the results of annotation using mAP.</p><p>We first randomly selected 100 positive and 300 negative labeled examples to make up the training set for each class, and we show the performance of the annotation based on the 30-dimensional subspace generated by LMIB, MMH, sKIE, SCA and CCA respectively in Fig. <ref type="figure" target="#fig_1">2</ref>. We can see that for most of the categories in these two data sets, LMIB outperforms other methods and demonstrates impressive improvements compared with SCA, CCA, and sKIE. Specifically, mAPs of LMIB show a 17:7 and a 6:8 percent increase compared with the mAPs of MMH on the PASCAL VOC'07 and MIR Flickr data sets. This suggests that the low-dimensional representations of multi-view examples generated by LMIB are much more discriminative, and the learned classifier is accurate via a large-margin approach.</p><p>To compare the performance of different algorithms under different low-dimension conditions, we projected the original multi-view features to f5; 20; 40; 60; 80; 120; 140g-dimensional subspaces and measured their performance using mAP. Fig. <ref type="figure" target="#fig_5">3</ref> illustrates the performance comparisons on the PASCAL VOC'07 and MIR Flickr data sets with respect to different low-dimensions. LMIB achieves the best performance in all the settings. Even in the projected subspaces of 5-dimension, the LMIB algorithm obtains mAPs near 0.4 on both data sets, while other methods have mAPs of 0.3 or worse. This suggests that LMIB can effectively exploit the intrinsic representations of multi-view examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Human Motion Recognition</head><p>We next applied the LMIB algorithm to the human motion recognition task on HMDB <ref type="bibr" target="#b53">[54]</ref>, and compared it with SVM, SCA, CCA, sKIE, and MMH. These experiments were conducted on the training and test splits suggested by <ref type="bibr" target="#b53">Kuehne et al. (2011)</ref>, and the performances averaged on these splits. We utilized different algorithms to project the multi-view video clips into the 50-dimensional subspaces and report the recognition accuracy based on these embedded examples. The confusion matrices for LMIB and MMH on HMDB are shown in Fig. <ref type="figure" target="#fig_6">4</ref>. LMIB provides performance improvements in most of the categories. The recognition accuracy of different algorithms based on various feature combinations is summarized in Table <ref type="table" target="#tab_1">1</ref>. Note that as an important baseline, our result of SVM using HOG and HOF together is 20:71 percent, which is similar to the result of 20:44 percent reported in <ref type="bibr" target="#b53">[54]</ref>. When compared with single-view SVM and SCA, the multi-view algorithms demonstrate variable performance improvements, as a result of the exploitation of complementary information underlying the multi-view features. In particular, LMIB obtains the best recognition result, due to the large-margin approach for training a multi-view classifier. As well as performance improvements in the multi-view setting, LMIB also demonstrates performance improvements compared to the conventional SVM method, when each single feature is used as input. This is because projecting examples into a low-dimensional subspace reduces noise and enlarges the discrimination between examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Publication Classification</head><p>We compared LMIB with other algorithms on a publication classification task on the Citeseer data set <ref type="bibr" target="#b54">[55]</ref>. A 3,703-dimensional vector describing the title and abstract of the paper, and a 3,309-dimensional vector representing the citing relationships between publications were employed as two different views in our multi-view setting. We used different algorithms to project the multiview publications into different low-dimensional subspaces and reported the classification accuracy on these subspaces in Fig. <ref type="figure" target="#fig_8">5a</ref>. We can see that the large-margin based multi-view LMIB algorithm performs much better than the single-view SCA method, which ignores the presence of multi-view features. Likewise, the performance of CCA and sKIE is limited, due to the neglect of the side information. Compared with MMH, which is also trained using a large-margin approach, the performance improvements of LMIB not only rely on the classifier trained using the large-margin approach, but also the information bottleneck principle to extract the   Therefore, LMIB stably outperforms other competitors in a range of settings. We show the confusion matrices in classification based on the subspaces obtained through different algorithms in Fig. <ref type="figure" target="#fig_9">6</ref>. Compared with the classification result in the original feature space (Fig. <ref type="figure" target="#fig_9">6f</ref>), projecting high-dimensional examples into lowdimensional subspaces is beneficial for performance improvements in classification. By considering the different characteristics of multiple views, we are able to find more optimal latent subspaces using multi-view algorithms such as LMIB (Fig. <ref type="figure" target="#fig_9">6a</ref>) and MMH (Fig. <ref type="figure" target="#fig_9">6b</ref>) for the classification task.</p><p>Figs. 5b and 5c compare the time efficiency of LMIB with other algorithms on the Citeseer data set. Since CCA is a general singular value problem, the training time of CCA is almost consistent. Though the classification performance of the single-view algorithm SCA is limited, the algorithm can be efficiently trained. The LMIB algorithm can be efficiently solved using the alternating direction method, and the training time is comparable to MMH. Both LMIB and MMH are much more efficient than sKIE, which requires time-consuming kernel density estimation in training and complicated conditional probability estimation in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PROOFS</head><p>In this section, we present the detailed proofs of the theoretical results on the convergence, the robustness and the generalization error bound for the LMIB algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Proof of Proposition 1</head><p>Denote the solution of the objective function F defined by <ref type="bibr" target="#b8">(9)</ref> at the iteration round k as ðw ðkÞ ; Q ðkÞ Þ. For the fixed Q ðkÀ1Þ with the value solved in the ðk À 1Þth step, the minimization of (9) w.r.t. w at the iteration round t is turned into <ref type="bibr" target="#b15">(16)</ref>, which is a standard SVM problem associated with a convex loss function. Therefore, we can deduce that at the iteration round k, the solution w ðkÞ satisfies F ðw ðkÞ ; Q ðkÞ Þ F ðw ðkÀ1Þ ; Q ðkÀ1Þ Þ, with Q ðkÞ ¼ Q ðkÀ1Þ . On the other hand, for the fixed w ðkÞ , minimizing ( <ref type="formula" target="#formula_8">9</ref>) is equivalent to minimizing <ref type="bibr" target="#b9">(10)</ref> for each view sequentially. Cover and Thomas <ref type="bibr" target="#b38">[39]</ref> proved that given pðxÞ, the mutual information IðX; T Þ is convex in pðtjxÞ, which is exactly the component of the transition matrix Q. Furthermore, because the hinge loss function is convex in pðtjxÞ, <ref type="bibr" target="#b9">(10)</ref> is convex. Then the solution Q ðkÞ satisfies F ðw ðkþ1Þ ; Q ðkþ1Þ Þ F ðw ðkÞ ; Q ðkÞ Þ, with w ðkþ1Þ ¼ w ðkÞ . Therefore, the objective function F ðw; QÞ will consistently decrease when k ! þ1 and the alternating iteration rounds converge.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Proof of Theorem 1</head><p>To prove the robustness of LMIB, let w Ã be the solution of w given the projected examples fT j g n j¼1 . Thus, we have</p><formula xml:id="formula_39">C 1 2 kw Ã k 2 þ C 2 n X n j¼1 ½1 À y j T j w Ã þ C 1 2 k0k 2 þ C 2 n X n j¼1 ½1 À y j T j 0 þ ¼ C 2 ; which implies kw Ã k ffiffiffiffiffiffi 2C 2 C 1 q</formula><p>. Given the covering number of X , we can partition X as N ðg; X ; j Á j 2 Þ sets, such that if X 1 and X 2 belongs to the same set, then y 1 ¼ y 2 and kX i 1 À X i 2 k g i for each view-i. Therefore, we have</p><formula xml:id="formula_40">jlðw Ã ; Q Ã ; X 1 Þ À lðw Ã ; Q Ã ; X 2 Þj ¼ 1 À y 1 w Ã 1 m X m i¼1 Q i X i 1 " # þ À 1 À y 2 w Ã 1 m X m i¼1 Q i X i 2 " # þ w 1 m X m i¼1 Q i X i 1 À 1 m X m i¼1 Q i X i 2 ! kwk 1 m X m i¼1 Q i ðX i 1 À X i 2 Þ 1 m ffiffiffiffiffiffiffiffi 2C 2 C 1 s X m i¼1 ffiffiffiffiffi ffi D i p g i :</formula><p>This completes the proof. t u</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Proof of Theorem 2</head><p>The standard Rademacher bound is given in the following lemma quoted in the form given in <ref type="bibr" target="#b49">[50]</ref>.</p><p>Lemma 2. Fix g &gt; 0 and d 2 ð0; 1Þ, and let F be a class of functions mapping from S to ½0; 1. Let ðX j Þ n j¼1 be drawn independently according to a probability distribution D. Then with probability at least 1 À d over random draws of samples of size n, every f 2 F satisfies</p><formula xml:id="formula_41">E D ½fðXÞ E S ½fðXÞ þ 1 g Rn ðF Þ þ 3 ffiffiffiffiffiffiffiffiffi ffi lnð 2 d Þ 2n s :</formula><p>This bound is quite general and is applicable to various learning algorithms if an empirical Rademacher complexity Rn ðF Þ of the function class F can be approximated.</p><p>For the function class of one particular view, we derive the following lemma to bound its empirical Rademacher complexity. Lemma 3. Let fX i 1 ; . . . ; X i n g be a sample of points from X i on view-i, whose dimension is denoted as D i , then given kwk B, the empirical Rademacher complexity of the class F i satisfies</p><formula xml:id="formula_42">Rn ðF i Þ 2BkQ i k 2 n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X n j¼1 À X i j Á T À X i j Á v u u t : Furthermore, considering kQ i k 2 ffiffiffiffiffi ffi D i p , we have Rn ðF i Þ 2B ffiffiffiffiffi ffi D i p n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X n j¼1 À X i j Á T À X i j Á v u u t :</formula><p>Proof. According to the definition of the empirical Rademacher complexity, we have</p><formula xml:id="formula_43">Rn ðF i Þ ¼ E sup kwk B 2 n X n j¼1 s j w T Q i X i j " # 2B n E X n j¼1 s j Q i X i j " # ¼ 2B n E ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X n j;k¼1 s j s k À Q i X i j Á T Q i X i k v u u t 2 4 3 5 2B n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi E X n j;k¼1 s j s k À X i j Á T ðQ i Þ T Q i X i k " # v u u t :</formula><p>Since the Rademacher variables are independent and E½s i ¼ 0 for all i, we get</p><formula xml:id="formula_44">Rn ðF i Þ 2B n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X n j¼1 À X i j Á T À Q i Á T Q i X i j v u u t :</formula><p>Note that the transition matrix Q i is composed of the conditional probabilities between the feature space on view-i and the subspace T , and thus the summation of each column in Q i equals 1, and</p><formula xml:id="formula_45">kQ i k 2 ffiffiffiffiffi ffi D i p kQ i k 1 ¼ ffiffiffiffiffi ffi D i p</formula><p>. The above inequality is bounded by</p><formula xml:id="formula_46">Rn ðF i Þ 2B ffiffiffiffiffi ffi D i p n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X n j¼1 À X i j Á T X i j v u u t :</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>Combining the results of Lemma 1 and Lemma 3, we can easily obtain Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Proof of Proposition 2</head><p>We first denote the expectation of Qk on kth view by</p><formula xml:id="formula_47">Q k ¼ E½ Qk ¼ 1 m X m i Q i X i ðX k Þ À1 ;<label>(22)</label></formula><p>where Q i is the true transition matrix composed of the conditional probabilities between the space on view-i and the projected subspace, and X i is the matrix composed of the examples on view-i. Applying Hoeffding's inequality, we first obtain the following lemma to measure the difference between the estimated Qk and its expectation Q k . Lemma 4. Fix d &gt; 0, and let X 1 ; . . . ; X n be a sequence of i.i.d.</p><p>random variables represented by m views. Define Qk ¼</p><formula xml:id="formula_48">1 mn P n j P m i c i ðX i j Þ and Q k ¼ E½ Qk , both of which are D T Â D k dimensional matrices. Given ½c i ðX i j Þ p 2 ½A; Aþ</formula><p>B, for some A and B &gt; 0, and then with probability at least 1 À d, the following bound holds: This completes the proof.</p><formula xml:id="formula_49">kQ k À Qk k 1 D T ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi B 2 ln</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>We now bound the derivation of the estimate Qk from the true Q k using the expectation Q k by the following lemma.</p><p>Lemma 5. Given Q k and Qk defined above, the perturbation of Q k satisfies,</p><formula xml:id="formula_50">kQ k À Qk k 1 1 mkX k k 1 X m i kQ k X k À Q i X i k 1 þ kQ k À Qk k 1 :</formula><p>Furthermore, if for each view-i, kQ k X k À Q i X i k 1 i , we have</p><formula xml:id="formula_51">kQ k À Qk k 1 1 mkX k k 1 X m i i þ kQ k À Qk k 1 :</formula><p>Proof.</p><formula xml:id="formula_52">kQ k À Qk k 1 ¼ kQ k À Q k þ Q k À Qk k 1 kQ k À Q k k 1 þ kQ k À Qk k 1 1 mkX k k 1 X m i kQ k X k À Q i X i k 1 þ kQ k À Qk k 1 :</formula><p>Furthermore, if for each view-i, kQ k X k À Q i X i k 1 i , we have</p><formula xml:id="formula_53">kQ k À Qk k 1 1 mkX k k 1 X m i i þ kQ k À Qk k 1 : t u</formula><p>Using Lemma 4 to bound the second term of Lemma 5, we achieve Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Proof of Corollary 1</head><p>According to Theorem 2, we assume that the maximum empirical Rademacher complexity of the function class is obtained on view-k, and then we get</p><formula xml:id="formula_54">RðfÞ 1 n X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffi R 2 =g 2 n r kQ k k 2 þ 11 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð mþ3 d Þ 2n s :</formula><p>If there exists perturbation ~Qk around the true transition matrix Q k , we get</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RðfÞ</head><p>1 n </p><formula xml:id="formula_55">X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffi R 2 =g 2 n r kQ k þ ~Qk k 2 þ 11 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ln ð mþ3 d Þ 2n s 1 n X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffi R 2 =g 2 n r ffiffiffiffiffiffi D k p kQ k k 1 þ ffiffiffiffiffiffi D k p k~Q k k 1 þ 11 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ln ð mþ3 d Þ 2n s 1 n X n j¼1 j þ 2B ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi D k R 2 =g 2 n r 1 þ 1 mkX k k 1 X m i i þ D T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have extended the theory of the information bottleneck and presented a novel algorithm for learning from examples represented by multi-view features. The idea is to model the multi-view learning problem by a communication system with multiple senders, and further strengthen the discrimination of the encoder through a margin maximization approach within the frame of coding theory. The proposed algorithm benefits from the IB principle and coding theory, and exhibits advantages that balance the accuracy and complexity of the multiview model and retain sufficient discrimination for classification. Theoretical results on the robustness and generalization error bound of the proposed algorithm are given and, to our knowledge, we are the first to theoretically analyze the consensus and complementary properties of multi-view features (more than two views). For the consensus, the empirical Rademacher complexity of the hypothesis class is limited, the accuracy of the solution can be enhanced, following which the generalization error bound of the algorithm is improved. For the complementarity, the accurate views counter the noisy views and ensure the algorithm's robustness. Finally, we effectively optimize the resulting objective function using the alternating direction method and evaluate it for annotation, recognition, and classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. LMIB: Large-margin multi-view information bottleneck.</figDesc><graphic coords="3,294.75,51.19,240.00,128.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 2 .</head><label>2</label><figDesc>Fix d &gt; 0, and let Qk be the estimate of Q k corresponding to the examples on view-k, where both Qk and Q k are D T Â D k dimensional matrices. Given</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:Corollary 1 .</head><label>1</label><figDesc>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi B 2 ln ð2D T D k =dÞ 2mn r Given Theorem 2 and Proposition 2, we obtain the following corollary to show the influence of the perturbation of Q k on the generalization error bound. Fix g &gt; 0 and d 2 ð0; 1Þ. Let fX j g n j¼1 be a sample of examples represented by m views drawn independently according to a probability distribution D. Assume the maximum empirical Rademacher complexity is obtained on view-k, and the corresponding transition matrix Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Harris corner is detected, and feature descriptors such as histogram of oriented gradient (HOG) and histogram of oriented flow (HOF) are computed. For classification, these space-time interest-point descriptors are clustered by k-means to obtain a set of k ¼ 4;000 visual words; hence, every clip can be represented by the 4,000-dimensional vector of HOG visual words histogram, and the 4,000-dimensional vector of HOF visual words histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A comparison of the proposed LMIB algorithm with other methods. AP scores of all categories are shown. (Top: PASCAL VOC'07; Bottom: MIR Flickr).</figDesc><graphic coords="8,105.53,51.37,369.27,199.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance comparison on (a) PASCAL VOC'07 and (b) MIR Flicker data sets on different low-dimensional subspaces generated by different methods.</figDesc><graphic coords="9,91.28,251.15,384.00,164.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Confusion matrices for HOG/HOF features of LMIB and MMH algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Classification accuracy, (b) training time, and (c) testing time of different methods for publication classification.</figDesc><graphic coords="10,88.27,198.88,390.24,222.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Confusion matrices of classification in different approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>:À2 mn 2 B 2 :</head><label>2</label><figDesc>ð2D T D k =dÞ 2mn r Proof. Considering the pth entry of the matrix, we have the following inequality using the Hoeffding inequality Pr Â jE½½ Qk p À ½ Qk p j ! Ã 2exp Setting the right hand-side of the inequality equal to d and solving for , we get Pr jE½½ Qk p À ½ Qk p j ! ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi B All the D T Â D k parameters of the matrix can be bounded by the union bound, and then we have Pr 9p : jE½½ Qk p À ½ Qk p j ! ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi B 2 ln ð2D T D k =dÞ 2mn each column in the matrix, we get Pr kQ k À Qk k 1 ! D T ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi B 2 ln ð2D T D k =dÞ 2mn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>;</head><label></label><figDesc>where the third inequality comes from applying Proposition 2 to constraint ~Qk . t u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 Average</head><label>1</label><figDesc>Performance of Different Algorithms low-dimensional intrinsic representations of examples.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>0162-8828 ß 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors greatly thank the handling Associate Editor and all anonymous reviewers for their positive support and constructive comments for improving the quality of this paper. The work was supported in part by Australian Research Council Projects FT130101457 and DP140102164, NBRPC 2011CB302400, NSFC 61121002, 61375026, and JCYJ 20120614152136201.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining Labeled and Unlabeled Data with Co-Training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Ann. Conf. Computational Learning Theory (COLT &apos;98)</title>
		<meeting>11th Ann. Conf. Computational Learning Theory (COLT &apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the Effectiveness and Applicability of Co-Training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth Int&apos;l Conf. Information and Knowledge Management (CIKM &apos;00)</title>
		<meeting>Ninth Int&apos;l Conf. Information and Knowledge Management (CIKM &apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Co-Regularization Approach to Semi-Supervised Learning with Multiple Views</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Learning with Multiple Views at ICML</title>
		<meeting>Workshop Learning with Multiple Views at ICML</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning the Kernel Matrix with Semi-Definite Programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large Scale Multiple Kernel Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sch€</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch€</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simplemkl</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-View Clustering via Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Ann. Int&apos;l Conf. Machine Learning (ICML&apos;09)</title>
		<meeting>26th Ann. Int&apos;l Conf. Machine Learning (ICML&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-View Regression via Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Kernel Method for Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint cs</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Probabilistic Interpretation of Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>688</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Dept. of Statistics, Univ. of California</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse Probabilistic Projections</title>
		<author>
			<persName><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">m-SNE: Multiview Stochastic Neighbor Embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grassmannian Regularized Structured Multi-View Embedding for Image Classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2646" to="2660" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorized Latent Spaces with Structured Sparsity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-View Latent Variable Discriminative Models for Action Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4610</idno>
		<title level="m">Manifold Relevance Determination</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel Information Embeddings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int&apos;l Conf. Machine Learning (ICML)</title>
		<meeting>23rd Int&apos;l Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shared Kernel Information Embedding for Discriminative Inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="778" to="790" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive Subspace Learning for Multi-View Data: A Large Margin Approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-Margin Predictive Latent Subspace Learning for Multi-View Data Analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2365" to="2378" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Survey on Multi-View Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>preprint 1304.5634</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiview Hessian Regularization for Image Annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2676" to="2687" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiview Vector-Valued Manifold Regularization for Multilabel Image Classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="709" to="722" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-Supervised Multiview Distance Metric Learning for Cartoon Synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4636" to="4648" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Canonical Correlation Analysis: An Overview with Application to Learning Methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized Multiview Analysis: A Discriminative Latent Space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimal Weighting of Multi-View Data with Low Dimensional Hidden States</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-Margin Multi-View Gaussian Process for Image Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int&apos;l Conf. Internet Multimedia Computing and Service (ICIMCS &apos;13)</title>
		<meeting>Fifth Int&apos;l Conf. Internet Multimedia Computing and Service (ICIMCS &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiview Fisher Discriminant Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Diethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Learning from Multiple Sources</title>
		<meeting>Workshop Learning from Multiple Sources</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Constructing Nonlinear Discriminants from Multiple Data Views</title>
		<author>
			<persName><forename type="first">T</forename><surname>Diethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Shawe</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="328" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-View Discriminant Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sufficient Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1307" to="1331" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sufficient Dimension Reduction via Squared-Loss Mutual Information Estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>Int&apos;l Conf. Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Medlda: Maximum Margin Supervised Topic Models for Regression and Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Ann. Int&apos;l Conf. Machine Learning (ICML)</title>
		<meeting>26th Ann. Int&apos;l Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Information Bottleneck Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>preprint physics/0004057</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large Margin Component Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast Alternating Linearization Methods for Minimizing the Sum of Two Convex Functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="349" to="382" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smooth Minimization of Non-Smooth Functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combining Statistical Learning with a Knowledge-Based Approach-A Case Study in Intensive Care Monitoring</title>
		<author>
			<persName><forename type="first">K</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brockhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int&apos;l Conf. Machine Learning (ICML&apos;99)</title>
		<meeting>16th Int&apos;l Conf. Machine Learning (ICML&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cutting-Plane Training of Structural SVMs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Libsvm: A Library for Support Vector Machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Trans. Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the o(1/n) Convergence Rate of the Douglas-Rachford Alternating Direction Method</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="700" to="709" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Distributed Linear Classification Algorithms via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Intelligence and Statistics</title>
		<meeting>Int&apos;l Conf. Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient Variable Selection in Support Vector Machines via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Intelligence and Statistics (AISTAS)</title>
		<meeting>Int&apos;l Conf. Artificial Intelligence and Statistics (AISTAS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robustness and Generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="391" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improved Loss Bounds for Multiple Kernel Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>Int&apos;l Conf. Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Two View Learning: SVM-2k, Theory and Practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multimodal Semi-Supervised Learning for Image Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">HMDB: A Large Video Database for Human Motion Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Eliassi</forename><surname>Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">His research interests include primarily in machine learning, multimedia search, and computer vision. Dacheng Tao (M &apos;07-SM &apos;12) is currently a professor of computer science at the Centre for Quantum Computation and Intelligent Systems and the Faculty of Engineering and Information Technology in the University of Technology, Sydney, Australia. He mainly applies statistics and mathematics for data analysis problems in data mining, computer vision, machine learning, multimedia, and video surveillance</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<date type="published" when="2010">2010. 2013</date>
			<biblScope unit="volume">5055</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">preprint 1009</note>
	<note>Chang Xu received the BE degree from Tianjin University in 2011. He is currently working toward the PhD degree at the Key Laboratory of Machine Perception (Ministry of Education) in the Peking University. Previously, he was a research intern with the Knowledge Mining group at Microsoft Research Asia. He won the Best Student Paper Award. He has authored and co-authored more than 100 scientific articles at top venues including IEEE T-PAMI, T-NNLS</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">with the best theory/algorithm paper runner up award in IEEE ICDM &apos;07. Chao Xu received the BE degree from Tsinghua University in 1988, the MS degree from University of Science and Technology of China in 1991, and the PhD degree from Institute of Electronics, Chinese Academy of Sciences in 1997. Between 1991 and 1994, he was employed as an assistant professor by the University of Science and Technology of China. Since 1997, he has been with the School of EECS at Peking University where he is currently a professor. His research interests include image and video coding, processing, and understanding. He has authored or co-authored more than 80 publications and 5 patents in these fields</title>
		<author>
			<persName><forename type="first">T-Ip</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nips</forename><surname>Icml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Icdm</forename><surname>Aistats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iccv</forename><surname>Cvpr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eccv; Acm T-Kdd</forename><surname>Kdd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multimedia</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
