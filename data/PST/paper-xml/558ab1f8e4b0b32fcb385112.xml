<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Localization and Border Detection Criteria Design in Edge-Based Image Segmentation: Automated Learning from Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Marek</forename><surname>Brejl</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Milan</forename><surname>Sonka</surname></persName>
							<email>milan-sonka@uiowa.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer En-gineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Localization and Border Detection Criteria Design in Edge-Based Image Segmentation: Automated Learning from Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E378A4A3B1303A00CF141FFB04DD8B80</idno>
					<note type="submission">received December 15, 1999; revised July 14, 2000. The Asso-</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image analysis</term>
					<term>image segmentation</term>
					<term>knowledge representation</term>
					<term>learning systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides methodology for fully automated model-based image segmentation. All information necessary to perform image segmentation is automatically derived from a training set that is presented in a form of segmentation examples. The training set is used to construct two models representing the objects-shape model and border appearance model.</p><p>A two-step approach to image segmentation is reported. In the first step, an approximate location of the object of interest is determined. In the second step, accurate border segmentation is performed. The shape-variant Hough transform method was developed that provides robust object localization automatically. It finds objects of arbitrary shape, rotation, or scaling and can handle object variability. The border appearance model was developed to automatically design cost functions that can be used in the segmentation criteria of edge-based segmentation methods.</p><p>Our method was tested in five different segmentation tasks that included 489 objects to be segmented. The final segmentation was compared to manually defined borders with good results [rms errors in pixels: 1.2 (cerebellum), 1.1 (corpus callosum), 1.5 (vertebrae), 1.4 (epicardial), and 1.6 (endocardial) borders].</p><p>Two major problems of the state-of-the-art edge-based image segmentation algorithms were addressed: strong dependency on a close-to-target initialization, and necessity for manual redesign of segmentation criteria whenever new segmentation problem is encountered.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>improved by utilizing advanced segmentation criteria that reflect higher level of knowledge about the segmented object. In most applications, however, this knowledge is not derived automatically, but introduced by an observer and then translated to a mathematical form. This approach requires expert knowledge of the given problem and is usually time consuming when new applications are designed. With the exception of active shape models, use of shape information is usually very limited or is not considered at all. Unfortunately, both traditional and new approaches require manual initialization-knowledge of approximate location of the object of interest-in order to succeed.</p><p>Previously, attempts were reported to design systems with partial ability to learn border properties from given examples. Barrett et al. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and Udupa et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> used two-dimensional (2-D) dynamic programming to develop interactive real-time border detection methods called live-wire and livelane. The method is based on interaction with the user who sets start and end points of the required border and the system has the ability to derive cost function parameters from a manually drawn border segment example. Then, it finds the least expensive path between the two points. While this method provides possibilities for automated cost function modification, it requires constant user interaction. The work of Morse et al. has addressed the trainability issues of border detection in this context <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr">Cootes et al.</ref> in their algorithm of active shape model use statistical modeling of border profiles in order to automatically design segmentation criteria <ref type="bibr" target="#b19">[20]</ref>. Comparison of the mean profile to image data is used to determine the cost of a particular segmentation instance. This approach provides fully automated scheme for design of segmentation criteria. The automated design approach is, however, tightly coupled with the active shape model segmentation process and, thus, cannot be directly used for design of image segmentation criteria for other segmentation algorithms.</p><p>We have previously introduced a neural-network-based method for automated design of segmentation criteria <ref type="bibr" target="#b20">[21]</ref>. Salient border features were organized into a feature vector that was constructed for every point along the border. Based on similarity, feature vectors were grouped and the exemplar (mean) feature vector was computed for every group. The cost function values were derived from the goodness of fit between the model and image data. This work did not, however, solve the problem of object localization and did not offer comprehensive treatment of border appearance-based cost function design.</p><p>The work reported here is attempting to solve the two main limitations of today's edge-based segmentation algorithms as described above-dependency on manual object localization and need to manually design segmentation criteria for accurate border detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>The new method for fully automated, model-based image segmentation consists of three main steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Set Design</head><p>By designing a training set (accurately defining borders of the objects of segmentation interest), the user defines the segmentation task. Images included in the training set must capture the main variability in both the object shape and the local border appearance that may be encountered during the segmentation. Landmarks are used in addition to borders to define correspondence between borders of different training objects. This correspondence is necessary for creation of both the shape and border appearance models. The landmark placement can be manual or computer-aided. In our case, a small number of landmarks are manually identified with a larger number of additional landmarks automatically determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training</head><p>Using segmentation examples, information about average shape of the objects of interest and observed shape variance is derived. This information is encoded in the shape model in a form suitable for shape-variant Hough transform (Section II-C). Similarly, the border appearance information is derived from the borders of the training objects. The created border appearance model is used to provide a measure of a likelihood that a particular image pixel described by its border appearance vector belongs to the border of a searched object.</p><p>Shape Model: Shape modeling allows object localization that is motivated by the ideas of the generalized Hough transform <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Generalized Hough transform can detect objects of arbitrary, however, a priori known shapes. Shape information is encoded in an R-Table, in which rows correspond to possible orientations of the boundary. Its columns carry information about the vectors connecting boundary points with a predefined reference point . For every boundary point , its local boundary direction is determined. In the R-Table row corresponding to direction , the vector connecting the boundary point with the reference point is computed and entered in the R-Table <ref type="bibr" target="#b2">[3]</ref>.</p><p>Mean Shape: There is a large number of papers dealing with averaging/aligning shapes into a common coordinate frame <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b27">[28]</ref>. In our work, a noniterative approach based on deriving a linear system of equations for computing the least squares fit to a set of shape instances was used <ref type="bibr" target="#b28">[29]</ref>. The shapes are derived from the training set of objects, each object being described by a set of landmarks. A set of transformation coefficients is determined that aligns the training shapes with the mean shape. The transformations are <ref type="bibr" target="#b0">(1)</ref> where , is the object aligned with the mean shape. The reference point is computed, for example as the center of gravity of the mean shape.</p><p>Shape Variance: Single shape model is typically used to create the R-Table in the generalized Hough transform. However, such approach does not consider shape variation. Fig. <ref type="figure" target="#fig_0">1</ref> shows a simple example of the Hough transform failure when the mean shape is used. Despite the fact that the training and testing objects are identical, the Hough transform is only able to correctly locate the object that is close to the mean shape. This represents a major problem in the applicability of the Hough transform to objects exhibiting shape variation.</p><p>To overcome this limitation, shape-variant Hough transform was developed that allows shape variance to be incorporated in the R-Table . For every landmark point on the border, the R-Table holds all border directions encountered in the training set and all vectors connecting the landmark with the reference point. The shape-variant Hough transform R-Table is constructed as follows. For every landmark position ;</p><p>, up to new entries into the R-Table can be made (one for every training object). The R-Table <ref type="table">values</ref> are computed as <ref type="bibr" target="#b1">(2)</ref> where is the th landmark in the shape from the aligned set of objects.</p><p>To record the value in the R-Table, direction corresponding to the landmark must be known. Gradient direction is used to determine the shape direction at a border point. Therefore, transformation defined by the vector [(1)] must be applied to the gradient direction in the original image. The direction corresponding to the aligned shape is then</p><p>The transformation defined by (1) can be viewed as translation by , rotation by and scaling by <ref type="bibr" target="#b3">(4)</ref> The rotation part of this transformation is given by (5)</p><p>Thus, the direction associated with the R-Table <ref type="table">entry</ref> is <ref type="bibr" target="#b5">(6)</ref> The entry into the R- Border Appearance Model: Local border appearance is described by a vector of image feature values. Border appearance vectors may consist for example of gray-level values sampled from a line perpendicular to the border. Fig. <ref type="figure" target="#fig_2">2</ref> shows examples of various possible border appearance vectors and demonstrates the difference between border appearance for different borders [Fig. <ref type="figure" target="#fig_2">2</ref>(a) versus Fig. <ref type="figure" target="#fig_2">2(b)</ref>]. The border appearance vector is computed for every border pixel of the training set. In order to create a statistical model of the border appearance, the set of training border appearance vectors has to be divided into groups representing the various patterns that occur in the population using fuzzy clustering <ref type="bibr" target="#b29">[30]</ref>. The mean appearance vector and appearance vector variance are computed for each of the groups.</p><p>Partitioning of a set of appearance vectors into fuzzy sets (clusters) is performed to minimize <ref type="bibr" target="#b6">(7)</ref> where expresses the degree to which the element belongs to the th cluster and is called the exponential weight which influences the degree of fuzziness of the membership (partition) matrix. The larger the number , the more fuzzy the partition matrix becomes; is the mean vector of th cluster. The clustering must satisfy standard fuzzy clustering requirements and is performed in an iterative way <ref type="bibr" target="#b7">(8)</ref> The algorithm stops when the one-step change in the partitioning matrix is below a specified limit.</p><p>Fit of image data to Border Appearance Model: The model of border appearance consists of the mean vectors and associated variances : , being the number of distinct patterns occurring in the population. <ref type="bibr" target="#b8">(9)</ref> where is the number of input vectors that have maximum membership value in the th cluster <ref type="bibr" target="#b9">(10)</ref> Following <ref type="bibr" target="#b14">[15]</ref>, border appearance vectors are assumed to be distributed as multivariate Gaussians. The quality of fit of a new border appearance vector to any of the model vectors is then given by <ref type="bibr" target="#b10">(11)</ref> where is a function performing some kind of inversion, for example <ref type="bibr" target="#b11">(12)</ref> The argument of the function in <ref type="bibr" target="#b10">(11)</ref> is the Mahalanobis distance of the sample from the model mean. Minimizing the Mahalanobis distance [which leads to maximizing ] is equivalent to maximizing the probability that comes from the distribution <ref type="bibr" target="#b14">[15]</ref>.</p><p>Comparing the input vector to all mean vectors, the best fit can be computed <ref type="bibr" target="#b12">(13)</ref> Equation ( <ref type="formula">13</ref>) provides a mechanism for generating the segmentation criterion. The value contains a comprehensive information about the border properties.</p><p>Border Appearance Variation Along Border: When the border appearance exhibits varying patterns along the border of a single object and when this pattern is consistent, it is possible to model such variance. The training borders are divided into several segments that have mutual correspondence between different objects. The division can be based on the registered landmarks. A nonambiguous correspondence, thus, exists between the segment location and each border appearance vector instance in the training set.</p><p>When such a local correspondence is established, the computation of values ( <ref type="formula">13</ref>) is modified. The appearance vector at an image point is consequently compared only to such appearance vectors from the Border Appearance Model that are associated with the border segment to which belongs <ref type="bibr" target="#b13">(14)</ref> where is a set of indexes denoting the clusters associated with the border segment . Local border correspondence must be known to take advantage of this feature during cost function construction. It means, however, that every pixel in an image would have to be associated with a particular border segment prior to the segmentation. This could be done, for example, by manually drawing aregion of interest (ROI) that consists of sections corresponding to the border segments. In the next section, a fully automated method is described that serves this purpose. Border Features: Despite its frequent utility, edge strength alone is not a powerful enough descriptor of border properties and more complex image features were designed to describe the border <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In our work, a variance matrix in the border appearance model provides feature weighting <ref type="bibr" target="#b10">(11)</ref> and the set of border features consists of three distinct groups: 1) orientation sensitive gradient profile, 2) border profile of average gray levels, and 3) average gray-level values on both sides of the border. To compensate for intensity variations across images, the feature values are normalized. Gradient magnitudes of the image are rescaled to the range and gray-level values are divided by the average gray-level values of the entire image.</p><p>Let us consider a case, when the border profile is oriented in direction. Let the border profile height be and let the number of pixels used to compute average gray-level values be (see Fig. <ref type="figure" target="#fig_3">3</ref>). The border appearance vector is computed as <ref type="bibr" target="#b14">(15)</ref> where is orientation sensitive gradient for otherwise <ref type="bibr" target="#b15">(16)</ref> ( is a vector describing gradient direction and is a normal vector to the object boundary), is computed as <ref type="bibr" target="#b16">(17)</ref> ( is maximum gradient magnitude in the entire image and is minimum gradient magnitude in the entire image.), is an average of intensity values in the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation</head><p>After the training stage is completed, object segmentation is performed in two steps. An approximate location of the object of interest is determined using shape-variant Hough transform and used for initialization of border detection.</p><p>Shape-Variant Hough Transform: The shape-variant Hough transform uses the R-Table derived from the training shapes to find an approximate location of the to-be-segmented object, thus, providing the required close-to-target initialization for the border detection.</p><p>For the functionality of the shape-variant Hough transform, a four-dimensional accumulator array has to be created. Every point of the accumulator array corresponds to a given location of a possible reference point, when the original object is scaled by and rotated by . For every image point that exhibits desired boundary properties, the possible boundary direction is determined. As is usual in Hough transform, for all considered rotations , a row in R-Table is determined that corresponds to the angle . For all vectors in the given R-Table <ref type="table">row</ref> and for all scaling parameters , the accumulator points are increased <ref type="bibr" target="#b17">(18)</ref> In the standard Hough transform, the usually corresponds to the edge magnitude and and are computed as <ref type="bibr" target="#b18">(19)</ref> When the entire image is processed, the maximum in the accumulator array is determined. The coordinates of the maximum value point are used to map the training shape back into the image data. One of the serious drawbacks of the original implementation <ref type="bibr" target="#b22">[23]</ref> of the generalized Hough transform is the fact that the edge strength used to determine the candidate boundary points is not descriptive enough. A straightforward solution to this problem is to use border appearance information represented by cost values determined from the border appearance model. Such cost values provide good distinguishing power between boundary and nonboundary points. In <ref type="bibr" target="#b17">(18)</ref>, the parameter is computed as <ref type="bibr" target="#b19">(20)</ref> where is the match between the appearance vector at the image point and the border appearance model <ref type="bibr" target="#b12">(13)</ref>, and is used to increase the difference in between points with high value of and points with low value of . Additionally, gradient direction is used as the direction information. Similarly as in the border appearance model, boundary direction has already been replaced by the gradient direction during the R-Table <ref type="table">construction</ref>. Therefore, consistency is maintained between the model creation and the segmentation processes (during which the true boundary location and direction are not known). Fig. <ref type="figure" target="#fig_4">4</ref> demonstrates the difference in the Hough transform results when only the gradient information is used, compared to employment of the Border Appearance Model.</p><p>From the coordinates of the maximum point in the accumulator array, transformation is obtained that is applied to the input shape model (the transformation is described by translation with respect to the reference point, rotation and scaling). In the case of modeling a single shape, the transformation is applied to the original shape to reconstruct the detected object. In the case of shape variability, possible shapes are available. In order to reconstruct the resulting object, the transformation is applied to all the aligned input shapes ; the transformed shapes are denoted by <ref type="bibr" target="#b20">(21)</ref> From the candidates, the shape is selected that exhibits the best overall fit to the Border Appearance Model. The overall fit is computed as</p><formula xml:id="formula_1">(22)</formula><p>The contour is selected so that <ref type="bibr" target="#b22">(23)</ref> The ability to include shape variance into the Hough transform represents a crucial step that leads to the method's applicability to real data. Figs. 1 and 5 demonstrate this importance. Fig. <ref type="figure" target="#fig_5">5</ref> shows an example of detecting corpus callosum in an MR image For snakes, the usual segmentation criterion form is <ref type="bibr" target="#b25">(26)</ref> where is an internal energy term that represents constraints on the object shape and is external energy term that is associated with the image data. The external energy is usually computed from magnitude of the image intensity gradient <ref type="bibr" target="#b26">(27)</ref> When the Border Appearance Model is used, the external energy term can reflect the border appearance (28)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL METHODS</head><p>The validation was designed to demonstrate performance of the reported approach. The ability of our method to automatically learn parameters of the shape and border appearance models from segmentation examples and to use the models to</p><p>• automatically find approximate position of segmented objects in new images, and • create cost functions that can be used by virtually any edge-based segmentation method; was determined.</p><p>To assess the performance of our method, border detection accuracy was determined in five separately trained MR segmentation tasks-localization and border detection of corpus callosum, cerebellum, vertebrae, and left and right heart ventricles. To demonstrate that our cost function design strategy is widely applicable, two distinct edge-based segmentation approaches were utilized and their results compared after performing identical localization and border criteria design steps [dynamic programming and snakes using cost functions defined in ( <ref type="formula">25</ref>), <ref type="bibr" target="#b25">(26)</ref>, and ( <ref type="formula">28</ref>)]. Note, however, that the purpose of our validation was not to compare the achieved results of dynamic programming and snakes. Rather, the general applicability of the automatically determined segmentation criteria was demonstrated in two substantially different border-based segmentation approaches.</p><p>In each of the training images, the borders of objects of interest were manually traced to create segmentation examples. A limited number of landmarks was identified on the borders. Additional landmarks were automatically placed equidistantly in-between the manually identified landmarks. In all testing images, borders were manually identified to form an independent standard. To assess segmentation performance, the automatically detected borders were compared with the manually identified borders in separate testing sets in a blinded fashion using the following border positioning indexes ;</p><p>where is the number of points on the detected border and is the signed distance of the th point on the detected border to the closest point on the manual border. According to <ref type="bibr" target="#b32">[33]</ref>, is computed as <ref type="bibr" target="#b28">(29)</ref> where is a point on the detected border and are points on the border that serves as an independent standard. The signed mean error is an indicator of a consistent bias in the detection. In our case, a positive signed mean error indicates that the detected object is smaller than expected. All error indexes are reported as mean standard deviation.</p><p>The training parameters (height of the border appearance profile, number of expected border patterns, etc.) were set based on the behavior observed in the training data. The search for the approximate location of objects of interest was done in the entire image area despite the possibility to narrow the search into a subarea derived from the training set. The purpose of the full-image search was to assess whether the algorithm gets attracted to other structures in the image.</p><p>Overall, our method's performance was assessed in segmentation of 489 objects. To demonstrate the independence of the training data selection, the cerebellum segmentation method was trained under five independent occasions. In each experiment, six training images were randomly selected from the entire set of 90 MR brain images.</p><p>In all comparative validation steps, significance of the differences was statistically assessed using a paired -test for error means. A value less than 0.05 was considered significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>Our method achieved good results in all performed segmentation tasks. Figs. <ref type="figure" target="#fig_8">6</ref><ref type="figure">7</ref><ref type="figure">8</ref>show the segmentation results achieved by our fully automated technique after its parameters were automatically determined. Figure <ref type="figure">panels</ref> show original testing images, manually identified borders defining independent standard, automatically determined approximate locations of the objects, and final segmentations using dynamic programming and snakes. The borders are displayed with an increased thickness to achieve better visualization.</p><p>The achieved segmentation accuracy is summarized in Tables I and II. Table <ref type="table">I</ref> provides the localization errors after the shape-variant Hough transform step-the errors between the automatically determined approximate locations and the independent standard. Objects of interest were correctly localized in all segmentation cases. Table <ref type="table">II</ref> shows the final segmentation accuracy of dynamic programming and snakes utilizing the automatically designed cost function. The segmentation was initialized by the automatically detected approximate object locations. Comparison of Tables I and II demonstrates the expected improvement in the segmentation accuracy after the second segmentation step. This improvement was statistically significant in ten of the 12 segmentation tests ( , moreover in eight of the 12 cases). In the two remaining cases, no significant improvement was observed ( -vertebrae and epicardial borders using snakes).</p><p>Dependency of the segmentation accuracy on the size of training set was studied by independent training in sets of five different sizes. Table <ref type="table" target="#tab_2">III</ref> demonstrates that once the main shape and border appearance variations are included in the training set, adding more training data does not lead to an increase of the final segmentation accuracy. Table <ref type="table" target="#tab_2">III</ref> gives the associated values demonstrating virtually no statistical significance of differences in rms errors between all pairs of experiments. Specifically, a larger number of training examples then nine did not improve the method's performance in the tested cases.</p><p>Table <ref type="table" target="#tab_3">IV</ref> shows that selection of the training data does not significantly influence the segmentation accuracy, as long as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>The proposed algorithms solve two important problems of edge-based image segmentation methods. First, based on both the shape model and the border appearance model, any segmentation algorithm can be automatically initialized. Second, the cost functions used in border-based image segmentation criteria can be designed automatically. Our method is applicable to image segmentation tasks with the following properties:</p><p>• shape and shape variation of objects of interest are well defined; • border appearance is reasonably consistent across the object instances (though consistent variability along the border is allowed); • representative set of examples is available for training; • edge-based image segmentation method is appropriate for the segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Border Appearance Model</head><p>To create the border appearance model, three important factors should be considered: number of clusters , height of the border profiles , and training data selection. The number of clusters should correspond with the number of expected appearance patterns. If the number of clusters is low, several distinct patterns will be represented by a single cluster. The mean appearance vector of such a cluster will be an average of the distinct patterns-an undesirable result. On the other hand, by creating too many clusters, the model would describe unimportant variations. As a result, falsely detected good fits to the appearance model can occur in nonborder pixels in noise-deteriorated parts of the image. Despite the fact that the choice of the cluster number should correspond with the number of border appearance patterns actually occurring in the training set, incorrect choice has negative influence on the results only if the mismatch is considerable. Even in such cases, the mismatches are merely lowering the differences between the border and nonborder pixel responses to the cost function.</p><p>Height of the border profile is used to create an appearance vector. The profile should be large enough to include all the pixels that consistently describe the border appearance pattern. If the height is too small, some important information may be missing. If the height is too large, the profile may contain pixels with inconsistent information about the border pattern. However, similarly as for the number of clusters, sensitivity of the final segmentation on the profile height is substantial only for large departures from the optimal values. Moreover, employing the multivariate Gaussian model <ref type="bibr" target="#b10">(11)</ref> to compute the values mostly solves this problem.</p><p>Selection of training data is another factor influencing how well the border appearance model is created. Similar to other machine learning situations, the main segmentation variability occurring in the images must be present in the training data. The model cannot be expected to recognize patterns that were not represented in the training set. However, we demonstrated that in shape and appearance consistent tasks, most of the variation is contained within a small set of examples and the results are quite independent of their actual selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shape-Variant Hough Transform</head><p>Some of the characteristics of the shape-variant Hough transform have roots in the generalized Hough transform <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The shape-variant Hough transform's main performance improvement was achieved by utilizing the border appearance model to determine pixel probability of being a border point and by including shape variance into the shape model.</p><p>The shape-variant Hough transform appears to be very robust. In none of the 489 segmentation tasks used for validation did it failed to find the correct object. However, the performance depends on the detection settings. The Hough transform search considers all possible scalings and rotations of the searched object. Since the search is not performed in an analytical manner, only a subset of all possible rotations and scalings is considered. The finer the search, the more accurate results can be obtained. Unfortunately, the computational requirements increase at the   same time. To search for objects with little shape variability (for example vertebrae in our validation sets), the resolution can be coarse and, thus, faster performance can be achieved (resolution steps of 10% for scaling and 10 for rotation were used).</p><p>To detect objects with significant shape variability (for example corpus callosum), the search has to be performed in a considerably finer resolution (resolution steps of 1% for scaling and 1 for rotation were employed).</p><p>To reduce the search space, the training data are used to automatically derive the expected ranges for possible rotation and scaling, as well as to determine the probable location of the object of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Time Requirements</head><p>Four different design and segmentation steps are worth discussing from the computational time requirement point of view: design of the shape model, design of the border appearance model, application of the shape-variant Hough transform, and computation of cost values.</p><p>In our validation studies, calculation of the shape model required 1-8 s. The time depends on the number of training images <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref>, as well as on the number of landmarks defined on every image (41-51).</p><p>Time required for the automated design of the border appearance model ranged from 30 s-3 min. The time depends on the number of training data, size of the border appearance vector (16-24 elements), and the number of expected border patterns <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref>.</p><p>The time required by the shape-variant Hough transform to detect approximate object location was highly variable. It depends on the size of the image, number of entries in the R-Table , 
and most importantly on the resolution used in the accumulator array. In our experiments, processing of one image lasted from 15 s-20 min. (Note that the time required here was influenced by the method design requiring to search the entire image and not to utilize the expected object location information provided by the training stage.)</p><p>The time required to compute the cost values using the border appearance model depends on the size of the image, on the number of clusters in the model and on the size of the appearance vectors. In our experiments, the typical processing time was between less than 1 s to about 5 s.</p><p>The reported computation times were achieved on an HP Kayak Workstation, Pentium II, 300 MHz. The implementation was not optimized for speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extension to Three Dimensions</head><p>The presented methodology can be extended to three dimensions. We discuss several key points in the following order: definition of training data, creation of the border appearance model, computation of cost values, creation of the shape model, and detection of an approximate object location.</p><p>In order to provide training data, manually outlined borders (surfaces) of the objects have to be provided along with the registration landmarks. Some of the difficulties include the complexity and time requirements for manual outlining of the 3-D surfaces. Similarly, 3-D correspondence of landmarks is not as obvious as it is in 2-D. Some approaches have been introduced in the literature to assist landmark placement and registration on 3-D objects <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>Provided that manual outlines of the objects and registered landmarks are available, an approach identical to that described for the 2-D case can be taken to construct the border appearance model. Border appearance vectors remain one-dimensional (1-D) and are derived from 1-D border profile vectors. The only difference is that in a volumetric application, border profile vectors are oriented in space instead of on a 2-D plane. To get the needed 3-D orientations, a 3-D edge detector has to be applied. We have derived a 3-D edge detector that can be applied directly to anisotropic image data <ref type="bibr" target="#b35">[36]</ref>. The detector outperforms a 3-D version of the Canny edge detector not only in the anisotropic images, but also in images with high level of superimposed noise.</p><p>The fuzzy c-means clustering algorithm determining the border appearance model remains unchanged. Local correspondence can be established in the same way as presented here, using defined landmark points. The fit of border appearance vectors to the border appearance model is computed just as in the 2-D case. The cost function is constructed based on the values with no changes.</p><p>To create a 3-D shape model, a 3-D version of the surface alignment must be used <ref type="bibr" target="#b33">[34]</ref>. The matrix sizes will increase considerably, since the number of points defining a 3-D object is considerably larger than the number of points defining a 2-D object. To search for the approximate object location, the 2-D version of the shape-variant Hough transform has to be extended into the 3-D space. The R-Table is no longer organized by a single angle of the border direction. The direction is described by two angles. Equation ( <ref type="formula">19</ref>) that was used to compute the reference points in an accumulator array must be rewritten to consider rotation, translation and scaling in the 3-D space. Accumulator array becomes six-dimensional (three space coordinates, scaling and two angles describing rotation). Despite the fact that the required changes can be easily mathematically derived, the computational requirements connected with these dimensionality changes may require some structural changes to the algorithm in order to make them applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper provides a methodology for fully automated model-based image segmentation. All the information necessary to perform image segmentation is automatically derived from a training set that is provided in a form of segmentation examples. The training set is used to construct two models representing the objects-shape model and border appearance model.</p><p>A two-step approach to image segmentation was proposed. In the first step, an approximate location of the object of interest is determined. In the second step, accurate border segmentation is performed. The approximate location of the object of interest determined in the first step is used to either create a ROI or to otherwise initialize the second-step algorithm that attempts accurate image segmentation.</p><p>The developed shape-variant Hough transform robustly localizes objects of arbitrary shape, rotation or scaling and can handle object variability. Our automatically created border appearance model is used to automatically design cost functions that can serve as the segmentation criteria of edge-based segmentation methods. The automated design of cost function greatly simplifies and speeds up design of new image segmentation applications.</p><p>Our method was tested in five different segmentation tasks that included 489 objects to be segmented (endocardial and epicardial borders of the heart, corpus callosum and cerebellum, and vertebrae in MR images). The automated detection of the approximate object locations consistently provided good starting initialization. The automatically designed cost function was applied to the dynamic programming and snakes segmentation algorithms to demonstrate the general applicability of our approach. The segmentation was compared to manually defined borders with good results (root mean square border positioning errors ranging from 1.1-1.6 pixels). The presented work solves two major problems of the state-of-the-art edge-based image segmentation algorithms: the strong dependency on a close-to-target initialization, and the necessity for manual redesign of segmentation criteria whenever new segmentation problem is encountered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )</head><label>1</label><figDesc>Training set design-selection of images, manual boundary tracing, and landmark identification. 2) Training • Shape model design-alignment of training contours, mean shape and shape variation represented in an R-Table. • Border appearance model design-statistical model of local border and nonborder appearance. 3) Segmentation • Shape-variant Hough transform-approximate object localization, using the shape model and border appearance model. • Edge-based object segmentation-accurate border detection using criteria derived from the shape model and border appearance model. The segmentation information is automatically derived from a training set provided in a form of segmented example images. The training set is used to create two statistical models representing the object(s) of interest-the shape model and the border appearance model. The models are used during the segmentation to detect an approximate location of the object of interest in a new image and to automatically design a cost function that can be used in the segmentation criterion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Importance of incorporating shape variability into the Hough transform. (a) Training and testing image; (b) maximum intensity projection to x-y plane of the Hough transform accumulator array and outlined resulting shapes (only the mean shape used); (c) maximum intensity projection to x-y plane of the shape-variant Hough transform accumulator array and outlined resulting shapes (shape variance incorporated).</figDesc><graphic coords="2,302.64,62.28,256.08,140.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of border appearance vectors (gray levels-left, orientation sensitive gradient magnitude-right). Two different definitions of the desired borders are given in (a) and (b), marked in white. In this example, the border properties remain approximately identical along the border in the same image, however, the border properties differ for different border definitions, compare (a) with (b).</figDesc><graphic coords="3,302.76,151.01,248.00,70.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Geometry of computing border appearance vectors.</figDesc><graphic coords="4,336.66,62.28,183.12,124.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Detection of vertebrae-comparison of results of the Hough transform (a) when only gradient information is used and (b) when the border appearance model is used.</figDesc><graphic coords="5,309.66,193.57,234.00,120.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of Hough transform results (a) when border appearance variation is used and (b) when it is not.</figDesc><graphic coords="6,79.86,215.53,171.00,133.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>• unsigned mean (L1 norm) [mean]: ; • signed mean [sgn. mean]: ; • root mean square error (L2 norm) [rms]: ; • maximum error (L norm) [max]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>Epicardial and endocardial borders. Total of 58 2-D MR images of thorax were available, 21 images were used for training and the remaining 37 images for testing. The images were collected at a resolution of 256 256 pixels, 1.5-1.8 mm/pixel. Each training image contained 3 landmarks manually defined on the endocardial border with the total of 41 epicardial and 51 endocardial landmarks. Endocardial and epicardial borders were treated separately during training as well as during segmentation. For the epicardial border, the number of expected distinct border patterns was 5, the border appearance vector height was 11 pixels ( ). Values of 10 and 5 were used for the endocardial borders that exhibited more appearance variability. • Corpus callosum in 2-D MR images of brain. Total of 90 images were available, out of which 15 were used for training. The images of resolution 320 256 pixels, 0.7 mm/pixel were obtained by interpolating the original volumetric data acquired with the 1.5-mm slice thickness and (0.7 0.7) mm coronal plane resolution. The contours of corpus callosum contained 50 landmarks, out of which three were defined manually. 6 and 4 were used. • Cerebellum in 2-D MR images of brain. The same set of 90 MR brain images was used here. Six images were used for training. Each training image contained manually outlined borders with 50 landmarks, out of which seven were defined manually. 5 and 3 were used. • Vertebrae in 2-D MR images of spine. Total of 55 images were available, resolution of pixels, 0.6 mm/pixel. Since this segmentation task required to segment multiple vertebrae from each image, more than 400 vertebrae were identifiable. Fifteen images were selected for training, each with example segmentation of a single vertebra. Each training image contained manually outlined borders with 50 landmarks, out of which four were defined manually. and were used. To assess sensitivity of the learning efficiency on the number of training examples, segmentation of the cerebellum was performed after training in 3, 6, 9, 12, 15, and 30 brain MR images. The first three training images were selected to represent the main variation in shape and border appearance as determined visually in the set of available images. Additional training sets were formed by randomly adding new images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Detection of corpus callosum and cerebellum. (a) Original image. (b) Manually traced borders. (c) Approximate location. (d) Final segmentation using dynamic programming. (e) Final segmentation using snakes. (Corpus callosum and cerebellum were segmented independently.) Subwindows of the analyzed image shown.</figDesc><graphic coords="8,172.14,481.95,250.00,190.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Detection of vertebrae. (a) Original image. (b) Manually traced borders. (c) Approximate location. (d) Final segmentation using dynamic programming. (e) Final segmentation using snakes. Subwindows of the analyzed image shown.</figDesc><graphic coords="9,132.66,379.20,325.00,291.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table is made only if it has not already been included. The R-Table, therefore, represents the shape model derived from training examples. The R-Table containing shape variance may include up-to times more elements than in the shape-invariant case.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III STUDY</head><label>III</label><figDesc>OF DEPENDENCY ON NUMBER OF TRAINING DATA: PAIRED TWO-SAMPLE t-TEST FOR MEANS (COMPARISON OF rms ERRORS), p VALUES ARE GIVEN. N = NUMBER OF TRAINING OBJECTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV STUDY</head><label>IV</label><figDesc>OF DEPENDENCY ON TRAINING DATA SELECTION: PAIRED TWO-SAMPLE t-TEST FOR MEANS (COMPARISON OF rms ERRORS), p VALUES ARE GIVEN. SIX RAMDOMLY SELECTED TRAINING OBJECTS WERE USED IN ALL CASES</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the following people and institutions for providing data for experiments and validation: J. W. Haller and L. Bolinger, Department of Radiology, and T. J. Grabowski, Department of Neurology, The University of Iowa Hospitals and Clinics for providing MR data of brains and spines as well as their object tracing expertise. J. H. C. Reiber, Department of Radiology, Leiden University Medical Center, The Netherlands for providing MR images of thorax together with manual tracings of epicardial and endocardial borders.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation techniques</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comp.Vision, Graphics, and Image Proc</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="100" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of some region growing operators for image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zamperoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Image Processing and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Cappelini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Marconi</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="204" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<title level="m">Image Processing, Analysis, and Machine Vision</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<publisher>PWS</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Princeton Univ. Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimal Control and Differential Games: Collection of Papers</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Pontriagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Amer. Math. Soc</publisher>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Use of active shape models for locating structures in medical images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="365" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">User-steered image segmentation paradigm: Live wire and live lane</title>
		<author>
			<persName><forename type="first">A</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Pennsylvania</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<date type="published" when="1995">1995</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
	<note>Tech. Rep. MIPG</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of 3-D objects using live wire</title>
		<author>
			<persName><forename type="first">A</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep. MIPG</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<date type="published" when="1997">1997</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive boundary detection using &apos;live-wire&apos; two-dimensional dynamic programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers in Cardiology</title>
		<meeting><address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc. Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="635" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast, accurate, and reproducible live-wire boundary extraction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Mortensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale medial analysis of medical images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical</title>
		<editor>
			<persName><forename type="first">Barrett</forename><surname>Imaging</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gmitro</forename></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="112" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zoom invariant vision of figural shape: The mathematics of cores</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Eberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Fritsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Eur. Conf. Computer Vision</title>
		<meeting>5th Eur. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Statistical models of appearance for computer vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Manchester, Manchester, U.K., Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparing active shape models with active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Br. Machine Vision Conf</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Pridmore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Elliman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User-steered image segmentation paradigms: Live wire and live lane</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Pennsylvania, Dept. Radiol</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<date type="published" when="1995">1995</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
	<note>Tech. Rep. MIPG</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boundary detection via dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing, Proc. SPIE</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1808</biblScope>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trainable optimal boundary finding using two-dimensional dynamic programming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Pennsylvania</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Philadelphia, PA, Tech</pubPlace>
		</imprint>
	</monogr>
	<note>Rep. MIPG-180</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trainable method of parametric shape description</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical image segmentation, automated design of border detection criteria from examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brejl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method and means for recognizing complex patterns</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V C</forename><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">654</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalizing the Hough transform to detect arbitrary shapes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Procrustes methods in the statistical analysis of shape</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="339" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Morphometric Tools for Landmark Data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The complex bingham distribution and shape analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="285" to="299" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Closed form solution of absolute orientation using unit quaternions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="629" to="642" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmentation and interpretation of MR brain images: An improved active shape model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1049" to="1062" />
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<title level="m">Pattern Recognition with Fuzzy Objective Function Algorithms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust simultaneous detection of coronary borders in complex images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Winniford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="1995-02">Feb. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmentation of intravascular ultrasound images: A knowledge-based approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siebes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bissing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dejong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="719" to="732" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A methodology for evaluation of boundary detection algorithms on medical images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chalana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active shape models and the shape approximation problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="601" to="607" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Development of a point-based shape representation of arbitrary three-dimensional medical objects suitable for statistical shape modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krahnstover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Int. Symp. Medical Imaging</title>
		<meeting>SPIE Int. Symp. Medical Imaging<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Directional 3-D edge detection in anisotropic data: Detector design and performance assessment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brejl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding (Special Issue on Analysis of Volumetric Images)</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="84" to="110" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
