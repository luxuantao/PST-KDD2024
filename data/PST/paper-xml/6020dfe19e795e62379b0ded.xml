<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Sparse Attention Network For Session-based Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Yuan</surname></persName>
							<email>jhyuan@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Song</surname></persName>
							<email>zhsong@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyou</forename><surname>Sun</surname></persName>
							<email>mysun@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
							<email>xlwang@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Intelligent Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Sparse Attention Network For Session-based Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session-based Recommendations recommend the next possible item for the user with anonymous sessions, whose challenge is that the user's behavioral preference can only be analyzed in a limited sequence to meet their need. Recent advances evaluate the effectiveness of the attention mechanism in the session-based recommendation. However, two simplifying assumptions are made by most of these attentionbased models. One is to regard the last-click as the query vector to denote the user's current preference, and the other is to consider that all items within the session are favorable for the final result, including the effect of unrelated items (i.e., spurious user behaviors). In this paper, we propose a novel Dual Sparse Attention Network for the sessionbased recommendation called DSAN to address these shortcomings. In this proposed method, we explore a learned target item embedding to model the user's current preference and apply an adaptively sparse transformation function to eliminate the effect of the unrelated items. Experimental results on two real public datasets show that the proposed method is superior to the state-of-the-art sessionbased recommendation algorithm in all tests and also demonstrate that not all actions within the session are useful. To make our results reproducible, we have published our code on https://github.com/SamHaoYuan/DSANForAAAI2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The core of personalized recommendation systems is to recommend different products or services to users by analyzing their past behaviors. Therefore, sufficient historical data and a complete user profile are critical for accurate recommendation results. However, in many practical application scenarios, the user often cannot be identified, and their related historical behaviors cannot be associated. In this case, the recommender system needs to accurately capture the user's intent and preference from a relatively short user session and generate the recommended results with the limited information. Thus, the session-based recommender system, which aims to predict the next possible click or consumption item based on the current session, has attracted the attention in academics and industries <ref type="bibr" target="#b10">(Ludewig and Jannach 2018;</ref><ref type="bibr" target="#b23">Zhao et al. 2020)</ref>. The critical problem of the session-based recommendation is how to make the best use of limited information to generate accurate recommendation results. Recently, the attention mechanism, which can automatically assign different influence weights to items to capture the related information, has demonstrated outstanding performance in sequence modeling and is widely applied in various session-based recommendation algorithms <ref type="bibr" target="#b8">(Li et al. 2017;</ref><ref type="bibr" target="#b9">Liu et al. 2018;</ref><ref type="bibr" target="#b22">Xu et al. 2019;</ref><ref type="bibr" target="#b11">Luo et al. 2020)</ref>. The weighted sum of the items within the session gives a better representation of the user preference and improves prediction accuracy, showing the importance of the attention mechanism in this task.</p><p>Although the attention-based methods significantly improve the prediction results, there are still some limitations. First, the last-click, which is the last user action within a session, is usually considered as the user's current interest <ref type="bibr" target="#b9">(Liu et al. 2018;</ref><ref type="bibr" target="#b22">Xu et al. 2019;</ref><ref type="bibr" target="#b11">Luo et al. 2020)</ref> or used as the query vector for the weight assignment in <ref type="bibr">RNN (Li et al. 2017)</ref>, MLP <ref type="bibr" target="#b9">(Liu et al. 2018)</ref>, and GNN models <ref type="bibr" target="#b21">(Wu et al. 2019</ref>), but it does not always accurately reflect the user's real preference. For example, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, if we use the last-click item iWatch as the query vector, the resulting attention coefficient is far from the score of using the item iPhone 11, which is the real target item. Since the real target item cannot be known in advance, an intuitive solution is to use each item in the candidate set as the query vector to find the best one. However, in the real world, the candidate set is vast, and the enormous computational overhead makes this choice unacceptable. Second, not all items in the session are related to the user's intent; for example, clicking one item by mistake or browsing through extraneous promotions. Even if we use the real target item, the conventional attention mechanism still give the unrelated item a small weight, as demonstrated by "book", "wallet", "lipstick", and "bag" in the second session of Figure <ref type="figure" target="#fig_0">1</ref>, which may add some useless information to the final representation of the session. Consequently, this strictly positive score is wasteful, making models less interpretable and assigning probability mass to many implausible outputs <ref type="bibr" target="#b14">(Peters, Niculae, and Martins 2019)</ref>.</p><p>To overcome the issue mentioned above, we propose a dual sparse attention network for the session-based recommendation. For simplicity, we name the proposed model DSAN. Specially, we first explore the interaction and correlation between each item within the session and learn a representation of the user's current preference based on the known information by a self-attention network. For convenience, we call this representation target embedding since it contains the position information of the target item. Noting that we do not use any other unknown information like the real index of the target item. Subsequently, we use this target embedding as a query vector and apply a vanilla attention network to distinguish the importance of different items within the current session. In this way, we could construct a more reliable session representation. Finally, we combine this learned target embedding and the entire session representation by a neural network to get a final representation, which is used to predict the user's next click. Moreover, to tackle the possible spurious user behaviors in the session, we introduce an adaptively sparse transformation function in both attention layers, which has context-dependent sparsity patterns to pick out more useful items, give higher weight to the critical item, and zero value to the useless item in the session.</p><p>The main contributions of this work are summarized as follows:</p><p>• We propose a novel framework based on a dual attention network composed of self-attention and vanilla attention. It learns a target embedding with the item-level collaborative information and uses different query, key, and value vectors for high-level information to model the sessionbased recommendation scenario effectively. • We introduce an adaptively sparse attention mechanism based on the context to find the possible unrelated item in the session and retain higher weight for the related items. • Experiments on two real datasets show that our proposed model can not only achieve better results than state-ofthe-art methods but also improves the model discernibility because of the sparse attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we review the related work about the sessionbased recommendation, including conventional methods and neural-network-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conventional Methods</head><p>Since the user's identification is unknown, the session-based recommendation is limited to the context within the sessions. Hence, for conventional methods, simple matrix factorization <ref type="bibr" target="#b13">(Mnih and Salakhutdinov 2008;</ref><ref type="bibr" target="#b8">Koren and Bell 2015)</ref> and Item- <ref type="bibr">KNN (Sarwar et al. 2001)</ref> are not suitable for the session-based scene because of ignoring the order of the user's behaviors. To be more consistent with the sequence scenario, Gu, Dong, and Zeng proposed the model based on Markov chains, making predictions by the sequential connections between adjacent clicks. For sequenceaware recommendation tasks, FPMC <ref type="bibr" target="#b16">(Rendle, Freudenthaler, and Schmidt-Thieme 2010)</ref> combines Markov chain and matrix factorization to simulate the sequential behavior between two adjacent clicks, which achieves a better result.</p><p>Recently, methods based on nearest-neighbors obtain competitive performance. SKNN <ref type="bibr" target="#b6">(Jannach and Ludewig 2017</ref>) is a session-based k-nearest-neighbors approach, which considers the sessions that contain any item of the current session as neighbors. STAN <ref type="bibr" target="#b2">(Garg et al. 2019</ref>) is an extension of SKNN that additionally considers three factors, e.g., the position. However, this kind of approach merely considers the item relevance information and ignores the transition of the user's interest reflected by the session sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural-Network-based Methods</head><p>In recent years, thanks to the powerful representation capability of deep With the help of GNN, the model is capable of capturing the complex transition between items and attains better item embeddings, which improves the performance to a great extent. However, all these neural-network-based methods with the attention model use the last-click or other heuristic ways to generate the query vector, which ignores the real user's current preference. Bert4Rec <ref type="bibr" target="#b17">(Sun et al. 2019</ref>) employs the deep bidirectional self-attention to model user behaviors and first tries to learn the user's current preferred target embedding, but the simple repetitive multi-layer structure makes the model lose the original information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>In this section, we formulate the problem of session-based recommendation and then introduce the sparse transformation function, which serves as a fundamental module of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>The typical task of session-based recommendation is to predict the item that the user most likely interact with next based solely on the action records (e.g., clicks) within a short period. Since the user's long-term preference profile is unknown, making decisions with limited information is a considerable challenge.</p><p>Let I = {i 1 , i 2 , ..., i m } denote the set of all unique items involved in all sessions and S = {s 1 , s 2 , ..., s n } denote a session ordered by timestamps, where s p ∈ I represents the p th clicked item of the user and n is the length of the session. In this paper, given a prefix of the session truncated at time step t, S t = {s 1 , s 2 , ..., s t }(1 &lt; t &lt; n), the goal of the proposed model is to predict the next click item s t+1 . To be exact, the proposed model learns to generate a score ŷi for each item i ∈ I and then items corresponding to the top-K scores will be presented to users as results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse transformation function</head><p>The two core components of the attention mechanism are the alignment model and the transformation function. One is used to compute attention weights, and the other is to transform weights into probabilities. Usually, the transformation function is a well-known function softmax <ref type="bibr" target="#b0">(Bridle 1990</ref>), which returns positive values and dense output probabilities. However, this nonzero probability may assign weights to the useless data, affecting the ability to find the relevant items.</p><p>Essentially, softmax is one of mappings from</p><formula xml:id="formula_0">R d to d−1 , where d−1 = {p ∈ R d |1 T p = 1, p ≥ 0} denotes the d-1 dimensional simplex.</formula><p>Sparse transformations tend to yield zero for the low-scoring in the vector, which is named sparsemax <ref type="bibr" target="#b12">(Martins and Astudillo 2016)</ref>:</p><formula xml:id="formula_1">sparsemax(x) = argmin p∈ d−1 ||p − x|| 2 (1)</formula><p>where x is the input vector and p is the output vector. In this paper, we introduce a novel family of the transformation function, namely α-entmax (Peters, Niculae, and Martins 2019), to replace the softmax, which has been proven useful for the application of NLP <ref type="bibr" target="#b2">(Garg et al. 2019;</ref><ref type="bibr" target="#b1">Correia, Niculae, and Martins 2019)</ref>,</p><formula xml:id="formula_2">α-entmax(x) = argmax p∈ d−1 p T x + H T α (p)</formula><p>, where where H T α (p) is the Tsallis α-entropies <ref type="bibr" target="#b19">(Tsallis 1988</ref>), which is a family of entropies parametrized by a scalar α &gt; 1. From the equation 2, we can prove that the softmax function equals 1-entmax and sparsemax is the 2-entmax, where the Shannon entropy and Gini entropy are the entropic regularizers respectively. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the parameter α controls the shape and sparsity of this function. When 1 &lt; α &lt; 2, this function tends to produce sparse probability distribution and has smooth corners. In this paper, in order to distinguish unrelated information in the session, we replace the transformation function with α-entmax in the attention mechanism. Moreover, we present a method to automatically learn α, allowing each session to choose the parameter based on the context adaptively.</p><formula xml:id="formula_3">H T α (p) =      1 α(α − 1) j (p j − p α j ), α = 1 H S (p), α = 1 (2) 2 1 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 = 1 (softmax) = 1.25 = 1.5 = 2 (sparsemax) = 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we present the proposed model in detail. This model has four main components: embedding layer to convert input session into two vectors, target embedding learning to exploit the item-level collaborative information by sparse self-attention, target attention layer to combine with initial information, and prediction layer for the final result. The complete pipeline of the calculation is demonstrated in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding layer</head><p>First of all, we present an embedding layer to convert the input session into two vectors. One is the item embedding, and the other is the positional embedding. We introduce a learnable positional embedding module <ref type="bibr" target="#b17">(Sun et al. 2019)</ref>, which is used to map the position index to a dense vector for capturing the temporal influence of the input. Formally, given the input session S t = {s 1 , s 2 , ..., s t }, for any element s p ∈ S t , the hidden representation of it is</p><formula xml:id="formula_4">c i = Concat(x i , p i ) (3)</formula><p>where x i ∈ R d is the embedding of the item, p i ∈ R d is the positional embedding of the item, and c i ∈ R 2d is the concatenated embedding of the item and position. Positional embeddings allow the proposed model to know about the portion of the session that it is dealing with, so the concatenated embedding could learn about the item's collaborative information of the item and position within the session. In order to learn the current user's preference representation without specifying the target item information, we append a special item index (e.g., the largest item index plus one) at the end of the input sequence, which is denoted by x s with its position embedding p s and is initialized with other item embeddings, to indicate the item that we need to predict. So the concatenated embedding is Ĉ = {c 1 , c 2 , ..., c t , c s }. c s , at the position of t + 1, is composed of x s and p s , including the information of the special item index and the position of the item to be predicted. The special item index in different sessions share the same embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Embedding Learning</head><p>As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, the outputs of the embedding layer are the concatenated embeddings Ĉ. Then we introduce how to learn the target embedding by self-attention to fuse the entire context of the session. Firstly, we adopt the Scaled Dot-Product Attention with the sparse transformation to capture the dependency between item pairs within the session:</p><formula xml:id="formula_5">Â = α-entmax( QK T √ 2d )V<label>(4)</label></formula><p>where Q contains representations of the queries, K is the key matrix, and V is the value matrix of the items attended.</p><p>In this layer, the sparse transformation is used to generate the attention weight for the first time to ensure that the learned target embedding and new item embeddings contain less unrelated information as much as possible. For each session, we learn its own α by:</p><formula xml:id="formula_6">α = σ(W α c s + b α ) + 1 (5)</formula><p>where W α ∈ R 1×2d denotes the weighting matrix, b ∈ R is the bias value, and σ denote the sigmoid function. In this way, α is determined by c s in different session, which included the special item index and the largest position. The value of α is also mapped to [1, 2], which has a smooth corner. Then we make K = V = Ĉ, but</p><formula xml:id="formula_7">Q = f ( ĈW Q + b Q ) (6)</formula><p>where</p><formula xml:id="formula_8">W Q ∈ R 2d×2d is the weighting matrix, b Q ∈ R 2d</formula><p>is the bias vector and f (•) denotes activate function ReLU. Through the projection, the representation of the item can be more flexible. For example, the dot product in self-attention satisfies the commutative law, that is, QK T is equal to KQ T if Q = K, leading to the different role of the item may have the same effect.</p><p>Although the self-attention mechanism learns the new representation of all items, it is mainly based on linear projections. Then we apply Position-wise Feed-Forward Network to endow the model with more non-linearity,</p><formula xml:id="formula_9">F F N ( Â) = max(0, ÂW self 1 + b 1 )W self 2 + b 2 (7) where W self 1 , W self 2 ∈ R 2d×2d are both weighting matri- ces, b 1 , b 2 ∈ R 2d</formula><p>are bias vectors, and all sessions will share the same parameters. After that, we add a residual connection and layer normalization on the result to alleviate the instability of the model training. We also add the dropout mechanism to alleviate the overfitting. For simplicity, we define the whole sparse self-attention network above as</p><formula xml:id="formula_10">E = SAN ( Ĉ)<label>(8)</label></formula><p>where E = {e 1 , e 2 , ..., e t , e s } is the final output of selfattention network. Additionally, {e 1 , e 2 , ..., e t } are new item embeddings of the session after the information extraction by the network. e s is the learned target embedding, which contains the special item index and fuses the entire session's information to denote the user's real current preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Attention Layer</head><p>The self-attention network can be considered as a feature extractor, in which each item embedding contains the information with each other, including the learned target embedding. However, it ignores the initial information after the information extraction. Therefore, in this layer, we directly apply a vanilla attention network based on the different query, key, and value to learn the entire session representation. Given E ∈ R | Ĉ|×2d , the target attention layer aims to learn the whole session representation based on the learned target embedding and the initial input. Formally, a feed-forward network is used to learn the weight:</p><formula xml:id="formula_11">β p = α-entmax(W 0 f (W 1 e p + W 2 e s + b a ))<label>(9)</label></formula><p>where W 1 , W 2 ∈ R 2d×2d , W 0 ∈ R 1×2d are the weighting matrices, b a ∈ R 2d is the bias vector, f (•) denotes the activation function ReLU, and α is given by</p><formula xml:id="formula_12">α = σ(W α e s + b α ) + 1.<label>(10)</label></formula><p>Noting that K = {e 1 , e 2 , ..., e t } is the key matrix and e s is the vector of query, which are both the outputs of the equation 8. Here we use the sparse transformation again to get rid of the unrelated items in the initial input session when learning the whole session representation.</p><p>In a word, we use the learned target embedding to be the query, and the new item embeddings learned by the selfattention network to be the key. β p represents the attention weight of the item e p with the learned query item e s . As a result, this attention weight is based on the known information of all clicked items and the target item. It can capture the correlations between the item within the session and the user's current preference.</p><p>After obtaining the attention score vector β = {β 1 , β 2 , ..., β t } with respect to the current session prefix S t , the session representation can be calculated as h s = p=1 β p c p , where h s ∈ R 2d denotes the session representation and c p is the value vector, which is initial item embedding before entering into the self-attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Layer</head><p>In this layer, we evaluate the probability of the next clicking item based on the outputs above. We first concatenate the learned target embedding e s and the session embedding h s , and then apply a feed-forward neural network to get the final representation of the proposed model,</p><formula xml:id="formula_13">h = Concat(e s , h s ) z = f (W z h + b z )<label>(11)</label></formula><p>where z ∈ R d denotes the final output of the proposed model, W z ∈ R d×4d is the weighting matrices, b z ∈ R d is the bias vector and f (•) is the non-linear activation function SELU <ref type="bibr" target="#b7">(Klambauer et al. 2017)</ref>. For each item i ∈ I, we get its probability as follows:</p><formula xml:id="formula_14">ẑ = w k L2N orm(z), xi = L2N orm(x i ) ŷi = sof tmax(ẑ T xi ) (12)</formula><p>where x i is the initial embedding of the item i and ŷi denotes the probability of the item in the candidate item set I. L2N orm is the L2 Normalization function and w k is the normalized weight. This weighted normalization <ref type="bibr" target="#b4">(Gupta et al. 2019</ref>) and the Regularizing Softmax loss (Zheng, Pal, and Savvides 2018) make the training process more stable and insensitive to hyper-parameters. Finally, the loss function is defined as the cross-entropy of the prediction and the ground-truth. It can be written as follows:</p><formula xml:id="formula_15">L(y, ŷ) = − m j=1 y i log( ŷi ) (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where y is the one-hot encoding vector of the ground truth item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Analysis</head><p>In this section, we first describe the setup of the experiments.</p><p>And then, we design experiments to prove the performance of our proposed DSAN and conduct detailed analysis under different experimental settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiemts setup</head><p>Datasets To evaluate the effectiveness of the proposed model, we use two real-world representative datasets, i.e., Diginetica<ref type="foot" target="#foot_0">1</ref> and Retailrocket<ref type="foot" target="#foot_1">2</ref> . For simplicity, we name them DN and RR. The DN dataset comes from CIKM Cup 2016, and we only used the released transaction data. The RR is a dataset on a Kaggle contest published by an e-commerce company, which contains the user's browsing activity within six months. Both two datasets are publicly available.</p><p>For a fair comparison, we follow the previous work <ref type="bibr" target="#b22">(Xu et al. 2019)</ref> to filter out all sessions of length less than 3 and items which are fewer than 5 occurrences in each dataset, and then we take last week in the two datasets as the test set. Furthermore, we conducted a segmentation preprocessing for the session. For each input session S = {s 1 , s 2 , ..., s n }, we have generated the corresponding session prefix and label pairs ([s 1 , s 2 ]; s 3 ), ([s 1 , s 2 , s 3 ]; s 4 ), ..., ([s 1 , s 2 , ..., s n−1 ]; s n ). It is worth noting that the session containing at least two items is more conducive to methods based on the GNN <ref type="bibr" target="#b22">(Xu et al. 2019)</ref>. The statistics of the three datasets after preprocessing are shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Metrics</head><p>We compare the proposed model DSAN with the following methods:</p><p>• S-POP recommends the most popular items in the current session. It is an improved version of the popularity-based method.</p><p>• FPMC (Rendle, Freudenthaler, and Schmidt-Thieme 2010) is a hybrid model combining matrix factorization and Markov chains.</p><p>• SKNN (Jannach and Ludewig 2017) is a session-based k-nearest-neighbors approach.</p><p>• STAN <ref type="bibr" target="#b2">(Garg et al. 2019</ref>) is an extension of SKNN approach with some additional factors.</p><p>• GRU4Rec <ref type="bibr" target="#b5">(Hidasi et al. 2016</ref>) is a session-based recommendation model based on GRU layers.</p><p>• STAMP <ref type="bibr" target="#b9">(Liu et al. 2018</ref>) is a short-term memory priority model, which exploits the user's current interest reflected by the last-click.</p><p>• SR-GNN <ref type="bibr" target="#b21">(Wu et al. 2019</ref>) is a session-based recommendation model that applies a graph neural network to learn the item and session representation.</p><p>• GC-SAN <ref type="bibr" target="#b22">(Xu et al. 2019</ref>) makes recommendations by combining a single-layer graph neural network and multilayer self-attention network.</p><p>Table <ref type="table">2</ref>: Performance of all recommendation models. The boldface is the best result over all methods, the underline is the best result of all baselines, and * denotes the significant difference for t-test .</p><p>Datasets Diginetica Retailrocket Metrics HR@5 HR@10 HR@20 MRR@5 MRR@10 MRR@20 HR@5 HR@10 HR@20 MRR@5 MRR@10 MRR@20 S-POP 0. • Bert4Rec <ref type="bibr" target="#b17">(Sun et al. 2019</ref>) is a method that employ the deep bidirectional self-attention to model user behaviors for sequential recommendation.</p><p>• CoSAN <ref type="bibr" target="#b11">(Luo et al. 2020</ref>) learn the session representation and predict the intent of the current session by investigating neighborhood sessions.</p><p>We adopt two common metrics on session-based recommendation in this paper, Hit Rate(HR@K) and Mean Reciprocal Rank (MRR@K). HR@K is the proportion of cases when the ground truth is ranked amongst the top-K items. It is used to evaluate unranked results. MRR@K is the average of reciprocal ranks of the desired items, which is the evaluation of ranked results. In this study, we consider the Top-K (K = 5, 10, 20) recommendation results.</p><p>Parameters setup In this paper, all hyper-parameters are optimized via grid search on each dataset, respectively. According to the experimental results, the optimal hyperparameters are {η : 0.001, ε : 0.5, w k : 20} on two datasets, where η is the learning rate, ε is the dropout rate and w k is the normalized weight. We use Adam as the model optimizer, and explore the case that embedding dimension d = 100 for a fair comparison, which is the hyper-parameter in the previous work. We implement the model by Pytorch, and the mini-batch settings are {batch size: 512, epoch: 50}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Overall performance To demonstrate the recommendation performance of our proposed model DSAN, we compare it with other state-of-the-art baselines. The experimental results on two datasets are shown in Table <ref type="table">2</ref>.</p><p>The first four methods, S-POP, FPMC, SKNN and STAN, are all improved conventional methods, but they have achieved significant results. Firstly, the performance of FPMC is poor since we consider each session as a user but do not have the long-term user profile. However, SKNN and STAN reach competitive results compared with neuralnetworks methods, especially on RR dataset. They are all improved version of the KNN method, proving that the conventional methods are not necessarily weaker than the neural-network-based methods when they incorporate with the session-level information.</p><p>Neural-network-based models can generally achieve better results except for GRU4Rec. GRU4Rec, the first proposed neural-network-based method to solve sessionbased recommendations, performs worst among all neuralnetwork methods. Furthermore, the STAMP model, which considers long-term and short-term memory fusion in the session and especially regards the last-click as the user's current preference, reaches a not very prominent result on RR dataset. SR-GNN and GC-SAN both construct graphs for sessions, but GC-SAN incorporates self-attention networks. Therefore, the performance of GC-SAN is better than SR-GNN, since it combines the long-range selfattention representation and the short-term interest of the last-click. Bert4Rec also has a learned embedding, including the known positional information, but they directly use the last layer to present the whole session, ignoring the initial information. As we can see, its result is competitive but still inferior to our proposed model. Similarly, CoSAN combines the self-attention network and the session-level collaborative information, which performs equally well or significantly better in some cases than other neural-network-based methods. It is noted that the three models with the self-attention network all have competitive performance, demonstrating the importance of this feature extraction method.</p><p>Our proposed model DSAN outperforms all baselines significantly. Compared to Bert4Rec, we introduce a dual attention network with a learned target embedding, which is applicable to different situations. Compared with CoSAN and GC-SAN, DSAN adopts the self-attention mechanism for the whole session representation and a learned target embedding to be the query vector for the user's real intent, leveraging all known information for the final representation. Without investigating neighborhood sessions and adopting the graph neural network, we can still achieve the best results.</p><p>Effect of the dual attention network In order to verify the effect of each module, we design three contrast models for comparison: DSAN-NS does not have the target embedding learning and uses the last-click as the query in a vanilla attention network. DSAN-NT does not have the target attention layer, which only uses the learned target embedding from the self-attention network as the whole session representation. DSAN-DA separates self-attention and vanilla at- tention into two parallel structures, which also use the lastclick as a query in the vanilla attention layer.</p><p>As shown in Table <ref type="table" target="#tab_3">3</ref>, we report the result on HR@20 and MRR@20. First of all, we can observe that DSAN-NS has the worst performance, indicating that the last-click does not exhibit the user's real intent and interest in all cases. In contrast, DSAN-NT has the second better performance, proving that the learned target embedding positively contributes to the result of a given session. DSAN-DA is better than DSAN-NS but worse than DSAN-NT, which further proves the problem of using the last-click. Both DSAN-NS and DSAN-NT only have one single attention network, and their performances are worse than the proposed model DSAN, indicating the importance of the dual attention module; indeed, it can get a better representation of the whole session. Also, the fact that DSAN performs best in all cases demonstrates the advantages of considering both the dual attention network and the learned target embedding. This experiment is also an ablation experiment for our proposed model, which proves that each module in the model structure is an indispensable content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with different transformation function</head><p>To demonstrate the utility of α-entmax, we compare the experimental results on two datasets using the proposed adaptive α and the fixed α. The range of fixed α is in {1, 1.2, 1.4, 1.6, 1.8}. We set the minimum session length of 15 since the longer session is more likely to contain unrelated click information. As illustrated in Figure <ref type="figure">4</ref>, the performance of the softmax function, which is α = 1, is relatively poor. We infer that the probability of users being affected by the external environment significantly increases in the longer session, so that the longer session may have more unrelated items, but the softmax function still gives small weight to them. The performance based on the adaptively sparse transformation is better than most fixed α, indicating that each session has its own best α based on the context. Besides, we notice that the overall performance of the longer session is worse than the result in Table <ref type="table">2</ref>, which proves that the task of session-based recommendation in the long sessions is more difficult. Therefore, the sparse transformation, which can effectively improve the prediction accuracy in long sessions, is of great significance.</p><p>Influence of the normalized weight w k In order to explore the stability of DSAN, we explore the influence of the hyper-parameter w k . Session-based recommendation suffers from popularity bias, and cross-entropy based on dot product makes target items with higher L 2 norm easier to be predicted. However, when we use L2 Normalization function, As shown in Figure <ref type="figure" target="#fig_3">5</ref>, the effectiveness of the proposed model is extremely related to this parameter. When w k = 1, it is equivalent to the cosine similarity and the model achieves the worst result, indicating that it is not a good measure. As w k increases, the result gradually improves on both datasets, and they both achieve the best result when w k = 20, proving that the appropriate value can make the training of this model more stable to get better performance. With w k further increase, the result has declined since a too larger value will lead to overfitting and thus affect the proposed model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future work</head><p>In this paper, we propose a dual sparse attention network for session-based recommendation. Specifically, we first use a self-attention network with the positional embedding to generate the target item embedding and then integrate a vanilla attention network to learn the entire session representation. Next, we combine the two vectors to rank all items. We also introduce a new adaptively sparse transformation function to replace the softmax, making useful information more focused. Extensive experimental analysis verified that our proposed model DSAN is superior to state-of-the-art methods. In the Future work, we plan to explore the way of using automated machine learning to delete the unrelated item in the session before entering into the network, so that the model can learn more useful information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A motivating example of session-based recommendation. This paper aims to directly model the real target item representation and alleviate the impact of unrelated items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of α-entmax in the two-dimensional case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The general architecture of the proposed model. The red dot line indicates a possible zero weight value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Experimental results with different transformation function on two metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets used in the experiments</figDesc><table><row><cell>Datesets</cell><cell># train</cell><cell># test</cell><cell># clicks # items</cell></row><row><cell>Diginetica</cell><cell cols="3">526,135 44,279 858,108 40,840</cell></row><row><cell cols="4">Retailrocket 433,648 15,132 710,586 36,968</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Impacts of the dual attention network. The boldface is the best result.</figDesc><table><row><cell>Datesets</cell><cell cols="2">DIGINETICA</cell><cell cols="2">RETAILROCKET</cell></row><row><cell>Metrics</cell><cell cols="4">HR@20 MRR@20 HR@20 MRR@20</cell></row><row><cell>DSAN-NS</cell><cell>0.6552</cell><cell>0.3095</cell><cell>0.5322</cell><cell>0.3000</cell></row><row><cell>DSAN-NT</cell><cell>0.6788</cell><cell>0.3228</cell><cell>0.5646</cell><cell>0.3040</cell></row><row><cell cols="2">DSAN-DA 0.6696</cell><cell>0.3220</cell><cell>0.5600</cell><cell>0.3010</cell></row><row><cell>DSAN</cell><cell>0.6821</cell><cell>0.3246</cell><cell>0.5654</cell><cell>0.3074</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://cikm2016.cs.iupui.edu/cikm-cup</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.kaggle.com/retailrocket/ecommerce-dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by NSFC grants (No. 61532021 and 61972155), the Science and Technology Commission of Shanghai Municipality (20DZ1100300 and 19511120200) and the Open Project Fund from Shenzhen Institute of Artificial Intelligence and Robotics for Society.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We promise this paper is our original work and no portion of this paper has been previously published. Our work is mainly focus on theoretical area of the recommender system based on anonymous users, so that we think it will not make any directly negative societal implications. The positive implication is that we aim to help ease the information overload of the recommender system and reduce the time for users to find desired items.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptively Sparse Transformers</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong; China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence and time aware neighborhood for sessionbased recommendations: Stan</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1069" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Increasing recommended effectiveness with markov chains and purchase intervals</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1162" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
		<idno>arXiv-1909</idno>
		<title level="m">NISER: Normalized Item and Session Representations to Handle Popularity Bias</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">When recurrent neural networks meet the neighborhood for session-based recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ludewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
				<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="306" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
	<note>Recommender systems handbook</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">STAMP: short-term attention/memory priority model for session-based recommendation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of sessionbased recommendation algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ludewig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="331" to="390" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative Self-Attention Network for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2591" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse Sequence-to-Sequence Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1504" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for nextbasket recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2001">2010. 2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th international conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved recurrent neural networks for session-based recommendations</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Possible generalization of Boltzmann-Gibbs statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tsallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical physics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="479" to="487" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Collaborative Session-Based Recommendation Approach with Parallel Memory Modules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SI-GIR&apos;19</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SI-GIR&apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph contextualized selfattention network for session-based recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf. Artif. Intell.(IJCAI)</title>
				<meeting>28th Int. Joint Conf. Artif. Intell.(IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3940" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno>ArXiv abs/2011.01731</idno>
		<title level="m">RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5089" to="5097" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
