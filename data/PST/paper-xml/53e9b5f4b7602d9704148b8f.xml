<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">INformation Systems</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<idno type="ISSN">1386-3681</idno>
					</monogr>
					<idno type="MD5">E4DD25CE3095C31A1AF6AACA7E900F8E</idno>
					<note type="submission">Multimedia Metadata on the Web (Part II</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>1998 ACM Computing Classification System: H.3.1</term>
					<term>H.3.7</term>
					<term>H.5.1</term>
					<term>H.5.4.</term>
					<term>I.7.2 Phrases: Semantic Web</term>
					<term>metadata production</term>
					<term>multimedia production process</term>
					<term>XML</term>
					<term>XML Schema</term>
					<term>RDF</term>
					<term>RDF Schema</term>
					<term>MPEG-4</term>
					<term>MPEG-7</term>
					<term>MPEG-21</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CWI's research has a theme-oriented structure and is grouped into four clusters. Listed below are the names of the clusters and in parentheses their acronyms. Probability, Networks and Algorithms (PNA) Software Engineering (SEN) Modelling, Analysis and Simulation (MAS) Information Systems (INS)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>That Obscure Object of Desire: Multimedia Metadata on the Web (part 2) Jacco van Ossenbruggen, Frank Nack, Lynda Hardman</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Intro</head><p>Part 1 of this article provided our vision of a media-aware Semantic Web in form of a business presentation scenario and derived from it a number of problems regarding the semantic content description of media units. We discussed the multimedia production chain, in particular emphasizing the role of progressive metadata production. As a result we distilled a set of media-based metadata production requirements and showed how current media production environments fail to address these. We then introduced those parts of the W3C and ISO standardization works that are relevant to our discussion. In part 2 we analyze the abilities of the W3C and ISO standardization works to define structures for describing media semantics, discuss syntactic and semantic problems, ontological issues for media semantics, and the problems of applying the theoretical concepts to real world problems. We conclude with implications of the findings for future action with respect to the activeties the community should take.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semantic Web versus MPEG-7: A Language Analysis</head><p>Machine-processable content is the main prerequisite for the more intelligent Web services that constitute the "Semantic Web". To be able to build tools that are aware of the semantics of both the content and the context of multimedia, we need a language that makes the semantics od media units explicit. In part one we discussed, based on our analysis of the metadata production process, the requirements for a language that facilitates the description of multimedia content. Our findings can be summarized as follows:</p><p>1. support the definition of syntactic rules to express and combine description structures at various levels of detail, which results in the provision of a rich set of syntactic, structural, cardinality and datatyping constraints 2. state spatial, temporal and conceptual relationships between the components of a description and between descriptions, so that a meaningful discourse about, or with, descriptions, through algebraic, logic, or functional means, is possible, 3. facilitate a diverse set of linking mechanisms between descriptions and the data that is described, which includes, in particular, means of segmentation for temporal media.</p><p>4. be platform and application independent and humanand machine-readable.</p><p>Ultimately, when describing multimedia content on the Web, one has to pick a language suitable for doing so. Despite the different representational goals in the ISO and W3C approaches, at least both use the same serialization language: XML. The two approaches differ, however, widely in the way XML is used to describe multimedia content.</p><p>In the following analysis of both description approaches we discuss various problems related to syntactic interoperability between the main languages used, namely XML, MPEG-7 DDL, RDF, RDF(S) and OWL. We then examine solutions and problems regarding semantic interoperability, in particular related to the definition and mapping of semantic-based descriptions. We analyze the ability of W3C and ISO technologies to address the expressiveness of media units to facilitate the process of audio-visual signification of multimedia. Finally we consider the practical applicability of the provided concepts, methods and technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntactic Interoperability: MPEG's DDL vs XML Schema</head><p>Within MPEG-7, the Description Definition Language (DDL) <ref type="bibr" target="#b7">[9]</ref> is intended to address the language requirements listed in bulletpoints 1 -4. The DDL provides basically the same structure-oriented language elements as XML-Schema<ref type="foot" target="#foot_0">1</ref> . The only extensions to XML-Schema cover the ability to define arrays and matrices and to provide two additional datatypes, basicTimePoint and basicDuration, which allow specific temporal descriptions (see <ref type="bibr" target="#b7">[9]</ref>, pp. 9 -14). Any available MPEG-7 parser addresses consequently only these  Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref> on the next page provide a small example of a piece of MPEG-7 metadata. The first halve of the example shows how to address the target video fragment. Note that in addition to the URI, the MediaTime is used to identify the first eight minute segment of the video file this piece of metadata applies to. Moreover, this part of the example shows how the coding format of the audiovisual component can be described by using the Medi-aFormat Descriptor. Here the description of the video covers its aspect ratio, the frame size and the frame rate per second.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> on the following page exemplifies how the segmentation of the video in scenes and subscenes (Temporal-Decomposition) can be achieved, where the Media-TimePoint provides the temporal start point of the audiovisual segment based on a Gregorian time scheme and the MediaDuration describes the temporal period of the segment. Moreover, the illustration also exemplifies a simple way to provide extra semantic annotations for a particular sequence supported by the semantic element.</p><p>The XML syntax underlying the DDL facilitates platform and application independence and human-and machinereadability. However, as it merely adopts the syntactic ele-ments of XML-Schema to represent structures in the form of schemata, the DDL 1. lacks particular media-based datatypes. The datatypes used in the example are either standard XML-Schema datatypes (such as integers etc.) or media-specific datatypes defined in part 5 of the MPEG-7 standard, the Multimedia Description Schemes (MDS) <ref type="bibr" target="#b10">[12]</ref>.</p><p>2. does not facilitate a diverse set of linking mechanisms between descriptions and the data that is described, which includes, in particular, means of segmentation for temporal media. Again, the locating and segmentation techniques used in the example are plain URLs combined with descriptors for time segments, also defined in the MDS.</p><p>3. does not support the definition of semantic relations, as does RDF Schema<ref type="foot" target="#foot_1">2</ref> , or ontology-based modeling, such as DAML+OIL <ref type="bibr" target="#b17">[19]</ref> or OWL <ref type="foot" target="#foot_2">3</ref> . Semantics of relations between the syntax constructs, such as being used in the example of Figure <ref type="figure" target="#fig_0">1</ref>, are often only defined by English prose in the text of the standard, and hence lack the formal semantics that the Semantic Web languages have.  The strength of the DDL, however, lies in supporting the definition and adaptation of schemata. This is used in MPEG-7 to define normative schemata that on the one side provide the necessary syntactic necessities but also facilitate the description of the semantics of a single multimedia object or collections in the form of a multimedia unit. These schemata, however, are not part of the description language, but of the MDS. Here we find a plethora of structures for:</p><p>• specific datatypes required for the description of form and substance of media expression [ISO MPEG-7 2001e, pp. 49-103]. Extensions are provided in the parts Visual <ref type="bibr" target="#b8">[10]</ref> and Audio <ref type="bibr" target="#b9">[11]</ref>;</p><p>• linking, identification and localization tools, mainly based on XPath but extended with particular temporal constructs, that provide a basic means of estab-lishing references within a description and linking to the associated multimedia data ( <ref type="bibr" target="#b10">[12]</ref>, pp. 74-103);</p><p>• graphs of relations, where the basic unit of a relation is built, similar to RDF, on a conceptual triple that allows the establishment of named relations between the parts in a description. The organization of relations is restricted to a defined set of 11 topological and set-theoretic graph-relation types ( <ref type="bibr" target="#b10">[12]</ref>, pp. 179-191);</p><p>• forms of spatio, temporal and spatio-temporal segmentations for video, audio, audio-visual, multimedia, and ink content, including a set of temporal and spatial relations ( <ref type="bibr" target="#b10">[12]</ref>, pp. 251-400 and 458-540);</p><p>• a set of 45 semantic relations that allow the description of narrative structures ( <ref type="bibr" target="#b10">[12]</ref>, pp. 401-457).</p><p>The syntactic description of general multimedia datatypes is thus not part of the description language, but is an integral part of concrete schemata with there specific semantics. The consequences are far reaching. As the essential semantic aspects for the description of multimedia are defined in standardized schemata they have to be used in the provided way and any modification, including the combination of schemata, will be outside the scope of the standard. More crucially, any modification on one of the "language related" schemata will not only alter the semantics of the description but also the description language itself. Such modifications are, however, unavoidable as a great number of schemata describe solutions for particular problems for a fraction of multimedia applications <ref type="foot" target="#foot_3">4</ref> . Thus, the MPEG-7 approach of fusing language syntax and schemata semantics is problematic and must be seen as a first step towards a language that facilitates the syntactic means for establishing semantic descriptions of multimedia. An issue that needs addressing is the identification of semantically relevant syntax elements in the semanticrelated schemata and to include them into the DDL. This would allow the semantic web to open up to the well defined semantic-loaded low-level descriptors of MPEG-7.</p><p>On the other hand, the lack of explicit semantics in MPEG-7 is, to some extent, inherent to the direct use of XML. The XML level of "self description" is limited to the extent that XML is only able to define the syntax of the elements in a language. There is no understanding of anything other than the hierarchical, syntactical structure of the document. What is needed is some way of specifying the semantics that is supposed to be communicated by the syntactical XML document structures <ref type="bibr" target="#b0">[1]</ref>. However, to make these semantics explicit, and to communicate them in a machine understandable way, XML in itself is insufficient. Other layers, built on top of XML, are required to accomplish this. Within the Semantic Web, however, the semantics of these upper layers are RDF-based, and envisioned to be themselves machinereadable as much as possible. As a result we have a problem of syntactical interoperability between the two main developments (XMLSchema in MPEG-7 and RDF-Schema for the Semantic web). As this is an important issue we investigate it now in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactic interoperability: RDF vs XML</head><p>Both the Semantic Web and MPEG-7 metadata build syntactically on top of XML. Unfortunately, this does not solve even the syntactic interoperability issues for applications that need to use both approaches simultaneously. Especially the use of RDF in most Semantic Web applications causes interoperability problems. While the decision to build the Semantic Web on top of RDF is often taken for granted, it results in a potentially large number of low level, pure syntax-oriented interoperability problems (that is, the kind of problems XML was supposed to solve).</p><p>Suppose that the "lifestyle video" fragment from the example scenario in part one of this article is published on the Web, distributed under an "Open Publication License". That this Web resource is indeed open content could, by interpreting the surrounding text on the HTML page from where it is linked to, be obvious to human readers, but not to a machine. To make this explicit, one could state this explicitly in RDF, and attach this statement as metadata to the Web page. In RDF triple terminology: the URL of the page (say, the (relative) URL yup lifestyle.mpg) would denote the resource, the "dc:rights" label the property, and the string "OPL" the value. Figure <ref type="figure" target="#fig_2">3</ref> shows the common graph notation. While RDF in itself is syntax neutral, it defines a XML serialization syntax for interchange. In addition, it defines an abbreviate form. As a result, even the simple, single triple defined above can be serialized to XML in two ways, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. Applications are expected to implement both forms and annotators are thus free to mix the two. In practice, many RDF files indeed use both forms simultaneously, which makes it hard to process RDF using generic XML tools (e.g. it is almost impossible to write an XSLT stylesheet for any but the most trivial RDF documents). In real life, this problem is made even worse by the fact that, in most cases, the order in which RDF triples are serialized is irrelevant for RDF applications (while it is relevant for XML applications). Similarly, an RDF application might decide to serialize descrip-tions in a nested form without changing the RDF semantics (while in XML, the nesting of elements is usually consider relevant and can thus not be changed).</p><p>So while RDF technically uses XML, it makes it very hard to use generic XML tools for RDF processing. Unfortunately, the reverse also holds. In practice, it is very hard to make RDF tools process generic XML <ref type="bibr" target="#b12">[14]</ref>. Suppose that in addition to the RDF metadata of our video fragment, our application has also access to the MPEG-7 metadata shown in Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>. Despite the fact that it is encoded in XML, most RDF-based Semantic Web applications will not even be able to parse this on a syntactic level, unless one uses a non-standardized translation from MPEG's XMLbased syntax to RDF, as advocated by Hunter <ref type="bibr" target="#b4">[6]</ref>.</p><p>The syntactic problems between the two major approaches in multimedia content description are, however, not the only issues that make it diffiult to merge in one way or another the generated metadata sets. There is also a different way on defining semantics, as is shown in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic interoperability: defining semantics</head><p>The Semantic Web stack itself does not define any multimedia specific semantics. For defining application-specific semantics the Semantic Web relies on third-party specifications. The meaning of the "dc:rights" property in Figure <ref type="figure" target="#fig_1">2</ref>, for example, is defined by the Dublin Core Metadata Initiative <ref type="bibr" target="#b1">[2]</ref>. Attaching RDF metadata to a particular segment of a video (as is done in the MPEG-7 example in Figure <ref type="figure" target="#fig_1">2</ref>) requires a way to specify that specific fragment. Specification of such an addressing scheme is not considered to be within the scope of RDF or the other Semantic Web languages. Instead, it is left to a third party to develop such a scheme.</p><p>The approach of defining semantics on the Semantic Web is to provide relative thin but generic layers that define increasingly complex semantic structures, and to defer the definition of domain and application specific ontologies to third parties. This approach can be contrasted to the MPEG-7 approach, which defines metadata syntax and semantics within the MPEG-7 standard. It also defines both the framework (including the DDL) and the actual ontologies. A large number of schemata in MPEG-7 establish ontological structures, as most schemata are inspired by the domain of broadcasting and audiovisual-based entertainment (see for example the VideoEditingSegment, the AgentDS, PlaceDS, or the user preference description schemata in the MDS <ref type="bibr" target="#b10">[12]</ref>). The large number of schemata, often describing similar aspects of the same semantic problem, and their interlocked nature, indicate the ontological role at least of the MDS. However, the attempt of abstraction to achieve domain independence makes it impossible to use those schemata as ontology items. Nevertheless, the advantage of the approach taken by MPEG-7 is that it provides a large vocabulary of description terms, developed especially for describing audiovisual material. A disadvantage is that the result is rather monolithic, with structures that are hard to be reused outside the MPEG-7 context. This problem cannot be underestimated, as the definition of semantics also points to the tribulation of mapping semantics.</p><p>Thus, with respect to the aim of the Semantic Web to make use of third-party specifications, the schemata developed in MPEG-7 are most relevant. Yet, as outlined earlier there are in particular language barriers that have to be removed before a full integration is possible. Moreover, it seems to us that the encapsulated nature of MPEG-7 needs opening, namely through further modularization, to allow easier accessibility of the available schemata.</p><p>Even if the disussed issues can be resolved, there is more to be considered. In part one of this article we pointed out that the nature of annotations is necessarily imperfect, incomplete, and preliminary because they accompany and document the dynamic progress of understanding a concept, which usually open up questions of subjective interpretation. Thus, there is a need for mechanisms to establish collective sets of descriptions growing over time. The problem we then face is that of mapping semantics to make use of such structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic interoperability: mapping semantics</head><p>The question to be asked, with respect to the Semantic Web, is: should an ontology layer use RDF(S) as its serialization syntax, or is it better to develop a (more concise) syntax directly in XML? In the RDF-based approach, one runs the risk of making integration with current and future XMLbased approaches harder. Needless to say, the majority of current Web applications is XML-based, and even the MPEG-7 metadata framework is based on XML, not RDF. In addition, by using RDF syntax, and building incremental syntax layers on top of that, one also need to make sure that the underlying semantics can be layered in a similar fashion (for example, consider the potential problems when a pure RDF application interprets the semantics of a OWL document using the RDF serialization syntax. Ideally, the conclusions of the RDF application should be a subset of the conclusions an OWL application would make, but the two should not contradict one another).</p><p>On the other hand, by building the ontology layer directly on XML, one runs the risk of the development of two incompatible Semantic Webs: an XML/ontology-based "knowledge" Web versus an RDF/RDF Schema-based "metadata" Web. Clearly, the OWL Working Group chose the RDF-based approach. But the XML vs RDF question (see section 2.2) is closely related to one of the big controversies surrounding the Semantic Web in general: the question of whether the advantages of developing a common Semantic Web language stack, such as proposed by Tim Berners-Lee<ref type="foot" target="#foot_4">5</ref> , really outweigh the more pragmatic approach of defining knowledge interchange formats directly in XML on a per application domain and per user community basis. The latter is the approach many E-business initiatives are currently taking. In theory, a Semantic Web-based approach would require less a priori commitment between the different user groups, and would promote the use of generic (free and commercial) tools. The Semantic Web would standardize more levels of the information stack, agreement about the semantics defined by these levels and the possibility of using off-the-shelf tools would thus come "for free". In the XML-based alternative, users from a specific community would need to agree on these levels first, and then develop their own tools. Second, when new users would join in, adding their own set of knowledge bases and tools, the Semantic Web promises a better infrastructure for interoperability between the two worlds. It still remains to be seen to what extent these promises prove to be realistic in practice.</p><p>MPEG-7's approach towards semantic interoperability is also critical because the standard acts as an ontology definition language as well as an ontology (see comments in section 2.1). This ambiguous conceptual state is a result of the decision to model the DDL on XML Schema rather than on RDF Schema. This choice was mainly political, as RDF Schema was at that time, and still is not at the time of writing, a W3C recommendation and was thus not referable (for more insights w.r.t. the relationship between XML Schema and RDF Schema, see <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">7]</ref>). The choice for XML-Schema as the serialization syntax had far reaching consequences. As a mere syntax oriented language, the DDL could not provide the basis for some basic reasoning services, mainly subsumption based reasoning on the class and properties hierarchies. This required formal semantics that had to be established elsewhere, resulting in the Semantic description tools section in the MDS ( <ref type="bibr" target="#b10">[12]</ref>, pp. 401-457). That MPEG-7 defined its own ontology environment is an asset with respect to interoperability within MPEG but it turned out as being a hurdle for the interoperability with other ontologies, as necessary mechanisms to connect into a source, such as an ontology, were not developed and the available linking mechanisms in MPEG-7 into external sources only cover other MPEG-7 documents or media items.</p><p>A potentially solution to overcome this problem of ontology interoperability is the Classification schema. The Classification schema facilitates the organizational wrapper for a controlled vocabulary built out of terms and the relations between them. The relations organize the terms in the form of a hierarchy, indicating if one term is broader or narrower in its meaning than another, a synonym or, in the given set of relations, the one of highest relevance. Thus, a classification schema in some sense covers aspects of a thesaurus. The classification schema allows the incorporation of other classification schema, though no indication is given, if this feature only takes account of the inclusion of other MPEG-7 classification schemata or also the insertion of or connection to other ontologies. Unfortunately, there is no information provided about how the mapping from previously unconnected terms should be achieved. Thus, the problem of mapping high-level media semantics is not solved yet and it remains questionable if the MPEG-7 approach of profiling schemata provides suitable solutions.</p><p>A final issue to be addressed is the strong focus of both the current HTML/XML Web and the future Semantic Web on XML text and page-based layout. Many of today's Web technology does not address the special needs of multimedia. The MPEG metadata framework was especially developed to address those multimedia-specific issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semantics for media expressiveness</head><p>In part one of the article we outlined that it is the process of audio-visual signification of multimedia objects that requires special attention when it comes to semantics. This process, though based on common human knowledge and thematic structures (expression form), provides its own temporalspatial realities based on patterns of juxtaposition of the media intrinsic parts (expression substance). The information provided on a perceptual level using objective measurements, such as those based on image or audio processing or pattern recognition, play an important role regarding the aesthetics of a multimedia unit and consequently its subjective interpretation. Take the incorporated video sequences from our business authoring scenario in part one. These videos were not only added to the presentation to strengthen the logical flow within the presentation by conveying the lifestyle of the new product's target audience but the material also has to express the expectations of the audience (in the example the board of managers) and has to fit into the overall style of the presentation.</p><p>Support for the form of expression requires a rich set of presentation models. The following discussion is predominantly focused on MPEG-7 as the standard is devoted to representing the form and substance of media expression, whereas the W3C standards are merely silent about these issues.</p><p>Despite the semantic relations, already introduced in section 2.1, MPEG-7 additionally suggests a set of schemata that provides structures for multimedia summaries, points of view, partitions and variations ( <ref type="bibr" target="#b10">[12]</ref>, pp. 458 -540) and various forms of collections on a probabilistic, analytical or classification level ( <ref type="bibr" target="#b10">[12]</ref>, pp. 541 -600). These schemata are very detailed, but they impose particular semantics on the user. In fact, the approach taken by the W3C, as exemplified through SMIL, representing a textual serialization of temporal and spatial aspects for multimedia presentations seems more promising because it is less rigid and thus more easily applicable.</p><p>Similar problematic is the the approach taken by MPEG-7 for representing substance of expression, i.e., the semantics of low-level audio and visual features as manifested in the parts Visual <ref type="bibr" target="#b8">[10]</ref>  <ref type="foot" target="#foot_5">6</ref> and Audio <ref type="bibr" target="#b9">[11]</ref> <ref type="foot" target="#foot_6">7</ref> . It must be clearly stated that it is not so much the conceptual ideas described in standard that are problematic. The dilemma is rather caused by the attempt to solve the challenge of representing the dynamic nature of audiovisual semantics by providing a binary (algorithmic) and textural (schema) description structure. The intention is that both representational forms provide the same information, since a requirement for the system specification of MPEG-7 is that "MPEG-7 data can be represented either in textual format, in binary format or a mixture of the two formats, depending on application usage. A bi-directional loss-less mapping between the textual and the binary representation is possible." ( <ref type="bibr" target="#b6">[8]</ref>, p. 10).</p><p>This, however, turns out not to be the case. Both parts provide many semantic descriptions relevant for the interpretation of the individual binary format of a schema. Take the ColorStructureType ([10], pp. 50-56) as an example. The descriptor specifies both color content (similar to that of a color histogram) and the structure of this content.</p><p>The binary format is accompanied by long textual and graphical descriptions giving detailed interpretational information about the extraction algorithm, re-quantization, color space and color quantization, and the raw ColorStructure histogram accumulation. All of that information is required to understand the meaning of every single element (bin) specified in the ColorStructure descriptor array of 8-bit integer values, h(m)f orm ∈ {0, 1, . . . , M -1}.</p><p>None of this, however, made it into the textual description. In fact, the schema merely provides the structure of the result space, that is the size of the matrix that contains the results of the extraction algorithm (see the DDL representation syntax on <ref type="bibr" target="#b8">[10]</ref>, p. 51). The assumption during the development of the audio and visual schemata was that an agent would know about the semantics of a bin in the Col-orStructure Schema and thus could react accordingly. The result is that the semantics of the array are not made explicit but are hidden in the standard document. However, for real analytic parity of audio-visual media within the Semantic Web it is of utter importance that the semantics of a media unit are made explicit, in particular as an XML-based parser is not able to evaluate the binary representation or the quasi binary representation of the current array content. While this problem may appear trivial, it has far reaching consequences because the use of low-level features for semanticbased descriptions is one of the few mechanisms available for the automatic annotation of media.</p><p>Having analyzed the conceptual ideas of the two standards it seems that, despite the fact that both built on XML, their significant incompatibilities make it very difficult to establish a general framework for describing the semantics of audio-visual informations units in a maschine accessible way. Yet, both approaches provide relevant solutions to address the general problems of metadata production. However, the majour issue within metadata production, namely its labour intensitivity, was not really addressed yet in our discussion. In part one of this article we, however, clearly stated that a Semantic Web can only emerge if the abstract idea of the media-aware Semantic Web can be turned into an environment that integrates the instantiation and maintenance of the dynamic structures into the actual working process. The next section reflects on theses issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Applicability of semantic structures</head><p>In the first part of this article we argued that a future mediaaware Semantic Web, where a great variety of media will be constantly generated, manipulated, analyzed, and commented on, could only emerge if people are provided with tools that support the dynamic nature of audio-visual media and the variety of data representations and their combinations. We also showed that current technologies to support the instantiation and maintenance of the dynamic structures are still in their infancy, as is the research for the Semantic Web. The question is: are the methodologies provided by the two major approaches capable to allow the emergence of a media-aware Semantic Web as desired?</p><p>Our discussion on syntactic and semantic interoperability in sections 2.1 -2.4 already demonstrated that the layer approach used in W3C technology seems to address the flexibility of descriptive structures, the essential requirement for intelligent media-and metaproduction better than the philosophy of the "universal" description schema for a domain as provided by MPEG-7. However, the current state of Semantic Web technology is still mainly text oriented.</p><p>Though MPEG-7 provides the better means to describe media content, its crucial dilemma is its structural complexity that obstructs the take-up of the standard. Instances of the complexity problem within MPEG-7 are:</p><p>• A description of a media item is basically forced into one document (see the definition of the root element in the MDS ( <ref type="bibr" target="#b10">[12]</ref>, pp. 17 <ref type="bibr">-19)</ref>. The instantiation of a complete description structure can be attached to the relevant media items and naturally, the resulting descriptions are consistent and interoperable within MPEG-7, even if the descriptions vary in their instantiated depth. Though the structure of the schema can be complex, once it is created and used in instantiations, its structure cannot be altered. Any modification would cause inconsistencies with existing documents <ref type="foot" target="#foot_7">8</ref> .</p><p>• Links in MPEG-7, do not provide any information about the semantics of the relationship between documents. MPEG-7 relations, which supply the desired semantics through the introduction of relationship elements, can only be applied within a document, which again results in encapsulating the required network structure in a single document.</p><p>• There are a great number of abstract elements, which are used to establish class structure <ref type="foot" target="#foot_8">9</ref> . However, abstract elements cannot appear in instantiations. When an element is declared to be abstract, a member of that element's substitutable class must appear in the instance document. To indicate that the derived type is not abstract, the XML namespace mechanism is used (xsi:type). Thus, a thorough understanding of schemata development is required, which makes instant schemata development for distinct domains hard, especially if the required schemata should cover simple descriptions, where the theoretically founded overhead is actually not required.</p><p>• The interlocked nature of schemata, resulting on the approach of providing an ontology-like but yet general set of schemata to describe media semantics, makes it very difficult for a user to identify the appropriate schemata and to use them in isolation. At the moment it is still not clear how the currently discussed MPEG-7 profile/level version 2.profiling will address this problem</p><p>• Due to the lack of a fundamental data model the provided structures show inconsistencies and duplications, which makes manual schemata generation difficult.</p><p>Compensating the structural complexity would require support tools that help during the complex process of schema development and maintenance, but few support tools exist for the manual generation of new schemata. The situation is more bleak with respect to semi-automated tools, such as technology that can handle (e.g., locate, transfer, integrate) multimedia segments and fragments, per the annotations, as described in the first part of this article. Note, that support of tools for W3C technology is in every day production environments is on a similar spare level. This fact indicates for both standardization activities that they still operate on a theoretical level where the everyday use has not the highest priority in the development agenda. For the nearer future we see an analog development as at the beginning of the WWW, where only the introduction of user applicable graphical tools turned the pure academic infrastructure into a public environment.</p><p>However, there are projects in real world domains, such as the TV Anytime Forum <ref type="bibr" target="#b2">[3]</ref>, that give an indication of how media-aware semantic structures, such as those provided by MPEG-7, will be used in the future. The TV Anytime Forum develops specifications for services based on consumer digital storage devices. The semantic structures, all written in XML Schema, are self-developments and cover the essential aspects of media description, i.e. content description, content referencing and location, rights management and protection, systems and transport. Though the TV Anytime schemata are similar to the equivalent structures in MPEG-7, they are less complex in their organizational structure. TV Anytime includes, for example, the MPEG-7 schemata on user-modeling, though without incorporating the complete MPEG-7 organizational overhead. Rather, TV Anytime uses MPEG-7 as a namespace and thus be able to incorporate just the required schemata <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Summary</head><p>The problem of describing the semantics of multimedia in a network-based environment is complex. The essential similarities and differences between the two main approaches that address the difficult issues involved is recapitulated in Table <ref type="table" target="#tab_2">1</ref> on the following page.</p><p>Our analysis showed that none of the approaches is providing what is required to make the media-aware Semantic Web happening. In fact, an ideal media-aware metadata language should be applicable outside the context it was initially designed for. Therefor, it should have a syntaxneutral basis and modular design. In particular the problems within MPEG-7 regarding the fusion of language syntax and schemata semantics clearly exemplified that a closed approach hinders the required modularity for description design, obstructing the needed interoperability on a syntactic and semantic level. Specific modules of such an ideal media description language, could adopt a number of description constructs from the visual and audio parts from MPEG-7. These could then be used to describe media aspects only and would allow the linking into conceptual and contextual descriptions expressed in semantic languages such as RDF, RDF Schema or OWL.</p><p>The flexible syntactic properties of this language, would facilitate re-use and inferencing about material for specific purposes, potentially leading to interoperability with other description formats, used in media-based standards, such as</p><p>• the Dynamic Metadata Dictionary-Unique Material Identifiers (UMIDs) <ref type="bibr" target="#b14">[16]</ref>. UMIDS provides for the link between the essence (video, audio, graphics, stills etc.) and the metadata and generates a time code and date (time-axis) for synchronizing this data.</p><p>• the Multimedia Home Platform (MHP) as part of the Digital Video Broadcasting (DVB) Project <ref type="bibr" target="#b13">[15]</ref>. MHP is a series of measures designed to promote the harmonized transition from analogue TV to a digital interactive multimedia future.</p><p>• the P/Meta Standard developed by the Production Technology Management Committee (PMC) of the European Broadcasting Union (EBU), using the Standard Media Exchange Framework (SMEF) by the British Broadcasting Corporation (BBC) and SMPTE outputs, provides a common exchange framework and a format between members (and others) <ref type="bibr" target="#b16">[18]</ref>.</p><p>• the TV Anytime Forum <ref type="bibr" target="#b2">[3]</ref>, The TV Anytime Forum is an association of organizations that develops specifications to enable audio-visual and other services based on mass-market, high-volume digital storage.</p><p>• the Dublin Core Metadata Initiative <ref type="bibr" target="#b1">[2]</ref>,</p><p>• NewsML <ref type="bibr" target="#b11">[13]</ref> is An XML-based standard to represent and manage news throughout its life cycle, including production, interchange, and consumer use.</p><p>• the Gateway to Educational Materials project <ref type="bibr" target="#b15">[17]</ref>. A U.S. Department of Education initiative that expands educators' capability to access Internet-based lesson plans, curriculum units and other educational materials.</p><p>• The Getty Research Institute's Vocabulary Databases (the Art &amp; Architecture Thesaurus(r), the Union List of Artist Names(r), and the Getty Thesaurus of Geographic Names(tm)) [5], contain terminology and other information about the visual arts, architecture, artists, and geographic places.</p><p>The most crucial activity, however, is to leave the laboratories and provide real world cases that show the applicability of the technology. Related to that is the provision of development and maintenance tools, but also technology that allows to make use of the established semantic descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion and future research</head><p>In this article we argued that the traditional linear approach of generating information and thus meaning is far too restrictive, as any form of information is necessarily imperfect, incomplete and preliminary. Metadata accompanies and document the progress of interpretation and understanding of a concept. Consequently, we described the need for flexible, collective sets of descriptions growing over time and being collected during the actual working process, including the generation, restructuring, representing, resequencing, repurposing or redistributing of media. The major points of our discussion were:</p><p>• There is need for copious annotation at the various phases of the multimedia production process, including those stages as restructuring, representing, resequencing, repurposing and redistributing media.</p><p>• Establishing such annotation either manually or semiautomatically is difficult, labor intensive, situated (quasisubjective)</p><p>• The two approaches towards machine-processable and semantic-based content description, namely the Semantic Web activity of the W3C and ISO's Multimedia Content Description Interface (MPEG-7), are in large parts incompatible. We showed, however, that both approaches provide the potential techniques to establish a media-aware Semantic Web. We illustrated that, though both approaches are XML-based, the differences on a philosophical and implementation level are substantial enough to make a merge between the two complicated but not impossible. We focused in particular on the problems emerging from syntactic interoperability, the definition and mapping of semantics, and the description of expressiveness of media and showed that the current developments in both approaches are but a small step towards the intelligent use and reuse of media-based information.</p><p>• Further developments towards a robust media-aware Semantic Wed depends on "resolution" technology, i.e., technology that can handle (e.g., locate, transfer, integrate) multimedia segments and fragments, per the annotations. Such technology does not exist yet on a required scale.</p><p>In particular the last point should be of most significant communities interested in a robust multiimedia web, yet the lack of this sort of technology is currently the major obstacle for swift developments. Thus, in our opinion the main task is to provide real world cases that show the applicability of semantic-aware technology, including maintenance tools, but also technology that allows to make use of the established semantic descriptions.</p><p>In fact, far more work is required on flexible formal annotation mechanisms and structures, but also on tools that first support human creativity to create the best material for the required task and additionally use the creative act to extract the significant syntactic, semantic and semiotic aspects of the content description. For that it is, however, required that those who develop technology get a better understanding of the domains they are developing for.</p><p>Of course, we'll encounter more problems and surprises in the Semantic Web arena, making it impossible to foresee what is coming. The best we can do as research community is investigate the basic conceptual, perceptual, and processable elements of that volatile thing called multimedia information and get the fundamental framework right so that we can satisfactorily exploit the evolutionary process of semantic-based multimedia information exchange.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a MPEG-7 sequential description (I): linking to a video fragment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of MPEG-7 sequential description (II) the actual annotations describing the content of the video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Simple graphical representation of an RDF triple</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>&lt;Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of two XML serializations of the same RDF statement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Multimedia metadata: MPEG-7 vs Semantic Web.</figDesc><table><row><cell></cell><cell>MPEG-7</cell><cell>Semantic Web</cell></row><row><cell>Syntax</cell><cell>XML</cell><cell>XML/RDF</cell></row><row><cell cols="3">Schema/ontology language MPEG-7 DDL/XML Schema RDF Schema/OWL</cell></row><row><cell>Composition</cell><cell>monolithic/big</cell><cell>small layers</cell></row><row><cell>Extensibility</cell><cell>? (version problems?)</cell><cell>designed to be extended</cell></row><row><cell>Multimedia ontologies</cell><cell>++</cell><cell>-(third party)</cell></row><row><cell>Linking into media items</cell><cell>++</cell><cell>-(media dependent)</cell></row><row><cell>Tool support</cell><cell>-</cell><cell>+</cell></row><row><cell>Real life applications</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.w3.org/TR/xmlschema-1/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.w3.org/TR/rdf-schema/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.w3.org/TR/owl-ref/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Moreover, dispersing language elements into description schemata asks for an evaluation complexity close to a validation level no parser can cope with. In fact, at the time of writing there is no MPEG-7 validator that can handle all the existing structures.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://www.w3.org/2000/Talks/1206-xml2k-tbl/ slide10-0.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Covered features among others are: color, texture, shape, motion, or localization</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Covered features among others are: series types (scalable, scalar, vector etc.), waveform, power, spectrum, harmonicity, silence, sound, spoken content, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>The "description unit" might be intended to play that role. The problem with this construct is that it is deficient in most of the conceptual overhead of the "complete description", among which the lack of linking mechanisms is the most serious. In fact, a "description unit" performs merely as a free-floating description unrelated to real</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>data.<ref type="bibr" target="#b7">9</ref> The fundamental problem of class and instance, where sometimes an instance should also be a class, is implicitly addressed in MPEG-7 and also forms part of the language problem described earlier</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Part of the research described here was funded by the Dutch national Token2000/CHIME and NWO/NASH projects, and Ontoweb, a thematic network of the European Comission. The authors wishes to thank in particular Wolfgang Putz from FHG-IPSI in Darmstadt and Jane Hunter from DSTC in Brisbane for insightful discussions and helpful comments. We also wish to thank our colleague Lloyd Rutledge for useful discussion during the development of this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimedia Metadata on the Web (Part II) Jacco van Ossenbruggen, Frank Nack, Lynda Hardman REPORT INS-E0309 DECEMBER 3, 2003 INS Information Systems CWI is the National Research Institute for Mathematics and Computer Science. It is sponsored by the Netherlands Organization for Scientific Research (NWO). CWI is a founding member of ERCIM, the European Research Consortium for Informatics and Mathematics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Semantic Web: The roles of XML and RDF</title>
		<author>
			<persName><forename type="first">S</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fensel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Broekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<ptr target="http://www.computer.org/internet/ic2000/w5063abs.htm" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2000-10">October 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dublin Core Element Set, Version 1.1</title>
		<author>
			<orgName type="collaboration">Dublin Core Community</orgName>
		</author>
		<ptr target="http://www.dublincore.org/documents/dces/" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The TV-Anytime Forum Home Page</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-A</forename><surname>Forum</surname></persName>
		</author>
		<ptr target="http://www.tv-anytime.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Specification Series: S3 On: Metadata Corrigenda 1 to S-3 V1.1. COR1 SP003v1.1</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-A</forename><surname>Forum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adding Multimedia to the Semantic Web -Building an MPEG-7 Ontology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hunter</surname></persName>
		</author>
		<ptr target="http://www.semanticweb.org/SWWS/program/full/paper59.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Working Symposium (SWWS)</title>
		<meeting><address><addrLine>Stanford University, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-08-01">July 30 -August 1, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining RDF and XML Schemas to Enhance Interoperability Between Metadata Application Profiles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lagoze</surname></persName>
		</author>
		<ptr target="http://www10.org/cdrom/papers/572/" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International World Wide Web Conference</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">May 1-5, 2001</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
	<note>IW3C2</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Text of ISO/IEC 15938-1/FDIS Information Technology Multimedia Content Description Interface Part 1: Systems</title>
		<ptr target=".ISO/IECJTC1/SC29/WG11/N4285" />
		<imprint>
			<date type="published" when="2001-03">March 2001</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
	<note>International Organization for Standardization/International Electrotechnical Commission</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Text of ISO/IEC 15938-2/FDIS Information Technology -Multimedia Content Description Interface -Part 2: Description Definition Language</title>
		<idno>ISO/IEC JTC 1/SC 29/WG 11 N4288</idno>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<publisher>International Organization for Standardization/International Electrotechnical Commission</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<idno>ISO/IEC JTC 1/SC 29/WG 11/N4358</idno>
		<title level="m">International Organization for Standardization/International Electrotechnical Commission. Text of ISO/IEC 15938-3/FDIS Information Technology -Multimedia Content Description Interface -Part 3 Visual</title>
		<meeting><address><addrLine>Sidney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<idno>IEC JTC 1/SC 29/WG 11/N4224</idno>
		<title level="m">International Organization for Standardization/International Electrotechnical Commission. Text of ISO/IEC 15938-4:2001(E)/FDIS Information Technology -Multimedia Content Description Interface -Part</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno>ISO/IEC JTC 1/SC 29/WG 11/N4242</idno>
		<title level="m">International Organization for Standardization/International Electrotechnical Commission. Text of ISO/IEC 15938-5/FDIS Information Technology -Multimedia Content Description Interface -Part 5: Multimedia Description Schemes</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The NewsML Home Page</title>
		<author>
			<persName><surname>Newsml</surname></persName>
		</author>
		<ptr target="http://www.newsml.org/" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Yin/Yang Web: XML Syntax and RDF Semantics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Siméon</surname></persName>
		</author>
		<ptr target="http://www2002.org/CDROM/refereed/231/" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International World Wide Web Conference</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">May 7-11, 2002</date>
		</imprint>
	</monogr>
	<note>IW3C2</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Digital Video Broadcasting Project Home Page</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D V B</forename><surname>Project</surname></persName>
		</author>
		<ptr target="http://www.dvb.org/latest.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Standard 330M-2000 for Television-Unique Material Identifier (UMID)</title>
	</analytic>
	<monogr>
		<title level="m">Standard 330M-2000 for Television-Unique Material Identifier (UMID)</title>
		<meeting><address><addrLine>White Plains, N.Y.</addrLine></address></meeting>
		<imprint>
			<publisher>SMPTE</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename></persName>
		</author>
		<ptr target="http://www.thegateway.org/welcome.html" />
		<title level="m">Educational Materials Consortium. The Gateway to Educational Materials Home Page</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Union. The European Broadcasting Union Home Page</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E B</forename></persName>
		</author>
		<ptr target="http://www.ebu.ch/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reference description of the DAML+OIL</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<ptr target="http://www.daml.org/2001/03/reference.html" />
	</analytic>
	<monogr>
		<title level="m">) ontology markup language</title>
		<imprint>
			<date type="published" when="2001-03">March 2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
