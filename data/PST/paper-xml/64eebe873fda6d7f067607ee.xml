<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TaskLAMA: Probing the Complex Task Understanding of Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-08-29">29 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Quan</forename><surname>Yuan</surname></persName>
							<email>yquan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
							<email>mehrankazemi@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isaac</forename><surname>Noble</surname></persName>
							<email>isaacn@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Vaiva</forename><surname>Imbrasaite</surname></persName>
							<email>vimbrasaite@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
							<email>ramachandrand@google.com</email>
						</author>
						<title level="a" type="main">TaskLAMA: Probing the Complex Task Understanding of Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-29">29 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2308.15299v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured Complex Task Decomposition (SCTD) is the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between them. SCTD is an important component of assistive planning tools, and a challenge for commonsense reasoning systems. We probe how accurately SCTD can be done with the knowledge extracted from Large Language Models (LLMs). We introduce a highquality human-annotated dataset for this problem and novel metrics to fairly assess performance of LLMs against several baselines. Our experiments reveal that LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37% over the base model. However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In their daily lives, people are involved in executing multiple tasks of different temporal granularity in order to achieve their varied goals. This may range from simpler tasks such as washing a cup which may take seconds or minutes to complete, to more complex tasks such as planning a wedding which may take many weeks or months to complete.</p><p>There is abundant evidence that consciously decomposing a complex task into smaller sub-tasks leads to more efficient and reliable execution. For example, <ref type="bibr" target="#b13">Kokkalis et al. (2013)</ref> show that people tend to achieve tasks better and faster when given a concrete plan with actionable steps. <ref type="bibr" target="#b5">Cheng et al. (2015)</ref> show that breaking a macro-task into micro-tasks for workers results in more accurate overall quality and allows for easier recovery from interruption. <ref type="bibr" target="#b6">Chilton et al. (2013)</ref> show that breaking down a task into sub-tasks is beneficial for taxonomy creation. <ref type="bibr" target="#b26">Teevan, Iqbal, and Von Veh (2016)</ref> show that breaking a task into sub-tasks can be beneficial for collaborative writing (e.g., writing a description of a shared project). <ref type="bibr" target="#b0">Allen and Peikoff (2001)</ref> argue in their book that "there is an inverse relationship between things on your mind and those things getting done". Transfer the egg mayonnaise into a bowl Task Graph Figure <ref type="figure">1</ref>: An example of a task graph for a complex task from the TaskLAMA dataset. Nine steps are outlined. The four steps with horizontal texts can be done in any order.</p><p>Given a complex task, the goal of structured complex task decomposition (SCTD) is to automatically find a complete set of necessary steps for achieving the task, and specify temporal dependencies between these steps (i.e. which steps should be done before which). The output can be described as a directed acyclic graph, which we term Task Graph, where the nodes represent the steps and the edges represent temporal dependencies. Any ordering of the nodes that respects the edges of the graph is a possible ordering of the steps to accomplish the task. Figure <ref type="figure">1</ref> demonstrates an example task graph for making egg-based mayonnaise. The importance of this problem has led to an extensive classic and modern literature for developing solutions based on the latest AI technologies available <ref type="bibr" target="#b20">(Newell, Simon et al. 1972;</ref><ref type="bibr" target="#b13">Kokkalis et al. 2013;</ref><ref type="bibr" target="#b1">Awadallah et al. 2014;</ref><ref type="bibr" target="#b31">Zhang et al. 2021)</ref>. These works were also motivated by the feature of SCTD being a quintessential or (informally speaking) AI-Complete problem, involving many features of human-level reasoning such as logical/defeasible reasoning, uncertainty and high context-dependence. The existing solutions in the literature are typically based on crowd-sourcing <ref type="bibr" target="#b13">(Kokkalis et al. 2013;</ref><ref type="bibr">Zhou et al. 2022b)</ref>, based on similarity or cooccurrence of user search queries <ref type="bibr" target="#b1">(Awadallah et al. 2014;</ref><ref type="bibr" target="#b19">Mehrotra and Yilmaz 2017;</ref><ref type="bibr" target="#b32">Zhang et al. 2015)</ref>, or based on summarizing the content of relevant web-pages found through web search <ref type="bibr" target="#b31">(Zhang et al. 2021)</ref>.</p><p>In this paper, we probe the extent to which such knowledge can be extracted from large language models (LLMs). Previous work has shown that LLMs contain a large amount of different types of knowledge <ref type="bibr" target="#b23">(Petroni et al. 2019;</ref><ref type="bibr" target="#b25">Sung et al. 2021;</ref><ref type="bibr" target="#b28">West et al. 2021</ref>) and can do various types of reasoning <ref type="bibr" target="#b27">(Wei et al. 2022;</ref><ref type="bibr" target="#b21">Nye et al. 2021;</ref><ref type="bibr">Kazemi et al. 2023a)</ref>. Our work extends this line of work by probing LLMs for their SCTD knowledge and reasoning abilities, and demonstrating their current strengths and limitations.</p><p>We create a high-quality human-annotated dataset for the SCTD problem. Following the current convention of naming language model probes as xLAMA, we name the dataset TaskLAMA. Specifically, we gather a set of 1612 tasks and ask human annotators to 1) write their assumptions to provide context for the task, 2) write the required steps for those tasks under the provided context, and 2) specify the temporal dependencies between the steps. This gives us a total of 12118 steps and 11105 temporal dependencies.</p><p>We identify a potential problem with the metrics used in previous work to measure the quality of the generated nodes: one can arbitrarily improve the metric by simply adding duplicate sub-steps. To solve this issue, we propose robust metrics and report results with them providing the first fair comparison of methods for this problem. We also develop novel metrics for measuring the quality of the generated temporal dependencies. These metrics are potentially generally applicable in other settings where annotated/labeled graphs produced by generative models must be compared.</p><p>We compare the performance of LLMs on TaskLAMA against heuristic-based, similarity-based, and query-based baselines. Our results reveal that the steps generated by an off-the-shelf LLM have higher quality than the baselines offering 15% -280% relative improvement compared to the best baseline in terms of different metrics. We also show that LLMs understand the context and can adapt the generated steps based on the context in which the complex task is to be done. We then propose a number of approaches to improve the performance of off-the-shelf LLMs even further, using the specialized structure of the SCTD problem. The combination of these solutions result in 7% -37% improvement over the base model, depending on the metric we use. We also measure the quality of the temporal dependencies produced by LLMs and observe that while LLMs are good at generating good sequences of steps, their ability in predicting pairwise temporal dependency remains unsatisfactory.</p><p>A summary of our main contributions follow: 1-we create TaskLAMA, a high-quality probe specifically focused on complex real-world task understanding, 2-we develop metrics for measuring model performance for SCTD, 3-we propose various LLM-based approaches for SCTD and compare against a number of baselines that do not leverage LLMs, 4-we conduct a comprehensive set of experiments showing that LLMs perform well at decomposing a complex task into a sequence of steps, but their understanding of temporal dependencies between these steps remains unsatisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We categorize the works from the literature that relate to our paper as follows:</p><p>Crowd-Sourced SCTD: One line of work uses crowdsourcing for obtaining the steps and their temporal order for complex tasks. <ref type="bibr" target="#b13">Kokkalis et al. (2013)</ref> develop a framework where users can find information about various tasks: if a similar task (measured using natural language processing tools) already exists in their database, the information about that task is shown to the user; otherwise, the task is sent for crowd-sourcing. <ref type="bibr">Zhou et al. (2022b)</ref> combine the information from the WikiHow website<ref type="foot" target="#foot_0">1</ref> to produce hierarchical task trees. While crowd-sourcing may lead to high-precision task graphs, it is costly and may suffer from low-recall as new task graphs cannot be built on-the-fly for novel tasks.</p><p>Query-based SCTD: Another commonly used approach for SCTD is by leveraging user search queries. <ref type="bibr" target="#b1">Awadallah et al. (2014)</ref> create sessions from the search queries of a commercial search engine and propose steps for complex tasks by finding the queries that frequently co-occurred with the complex task in different sessions. <ref type="bibr" target="#b19">Mehrotra and Yilmaz (2017)</ref> propose a hierarchical clustering approach for search queries where the queries higher in the hierarchy correspond to tasks and their children represent steps to those tasks. <ref type="bibr" target="#b32">Zhang et al. (2015)</ref> map queries to demands using external knowledge and mine frequent demand patterns. In this work, we compare against a number of query-based approaches.</p><p>Summarization-based SCTD: <ref type="bibr" target="#b31">Zhang et al. (2021)</ref> proposed a summarization-based approach to SCTD where for a given complex task, first a web search is done to identify relevant web pages, and then a language model is trained to summarize the contents of those web-pages into task graphs. In this work, we take a different approach by measuring how much of the information can be directly obtained from the LLM itself.</p><p>LLM Knowledge Probing: Previous work has shown that LLMs contain a large amount of different types of knowledge. This includes factual <ref type="bibr" target="#b23">(Petroni et al. 2019;</ref><ref type="bibr" target="#b11">Jiang et al. 2020)</ref>, commonsense <ref type="bibr" target="#b35">(Zhou et al. 2020;</ref><ref type="bibr" target="#b8">Davison, Feldman, and Rush 2019;</ref><ref type="bibr" target="#b29">Yin et al. 2022)</ref>, biomedical <ref type="bibr" target="#b25">(Sung et al. 2021)</ref>, numerical <ref type="bibr" target="#b16">(Lin et al. 2020)</ref>, scale <ref type="bibr" target="#b30">(Zhang et al. 2020)</ref>, and many other types of knowledge. Most related to our work, it has been shown that LLMs perform well in breaking a simple goal into specific low-level actions a robot needs to take to achieve the goal <ref type="bibr" target="#b10">(Huang et al. 2022</ref>) (e.g., providing the low-level steps for a goal such as throw away garbage); they also perform well in simple, advice-seeking scripts <ref type="bibr" target="#b24">(Sakaguchi et al. 2021;</ref><ref type="bibr" target="#b18">Madaan et al. 2022;</ref><ref type="bibr" target="#b2">Brahman et al. 2023</ref>) (e.g., go out with friends, live somewhere warmer, etc.). Our work is in the same vein with these works, but extends them by measuring the amount of information one can extract from LLMs for complex tasks requiring multiple (potentially complex) steps to be completed (see Table <ref type="table" target="#tab_1">1</ref> for a sample of such tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The TaskLAMA Probe</head><p>We create a dataset of task graphs for 1630 complex tasks. Following the LAnguage Model Analysis (LAMA) naming convention, we call our dataset TaskLAMA. Before describing the dataset creation process, we start with defining our notation. We represent a graph G = (V, E) as a tuple with A task graph is defined as follows:</p><p>Definition 1 (Task Graph). A task graph for a complex task T is a graph G = (V, E) where each node v i ? V represents a step required for accomplishing T and each edge (v i , v j ) ? E represents a temporal dependence between v i and v j , indicating that v i should be done before v j .</p><p>The problem we study in this paper is the following: Given a complex task T (and sometimes an extra context about the task) as input, generate a task graph G as output. TaskLAMA provides a probe for this problem with (T i , G i ) pairs. The creation of TaskLAMA involves four main components: 1-selecting a set {T 1 , . . . , T ? } of complex tasks, 2gathering steps V i involved in the execution of these tasks, 3gathering temporal dependencies E i between the steps, and 4-splitting the dataset into train, validation, and test sets. In what follows, we explain each component in detail<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting a set of complex tasks</head><p>We obtain a varied and representative set of complex tasks performed by humans from the following two sources.</p><p>The MSComplexTasks dataset <ref type="bibr" target="#b31">(Zhang et al. 2021</ref>): There are 711 distinct tasks in this dataset coming from the logs of Wunderlist, a popular task management application. These tasks are selected from a bigger pool of tasks using a number of filters, most notably filtering simple tasks.</p><p>Popular How To search queries: From the logs of a commercial search engine, we extracted popular search queries that start with How To. To ensure anonymity, we removed any query issued by fewer than 1000 unique users. We then deduplicated these tasks and applied a number of other filters to remove tasks involving sensitive and harmful topics, tasks requiring medical advice, or tasks that did not deem complex (e.g., tasks that did not involve multiple steps). We labeled the remaining tasks based on topic and sampled from each topic to avoid over-presence or under-presence from certain topics. At the end, we obtained 901 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gathering steps for the tasks</head><p>Complex tasks can be typically accomplished in multiple different ways, with a potentially different set of steps involved each time depending on the context (e.g., Make a burger can have different steps depending on whether we do it On a charcoal grill or In an air fryer). To gather the steps V for a task T while taking the context into account, we instructed annotators to do the following: for each task, write down the assumptions they are making and then the set of steps for the task under those assumptions. Some examples of tasks and assumptions are presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>The annotators were allowed to search online and learn about the steps, but were required to then write the steps in their own words and based on their own understanding. The annotators were also instructed to make sure that each step starts with a verb, corresponds to exactly one action, is meaningful as a standalone sentence/does not contain anaphora (e.g., avoid put it on the grill), is actionable as opposed to general advice, and is applicable in the context of the assumptions made. If the annotator was unfamiliar with the task, they were instructed to skip it; the task was then sent to another annotator.</p><p>We trained the annotators over three rounds of pilot study, each time having them annotate a small number of tasks and then explaining to them the mistakes they made. Some of the dominant mistakes in the initial rounds included directly copying search results, failing to follow one of the rules mentioned above, and/or misunderstanding how to provide assumptions. These issues were mostly resolved over the three rounds of training. In the final round, the workers spent an average of 892 seconds on each task. Through the above process, we gathered a total of 12118 steps for our 1612 tasks, with an average of 7.5 steps per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gathering temporal dependencies</head><p>Once we gathered the assumptions and steps for each task, a separate set of annotators were asked to specify the order dependencies E for the steps of each task. The annotators were instructed to first draw the graph on a piece of paper and then submit the edges one by one. Similar to the previous case, we trained the workers over three rounds of pilot studies. The mistakes in the initial rounds ranged from producing a linear sequence instead of a graph, only providing a partial graph (i.e. only a subset of the edges), and misunderstanding the concept of temporal dependence. These issues were mostly resolved over the three rounds of training. In the final round, the workers spent an average of 1138 minutes on each task. Through this process, we gathered a total 11105 temporal dependencies for our 1612 tasks, with an average of 6.9 dependencies per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset splitting</head><p>We split the data into train, validation, and test sets in such a way that the tasks are conceptually different in the three sets. Towards this goal, we first grouped the 1612 tasks into 621 clusters based on their textual similarity and then randomly split the clusters into train, validation, and test sets. This splitting strategy ensures some amount of difference in the tasks in each set. Following this splitting strategy, we ended up with 965 examples in the training set, 169 in the validation set, and 478 in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To generate task graphs for a given complex task, a model needs to 1) generate the steps, and 2) decide the order dependency between the steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating the steps</head><p>To generate the steps for a task, we experiment with the following strategies:</p><p>In-Context Learning (ICL): In ICL <ref type="bibr" target="#b3">(Brown et al. 2020</ref>), one provides a few demonstrations each containing an input and the expected output, followed by the query for which the model has to generate the output. The model learns the relation between the input and the output in context, and uses that to generate an output for the provided query.</p><p>Multiple Sequences (MultSeq): We notice that when we generate multiple sequences of steps for a task using the ICL approach, the sequences sometimes have complementary steps between them. To leverage this intuition, we generate k sequences of steps using the ICL approach, setting the decoding temperature to 0.5 to allow for diverse generations. Then we deduplicate the steps and combine the remaining steps to obtain the final set of steps. The deduplication procedure is explained in the Appendix.</p><p>Sample and Filter (S&amp;F): We notice that when we generate multiple sequences of steps for a task using the ICL approach, some of the sequences have higher quality than the others. To leverage this, we first train a separate model that scores the sequences generated by the ICL approach. Then, we generate multiple sequences of steps and select the one with the highest score according to the trained model. To train a model that can score the generated sequences, we first generate 16 sequences per task for the tasks in our training set, then we evaluate each of the generated sequences with respect to the golden sequence and obtain a single number indicating how good that sequence is. This gives us a dataset of (Task, Sequence of Steps, Score). We then train a model that given a task and a sequence of steps predicts the score.</p><p>Soft-Prompt Tuning (SPT): In the case of ICL, the incontext demonstrations we provide as input get mapped to the corresponding token embeddings that are then fed into the LLM. Recently, it has been shown that instead of using a fixed set of token embeddings as the in-context demonstrations, one can learn those embeddings based on training data to enable better in-context examples. This technique is typically referred to as soft-prompt tuning <ref type="bibr" target="#b15">(Lester, Al-Rfou, and Constant 2021)</ref>. We learn the prompt embedding based on our training data and decide the size of the prompt based on performance on our validation set.</p><p>MultSeq + S&amp;F: We generate k ? sequences of steps using the ICL approach, then select and combine the top k sequences ranked by the S&amp;F model.</p><p>MultSeq + SPT: We combine k sequences from the SPT model instead of the ICL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S&amp;F + SPT:</head><p>We use the S&amp;F model to score the sequences of steps generated by SPT and then select the best.</p><p>MultSeq + S&amp;F + SPT: This is similar to S&amp;F + SPT except that we combine the steps from the top k sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating the order dependencies</head><p>While task graphs are directed acyclic graphs (e.g., see Figure <ref type="figure">1</ref>), when using an LLM to generate the steps for a task we get a sequence of steps. We compare a few approaches that can turn the sequence of steps generated by the LLM into a task graph.</p><p>Linear: We use the linear order of steps produced by the LLM as the final task graph.</p><p>ICL: We provide multiple examples as demonstrations each containing a task, two of its steps, and the label indicating whether the first step should be done before the second one. We then provide a new task and two of its steps and ask for the label.</p><p>ICL with Chain-of-Thought: Chain-of-thought (CoT) prompting <ref type="bibr" target="#b27">(Wei et al. 2022</ref>) is a technique where besides providing the input and the label, the demonstrations also provide a rationale for the label. We test a version of ICL with CoT where the rationale for the demonstrating examples are written manually.</p><p>SPT: We soft-prompt tune the LLM on the training data to learn to predict the label given a task and two of its steps.</p><p>LLM Scoring: Given the initial linear order produced by the LLM, we generate m sequences by randomly swapping the order of two steps and use the LLM to score the sequences<ref type="foot" target="#foot_2">3</ref> . We then sort the sequences descendingly based on their LLM score and select the highest scoring sequences. Then, for two steps v i and v j , if we see v i before v j in some sequences and v j before v i in the other sequences, we assume v i and v j can be done in any order; otherwise, if v i always appears before v j (or vice versa), we assume v i has to be done before v j (or vice versa). We turn the sequences into a graph following the above strategy.</p><p>In the case of cycles, i.e. if a model predicts that A should be done before B, B should be done before C, and C should be done before A, we remove the dependencies assuming that each of the steps can be done before the other one so they can be done in any order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>For evaluation, we need two sets of metrics: one for measuring the quality of the steps (nodes) and one for measuring the quality of the temporal dependencies (edges). We discuss each of these separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Metrics</head><p>To compare the generated steps and measure their quality with respect to the golden steps, let V G = {v 1 , . . . , v n } be the steps in the golden graph, U M = {u 1 , . . . , u m } be the steps in the generated graph, and S be a pairwise similarity function of the steps (we use the cosine similarity of the universal sentence encodings <ref type="bibr" target="#b4">(Cer et al. 2018</ref>) of the steps).</p><p>Previous work has proposed to compute precision as  2021). We find, however, that with these metrics, one can arbitrarily increase the precision without sacrificing recall.</p><p>Consider the case where</p><formula xml:id="formula_0">V G = {v 1 , v 2 }, U M1 = {u 1 , u 2 },<label>and</label></formula><formula xml:id="formula_1">U M2 = {u 1 , u ? 1 , u 2 }</formula><p>and let S(u 1 , v 1 ) = 0.6 and zero for other pairs, and u ? 1 be a near-duplicate of u 1 . Ideally, the output of M 1 should be preferred to the output of M 2 because they provide the same information but M 1 has no duplicates. However, using the above formulae, M 1 will have a precision of (0.6+0.0) /2 = 0.3 and a recall of (0.6+0.0) /2 = 0.3, whereas M 2 will have a precision of (0.6+0.6+0.0) /3 = 0.4 and a recall of (0.6+0.0) /2 = 0.3. We observed this issue in our experiments too, where combining the steps from multiple sequences (without deduplication) increased both precision and recall.</p><p>The problem described above is due to the one-to-many mapping formulation of precision and recall. To solve this issue, we use a Hungarian matching <ref type="bibr" target="#b14">(Kuhn 1955)</ref> of the steps that enforces a one-to-one mapping. We note, however, that in some cases, one step in the golden graph may correspond to more than one steps in the generated graph and vice versa, in which case a one-to-one mapping may be too restrictive. For example, the golden graph may contain two steps add salt and add pepper and the generated graph may contain a step add salt and pepper. To account for such cases, we also report a relaxed version of Hungarian matching where we allow a one-to-two mapping 4 .</p><p>Once the precision and recall are computed using (relaxed) Hungarian matching, we compute the F 1 score and report it. We note that some of the generated steps that do not appear in the golden steps may still be good steps (e.g. because they provide more detail that is not in the golden graph). For this reason, recall might be a more informative metric than precision. To account for this, we also report an F 2 score where we weigh recall twice as much as precision.</p><p>Following previous work <ref type="bibr" target="#b31">(Zhang et al. 2021;</ref><ref type="bibr" target="#b18">Madaan et al. 2022)</ref>, we also concatenate the steps for each task and create a single document, and then report Rouge (F1 and F2) scores for these documents. 4 We could also report more relaxed versions such as one-tothree, but we observe that the cases where a single step corresponds to more than two steps are rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Metrics</head><p>To compare the generated edges with those of the ground truth graph, we first match the nodes from the generated graph to the nodes from the golden graph using Hungarian matching. If a node in one graph does not have a match in the other graph, then we connect it to a dummy singleton node. Then, for each pair of matched nodes (v i , u j ), let P i and C i be the parents and children of v i respectively, and P j , C j be the parents and children of u j respectively. We measure the amount of overlap between P i and P j as well as the amount of overlap between C i and C j , both computed in terms of Rouge score. Then we report two metrics: 1-In-Degree: the average overlap between P i and P j over all matched pairs of steps, 2-Out-Degree: the average overlap between C i and C j over all matched pairs of steps. Intuitively, by measuring the overlap between P i and P j , we measure the amount of overlap between the (immediate) preconditions of the two matched nodes. Moreover, by measuring the overlap between C i and C j , we measure the amount of overlap between the steps that become executable (immediately) after we execute the matched nodes.</p><p>Furthermore, we also report another metric, which we term step proximity, computed as the average overlap between P i ? C i and P j ? C j . Note that this metric does not evaluate the order of the temporal dependencies; instead, it evaluates whether the steps that should be done in close proximity to each other are indeed placed close to each other in the generated graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Decomposition Results</head><p>We compare against a number of baselines outlined below.</p><p>Repeat Task: This baseline repeats the task m times; m is a hyperparameter that we tune on the validation set.</p><p>Repeat Similar: This baseline works similarly as the previous baseline, but for the i-th step, we randomly select one of the (non-stop word) tokens in the step and replace it with a semantically similar token. The similarity is computed based on GloVe embeddings <ref type="bibr" target="#b22">(Pennington, Socher, and Manning 2014)</ref> of the tokens. The rationale for this substitution is that those words are likely to appear in the steps. For example, for the task make a smoothie, some of the most similar words to smoothie include yogurt, juice, strawberry and granola which are likely to appear in the steps of the task.</p><p>Search Query Co-occurrence: We cluster a large set of queries from a commercial search engine aiming at placing near-duplicate queries into the same cluster. Then, inspired by <ref type="bibr" target="#b1">Awadallah et al. (2014)</ref>, for a given task T we find the cluster C t that is most similar to T and we rank the other clusters based on the co-occurrence of their queries with those of C t and pick the top k clusters. We select a representative query from each of these cluster to serve as a step for the task T. Here, k is a hyperparameter that is tuned on the validation set.</p><p>Search Query Hierarchy: We perform a second level of clustering on top of the clustering from the previous baseline, where we aim to put similar-intent queries into the same cluster. Then, inspired by <ref type="bibr" target="#b19">Mehrotra and Yilmaz (2017)</ref>, for a given task T we find the clusters C L1 and C L2 from our level one and level two clustering (note that C L1 is a child of C L2 ) where T should belong, and then obtain steps by selecting a query from each of the top k sibling clusters of C L1 . Here, k is a hyperparameter that is tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The results are reported in Table <ref type="table" target="#tab_2">2</ref>. We observe that even the ICL approach significantly outperforms the other baselines on all metrics. For example, we observe a relative improvement of 280% over the best baseline in terms of Rouge2 F2-Score, and 15% in terms of Hungarian matching F1-score. This establishes LLMs as a powerful source for extracting information about the steps of a task. Our solutions further improve upon the ICL results. In particular, we observe that both S&amp;F and SPT result in improvements compared to ICL across various metrics, with SPT providing more improvement compared to S&amp;F. The MultSeq method brings improvement mostly for the F2-scores, due to having higher recall, showing that the steps generated in multiple LLM calls could be complementary as the combination of them improves recall, and hence the F2-score.</p><p>The approaches that mix two solutions perform better than the individual approaches in isolation in many cases. Among these approaches, we observe that MultSeq + SPT works best in terms of the F2-score and S&amp;F+SPT performs best in terms of F1-score. The combination of all three solutions also works well, but is often dominated by one of the approaches that combines two solutions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Understanding Results</head><p>The steps for completing a complex task can be different depending on the context. For example, recover deleted photos has different steps depending on the device. We measure the ability of LLMs in providing contextualized steps for a task.</p><p>Recall that in our dataset, the steps for a task are written under certain assumptions that provide the context. To measure how well LLMs can providing contextualized steps for a task, we compare their performance with and without the assumption/context being provided to them. We test both settings with ICL: in one case we only provide the task and the steps and in the other we provide the task, assumptions/context, and the steps.</p><p>According to Figure <ref type="figure" target="#fig_1">2</ref>, when we provide extra context, the model performs better across all metrics. This shows that LLMs are able to adjust the steps based on the context for a task. In Figure <ref type="figure" target="#fig_3">3</ref>, we provide an example model output with and without the context provided to the model. We can see that when no context is provided to the model, the model either provides general steps or selects a specific approach (using vinegar), but when the context is provided the model provides steps that are more specific to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Dependency Results</head><p>We next verify how well LLMs can predict the temporal dependencies between the steps of the tasks (i.e. the edges of the task graphs). Initially, we compare the ICL, ICL+CoT, and SPT approaches on the golden nodes and edges (note that the other two edge prediction approaches are only applicable to generated graphs). In this case, for each pair of nodes in each of the golden task graphs in the test we ask the model to predict if one step should be done before the other one and report the accuracy. The results are reported in Table <ref type="table" target="#tab_4">4</ref>. According to the results, both ICL and ICL+CoT perform quite poorly, but the performance improves massively after soft-prompt tuning, thus showing that temporal dependency understanding does not surface out of the box from LLMs but requires tuning on some data.</p><p>We next evaluate the performance of the approaches on the generated graphs. To this end, we conduct two experiments in one case we fix the generated steps to those of the SPT model (i.e. there is only one sequence of steps) and in the second we fix the generated steps to those of the SPT+MultSeq model (i.e. there are multiple sequences of steps). We then use the aforementioned approaches to decide the links. The results are reported in Table <ref type="table" target="#tab_3">3</ref>. According to the results, in the case where we use only one sequence, we observe that the linear order produced by the LLM is quite a strong baseline: it outperforms the other models for step proximity (except for LLM Scoring where the two models are on-par) and only slightly underperforms in terms of indegree and out-degree metrics. We also tried a version of the SPT where we combine it with the linear order of the LLM by making the following assumption: if the LLM produced step A before step B, then we assume either A should be done before B, or A and B can be done in any order (i.e. we rule out the possibility that B should be done before A). Even in this case, we observe that the linear model alone still gives a better performance. Our results show that while LLMs are good at generating the sequence of steps for a task in the right order, they are not particularly good at individually deciding which step of a task should be done before the other or if two steps can be done in any order. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>In Figure <ref type="figure" target="#fig_4">4</ref>, we demonstrate an LLM-generated task graph. We can see that many of the steps and the temporal depenare sensible; however, there may also be some problems present. For example, one probably needs to complete the home study before attending the interview. More qualitative examples are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We studied the power of large language models (LLMs) for structured complex task decomposition (SCTD), i.e. the problem of decomposing a complex real-world task into multiple steps and determining the temporal dependencies between the steps. To this end, we created a probe named TaskLAMA, developed metrics for measuring the performance, tested various task decomposition and temporal dependency prediction models and compared against baselines. Our results indicate that LLMs are strong in task decomposition. For predicting temporal dependencies, however, while they are able to produce a good sequence of steps with the right order of steps, their ability in predicting pairwise temporal dependencies still lags behind. Future work can find ways of improving the temporal dependency understanding of LLMs, or develop new approaches to improve LLM-based SCTD, e.g. by generating the entire task graph at once (see <ref type="bibr" target="#b24">Sakaguchi et al. (2021)</ref>; <ref type="bibr" target="#b18">Madaan et al. (2022)</ref>) or by recursively breaking the complex tasks into simpler tasks and then solving those simpler task (see <ref type="bibr">Zhou et al. (2022a)</ref>; <ref type="bibr">Kazemi et al. (2023b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The quality of existing datasets</head><p>MSComplexTask <ref type="bibr" target="#b31">(Zhang et al. 2021)</ref> is the only dataset we are aware of that includes steps and temporal dependencies for complex tasks. However, we found several quality issues in the dataset that motivated us to create a new probe. Here, we categorize some of the main quality issues and provide a few examples for each category in Table <ref type="table" target="#tab_6">5</ref>.</p><p>? Train/Test Overlap: We found that for a large number of tasks in the test set, there exists one or more near duplicate (or closely related) task in the training set. To quantify the extent of this issue, for each task in the test set we first identified a set of highly similar tasks from the training set with a combination of manual search and automatic textual similarity. Then, we showed pairs of tasks to a human annotator who judged if the two tasks are 1-near duplicate (the two tasks being identical despite differences in the textual description of the task), 2-closely related (the two tasks are not identical but may share several sub-steps), or 3-other (the two tasks may not share several sub-steps). We found that for 21% of the tasks in the test set, there was at least one near duplicate task in the training set, and for another 13% there was a closely related task in the training set, amounting to 34% leakage in total.   ? Irrelevant steps: For many tasks, we found that some of the provided steps are irrelevant to the task. ? Parsing issues: Since the initial steps have been mined automatically from the web, we found several parsing issues resulting in redundant text, incomplete steps, extra information with respect to the order (potentially leading to leakage for determining order dependency), etc. ? Duplicate steps: Since the initial steps come from multiple sources, we found that sometimes a task has several duplicated steps. ? Coreferences: Many steps contain pronouns that may refer either to something in the other steps or to something that is not even mentioned in the other steps, making the steps not standalone. ? (Irrelevant) advice instead of action: Some of the steps provided for a task do not correspond to an action that the user has to take, but rather to a (sometimes irrelevant) advice.</p><p>Another related dataset is the ProScript <ref type="bibr" target="#b24">(Sakaguchi et al. 2021)</ref>. This dataset contains both steps and temporal dependencies. However, we find that the inputs are mostly simple or advice-seeking scripts where one of the many options can be selected. Examples include take a shower after work, try daring foods, see the forest, live somewhere warmer, go to a bar one day, etc. We also find that many steps for the script are based on imaginary situations (e.g., take the elevator downstairs is a step for the script meet for lunch, which might be based on an imaginary situation where the person needs to take the elevator and go downstairs to meet for lunch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Results</head><p>More qualitative examples: Figures <ref type="figure" target="#fig_5">5</ref> and<ref type="figure" target="#fig_6">6</ref> show examples of LLM-generated task graphs where the nodes and   edges are mostly meaningful. One possible edge mistake in 5 could be the one between Build a clientele and Promote your business, where one may expect to first promote the business and then build a clientele. Figures 7 shows an example of a lower-quality LLM-generated task graph, where the steps are very generic and do not contain much information, and there are steps for applying for both bartender and barback positions, whereas the task is to become a bartender.</p><p>Precision and Recall: In the main text, for complex task decomposition we reported the F1 and F2 results. For completeness sake, in Table <ref type="table" target="#tab_7">6</ref> we report the precision and recall results. Similar conclusions can be derived as before with the S&amp;F and SFT models helping improve both precision and recall and the MultSeq model mostly helping recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The experiments were done with the PaLM 62B model <ref type="bibr" target="#b7">(Chowdhery et al. 2022)</ref> on TPUs of version 4. Due to the high cost of the experiments, the reported results are for one run only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Decomposition</head><p>In the case of the Repeat Similar baseline, we sampled from the top-20 most similar tokens to that of the token that was to be replaced. In the case of ICL, we provided 10 demonstrating examples from the training set as our in-context demonstrations. We select these 10 examples randomly and leave more sophisticated selection approaches, e.g. see <ref type="bibr" target="#b17">(Luo et al. 2023</ref>) as future work. We used the following prompt: For the soft-prompt tuned model, we tested prompt sizes from {100, 50, 10} and selected the one that performed best on the validation set. We set the learning rate to 0.1, batch size to 8, and total number of training steps to 10000.</p><p>For the S&amp;F model, we trained a BERT-base model <ref type="bibr" target="#b9">(Devlin et al. 2018)</ref> for sequence scoring with a learning rate of 1e -5 and batch size of 16, over 10 epochs. We also tried BERT-large and BERT-small; while BERT-base was slightly better than BERT-small, the difference between BERT-base and BERT-large was negligible.</p><p>For deduplication in the MultSeq model, we embed the steps into a vector representation using the universal sentence encoding <ref type="bibr" target="#b4">(Cer et al. 2018</ref>) and then consider two steps as being near-duplicates if the cosine similarity of their embedding is higher than a threshold. To decide the threshold, we generated multiple sequences of steps for the tasks in our training set, manually labeled some pairs of steps as being near-duplicates or being different, and then we selected a threshold that maximized the classification accuracy on the labeled set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Dependencies</head><p>In the case of ICL, we provided 10 demonstrating examples using the following prompt: For the soft-prompt tuned model, we set the prompt size to 10, the learning rate to 0.1, batch size to 4, and the total number of training steps to 14000. For the LLM Scoring model, we generate sequences by considering all possible ways of swapping two steps (truncated at 128 sequences at most), rank them based on their LLM score and filter out the bottom half, and construct the graph based on the remaining sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Measuring the complex task understanding of models is a challenging problem both in terms of dataset construction and evaluation. While we took a step toward this goal, we outline several limitations of our work that future work can resolve:</p><p>? Conditional Task Graphs: While TaskLAMA provides unconditional task graphs, the steps (and order dependencies) in complex real-world tasks are typically conditional. As an example, after completing some the steps, the following steps might be different depending on the outcome of the previous steps. Future work can develop probes and benchmark models for conditional task graph generation. ? Granularity: One important challenge for developing models (or measuring their performance) for task understanding is that Task Graphs can be generated at different levels granularity, making evaluation difficult. In this work, we only generated Task Graph at one level of granularity. Future work can find ways of generating and evaluating task graphs at different levels of granularity. ? Similarity vs entailment: For evaluation, following previous work we measured step overlap in terms of similarity. However, a similarity-based approach fails to capture cases where two steps are not textually similar but one entails the other. Future work can use entailment models in place of similarity models for evaluation. ? Simultaneously generating steps and temporal dependencies: In this work, we only examined the approaches where the steps (nodes) and temporal dependencies (edges) are produced independently. Future work can measure the performance of the models that produce both of them simultaneously.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>i</head><label></label><figDesc>maxj S(ui,vj ) |U M | and recall as j maxi S(uj ,vi) |V G | (Zhang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The model performance with and without the context provided as input. When the context is provided to the model, it performs better on all metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Task:</head><label></label><figDesc>Clean a mattress Context: Using baking soda Model Output w/o Context: Vacuum the mattress, wash the mattress cover, wash the mattress pad, dampen a cloth with vinegar, wipe the mattress with the cloth, allow to dry, remove any stains.Model Output w/ Context: Vacuum the mattress, mix baking soda and water in a bowl, apply the mixture to the mattress, allow the baking soda to dry, vacuum the mattress again.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of model outputs for a task with and without the context provided as input to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of an LLM-generated task graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of an LLM-generated task graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example of an LLM-generated task graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example of an LLM-generated task graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>are the steps to [TEST TASK]? In the case where we also provide an additional context, each example is as follows: What are the steps to [DEMONSTRATING TASK k] in the following context: [CONTEXT k]? STEP1 [S1], STEP2 [S2], ..., STEPn [Sn].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>TASK 1], should I do sub-step "[Si]" before sub-step "[Sj]"? [Yes/No]. ... EXAMPLE 10 For [DEMONSTRATING TASK 10], should I do sub-step "[Si']" before sub-step "[Sj']"? [Yes/No]. EXAMPLE 11 For [TEST TASK], I do sub-step "[Si'']" before sub-step "[Sj'']"? In the case where we add chain-of-thought, each example is as follows: For [DEMONSTRATING TASK k], should I do sub-step "[Si]" before sub-step "[Sj]"? [Rationale] therefore [Yes/No].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Sample complex tasks and extra assumptions (context) from TaskLAMA (-means no extra assumption).</figDesc><table><row><cell>Complex Task</cell><cell>Assumption / Context</cell></row><row><cell>Build a curved retaining wall</cell><cell>Using concrete</cell></row><row><cell>Start a property management company</cell><cell>In Florida</cell></row><row><cell>Write a grant proposal</cell><cell>For non-profit</cell></row><row><cell>Cook lobster tails at home</cell><cell>Grilled</cell></row><row><cell>Install a light switch</cell><cell>-</cell></row><row><cell>Get a real estate license</cell><cell>In Texas</cell></row><row><cell>Recover deleted photos</cell><cell>From iPhone</cell></row><row><cell>Plan a wedding</cell><cell>In Italy</cell></row><row><cell>Become a travel agent</cell><cell>Online agent</cell></row><row><cell cols="2">V = {v 1 , v 2 , . . . , v n } representing the nodes and E ? V 2</cell></row><row><cell cols="2">represent edges. Throughout this paper, we work with di-</cell></row><row><cell cols="2">rected graphs where, for an edge (v i , v j ), the order of the</cell></row><row><cell>nodes is important.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of different models for task step generation measured in terms of multiple metrics.</figDesc><table><row><cell>-</cell><cell cols="2">Rouge1</cell><cell cols="2">Rouge2</cell><cell cols="2">RougeL</cell><cell cols="2">Hungarian</cell><cell cols="2">Relaxed Hung.</cell></row><row><cell>Model</cell><cell>F1</cell><cell>F2</cell><cell>F1</cell><cell>F2</cell><cell>F1</cell><cell>F2</cell><cell>F1</cell><cell>F2</cell><cell>F1</cell><cell>F2</cell></row><row><cell>Repeat Task</cell><cell>12.2</cell><cell>10.3</cell><cell>1.7</cell><cell>1.5</cell><cell>12.1</cell><cell>10.2</cell><cell>34.6</cell><cell>33.0</cell><cell>41.3</cell><cell>37.7</cell></row><row><cell>Repeat Sim</cell><cell>11.6</cell><cell>10.2</cell><cell>1.5</cell><cell>1.3</cell><cell>10.8</cell><cell>9.5</cell><cell>33.</cell><cell>33.4</cell><cell>40.1</cell><cell>38.4</cell></row><row><cell>Co-occur</cell><cell>16.4</cell><cell>15.0</cell><cell>2.7</cell><cell>2.5</cell><cell>13.6</cell><cell>12.5</cell><cell>34.8</cell><cell>34.7</cell><cell>41.7</cell><cell>40.0</cell></row><row><cell>Hierarchical</cell><cell>14.6</cell><cell>12.6</cell><cell>2.3</cell><cell>2.0</cell><cell>12.7</cell><cell>11.0</cell><cell>34.3</cell><cell>34.1</cell><cell>41.2</cell><cell>39.3</cell></row><row><cell>ICL</cell><cell>33.1</cell><cell>31.7</cell><cell>10.0</cell><cell>9.5</cell><cell>24.0</cell><cell>23.0</cell><cell>40.1</cell><cell>40.3</cell><cell>48.1</cell><cell>47.3</cell></row><row><cell>MultSeq</cell><cell>32.5</cell><cell>37.2</cell><cell>10.5</cell><cell>12.1</cell><cell>22.9</cell><cell>26.3</cell><cell>35.3</cell><cell>42.8</cell><cell>47.5</cell><cell>50.4</cell></row><row><cell>S&amp;F</cell><cell>36.6</cell><cell>34.3</cell><cell>11.3</cell><cell>10.6</cell><cell>25.6</cell><cell>24.1</cell><cell>42.7</cell><cell>41.9</cell><cell>50.2</cell><cell>49.1</cell></row><row><cell>SPT</cell><cell>38.7</cell><cell>36.3</cell><cell>13.2</cell><cell>12.3</cell><cell>26.7</cell><cell>25.1</cell><cell>43.6</cell><cell>42.4</cell><cell>51.5</cell><cell>50.3</cell></row><row><cell>MultSeq + S&amp;F</cell><cell>37.6</cell><cell>38.3</cell><cell>12.1</cell><cell>12.3</cell><cell>25.2</cell><cell>25.7</cell><cell>41.7</cell><cell>44.4</cell><cell>50.5</cell><cell>51.1</cell></row><row><cell>MultSeq + SPT</cell><cell>36.8</cell><cell>41.3</cell><cell>13.4</cell><cell>15.0</cell><cell>25.3</cell><cell>28.4</cell><cell>39.6</cell><cell>46.2</cell><cell>51.3</cell><cell>53.2</cell></row><row><cell>+ SPT</cell><cell>39.0</cell><cell>38.2</cell><cell>13.4</cell><cell>13.1</cell><cell>26.7</cell><cell>26.2</cell><cell>43.8</cell><cell>43.2</cell><cell>51.5</cell><cell>50.4</cell></row><row><cell>MultSeq + S&amp;F + SPT</cell><cell>38.7</cell><cell>40.0</cell><cell>13.7</cell><cell>14.1</cell><cell>25.8</cell><cell>26.7</cell><cell>42.5</cell><cell>44.5</cell><cell>51.6</cell><cell>51.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of different models for edge evaluation on the generated graphs (#seqs=1 corresponds to the SPT model and #seqs=2 to the SPT+MultSeq model).</figDesc><table><row><cell>-</cell><cell>-</cell><cell></cell><cell>In-Degree</cell><cell></cell><cell></cell><cell>Out-Degree</cell><cell></cell><cell></cell><cell>Step Proximity</cell><cell></cell></row><row><cell>Model</cell><cell>#Seqs</cell><cell cols="9">Rouge1 Rouge2 RougeL Rouge1 Rouge2 RougeL Rouge1 Rouge2 RougeL</cell></row><row><cell>Linear Order</cell><cell>1</cell><cell>18.3</cell><cell>8.8</cell><cell>17.7</cell><cell>17.3</cell><cell>7.5</cell><cell>16.7</cell><cell>20.2</cell><cell>5.9</cell><cell>18.2</cell></row><row><cell>ICL</cell><cell>1</cell><cell>13.6</cell><cell>8.0</cell><cell>13.3</cell><cell>13.6</cell><cell>7.6</cell><cell>13.3</cell><cell>15.2</cell><cell>4.0</cell><cell>13.8</cell></row><row><cell>ICL with CoT</cell><cell>1</cell><cell>14.0</cell><cell>6.4</cell><cell>13.5</cell><cell>13.7</cell><cell>5.9</cell><cell>13.2</cell><cell>18.2</cell><cell>4.8</cell><cell>16.2</cell></row><row><cell>SPT</cell><cell>1</cell><cell>18.0</cell><cell>8.9</cell><cell>17.3</cell><cell>17.4</cell><cell>8.2</cell><cell>16.7</cell><cell>19.9</cell><cell>5.6</cell><cell>17.8</cell></row><row><cell>SPT + Linear</cell><cell>1</cell><cell>18.1</cell><cell>8.9</cell><cell>17.4</cell><cell>17.1</cell><cell>8.1</cell><cell>16.4</cell><cell>20.0</cell><cell>5.7</cell><cell>17.9</cell></row><row><cell>LLM Scoring</cell><cell>1</cell><cell>18.4</cell><cell>8.8</cell><cell>17.7</cell><cell>17.2</cell><cell>7.5</cell><cell>16.6</cell><cell>20.2</cell><cell>5.9</cell><cell>18.2</cell></row><row><cell>ICL</cell><cell>2</cell><cell>10.3</cell><cell>4.4</cell><cell>10.0</cell><cell>10.1</cell><cell>4.2</cell><cell>9.8</cell><cell>13.6</cell><cell>3.5</cell><cell>12.3</cell></row><row><cell>ICL with CoT</cell><cell>2</cell><cell>10.8</cell><cell>3.8</cell><cell>10.3</cell><cell>10.5</cell><cell>3.9</cell><cell>10.0</cell><cell>14.4</cell><cell>3.7</cell><cell>12.8</cell></row><row><cell>SPT</cell><cell>2</cell><cell>13.1</cell><cell>5.8</cell><cell>12.7</cell><cell>12.8</cell><cell>5.5</cell><cell>12.3</cell><cell>15.1</cell><cell>4.2</cell><cell>13.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The performance of different models for edge evaluation on the golden graphs. The Majority Class baseline always predicts no dependency.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Majority Class</cell><cell>53.8</cell></row><row><cell>ICL</cell><cell>47.5</cell></row><row><cell>ICL with CoT</cell><cell>49.6</cell></row><row><cell>SPT</cell><cell>78.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quality issues in the MSComplexTask dataset.</figDesc><table><row><cell>Issue</cell><cell>Example 1</cell><cell>Example 2</cell><cell>Example 3</cell></row><row><cell>Train/Test Overlap</cell><cell>Test: build a wooden privacy fence Train: build a wood privacy fence</cell><cell>Test: build an above ground pool deck Train: Build a deck for an above ground pool</cell><cell>Test: Set up an etsy shop Train: Start an etsy business</cell></row><row><cell>Irrelevant Steps</cell><cell>Task: Apply for citizenship uscis Step: View a larger version of the infographic</cell><cell>Task: Paint front door Step: Pin this to Pinterest</cell><cell>Task: Apply for fmla Step: View the archived webinar</cell></row><row><cell>Parsing issues</cell><cell>Task: Apply for social security card Step: Allowed To Work(See Instructions On Page 3)</cell><cell>Task: Register a business in Texas Step: Doing Business As (DBA</cell><cell>Task: Learn to hack Step: Step-3: Learn Programming</cell></row><row><cell>Duplicate steps</cell><cell>Task: Paint front door Step: Choosing a color</cell><cell>Task: Add music to google slideshow Step: Add a YouTube Video</cell><cell>Task: fix a garage door Step: Close the door</cell></row><row><cell></cell><cell>Step: Choose New Paint Color</cell><cell>Step Add music from a YouTube video</cell><cell>Step: Close the garage door</cell></row><row><cell></cell><cell></cell><cell>Step: Method 2:-Add Music From YouTube</cell><cell></cell></row><row><cell>Pronouns</cell><cell>Task: Sell artwork Step: Leave room for them to ask questions</cell><cell>Task: Apply for social security card Step: Use this form to apply for a new or replacemet SSN card</cell><cell>Task: Build a murphy bed cheap Step: Attach them with 2 201d screws</cell></row><row><cell>(Irrelevant) Advice</cell><cell>Task: Sell artwork</cell><cell>Task: Write a job description</cell><cell>Task: Apply for fmla</cell></row><row><cell>instead of Action</cell><cell>Step: Change your art practice</cell><cell>Step: Use bullet points</cell><cell>Step: Work for a covered employer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The precision and recall of different models for task step generation measured in terms of multiple metrics.</figDesc><table><row><cell>-</cell><cell></cell><cell cols="2">Rouge1</cell><cell cols="2">Rouge2</cell><cell cols="2">RougeL</cell><cell cols="2">Hungarian</cell><cell cols="2">Relaxed Hung.</cell></row><row><cell>Model</cell><cell></cell><cell>Prec.</cell><cell>Recall</cell><cell>Prec.</cell><cell>Recall</cell><cell>Prec.</cell><cell>Recall</cell><cell>Prec.</cell><cell>Recall</cell><cell>Prec.</cell><cell>Recall</cell></row><row><cell>Repeat Task</cell><cell></cell><cell>20.0</cell><cell>9.4</cell><cell>2.7</cell><cell>1.3</cell><cell>19.9</cell><cell>9.3</cell><cell>38.7</cell><cell>32.2</cell><cell>49.5</cell><cell>35.7</cell></row><row><cell>Repeat Sim</cell><cell></cell><cell>1.7</cell><cell>9.5</cell><cell>2.0</cell><cell>1.2</cell><cell>15.7</cell><cell>8.9</cell><cell>34.8</cell><cell>33.5</cell><cell>43.6</cell><cell>37.4</cell></row><row><cell>Co-occur</cell><cell></cell><cell>20.9</cell><cell>14.4</cell><cell>3.4</cell><cell>2.4</cell><cell>17.4</cell><cell>12.0</cell><cell>36.1</cell><cell>34.9</cell><cell>45.4</cell><cell>39.0</cell></row><row><cell>Hierarchical</cell><cell></cell><cell>21.5</cell><cell>11.7</cell><cell>3.4</cell><cell>1.9</cell><cell>18.7</cell><cell>10.2</cell><cell>35.5</cell><cell>34.2</cell><cell>45.2</cell><cell>38.2</cell></row><row><cell>ICL</cell><cell></cell><cell>39.4</cell><cell>31.4</cell><cell>12.0</cell><cell>9.4</cell><cell>28.6</cell><cell>22.7</cell><cell>41.9</cell><cell>40.9</cell><cell>49.9</cell><cell>46.9</cell></row><row><cell>MultSeq</cell><cell></cell><cell>28.2</cell><cell>42.4</cell><cell>9.0</cell><cell>13.9</cell><cell>19.8</cell><cell>30.1</cell><cell>28.0</cell><cell>51.0</cell><cell>44.0</cell><cell>52.8</cell></row><row><cell>S&amp;F</cell><cell></cell><cell>43.5</cell><cell>33.3</cell><cell>13.6</cell><cell>10.3</cell><cell>30.4</cell><cell>23.4</cell><cell>45.7</cell><cell>41.7</cell><cell>52.6</cell><cell>48.4</cell></row><row><cell>SPT</cell><cell></cell><cell>46.5</cell><cell>35.1</cell><cell>15.9</cell><cell>11.9</cell><cell>32.1</cell><cell>24.3</cell><cell>47.8</cell><cell>41.9</cell><cell>53.8</cell><cell>49.7</cell></row><row><cell>MultSeq + S&amp;F</cell><cell></cell><cell>39.0</cell><cell>39.4</cell><cell>12.7</cell><cell>12.6</cell><cell>26.3</cell><cell>26.5</cell><cell>39.5</cell><cell>47.2</cell><cell>50.0</cell><cell>51.6</cell></row><row><cell>MultSeq + SPT</cell><cell></cell><cell>32.5</cell><cell>45.8</cell><cell>11.8</cell><cell>16.7</cell><cell>22.2</cell><cell>31.6</cell><cell>32.9</cell><cell>52.8</cell><cell>48.9</cell><cell>54.8</cell></row><row><cell>S&amp;F + SPT</cell><cell></cell><cell>42.6</cell><cell>38.1</cell><cell>14.6</cell><cell>13.0</cell><cell>29.1</cell><cell>26.2</cell><cell>46.6</cell><cell>43.2</cell><cell>53.6</cell><cell>49.8</cell></row><row><cell cols="2">MultSeq + S&amp;F + SPT</cell><cell>39.8</cell><cell>41.7</cell><cell>14.4</cell><cell>14.7</cell><cell>26.6</cell><cell>27.8</cell><cell>41.8</cell><cell>46.8</cell><cell>51.5</cell><cell>52.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Task: Become a bartender</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Generated Task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Graph</cell><cell>Choose a bar to work at</cell><cell>Apply for a job as a barback Apply for a bartender position</cell><cell>few months</cell><cell>Work as a barback for a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.wikihow.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The full dataset can be downloaded from https://storage. googleapis.com/gresearch/tasklama/tasklama.zip</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that LLMs can be used both to generate an output and also to score a provided output.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peikoff</surname></persName>
		</author>
		<title level="m">Getting Things Done: The Art of Stress-Free Productivity</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supporting complex search tasks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on conference on information and knowledge management</title>
		<meeting>the 23rd ACM international conference on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19472</idno>
		<title level="m">PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Break it down: A comparison of macro-and microtasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4061" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade: Crowdsourcing taxonomy creation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1999">2013. 1999-2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9118" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Imbrasaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07934</idno>
		<title level="m">A Dataset for Natural Language Reasoning with Contradictory Information</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taskgenies: Automatically providing action plans helps people complete tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kokkalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>K?hn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013">2023. 2013</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
	<note>Lambada: Backward chaining for automated reasoning in natural language</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00683</idno>
		<title level="m">Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Imbrasaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14128</idno>
		<title level="m">Dr. ICL: Demonstration-Retrieved In-context Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language models of code are few-shot commonsense learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07128</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting hierarchies of search tasks &amp; subtasks via a bayesian nonparametric approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human problem solving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">104</biblScope>
			<pubPlace>Prentice-hall Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00114</idno>
		<title level="m">Show your work: Scratchpads for intermediate computation with language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08251</idno>
		<title level="m">proscript: Partially ordered scripts generation via pre-trained language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07154</idno>
		<title level="m">Can Language Models be Biomedical Knowledge Bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supporting collaborative writing with microtasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Veh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI conference on human factors in computing systems</title>
		<meeting>the 2016 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2657" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chain-ofthought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07178</idno>
		<title level="m">Symbolic knowledge distillation: from general language models to commonsense models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monajatipoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12247</idno>
		<title level="m">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05345</idno>
		<title level="m">Do language embeddings capture scales? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to decompose and organize complex tasks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2726" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Task-based recommendation on a web-scale</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tat-Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="827" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">2022a. Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sch?rli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10625</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07264</idno>
		<title level="m">Show me more details: Discovering hierarchies of procedures from semi-structured web data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating commonsense in pre-trained language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9733" to="9740" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
