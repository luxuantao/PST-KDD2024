<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpamHunting: An instance-based reasoning system for spam labelling and filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-01-03">3 January 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">F</forename><surname>Fdez-Riverola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Informática</orgName>
								<orgName type="department" key="dep2">Escuela Superior de Ingeniería Informática</orgName>
								<orgName type="institution">University of Vigo</orgName>
								<address>
									<addrLine>Edificio Politécnico, Campus Universitario As Lagoas s/n</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Iglesias</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Informática</orgName>
								<orgName type="department" key="dep2">Escuela Superior de Ingeniería Informática</orgName>
								<orgName type="institution">University of Vigo</orgName>
								<address>
									<addrLine>Edificio Politécnico, Campus Universitario As Lagoas s/n</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Díaz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. Informática</orgName>
								<orgName type="institution" key="instit1">University of Valladolid</orgName>
								<orgName type="institution" key="instit2">Escuela Universitaria de Informática</orgName>
								<address>
									<addrLine>Plaza Santa Eulalia, 9-11</addrLine>
									<postCode>40005</postCode>
									<settlement>Segovia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Méndez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Informática</orgName>
								<orgName type="department" key="dep2">Escuela Superior de Ingeniería Informática</orgName>
								<orgName type="institution">University of Vigo</orgName>
								<address>
									<addrLine>Edificio Politécnico, Campus Universitario As Lagoas s/n</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
							<email>corchado@usal.es</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. Informática y Automática</orgName>
								<orgName type="institution" key="instit1">University of Salamanca</orgName>
								<orgName type="institution" key="instit2">Plaza de la Merced s/n</orgName>
								<address>
									<postCode>37008</postCode>
									<settlement>Salamanca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SpamHunting: An instance-based reasoning system for spam labelling and filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-01-03">3 January 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">41DE7C2959ADFCC8BE82C68ACE11C539</idno>
					<idno type="DOI">10.1016/j.dss.2006.11.012</idno>
					<note type="submission">Received 13 July 2005; received in revised form 13 November 2006; accepted 27 November 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>IBR system</term>
					<term>Automatic reasoning</term>
					<term>Anti-spam filtering</term>
					<term>Model comparison</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we show an instance-based reasoning e-mail filtering model that outperforms classical machine learning techniques and other successful lazy learners approaches in the domain of anti-spam filtering. The architecture of the learningbased anti-spam filter is based on a tuneable enhanced instance retrieval network able to accurately generalize e-mail representations. The reuse of similar messages is carried out by a simple unanimous voting mechanism to determine whether the target case is spam or not. Previous to the final response of the system, the revision stage is only performed when the assigned class is spam whereby the system employs general knowledge in the form of meta-rules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and motivation</head><p>Though many of us may think of spam (UCE, Unsolicited Commercial E-mail) as a new problem, it goes back at least to 1975, as observed by Jon Postel <ref type="bibr" target="#b27">[28]</ref>. Initially, spam mostly referred to Usenet newsgroup posts that got out of hand, whereby someone would mail a message to many newsgroupsa message that was unrelated to most of the newsgroups to which it was mailed. Then, social and administrative action was sufficient: the guilty party was punished privately or publicly and repeat offenders would be added to kill lists. As such, early spam filtering simply identified bad senders.</p><p>The huge expansion of Internet usage in recent years has increased marketing opportunities. As a result the problem of spam has grown astronomically, and the earlier techniques for keeping it under control no longer work. Unsolicited commercial communications now represent more than 50% of the e-mail traffic in the European Union and around the world.</p><p>Spam is beginning to undermine the integrity of e-mail and even to discourage its use. The great majority of Internet users' mailboxes are swamped by unwanted messages over which they have no control. In large numbers, Internet users have reported that they trust e-mail less, and 29% of users even say they do not use e-mail as much as they used to because of spam <ref type="bibr" target="#b13">[14]</ref>. Users worry that the growing volume of spam is getting in the way of their ability to safely send and receive e-mail.</p><p>In order to reduce the inconveniences continually imposed by spam on individuals, users, subscribers, consumers, companies, direct marketers, internet service providers, traders, employers, organizations, public bodies, and, in the end, the Internet itself, advances are taking place on two different fronts: (i) legal measures and (ii) anti-spam filter software. Software developments are typically based on placing filters in the mail agents, servers and clients, able to detect and to stop illegitimate mail. In this sense, two approaches exist when developing technical solutions that facilitate the control of spam: (i) collaborative applications <ref type="bibr" target="#b18">[19]</ref> and (ii) content-based techniques.</p><p>The collaborative approach depends on the cooperation of user's groups who share information about spam. When a new spam message appears, the final user of the e-mail shares a signature (using hash codes) for the message with the rest of the group. If other users also receive the same message, they can identify it as spam based on the shared signature. Although it is a good idea, this type of technique presents two main problems. Firstly, spammers insert random characters into messages to foil signatures. Secondly, a process for sharing these signatures needs to be developed. The alternative is the content-based approach, where the classification is based on the content analysis of the features extracted from the e-mail body and header.</p><p>The success of machine learning (ML) techniques in text categorization <ref type="bibr" target="#b32">[33]</ref> has led researchers to explore learning algorithms in anti-spam filtering. However, anti-spam filtering has a further complication because it is a cost-sensitive problem: the cost of accidentally blocking a legitimate message can be higher than letting a spam message pass the filter, and this difference must be taken into account during both training and evaluation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Another important aspect of anti-spam filtering domain is the necessity to manage concept drift problem. While much of the research on machine learning has focused on static problems <ref type="bibr" target="#b34">[35]</ref>, a significant issue in many real-world domains is its changing environment <ref type="bibr" target="#b22">[23]</ref>. In those situations, the target concepts (spam or legitimate e-mails) may depend on some hidden context and usually this context is dynamic. Changes in the hidden context can induce changes in the target concept, which is generally known as concept drift <ref type="bibr" target="#b36">[37]</ref>. Concept drift in spam is particularly difficult as the spammers actively change the nature of their messages to elude spam filters.</p><p>In this paper we present our SpamHunting system, a lazy learning model based on an Instance-Based Reasoning (IBR) approach <ref type="bibr" target="#b35">[36]</ref> to accurately solve the problem of spam labelling and filtering. Machine learning techniques usually aim at compressing available sample data into more compact representations called models. These models can then be used for solving different explorative (data mining) or predictive inference tasks. As opposed to traditional modelbased machine learning, lazy learning <ref type="bibr" target="#b1">[2]</ref> refers to methods that defer all essential computation until the specific prediction task is completely determined. This type of predictive inference is also known as casebased or instance-based reasoning, reflecting the fact that the methods use a set of sample instances (the case base) in a central role in the predictive inference process. The main contributions of this study to the research in the area are the definition of a new memory structure called EIRN (Enhanced Instance Retrieval Network) capable of storing a flexible e-mail representation and the implementation of a revision stage in the IBR process. The proposed system incorporates the EIRN model which facilitates the indexation of instances and the selection of those that are most similar to the incoming e-mails. The reuse of similar messages is carried out by a unanimous voting algorithm which generates an initial solution by creating a model with the retrieved instances. The revision stage in the proposed model is only carried out in the case of spam messages. For this purpose, the system employs general knowledge in the form of meta-rules that are extracted from the e-mail headers. Finally, the learning stage is carried out updating the knowledge structure of the whole system.</p><p>Following the advantages of instance-based reasoning as lazy learning technique mentioned above, we have compared our model with several artificial intelligence (AI) techniques currently used for the construction of anti-spam filters. Starting from the experiments carried out using publically available data, we show extremely promising results, with our system obtaining better indicators compared to the rest of the models analyzed.</p><p>The rest of the paper is organized as follows: Section 2 introduces the spam problem domain taking into account previous related research work; Section 3 discusses in detail our SpamHunting system, explaining the basis that conform each phase of the model; Section 4 introduces our experimental results, investigating separately the effect of several evaluation metrics with different models and corpus; Finally, Section 5 concludes and suggests new directions for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work on spam filtering</head><p>In recent years, a wide variety of learning approaches have been applied to the anti-spam problem. Those techniques mainly include ML algorithms as well as case and memory-based systems. Successful applications have been developed: encoding of probabilistic classifiers like Naïve Bayes <ref type="bibr" target="#b29">[30]</ref> or its improved version called Flexible Bayes <ref type="bibr" target="#b21">[22]</ref>; boosting of C4.5 <ref type="bibr" target="#b31">[32]</ref>; C4.5 with PART <ref type="bibr" target="#b19">[20]</ref>; generation of classification rules using different algorithms like Ripper <ref type="bibr" target="#b5">[6]</ref> or Rocchio <ref type="bibr" target="#b20">[21]</ref>; or the well-known Support Vector Machine (SVM) binary classifier <ref type="bibr" target="#b7">[8]</ref>.</p><p>Most of these algorithms try to minimize the number of errors in the classifier. However, these algorithms assign the same importance to all errors, which does not apply to the environment of spam, where the errors made by the classifier have varying significance for the end users. As such, assuming that the positive class to detect is spam, a False Positive error (FP, classifying a legitimate e-mail as spam) incurs a greater cost than a False Negative error (FN, classifying a spam message as legitimate). In order to create cost-sensitive learning methods, successful adaptations of existing algorithms have been made <ref type="bibr" target="#b33">[34]</ref>. Another alternative is the generation of a cost-sensitive classifier starting from a learning algorithm plus a training collection and a cost distribution <ref type="bibr" target="#b17">[18]</ref>.</p><p>The Naïve Bayes learner is the most widely used algorithm. Although the independence assumption is over-simplistic, studies in anti-spam filtering have found Naïve Bayes to be effective <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. At run time, Naïve Bayes requires O( p) computations for each test message.</p><p>Flexible Bayes provides an alternative approach to Naïve Bayes for continuous attributes. Instead of using a single normal distribution, the a posteriori probability of each attribute is calculated as the average of normal distributions. Despite its weaker model bias, the computational complexity of Flexible Bayes is the same as Naïve Bayes. However, there is an increase in the number of computations at run time.</p><p>SVMs constitute a family of algorithms for classification and regression developed by V. Vapnik <ref type="bibr" target="#b34">[35]</ref> and is one of the most widely used ML techniques. SVMs can use all the terms of the messages. It is not necessary to carry out a previous selection, as in other learning algorithms, because their learning capacity does not degrade even if many characteristics exist <ref type="bibr" target="#b12">[13]</ref>.</p><p>The boosting algorithms are techniques based on the use of weak learners, that is to say, algorithms that learn with a next error rate of up to 50%. The classification trees C4.5 <ref type="bibr" target="#b28">[29]</ref> are good candidates to be weak learners, although their error rates are much better that 50%. Different boosting algorithms have been developed for classification tasks, both binary and multi-class. Among them we could highlight Adaboost <ref type="bibr" target="#b31">[32]</ref>, which boosts using C4.5 trees <ref type="bibr" target="#b12">[13]</ref> or LogitBoost <ref type="bibr" target="#b16">[17]</ref>.</p><p>Ripper <ref type="bibr" target="#b5">[6]</ref> induces classification rules from a set of examples. Unlike the algorithms above, it does not need a feature vector. It forms if-then rules which are disjunctions of conjunctions. Rocchio <ref type="bibr" target="#b20">[21]</ref> uses normalized TF-IDF (Term Frequency-Inverse Document Frequency) representation of the training vectors. The advantage of this algorithm is its fast training and testing stages.</p><p>As part of the study of anti-spam filtering techniques, there has also been intensive research into the use of memory-based classifiers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>. In general, using memory-based anti-spam filters are obtaining better results than ML algorithm-based approaches, mainly when the cost of the FP errors is high <ref type="bibr" target="#b30">[31]</ref>. Moreover, case-based approaches are suitable for spam classification because they offer a natural framework to unify learning and collaboration approaches and to continually learn in the presence of new knowledge <ref type="bibr" target="#b8">[9]</ref>.</p><p>Memory-based (or instance-based) methods store all training instances in a memory structure and use them directly for classification <ref type="bibr" target="#b3">[4]</ref>. They do not construct a unique model for each category, but simply store the training examples. In its simplest form, memory-based learning treats instances as points in a multi-dimensional space defined by the features that have been selected. Each training instance is represented as a point in that space.</p><p>Test instances are classified by estimating their similarity (distance) to the stored examples. Classification is usually performed through a variant of the basic k-nearest-neighbour (k-nn) algorithm. k-nn assigns to each test instance, the majority class of its k closest training instances.</p><p>TiMBL <ref type="bibr" target="#b9">[10]</ref> provides an implementation of a basic memory-based classification algorithm with a variant of k-nn. One important difference from k-nn basic is in the definition of the k-neighbourhood. TiMBL considers that all training instances at the k closest distances form the unseen instance.</p><p>In general, memory-based approaches appear to be more appropriate than ML techniques for anti-spam filtering, especially when the assigned cost of a FP error is high. However, these systems need to configure the filter appropriately <ref type="bibr" target="#b9">[10]</ref>.</p><p>Case-based approaches outperform previous techniques to anti-spam filtering <ref type="bibr" target="#b10">[11]</ref>. This is because spam is a disjoint concept: spam about porn has little in common with spam offering rolexes. Case-based classification works well for disjoint concepts whereas ML techniques try to learn a unified concept description. Another advantage of this approach is the ease with which it can be updated to catch the concept drift in spam.</p><p>[11] has presented a case-based system for anti-spam filtering called ECUE (E-mail Classification Using Examples) that can learn dynamically. In ECUE each e-mail is a case represented as a vector of binary features. The system use a similarity retrieval algorithm based on the utilization of Case Retrieval Nets (CRN) <ref type="bibr" target="#b24">[25]</ref>. CRN networks are equivalent to the k-nn algorithm but are computationally more efficient in domains where there is feature-value redundancy and missing features in cases, such as in spam. The ECUE is a system evolved from a previously successful system <ref type="bibr" target="#b8">[9]</ref> designed by the same authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IBR SpamHunting proposed architecture</head><p>The idea behind case-based reasoning (CBR) is that people rely on concrete previous experiences when solving new problems <ref type="bibr" target="#b0">[1]</ref>. This fact can be tested on any day to day problem by simple observation or even by psychological experimentation. A CBR system solves new problems by adapting solutions that were used to solve old problems. The case base holds a number of problems with their corresponding solutions. Once a new problem arises, the solution to it is obtained by retrieving similar cases from the case base and studying the similarity between them. A CBR system is a dynamic system in which new problems are added to the case base, redundant ones are eliminated and others are created by combining existing ones. A typical CBR system is composed of four sequential steps which are recalled every time a problem needs to be solved: retrieve the most relevant case(s), reuse the case(s) to attempt to solve the problem, revise the proposed solution if necessary, and retain the new solution as a part of a new case. Each of the steps of the CBR life cycle requires a model or method in order to perform its mission.</p><p>CBR is an incremental learning approach because every time a problem is solved a new experience can be retained and made immediately available for future retrievals. The nature of the problem and the expertise of the CBR designers determine how the CBR should be built. According to Aamodt and Plaza <ref type="bibr" target="#b0">[1]</ref> there are five different types of CBR systems, and although they share similar features, each of them is more appropriate for a particular type of problem: exemplar-based reasoning, instance-based reasoning, memory-based reasoning, analogy-based reasoning and typical case-based reasoning. Instance-based reasoning can be considered to be a type of exemplar-based reasoning in highly syntaxdependent problem areas <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. This type of CBR system focuses on problems in which there are a large number of instances which are needed to represent the whole range of the domain and where there is a lack of general background knowledge. The case representation can be made with feature vectors and the phases of the CBR cycle are normally automated as much as possible, eliminating human intervention.</p><p>An Instance-Based Reasoning Architecture has been developed for Spam classification. In order to correctly classify an incoming e-mail, a message descriptor should be generated. This message descriptor consists of a sequence of N features that better summarize the information contained in the e-mail. For this purpose, we use data from two main sources: (i) information obtained from the header of the e-mail (see Table <ref type="table" target="#tab_0">1</ref>) and (ii) those terms that are more representative of the subject, body and attachments of the message (see Table <ref type="table" target="#tab_1">2</ref>).</p><p>The selected features shown in Table <ref type="table" target="#tab_0">1</ref> are extracted from the e-mail header and stored as is. Those features represent global knowledge about a message. Spam-Hunting uses those features in the revision stage, where meta-rules are generated in order to ascertain the initial solution proposed by the system.</p><p>In order to obtain an accurate representation of each one of the e-mails, we calculate the frequency of each feature (term) within the actual message. This information allows us to obtain a more precise knowledge about the contents of the e-mail and gain a deeper insight into its structure.</p><p>Based on the information compiled in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> and taking into account the class of the message (spam or legitimate), we construct an instance representation for each message existing in the original corpus. Fig. <ref type="figure">1</ref> shows how we represent each one of the e-mails in the SpamHunting system.</p><p>Analyzing Fig. <ref type="figure">1</ref>, we can see that the size of the Frequency-Term Descriptor array is variable and depends on each e-mail. The value of the parameter n is stored in the Total Terms field of the instance-message descriptor (longest dotted arrow in Fig. <ref type="figure">1</ref>). Moreover, from the Frequency-Term Descriptor array we are interested in selecting the most relevant terms of each e-mail for message indexing and retrieval. As in the case of the n parameter, the number of relevant terms is different from e-mail to e-mail and it is stored in the Relevant Terms field.</p><p>Fig. <ref type="figure">2</ref> illustrates the life-cycle of the IBR SpamHunting system as well as its integration within a typical user environment. In the upper part of Fig. <ref type="figure">2</ref>, the mail user agent (MUA) and the mail transfer agent (MTA) are in charge of dispatching the requests generated by the user. Between these two applications, SpamHunting captures all the incoming messages (using POP3 protocol) in order to identify, tag and filter spam.</p><p>Whenever SpamHunting receives a new e-mail, the system evolves through the four steps depicted in the lower part of Fig. <ref type="figure">2</ref> as shadowed rectangles. Initially the system identifies those e-mails that best represent the new incoming message (left upper quadrant in Fig. <ref type="figure">2</ref>), only taking into account the set of messages with the highest number of terms in common. Each one of the previous selected messages contributes with one vote to the final class (left bottom quadrant in Fig. <ref type="figure">2</ref>). The revision stage is only carried out when the system proposes an e-mail as spam. For this purpose, SpamHunting uses previous encoded header metarules (right bottom quadrant in Fig. <ref type="figure">2</ref>). Every time the user checks his mailbox and provides feedback to the system about a previous e-mail classification, Spam-Hunting stores a new instance (or modifies an existing one) in the e-mail base for future use (right upper quadrant in Fig. <ref type="figure">2</ref>).</p><p>The retrieval stage is carried out using our Enhanced Instance Retrieval Network model. The EIRN network facilitates the indexation of instances and the selection of those that are most similar to the instance-message. The reuse of similar e-mails is carried out by means of the utilization of a unanimous voting mechanism (as proposed in the work of <ref type="bibr" target="#b11">[12]</ref>), which generates an initial solution by creating a model with the retrieved instances. In the revision stage the system employs general knowledge in the form of meta-rules that are extracted from the e-mail headers. Finally, the retain (learning) stage is carried out whenever the system classifies an incoming e-mail, updating the knowledge structure of the whole system. The proposed system also takes into account the feedback of the user when it receives an incorrectly classified e-mail (dotted arrow from the user computer to the e-mail base in Fig. <ref type="figure">2</ref>). The rest of this section is structured as follows: Subsection 3.1 details with the main aspects of defining a measure to evaluate the relevance of a term within a message. Subsection 3.2 introduces our model representation for indexing and retrieval of similar e-mails. Finally, Subsection 3.3 comments on the process of classifying new incoming e-mails by reusing similar instances and details the revise and retention stages of the proposed IBR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Selecting relevant terms for e-mail representation</head><p>Starting from the pre-processed list of words contained in an e-mail, we are interested in selecting the most relevant terms in this e-mail. Without any other information, the only way of doing this is to base it on the frequency of each word in the message. But, if we have available a set of e-mails (a corpus), we can use information about the underlying distribution of the corpus in relation to the target concept (spam or legitimate) to modulate the relevance of each word inside a specific message.</p><p>Therefore, we are interested in defining a criterion about the relevance of each term, T i , which appears in a specific e-mail, e, of a corpus K. In order to define this measure the following reasoning is carried out. First, the probability that the e-mail e is a spam message can be expressed as:</p><formula xml:id="formula_0">pðsjeÞ ¼ X t i ae pðsjT i ; eÞpðT i jeÞ ð<label>1Þ</label></formula><p>The expression p(T i | e) is known, given the e-mail e. Although the expression p(s | T i , e) is unknown, it can be estimated by the probability p(s | T i , K ). That is, it can be approximated by the probability that an e-mail in the corpus K is a spam message if the term T i is present in that e-mail. Therefore, Expression (1) can be approximated by: pðsjeÞc</p><formula xml:id="formula_1">X T i ae pðsjT i ; KÞpðT i jeÞ ð<label>2Þ</label></formula><p>After this, and applying the Bayes' rule, the probability p(s | e) can be expressed as: </p><formula xml:id="formula_2">pðsjeÞc</formula><p>Moreover, we are interested in truly discriminating between spam terms and legitimate terms (one term may have approximately equal probability to appear in a spam e-mail as it does in a legitimate e-mail). Therefore, the relevance measure of a term would be able to stress one term which is probably only in spam messages or only in legitimate messages, but is not equally probable in both kinds of e-mail, simultaneously. This fact can be modelled by means of the difference between the Expressions (3) and ( <ref type="formula" target="#formula_3">4</ref>), and each term of the sum can be interpreted as a measure of the contribution of each term in the final result, namely, a measure of the relevance of each term. Moreover, if we are not interested in the sign of the contribution (positive if the term helps to classify an e-mail as spam or negative if it helps to classify one as legitimate), the relevance of each term of the e-mail can be defined as follows:</p><p>rðT i ; eÞ ¼ jpðsjKÞpðT i js; KÞ-pðljKÞpðT i jKjÞj pðT i jKÞ pðT i jeÞ:</p><p>The relevance term r(T i , e) tries to conjugate the local and global relevance of the term T i . The first factor in r (T i , e) depends on the whole corpus K and expresses the utility of the term T i in order to discriminate among spam or legitimate e-mails and therefore it evaluates the global relevance of T i . The second factor in r(T i , e) only depends on the specific e-mail which is being processed and therefore it can be viewed as a measure of the local relevance of T i . As consequence of this definition, firstly, the relevance of a term T i which appears in two different e-mails only depends on the local relevance (since the first factor of Expression (5) will be the same). Secondly, the relative relevance of two terms T i and T j which appear in a specific e-mail e, not only depends on the local information, but also depends on the global information given by the first factor of Expression ( <ref type="formula" target="#formula_4">5</ref>), which will be probably different for both terms. The last fact is relevant because we are interested in ordering (by relative relevance) different terms in a specific e-mail in order to select the most relevant ones. Finally, this formulation can be used to select the most relevant terms in two ways: (i) a fixed number of terms ordered with respect to r(T i | e) or (ii) a variable number of terms depending on a fixed percentage of the whole sum of individual relevancies (if the terms of an e-mail e are ordered descending by |r(T i , e)| and R is the sum of |r(T i , e)| over all the terms T i belonging to e, then fixed a percentage α, the first k α terms, whose partial sum of relevancies exceed the quantity of αR, will be selected as the most relevant terms). In the successive experimentation only the first way will be used for the evaluation of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Indexing with the EIRN model</head><p>Based on the previous formulation for selecting relevant terms of each message in a corpus K, we present here our EIRN model for efficient as well as flexible instance indexing and retrieval. Our EIRN memory structure is inspired by the basic architecture and concepts of CRN networks <ref type="bibr" target="#b24">[25]</ref>, but without taking into account the similarly weighted arcs that connect information entity nodes among them.</p><p>Based on the CRNs case indexing properties, our model defines two measurements: (i) Term Confidence and (ii) E-mail Confidence used for maintaining as much information as possible concerning existing data (terms and e-mails). Fig. <ref type="figure" target="#fig_0">3</ref> depicts an example of our EIRN model to e-mail retrieval. The EIRN network used in this work is characterized by a two-dimensional space, where the terms (cells) are connected and organized according to the probability of representing spam and legitimate messages. Each cell in the network is associated with a term confidence, tc, which represents a measure of how much we can trust it to classify an incoming e-mail. The value of tc for a given term T i is given by Eq. ( <ref type="formula">6</ref>). tc i ¼ pðT i js; KÞ-pðT i jl; KÞ ð 6Þ</p><p>where p(T i | s, K ) and p(T i | l, K ) stand for the probability of the term T i belonging to spam and legitimate e-mails, respectively. The basic learning process in the EIRN network consists in topology modification and term confidence adaptations. Based on a corpus K, of m k training e-mails or training instances, learning in an EIRN network is carried out presenting all training e-mails, K, in a sequential fashion. For each training instance presentation, the network performs a so-called learning cycle, which may result in term confidence adaptation and topology modification. In the first step of each learning cycle, the relevant terms, rt, of the actual input instance e j , are linked with the terms present in the network, adding new terms to the model if necessary. Each new connection is weighted up with a relevant value, rv i , which represents the importance of this term to the actual instance. The value of rv i depends on the relevant terms (rt j ) of the actual input instance e j and the actual term T i . rv i is calculated using Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_5">rv i ¼ W k 2 i-1<label>ð7Þ</label></formula><p>where T i is the actual term and w k is a constant given by Eq. ( <ref type="formula" target="#formula_6">8</ref>).</p><formula xml:id="formula_6">W k ¼ 2 rt j -1 2 rt j -1 :<label>ð8Þ</label></formula><p>The second step consists of the adaptation of the term confidence belonging to those terms affected in the previous step and the calculation of the actual e-mail confidence, ec. The parameter ec represents a measure of the message coherence represented by its relevant terms and aids in the identification of rare e-mails. The value of ec for a given pair 〈e j , c j 〉 is calculated using Eq. ( <ref type="formula" target="#formula_7">9</ref>).</p><formula xml:id="formula_7">ec j ¼ P rt j i¼1 pðT i jc j ; KÞrv i rt j<label>ð9Þ</label></formula><p>where c j represents the actual class of the e-mail e j , rt j stands for the number of relevant terms for e j , p(T i | c j , K ) represents the probability of the term T i belonging to a message with the same class as e j (c j ) and rv i is calculated using Eq. ( <ref type="formula" target="#formula_5">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">E-mail classification by reusing similar instances. retrieval, revising and learning</head><p>Every time a new e-mail arrives at the user mailbox, the EIRN network obtains a set M′ composed of the instance-messages most similar to the target e-mail, e′. In this sense, we can conceive the EIRN memory structure as a dynamic k-nn mechanism able to retrieve different numbers of neighbours depending on the selected terms belonging the target e-mail, e′. This is done by selecting the relevant terms of the incoming message as described in Subsection 3.1 and projecting them into the network term space (see Fig. <ref type="figure" target="#fig_0">3</ref>). To perform this selection stage, the system accomplishes two sequential steps: (i) calculating the distance between e′ and the set of messages that shares the greatest number of common terms (cf′) and (ii) selecting those emails with a similarity value greater than the mean average value.</p><p>In order to calculate the similarity between two emails given a set of shared relevant terms, we use a weighted distance metric that takes into account the relevance of each common term. The underlying idea is to weigh those terms that are more relevant to the target e-mail e′, using the position that occupies each of them in the arrows coming from the target e-mail to the memory structure in Fig. <ref type="figure" target="#fig_0">3</ref>. The value of the distance between the target e-mail e′ and a given e-mail e j is calculated using Eq. <ref type="bibr" target="#b9">(10)</ref>.</p><formula xml:id="formula_8">Dðe V; e j Þ ¼ X cf V i¼1 dðe V i ; e ji Þrv i<label>ð10Þ</label></formula><p>where cf ′ is the number of common terms between M′ and e′, rv i represents the importance of each term to the target e-mail e′ and d(e i ′, e ji )measures the distance between the position assigned to the actual common term i in the two e-mails, calculated as the difference between the situation of this term in the arrows coming from the target e-mail e′ and a given e-mail e j to the memory structure in Fig. <ref type="figure" target="#fig_0">3</ref>.</p><p>Given the distance between two e-mails, the similarity is obtained by means of Eq. ( <ref type="formula" target="#formula_9">11</ref>), where the e-mail coherence is used to consider those messages which are most consistent with the instance base.</p><formula xml:id="formula_9">Sðe V; e j Þ ¼ 1 Dðe V; e j Þ ec j :<label>ð11Þ</label></formula><p>Every time IBR SpamHunting executes the aforementioned instance retrieval stage by selecting those emails with higher values for the similarity with the target e-mail e′, the system assigns a class label to the incoming e-mail e′ based on a unanimous voting algorithm. Each message in M′ returns one vote and by means of recounting the existing votes, an initial classification is provided by the system.</p><p>Previous to the final response of the system, a revision stage is carried out when the assigned class is spam. This re-evaluation is carried out with the goal of guaranteeing the accuracy of the solution proposed <ref type="bibr" target="#b25">[26]</ref>.</p><p>In these situations, SpamHunting uses the knowledge extracted from the message header (see Table <ref type="table" target="#tab_0">1</ref>) in order to generate a final answer. Concretely, the system searches the instance base looking for spam e-mails written in the same language as the new one. Starting from the previous selected messages, the system compares the from and the return path fields with the incoming e-mail, selecting those messages with almost one feature in common. From the latest collection of emails, SpamHunting verifies the existence of messages with the same number and content type of attachments.</p><p>If some e-mail exists that fulfills all these requisites, SpamHunting classifies the incoming message as spam, otherwise it labels the e-mail as legitimate. If the incoming e-mail was assigned as legitimate at the adaptation stage, the revision phase does nothing.</p><p>Before presenting the proposed classification to the user, the SpamHunting system calculates the consistency of the solution (SC). For this purpose we take into account the term coherence (tc) of each term belonging to the target e-mail e′. The sign of Expression (12) tells the user whether the assigned class is consistent with the selected terms of the target e-mail or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCðe VÞ</head><formula xml:id="formula_10">¼ P rt V i¼1 tc i rv i rt V<label>ð12Þ</label></formula><p>where rt′ stands for the number of relevant terms selected for e′, tc i represents the term coherence value of each term and rv i is calculated using Eq. ( <ref type="formula" target="#formula_5">7</ref>). Every time SpamHunting revises the proposed solution for an incoming e-mail, a new instance-message containing the instance-descriptor and the solution (assigned class) is constructed and stored in the instance base for future reuse. Whenever this happens, the relevant terms of the new instance-message are selected and projected into the network term space. For those terms, the EIRN network updates their tc i values and recalculates the p(T i | l, K ) and p(T i | s, K ) parameters belonging to the affected nodes. Once the new message has been correctly saved and the EIRN network  architecture updated, the system computes the value of the e-mail confidence for the new solved instancemessage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System evaluation</head><p>This section introduces our evaluation of the SpamHunting system. We carry out an evaluation of the performance of the system from a classical static point of view, where new messages are simply classified and no update of the model occurs. The experiments carried out were offline evaluations using e-mails collected over an extended period of time.</p><p>For this purpose, we use six well-known metrics to evaluate the performance (efficacy) of our IBR SpamHunting system: percentage of correct classifications (%OK), percentage of false positives (%FP), percentage of false negatives (%FN), spam recall, spam precision and total cost ratio (TCR) at three different usage scenarios. Moreover, we introduce a measure of the computational complexity (efficiency) of each algorithm by measuring the time spent for each technique in completing the test bed experiments. Finally, we show how our model generalizes the information existing in the training sets of e-mails indicating the mean value of message and model terms for several configurations of the EIRN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>A key objective was to evaluate the performance of our instance-based reasoning system over other wellknown approaches in the domain of spam filtering. For this reason, we used 10-fold stratified cross-validation <ref type="bibr" target="#b23">[24]</ref> in all of our experiments, a technique that increases the confidence of experimental findings when using small datasets. In other words, SpamAssassin corpus was partitioned into 10 parts, with each part maintaining the same ratio of legitimate and spam messages as the entire corpus. Each experiment was repeated 10 times, each time reserving a different part as the testing corpus and using the remaining 9 parts as the training corpus. Previous performance scores were then averaged over the 10 iterations.</p><p>Table <ref type="table" target="#tab_3">3</ref> describes the SpamAssassin corpus (created by Justin Mason of Network Associates, and public available for download at http://www.spamassassin.org/ publiccorpus/) employed in our experiments. The selected messages were received from January 2002 up to and including December 2003. We append to the subject of each one of the e-mails, the body of the message as well as the attachments. In order to process the diverse formats of the attached files, we use different techniques in each case taking into account the "contenttype" header information. At this point, we only applied stop word removal. The e-mails were not altered to carry out stemming of words. In addition, we do not store each word with its part-of-speech tag and we disregard the discourse information of spoken material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Static evaluation</head><p>In order to exhaustively evaluate our IBR Spam-Hunting system and to verify previously published results <ref type="bibr" target="#b26">[27]</ref>, we compare our model against five wellknown widely referenced techniques in the domain literature. These techniques are Naïve Bayes classifier, Adaboost, SVM and two successful case-based reasoning systems: ECUE <ref type="bibr" target="#b11">[12]</ref> and a previously developed CBR system belonging to the same authors <ref type="bibr" target="#b8">[9]</ref>, which we call Cunn Odds Rate.</p><p>All these models except Cunn Odds Rate use IG <ref type="bibr" target="#b28">[29]</ref> to select the most predictive features as it has been shown to be an effective technique in aggressive feature removal in text classification <ref type="bibr" target="#b38">[39]</ref>. For our comparisons, we have selected the best performance model of each technique varying between 100 and 2000 features. For Cunn Odds Rate model, we have maintained the original technique of selecting 30 words representing spam e-mails plus 30 words representing legitimate messages. The algorithm employed for sorting the vocabulary is based on the odds-ratio described in <ref type="bibr" target="#b8">[9]</ref>. Fig. <ref type="figure" target="#fig_2">4</ref> shows the percentage of correct classifications (%OK), percentage of false positives (%FP) and Fig. <ref type="figure">6</ref>. TCR values for the analyzed models varying the λ parameter over the SpamAssassin corpus. percentage of false negatives (%FN) belonging to the six models analyzed. From Fig. <ref type="figure" target="#fig_2">4</ref> we can surmise that the model producing a higher percentage of correct answers is SVM working with 2000 terms (features). Nevertheless, the model that presents a better ratio between FP and FN errors is the IBR SpamHunting system, which outperforms the rest of the systems. Table <ref type="table" target="#tab_4">4</ref> clarifies this situation.</p><p>As we can see from Table <ref type="table" target="#tab_4">4</ref> when comparing Cunn Odds Rate against the IBR SpamHunting system, our system significantly reduces the mean value of FNs while maintaining the same statistical performance in FPs. The next model, SVM working with 2000 terms, reduces the mean value of FNs but significantly increases the number of FPs.</p><p>In order to obtain a deeper insight into the operation of the different models, we calculate the recall (Fig. <ref type="figure">5a</ref>) and precision (Fig. <ref type="figure">5b</ref>) scores for the six techniques analyzed. From Fig. <ref type="figure">5a</ref> (filter effectiveness) it can be seen that the worst model is Cunn Odds Rate, while the best result was obtained by the SVM algorithm. The rest of the models are within the same interval of correctly classified spam messages, following the best technique closely and with a high value of spam recall.</p><p>From Fig. <ref type="figure">5b</ref> (filter safety) it can be seen that the technique that best classifies spam messages is the IBR SpamHunting system that envelops the EIRN model. In this case, the model with worst precision is Naïve Bayes working with 1000 terms. As in the case of recall, the rest of the models closely follow the best technique.</p><p>In general, a high spam recall value indicates low FN error rate, and a high spam precision value implies low FP error rate. These two metrics are straightforward to understand, but do not reflect differential treatment of the two types of errors. The TCR score is introduced for this reason, where higher TCR values indicate better performance of the models <ref type="bibr" target="#b4">[5]</ref>. Now, let us assume that FP errors are λ times more costly than FN errors, where λ depends on the usage scenario. Three different usage scenarios are used in our experiments. In the first one, the filter flags messages when it suspects them to be spam, without removing them. In this case, λ = 1. The second scenario assumes that messages classified as spam are returned to the sender. In this scenario λ = 9 is considered. That is, mistakenly blocking a legitimate message was taken to be as bad as letting 9 spam messages pass the filter. In the third scenario messages classified as spam are deleted automatically without further processing. Now λ = 999 is used. Fig. <ref type="figure">6</ref> shows the results taking into account the TCR score and varying the λ parameter as commented above.</p><p>From Fig. <ref type="figure">6</ref> we can see that when FP errors are assigned the same importance as FN errors (a nonrealistic point of view for a final user) the best model is the SVM algorithm, followed by the IBR SpamHunting and the ECUE systems. As soon as one increases the importance of classifying legitimate mail correctly (considering a FP error to be more costly than a FN error) the situation changes drastically and the IBR SpamHunting System produces much better results. This circumstance is supported by the high precision score obtained by the IBR SpamHunting system shown in Fig. <ref type="figure">5b</ref> and the better ratio between FP and FN errors demonstrated in Table <ref type="table" target="#tab_4">4</ref> and Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>Moreover, another relevant factor in the design of online anti-spam filters is the speed of the algorithms in both training and run time stages. Regarding the computational complexity of the analyzed models, Table <ref type="table" target="#tab_5">5</ref> shows the time spent (measured in minutes) in the execution of the 10-fold cross-validation. All the models were created and executed in a 2.8 GHz Pentium IV processor with hyper-threading technology. The Spam-Hunting, ECUE and Cunn Odds Rate systems have been implemented using Java language (JDK 1.4.x) whereas Naïve Bayes, Adaboost and SVM algorithms have been tested with WEKA 3 <ref type="bibr" target="#b37">[38]</ref>.</p><p>From Table <ref type="table" target="#tab_5">5</ref> we can surmise that the model producing the fastest answer is the IBR SpamHunting system, and from a practical point of view, our filter overcomes the advantages typically granted to the Naïve Bayes classifier.</p><p>Once we have compared both efficiency and efficacy of our IBR SpamHunting system against other wellknown techniques for anti-spam filtering, it is interesting to show how our EIRN network stores the information that it captures from the training messages. Columns in Table <ref type="table" target="#tab_6">6</ref> represent the total of terms of frequency selected for each e-mail. Note that each column corresponds to a different EIRN configuration. The first row of Table <ref type="table" target="#tab_6">6</ref> shows for each EIRN configuration, the mean value for the number of selected terms for a randomly chosen e-mail in the model. The second row of Table <ref type="table" target="#tab_6">6</ref> indicates the number of different terms indexed by the corresponding EIRN network. For the experiments carried out in this paper, the best performance of the EIRN network was achieved storing only the 30% of the total terms of frequency of each e-mail. This led to indexing 13,021 terms of the whole corpus and to representing each e-mail using an average of 16 terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and further work</head><p>In this paper we have presented a successful implementation of an IBR system for anti-spam filtering. The system employs an Enhanced Instance Retrieval Network for e-mail indexing and retrieval. The reuse of similar messages is carried out by means of a simple unanimous voting mechanism to determine whether the target case is spam or not. The revision stage is only carried out in the case of spam messages where the system employs general knowledge in the form of meta-rules that were extracted from the e-mail headers to assign a final class.</p><p>Our thorough investigation examined various performance aspects of several well-known machine learning techniques and documented successful anti-spam filters. For this purpose we studied and used a variety of measurements in our experiments to report performance. In this sense, the preliminary results obtained from a static evaluation of the analyzed models showed how our IBR SpamHunting system obtains a better ratio between FP and FN errors as well as in the precision score.</p><p>Another issue in anti-spam filtering is the cost of different misclassification errors in normal operation. In this sense we tested the performance of our model against other well-known classifiers in three different cost scenarios. With the exception of assigning the same importance to FP errors than FN errors (first unrealistic scenario), the SpamHunting system obtained significantly better results. Our experiments also showed that the real-life computational cost of running a SpamHunting system is always lower than other approaches.</p><p>As the core of the SpamHunting system is the EIRN network, we investigated how our memory structure stores the information that it captures from the training messages. The experiments carried out in this sense showed how the SpamHunting system attained the better performance storing only the terms belonging to each e-mail and representing 30% of the message frequency.</p><p>The results obtained from the experiments carried out are very promising and they back up the idea that instance-based reasoning systems can offer a number of advantages in the spam filtering domain. Spam is a disjoint concept and IBR classification works well in this domain. In addition IBR systems can learn over time simply by updating their memory with new instances of spam or legitimate e-mail. Moreover, it provides seamless learning capabilities without the need for a separate learning process and facilitates extending the learning process over different levels of learning.</p><p>Since SpamHunting is a long-life IBR spam filtering software, a key challenge for us in order to improve our obtained successful results is the development of a policy for instance-base maintenance. Instance editing techniques involve reducing an instance-base or training set to a smaller number of instances while endeavouring to maintain or even improve the generalization accuracy of the system. In this sense, we are working on the definition of an evolving sliding window in order to better model continuous updating of the system. Further work in this area will also include the comparison of our SpamHunting system with the more common ensemble approach to handling concept drift.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>clarifies this situation showing those cells with closest values for p(T i | l, K ) and p(T i | s, K ) parameters located in nearby points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Enhanced Instance Retrieval Network for e-mail indexing.</figDesc><graphic coords="8,114.12,67.29,311.99,221.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Percentage of correct classifications, false positives and false negatives from validation over the SpamAssassin corpus.</figDesc><graphic coords="9,291.91,67.29,210.20,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,89.97,526.00,368.62,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,128.30,67.29,283.64,278.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,71.49,499.80,397.40,170.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,58.51,67.29,431.98,311.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Representation of header features stored in the case of SpamHunting</figDesc><table><row><cell>Variable</cell><cell>Type</cell><cell>Description</cell></row><row><cell>From</cell><cell>String</cell><cell>Source mailbox</cell></row><row><cell>Return path</cell><cell>String</cell><cell>Indicates the address that the message will be returned to if one chose to reply</cell></row><row><cell>Date</cell><cell>Date</cell><cell>Date in which the message was sent</cell></row><row><cell>Language</cell><cell>String</cell><cell>Particular tongue of the message</cell></row><row><cell>Attached files</cell><cell>Integer</cell><cell>Indicates the number of attached files</cell></row><row><cell>Content type</cell><cell>String</cell><cell>MIME type</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Information features belonging to the subject, body and attachments of an e-mail</figDesc><table><row><cell>Variable</cell><cell>Type</cell><cell>Description</cell></row><row><cell>Relevant terms</cell><cell>Integer</cell><cell>Number of selected features to cluster the message</cell></row><row><cell>Total terms</cell><cell>Integer</cell><cell>Number of features contained in the message</cell></row><row><cell>Frequency-term descriptor</cell><cell>Array of feature-frequency pairs</cell><cell>Storing for each feature a measure of their frequency in the message</cell></row></table><note><p>Fig. 1. Instance-message representation for each e-mail in SpamHunting.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, the probability that an e-mail is legitimate given the e-mail, p(l | e), can be determined in a similar way:</figDesc><table><row><cell>pðljeÞc</cell><cell>X t i ae</cell><cell cols="3">pðT i jl; KÞpðljKÞ pðT i jKÞ</cell><cell>pðT i jlÞ</cell></row><row><cell cols="3">¼ pðljKÞ</cell><cell>X T i ae</cell><cell>pðT i jl; KÞpðT i jeÞ pðT i jKÞ</cell><cell>:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>X t i ae</cell><cell>pðT i js; KÞpðsjKÞ pðT i jKÞ</cell><cell>pðT i jeÞ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>¼ pðsjKÞ</cell><cell>X</cell></row></table><note><p><p>T i ae pðT i js; KÞpðT i jeÞ pðT i jKÞ ð3Þ Fig. 2. SpamHunting architecture and model integration.</p>Secondly</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Description of the SpamAssassin corpora of e-mails</figDesc><table><row><cell>Year</cell><cell>Legitimate</cell><cell>Spam</cell><cell>L:S</cell><cell>Total</cell></row><row><cell></cell><cell>messages</cell><cell>messages</cell><cell>ratio</cell><cell>messages</cell></row><row><cell>2002</cell><cell>2801 (84.9%)</cell><cell>498 (15.1%)</cell><cell>5.62</cell><cell>3299</cell></row><row><cell>2003</cell><cell>4150 (68.8%)</cell><cell>1883 (31.2%)</cell><cell>2.20</cell><cell>6033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Mean value of false positives and false negatives with 10-fold cross-validation</figDesc><table><row><cell></cell><cell>Cunn Odds Rate [60]</cell><cell>SpamHunting</cell><cell>SVM [2000]</cell><cell>ECUE [700]</cell><cell>Adaboost [700]</cell><cell>Naïve Bayes [1000]</cell></row><row><cell>False positives</cell><cell>0.6</cell><cell>1.6</cell><cell>5.7</cell><cell>7.9</cell><cell>12.3</cell><cell>60.9</cell></row><row><cell>False negatives</cell><cell>172.3</cell><cell>32.9</cell><cell>6.3</cell><cell>27.9</cell><cell>35.6</cell><cell>29.5</cell></row></table><note><p>Fig. 5. Recall and precision values for the analyzed models.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Time spent (in minutes) by the models analyzed in feature selection and training</figDesc><table><row><cell></cell><cell>Naïve Bayes [1000]</cell><cell>Adaboost [700]</cell><cell>SVM [2000]</cell><cell>Cunn Odds Rate [60]</cell><cell>ECUE [700]</cell><cell>SpamHunting</cell></row><row><cell>Feature selection</cell><cell>4.86</cell><cell>4.30</cell><cell>6.71</cell><cell>2.64</cell><cell>3.56</cell><cell>-</cell></row><row><cell>Model training</cell><cell>4.77</cell><cell>243.48</cell><cell>51.62</cell><cell>0.22</cell><cell>2.49</cell><cell>2.71</cell></row><row><cell>Total</cell><cell>9.63</cell><cell>247.78</cell><cell>58.33</cell><cell>2.86</cell><cell>6.05</cell><cell>2.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Mean value of message and model terms for several configurations of the EIRN network</figDesc><table><row><cell></cell><cell>EIRN 5%</cell><cell>EIRN 10%</cell><cell>EIRN 15%</cell><cell>EIRN 20%</cell><cell>EIRN 25%</cell><cell>EIRN 30%</cell><cell>EIRN 35%</cell><cell>EIRN 40%</cell></row><row><cell>Message terms</cell><cell>1.59</cell><cell>3.18</cell><cell>5.93</cell><cell>8.25</cell><cell>11.82</cell><cell>16.04</cell><cell>21.00</cell><cell>26.63</cell></row><row><cell>Model terms</cell><cell>2217</cell><cell>3974</cell><cell>6060</cell><cell>8268</cell><cell>10,551</cell><cell>13,021</cell><cell>15,518</cell><cell>17,787</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>F. Fdez-Riverola et al. / Decision Support Systems 43 (2007) 722-736</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the Spanish Council for Science and Technology (MEC) in projects TIC2003-07369-C02-02. The authors want to thank the work carried out by the referees for their reviews and comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Case-based reasoning: foundational issues, methodological variations, and system approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lazy learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An evaluation of Naïve Bayesian anti-spam filtering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning</title>
		<meeting>of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to filter spam e-mail: a comparison of a Naïve Bayesian and a memory-based approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sakkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stamatopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Machine Learning and Textual Information Access, 4th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>of the Workshop on Machine Learning and Textual Information Access, 4th European Conference on Principles and Practice of Knowledge Discovery in Databases<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to filter unsolicited commercial e-mail</title>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michelakis</surname></persName>
		</author>
		<ptr target="http://www.iit.demokritos.gr/skel/i-config/publications/" />
	</analytic>
	<monogr>
		<title level="m">NCSR &quot;Demokritos</title>
		<imprint>
			<date type="published" when="2004-02">2004/2. 2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-sensitive learning methods for text categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IBR retrieval method based on topology preserving mappings</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Corchado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental and Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="160" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A case-based approach to spam filtering that can track concept drift</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haahr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICCBR&apos;03 Workshop on Long-Lived CBR Systems</title>
		<meeting>of the ICCBR&apos;03 Workshop on Long-Lived CBR Systems<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TiMBL: Tilburg Memory Based Learner, Version 2.0, Reference Guide, ILK, Computational Linguistics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Der Sloot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Bosch</surname></persName>
		</author>
		<ptr target="http://ilk.kub.nl/~ilk/papers/ilk9901.ps.gz" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Tilburg University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An assessment of casebased reasoning for spam filtering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Fifteenth Irish Conference on Artificial Intelligence and Cognitive Science</title>
		<meeting>of Fifteenth Irish Conference on Artificial Intelligence and Cognitive Science<address><addrLine>Co. Mayo, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A case-based technique for tracking concept drift in spam filtering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th SGAI Int. Conf. on Innovative Techniques and Applications of Artificial Intelligence</title>
		<meeting>of the 24th SGAI Int. Conf. on Innovative Techniques and Applications of Artificial Intelligence<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support vector machines for spam categorization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1048" to="1054" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Internet users and spam: what the attitudes and behavior of internet users can tell us about fighting spam</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Conference on Email and Anti-Spam</title>
		<meeting>of the First Conference on Email and Anti-Spam<address><addrLine>Mountain View, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CBR based system for forecasting red tides</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fdez-Riverola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FSfRT: forecasting system for red tides</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fdez-Riverola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="374" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Categorización de Texto Sensible al Coste Para el Filtrado de Contenidos Inapropiados en Internet</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Puertas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Carrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Buenaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="13" to="20" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Personalised collaborative spam filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haahr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Conference on Email and Anti-Spam</title>
		<meeting>of the First Conference on Email and Anti-Spam<address><addrLine>Mountain View, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining text and heuristics for cost-sensitive spam filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Sanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Computational Natural Language Learning Workshop</title>
		<meeting>of the 4th Computational Natural Language Learning Workshop<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Conference on Machine Learning</title>
		<meeting>of the 14th International Conference on Machine Learning<address><addrLine>Nashville, TN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating continuous distributions in Bayesian classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>of the 11th Conference on Uncertainty in Artificial Intelligence<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The impact of changing populations on classifier performance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 5th International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Diego, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Joint Conference on Artificial Intelligence</title>
		<meeting>of the 14th International Joint Conference on Artificial Intelligence<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diagnosis and decision support</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Auriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1400</biblScope>
			<biblScope unit="page" from="51" to="90" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explaining the pros and cons of conclusions in CBR</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th European Conference on Case-Based Reasoning</title>
		<meeting>of the 7th European Conference on Case-Based Reasoning<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analyzing the impact of corpus preprocessing on antispam filtering software</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Méndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fdez-Riverola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Computing Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Internet Engineering Task Force</title>
		<author>
			<persName><forename type="first">J</forename><surname>Postel</surname></persName>
		</author>
		<ptr target="http://www.faqs.org/rfcs/rfc706.html/" />
	</analytic>
	<monogr>
		<title level="j">RFC</title>
		<imprint>
			<biblScope unit="volume">706</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note>On the Junk Mail Problem</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5 Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<title level="m">A Bayesian approach to filtering junk e-mail, Learning for Text Categorization -Papers from the AAAI Workshop</title>
		<meeting><address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A memory-based approach to anti-spam filtering for mailing lists</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sakkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stamatopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="73" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BoosTexter: a boosting-based system for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inducing cost-sensitive trees via instance weighting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd European Symposium on Principles of Data Mining and Knowledge Discovery</title>
		<meeting>of the 2nd European Symposium on Principles of Data Mining and Knowledge Discovery<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics for Engineering and Information Science</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<pubPlace>New York, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Applying case-based reasoning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Techniques for Enterprise Systems</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning in the presence of concept drift and hidden contexts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="101" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fourteenth International Conference on Machine Learning</title>
		<meeting>of the Fourteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
