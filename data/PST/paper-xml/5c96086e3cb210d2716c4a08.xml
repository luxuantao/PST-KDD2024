<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Make the Most out of Last Level Cache in Intel Processors</title>
				<funder>
					<orgName type="full">Knut and Alice Wallenberg Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Wallenberg AI, Autonomous Systems and Software Program</orgName>
					<orgName type="abbreviated">WASP</orgName>
				</funder>
				<funder ref="#_Jcp7cNT">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Swedish Foundation for Strategic research</orgName>
					<orgName type="abbreviated">SSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alireza</forename><surname>Farshin</surname></persName>
							<email>farshin@kth.se</email>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Roozbeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
							<email>maguire@kth.se</email>
						</author>
						<author>
							<persName><forename type="first">Dejan</forename><surname>Kosti?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Make the Most out of Last Level Cache in Intel Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3302424.3303977</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Slice-aware Memory Management</term>
					<term>Last Level Cache</term>
					<term>Non-Uniform Cache Architecture</term>
					<term>CacheDirector</term>
					<term>DDIO</term>
					<term>DPDK</term>
					<term>Network Function Virtualization</term>
					<term>Cache Partitioning</term>
					<term>Cache Allocation Technology</term>
					<term>Key-Value Store</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In modern (Intel) processors, Last Level Cache (LLC) is divided into multiple slices and an undocumented hashing algorithm (aka Complex Addressing) maps different parts of memory address space among these slices to increase the effective memory bandwidth. After a careful study of Intel's Complex Addressing, we introduce a sliceaware memory management scheme, wherein frequently used data can be accessed faster via the LLC. Using our proposed scheme, we show that a key-value store can potentially improve its average performance ?12.2% and ?11.4% for 100% &amp; 95% GET workloads, respectively. Furthermore, we propose CacheDirector, a network I/O solution which extends Direct Data I/O (DDIO) and places the packet's header in the slice of the LLC that is closest to the relevant processing core. We implemented CacheDirector as an extension to DPDK and evaluated our proposed solution for latency-critical applications in Network Function Virtualization (NFV) systems. Evaluation results show that CacheDirector makes packet processing faster by reducing tail latencies (90-99 t h percentiles) by up to 119 ?s (?21.5%) for optimized NFV service chains that are running at 100 Gbps. Finally, we analyze the effectiveness of slice-aware memory management to realize cache isolation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the known problems in achieving high performance in computer systems has been the Memory Wall <ref type="bibr" target="#b44">[43]</ref>, as the gap * Both authors contributed equally to the paper.</p><p>? This author has made all open-source contributions.</p><p>between Central Processing Unit (CPU) and Direct Random Access Memory (DRAM) speeds has been increasing. One means to mitigate this problem is better utilization of cache memory (a faster, but smaller memory closer to the CPU) in order to reduce the number of DRAM accesses. This cache memory becomes even more valuable due to the explosion of data and the advent of hundred gigabit per second networks (100/200/400 Gbps) <ref type="bibr" target="#b8">[9]</ref>. Introducing faster links exposes processing elements to packets at a higher rate-for instance, a server receiving 64 B packets at a link rate of 100 Gbps has only 5.12 ns to process the packet before the next packet arrives. Unfortunately, accessing DRAM takes ?60 ns and the performance of the processors is no longer doubling at the earlier rate, making it harder to keep up with the growth in link speeds <ref type="bibr">[4,</ref><ref type="bibr" target="#b59">58]</ref>. In order to achieve link speed processing, it is essential to exploit every opportunity to optimize computer systems. In this regard, Intel introduced Intel Data Direct I/O Technology (DDIO) <ref type="bibr" target="#b54">[53]</ref>, by which Ethernet controllers and adapters can send/receive I/O data directly to/from Last Level Cache (LLC) in Xeon processors rather than via DRAM. Hence, it is important to shift our focus toward better management of LLC in order to make the most out of it.</p><p>This paper presents the results of our study of the nonuniform cache architecture (NUCA) <ref type="bibr" target="#b36">[35]</ref> characteristics of LLC in Intel processors where the LLC is divided into multiple slices interconnected via a bi-directional ring bus <ref type="bibr" target="#b86">[84]</ref>, thus accessing some slices is more expensive in terms of CPU cycles than access to other slices. To exploit these differences in access times, we propose slice-aware memory management that unlocks a hidden potential of LLC to improve the performance of applications and bring greater predictability to systems. Based on our proposed scheme, we present CacheDirector, an extension to DDIO, which enables us to place packets' headers into the correct LLC slice for user-space packet processing, hence reducing packet processing latency. Fig. <ref type="figure" target="#fig_1">1</ref> shows that CacheDirector can cut the tail latencies (90-99 t h percentiles) by up to ?21.5% for highly tuned NFV service chains running at 100 Gbps. This is a significant improvement for such optimized systems,  which can facilitate service providers meeting their Service Level Objectives (SLO). We believe that we are the first to: (i) take a step toward using the current hardware more efficiently in this manner, and (ii) advocate taking advantage of NUCA characteristics in LLC and allowing networking applications to benefit from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges.</head><p>We realize slice-aware memory management by exploiting the undocumented Complex Addressing technique used by Intel processors to organize the LLC. This addressing technique distributes memory addresses uniformly over the different slices based on a hash function to increase effective memory bandwidth, while avoiding LLC accesses becoming a bottleneck. However, exploiting Complex Addressing to improve performance is challenging for a number of reasons. First, it requires finding the mapping between different physical addresses and LLC slices. Second, it is difficult to adapt the existing in-memory data structures (e.g., for a protocol stack) to make use of the preferentially placed content (e.g., packets). Finally, we have to find a balance between performance gains due to placing the content in a desirable slice vs. the computational or memory overhead for doing so. Contributions. First, we studied Complex Addressing's mapping between different portions of DRAM and different LLC slices for two generations of Intel CPU (i.e., Haswell and Skylake) and measured the access time to both local and remote slices. Second, we proposed slice-aware memory management, thoroughly studied its characteristics, and showed its potential benefits. Third, we demonstrated that a key-value store can potentially serve up to ?12.2% more requests by employing slice-aware management. Fourth, this paper presents a design &amp; implementation of CacheDirector applied as a network I/O solution that implements sliceaware memory management by carefully mapping the first 64 B of a packet (containing the packet's header) to the slice that is closest to the associated processing core. While doing so, we address the challenge of finding how to incorporate slice-aware placement into the existing Data Plane Development Kit (DPDK) <ref type="bibr" target="#b15">[15]</ref> data structures without incurring excessive overhead. We evaluated CacheDirector's performance for latency-critical NFV systems. By using CacheDirector, tail latencies (90-99 t h percentiles) can be reduced by up to 119 ?s (?21.5%) in NFV service chains running at 100 Gbps. Finally, we showed that slice-aware memory management could provide functionality similar to Cache Allocation Technology (CAT) <ref type="bibr" target="#b52">[51]</ref>.</p><p>The remainder of this paper is organized as follows. ?2 provides necessary background and studies Intel Complex Addressing and the characteristics of different LLC slices regarding access time. ?3 elaborates the principle of sliceaware memory management and its potential benefits. Next, ?4 presents CacheDirector and discusses its design &amp; implementation as an extension to DPDK; while ?5 evaluates CacheDirector's performance. Moreover, ?6 and ?7 discuss the portability of our solution and cache isolation via sliceaware memory management. ?8 addresses the limitations of our work. Finally, we discuss other efforts relevant to our work and make concluding remarks in ?9 and ?10, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Last Level Cache (LLC)</head><p>A computer system is typically comprised of several CPU cores connected to a memory hierarchy. For performance reasons, each CPU needs to fetch instructions and data from the CPU's cache memory (usually very fast static randomaccess memory (static RAM or SRAM)), typically located on the same chip. However, this is an expensive resource, thus a computer system utilizes a memory hierarchy of progressively cheaper and slower memory, such as DRAM and (local) secondary storage. The effective memory access time is reduced by caching and retaining recently-used data and instructions. Modern processors implement a hierarchical cache as a level one cache (L1), level two cache (L2), and level 3 cache (L3), also known as the Last Level Cache (LLC). In the studied systems, the L1 and L2 caches are private to each core, while LLC is shared among all CPU cores on a chip. Caches at each level can be divided into storage for instructions and data (see Fig. <ref type="figure" target="#fig_2">2</ref>).</p><p>We consider the case of a CPU cache that is organized with a minimum unit of a 64 B cache line. Furthermore, we assume that this cache is n-way set associative ("n" lines form one set). When a CPU needs to access a specific memory address, it checks the different cache levels to determine whether a cache line containing the target address is available. If the data is available in any cache level (aka a cache hit), the memory access will be served from that level of cache. Otherwise, a cache miss occurs and the next level in the cache hierarchy will be examined for the target address. If the target address is unavailable in the LLC, then the CPU requests this data from the main memory. A CPU can implement different cache replacement policies (e.g., different variations of Least Recently Used (LRU)) to evict cache lines in order to make room for subsequent requests <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b82">80]</ref>. Physical memory addresses are logically divided into different portions (based upon an offset, set index, and tag, see Fig. <ref type="figure" target="#fig_3">3</ref>). The set index defines which set in the cache can hold the data corresponding to a given address. By concurrently comparing the tag portion of a given address with the tag portion of the address of the cache lines available in one set, the system can determine whether the data corresponding to that address is present in the cache.  The CPU cores and all LLC slices are interconnected by a bi-directional ring bus * . However, due to the differences in paths between a given core and the different slices (aka NUCA), accessing data stored in a closer slice is faster than accessing data stored in other slices. ?2.2 validates and quantifies this behavior by measuring access times from one core to different LLC slices. Although each of the LLC slices operates and is managed as a standard cache, all slices are addressable and accessible by all cores as a single logical LLC. Additionally, each LLC slice is equipped with Intel's hardware performance counters which monitor the CBo ? (or C-Box) register for each slice, see Fig. <ref type="figure" target="#fig_2">2</ref>. Each C-Box can be configured to measure a different event for a slice, e.g., count total number of LLC lookups or number of LLC misses.</p><p>The physical memory address determines the slice into which data will be loaded. Intel uses an undocumented hash function that receives the physical address as an input and determines which slice should be associated with that particular address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mapping between Physical Addresses and Slices</head><p>There have been many attempts to find the slice mapping and reverse-engineer Intel's Complex Addressing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b86">84]</ref>. In our test system, a server equipped with an Intel Xeon-E5-2667-v3, we followed the approach proposed by Cl?mentine Maurice, et al. <ref type="bibr" target="#b43">[42]</ref>. This approach can be divided into two parts:</p><p>Polling. This part is used to find the mapping between different physical addresses and LLC slices. For this, the C-Box counters (see ?2) are configured to count all accesses to each slice. Next, a specific physical address is polled several times, thus a C-Box counter showing a larger number of lookups will identify that the slice is mapped to that particular physical address. By applying the same technique to different physical addresses, the mapping will be found. This technique can be applied to any processor with any number of cores, which are equipped with uncore performance monitoring unit (e.g., C-Box counters).</p><p>Constructing the hash function. Although using polling is sufficient to learn the mapping, it can be expensive in terms of time. Hence, it would be convenient to know the hash function used in Complex Addressing. According to <ref type="bibr" target="#b43">[42]</ref>, the LLC hash function for a CPU with 2 n cores can be defined as a XOR of multiple bits. Therefore, one can compare the slices found, acquired by polling, for different addresses that differ in only one bit and then determine whether that bit is part of the hash function or not. If two addresses are mapped to different slices, then that bit is assumed to be one of the inputs to the hash function. By performing the above steps, the hash function can be constructed and then verified by assessing a wide range of address and comparing the output of hash function with the actual mapping between memory addresses and slices. We note that the hash function found for our test machine is the same function founded by <ref type="bibr" target="#b43">[42]</ref> for other Intel CPUs with 2 n cores, which is shown in Fig. <ref type="figure">4</ref>.</p><p>Where PA is Physical Address <ref type="bibr">Figure 4</ref>. Reverse-engineered Hash Function of Intel Xeon-E5-2667-v3 CPU with 8 cores -Dark blue cells correspond to the bits that are included in the hash function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Access Time to different Slices in LLC</head><p>As discussed previously, due to the difference in paths from each core to the different slices in the LLC, we expected to experience a difference in access time. To verify this hypothesis, we designed a test application to measure the number of cycles needed to access cache lines residing in different slices of LLC from a single core. All of these measurements were made on a system running Ubuntu 16.04 (linux kernel-4.4.0-104) with 128 GB of RAM and two Intel Xeon-E5-2667-v3 processors. Each processor has 8 cores running at 3.2 GHz. The specification of the cache hierarchy in Xeon-E5-2667-v3 is shown in Table <ref type="table" target="#tab_1">1</ref>. To measure the access time from one specific core to a LLC slice, we pin our test application to that core. Then, we allocate a buffer backed by a 1 GB hugepage by using mmap and then acquire the physical address of the allocated hugepage via /proc/self/pagemap <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b72">71]</ref>. Next, we try to fill a specific set in L1, L2, and our desired LLC slice. In our test application, only twenty cache lines have been selected because of the set-associativity of this processor's LLC. Thereafter, we write a fixed value into all of these cache lines and then flush the cache hierarchy by calling the clflush instruction to push all of the cache contents to main memory.</p><p>To ensure that all twenty cache lines are available in our desired LLC slice we read all of the selected cache lines. As the set-associativity of the L2 and L1 caches is only eight, we start by reading the first eight cache lines, as they probably are not available in the L2 or L1 caches. By measuring the number of cycles needed to read the first eight cache lines, we learn the access time to a specific slice in the LLC. These steps were repeated for all of the cores and all of the slices to find the access time from each core to all LLC slices. Measurements of the number of cycles used the rdtscp and rdtsc instructions following Intel's guidelines <ref type="bibr" target="#b55">[54]</ref>. To increase the measurement's accuracy and to prevent other tasks/processes from interfering with these measurements, a single CPU socket was isolated.</p><p>We ran the experiment 1000 times for each core and LLC slice pair. Results for all of the cores follow the same behavior. Fig. <ref type="figure" target="#fig_18">5a</ref> shows the results for core 0 when the cache lines are read from different LLC slices. These results suggest that LLC access times are bimodal since the caches are located on a physical ring bus, e.g., accessing slices 0, 2, 4, and 6 require fewer CPU cycles. Additionally, these results show that reading data from the appropriate slice (that is closest to the CPU core) can save up to ?20 cycles in each access to LLC * , which is equal to 6.25 ns. This saving could be aggregated, as cache misses in lower levels is inevitable for some real-world applications. The aggregated savings can be used to execute useful instructions instead of stalling, i.e., waiting for data to be available to the CPU. Furthermore, the amount of saving is comparable with the time budget for processing small packets being sent at 100 Gbps (i.e., 5.12 ns). Note that the addresses of the cache lines used in this experiment are saved in an array of pointers. Therefore, the measured values may include an additional memory/cache access and these access times are different from the nominal LLC access times stated by Intel (e.g., 34 cycles for the Haswell architecture <ref type="bibr" target="#b11">[12]</ref>). However, this extra overhead shows the actual impact of access time on real-world applications, as using pointers is common when programming.  We repeated the same experiment for write operations. These results are shown in Fig. <ref type="figure" target="#fig_5">5b</ref>. Note that there is no difference in latency for write operations as the updating policy of the CPU is write-back. This policy directs write operations to the L1 cache and upon completion the writeback will be immediately confirmed to the host <ref type="bibr" target="#b70">[69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Slice-aware Memory Management</head><p>In this section, we introduce slice-aware memory management, by which an application can ask for memory regions that are mapped to specific LLC slice(s). Applications can utilize our memory management scheme to improve * Using rdtscp and rdtsc instructions adds around 32 extra cycles to all measurements, hence we have subtracted this value from all of the results that are reported. their performance by allocating memory that is mapped to the most appropriate LLC slice(s), i.e., that have lower access latency. Moreover, slice-aware memory management can also be used to mitigate the noisy neighbor effect and realize isolation, as discussed in ?7.</p><p>In order to demonstrate the impact of this memory management scheme on the performance of applications, we designed an experiment as follows: (i) a 1 GB hugepage was used to allocate 1.375 MB * non-contiguous memory which maps to a specific slice, (ii) locations in this memory are read/written randomly (with uniform distribution) for a total of 10000 times in each run. This experiment was run 100 times and compared with normal memory allocation using contiguous memory. Fig. <ref type="figure" target="#fig_18">6a</ref> indicates the average speedup in slice-aware memory management for read operations. This result correlates with our previous findings (see Fig. <ref type="figure" target="#fig_18">5a</ref>). Although the results in ?2.2 showed that writing to different slices did not change the number of cycles per write, Fig. <ref type="figure" target="#fig_8">6b</ref> demonstrates that the difference in access times becomes visible with an increasing number of write operations. This behavior is related to the write-back policy since modified cache lines accumulate in L1 and they need to be written to higher level caches, specifically L2 and LLC, when there is not enough space in L1 for newer cache lines. Both experiments use 1 GB hugepages, hence the improvements are not due to fewer TLB misses. It is expected that one would observe the same improvement when using 4 kB or 2 MB pages.   Using multiple cores and larger datasets. To further investigate the potential benefits of slice-aware memory management, we ran the same experiment for different array sizes while running on multiple cores (see Fig. <ref type="figure" target="#fig_10">7</ref>). Both Fig. <ref type="figure" target="#fig_18">7a</ref> and Fig. <ref type="figure" target="#fig_10">7b</ref> suggest that using slice-aware memory management would lead to performance improvement when the working dataset in any given period can be fit into a slice (i.e., 2.5 MB in this architecture). Furthermore, applications with larger datasets can still take advantage of this scheme by putting their most frequently used data in the preferable LLC * Corresponding to half the size of each slice plus the size of L2. slice(s). Although we ran these experiments on the Haswell architecture, slice-aware memory management produces the same improvement on the newer architecture (i.e., Skylake), see ?6.  Average Operations Per Second (OPS) of the system for different array sizes while using 8 cores on a CPU with Haswell architecture. For slice-aware, each core is allocating the array using the memory mapped to the closest LLC slice. The array elements has been read/written randomly with uniform distribution generated by uniform_int_distribution class in C++11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Applicability</head><p>The experiments described in this section show that knowing the mapping between physical addresses and LLC slices can enable developers to further improve the applications' performance with minor effort. As shown in Fig. <ref type="figure" target="#fig_10">7</ref>, improvements are tangible when the per-core working dataset fits into an LLC slice. Many applications can benefit from our proposed memory management scheme, two examples are Key-Value Stores (KVS) and NFV. In this paper, we have focused on NFV, but we will briefly discuss the expected improvements in a KVS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KVS.</head><p>In-memory KVS is a type of database in which clients send a request for a key and server(s) reply with a value. Two common operations supported by a KVS are read &amp; write requests, also known as GET &amp; SET. Real-world KVS workloads are usually skewed * and follow a Zipfian (Zipf) distribution <ref type="bibr" target="#b1">[2]</ref>, i.e., some keys are more frequently accessed, making KVS a candidate for our solution.</p><p>We implemented a test application running on top of DPDK to emulate the behavior of a KVS, in which the size of keys and values are 64 B. We ran experiments for different workloads with/without slice-aware memory management. In our setup, the server serves a request only with one CPU core and a client sends requests encapsulated in 128 B TCP packets at high rate to stress the server. We measured the performance of our emulated KVS on the server side so that we could ignore the networking bottlenecks while measuring the impact on request serving rate. Fig. <ref type="figure">8</ref> shows the average Transactions Per Second (TPS) for different GET/SET ratios. For uniform key distribution, the probability of requesting the same key is quite low, which hides the benefits, as most of the requests must be served from DRAM. However, for a skewed workload (i.e., which accesses some keys more frequently), the probability of having a value for a requested key in LLC is higher. In our approach, these values would be available in the closest LLC slice; therefore, a CPU core can serve the requests for the popular keys faster compared to the normal scenario and slice-aware memory management can improve performance by up to ?12.2%. Our measurements show that the average number of cycles required to serve a request while doing 100% GET with skewed distribution is ?160 cycles, which is 34 cycles fewer (?17.5%) than for normal memory management. We believe these results motivate further investigation, as it shows the potential improvements that can be achieved by a slice-aware KVS. However, our experiment does not represent a real-world KVS for several reasons: (i) we have only used one CPU core for receiving &amp; serving the requests, (ii) we have used small keys &amp; values (i.e., 64 B ? ), and (iii) our emulated KVS does not implement all available functions of a KVS. Additional functions might lead to more cache eviction, as they might have a larger memory footprint, which in turn might decrease the expected improvements. A more complete implementation and evaluation of slice-aware KVS remains as future work. NFV. Network Functions (NF) typically perform operations on packets, mostly on packet headers (which can fit into one LLC slice). As a packet is frequently processed by different NFs in a service chain, NFs can potentially take * Skewness is the degree of distortion from the normal distribution, or more precisely it describes the lack of symmetry and there is a formula to calculate the skewness of any given workload <ref type="bibr" target="#b20">[20]</ref>.</p><p>? The current implementation of KVS cannot map values greater than 64 B to the appropriate LLC slice, see ?8.  Figure <ref type="figure">8</ref>. Average Transaction per Second (TPS) at server side for an emulated KVS implemented by using DPDK and running on 1 core. We allocated 1 GB memory, which is equal to 2 24 ? 64 B values. We used MICA's library <ref type="bibr" target="#b38">[37]</ref> to generate skewed (0.99) keys in the range of [0, 2 24 ).</p><p>advantage of slice-aware memory management. The rest of this paper proposes CacheDirector to exploit slice-aware memory management and discusses how it can be used to improve the performance of NFV service chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CacheDirector Design &amp; Implementation</head><p>This section advances state-of-the-art networking solutions by exploiting Intel's LLC design together with slice-aware memory management in user-space packet processing. We propose CacheDirector, a network I/O solution that extends DDIO and sends each packet's header directly to the appropriate slice in the LLC; hence, the CPU core that is responsible for processing a packet can access the packet header in fewer CPU cycles. To show the benefits of CacheDirector, we implement this solution as an extension to DPDK <ref type="bibr" target="#b15">[15]</ref>. Note that the concept behind CacheDirector could be applied to other packet processing frameworks.</p><p>We used DPDK as it was easier to prototype CacheDirector in user-space, but the same approach could be used for kernel network stack optimization. The section begins with some background about DPDK &amp; its memory management libraries and then elaborates the design principles &amp; implementation of CacheDirector. During DPDK initialization, the NIC is unbound from the Linux kernel (e.g., Intel NICs) or it uses bifurcated drivers (e.g., Mellanox drivers) to make user-space interaction with the NIC possible. After initialization, one or more memory pools are allocated from hugepage(s) in memory. These memory pools (aka mempools) include fixed-size elements (objects), created by the librte_mempool library. DPDK's memory management is non-uniform memory access (NUMA) aware and it applies memory alignment techniques to improve performance. In DPDK, network packets are represented by packet buffers (mbufs) through the rte_mbuf structure. Buffer Management allocates and initializes mbufs from available elements in mempools. Each mbuf contains metadata, a fixed-size headroom, and a data segment (used to store the actual network packet), see Fig <ref type="figure" target="#fig_12">9</ref>.</p><p>The metadata includes message type, length, starting address of the data segment, and userdata. It also contains a pointer to the next buffer. This pointer is needed when using multiple mbufs to handle packets whose size is larger than the data area of a single mbuf. After initializing a driver for all of the receiving and transmitting ports, one or more queues are configured for receiving/transmitting network packets from/to the NIC. These queues are implemented as ring buffers from the available mbufs in mempools. Finally, the receiving ports are set with correct MAC addresses or to promiscuous mode and then DPDK is ready to send and receive network packets.</p><p>Communication between an application and NIC is managed in DPDK through a Poll Mode Driver (PMD). PMD provides Application Programming Interfaces (APIs) and uses polling to eliminate the overhead of interrupts. PMD enables DPDK to directly access the NIC's descriptors for both receiving and transmitting packets. To receive packets, DPDK fetches packet(s) from the NIC's RX descriptor into its receiving queues when the application periodically checks for new incoming packets. To send packets, the application places the packets into transmitting queues from which DPDK takes packet(s) and pushes them into the NIC's TX descriptor, see Fig. <ref type="figure" target="#fig_12">9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CacheDirector</head><p>The main objective of CacheDirector is to bring awareness of Intel's LLC Complex Addressing to DPDK. More specifically, incoming packets are placed into the appropriate LLC slice, thus the core responsible for processing these packets can access them faster. To achieve this goal, the buffer and memory pool manager in DPDK initialize the mbufs so that they will be mapped to the appropriate slice. However, implementing this idea faces some challenges. These challenges and the ways CacheDirector tackles them are described below.</p><p>Small chunks. Dynamic headroom. CacheDirector can dynamically change the amount of headroom such that the starting address of the data area of an mbuf is at an address which is mapped to the desired LLC slice for each CPU core using that mbuf at runtime, see Fig. <ref type="figure" target="#fig_13">10</ref>. However, since DPDK assumes that the headroom is fixed (e.g., 128 B), setting the headroom size to values greater than this will result in a reduction of the data area of mbufs (the default size is 2 kB).</p><p>If the remaining data area is less than the packet's size, then DPDK uses multiple mbufs for one packet, which might be an expensive operation as an application needs to traverse a linked-list to access the whole packet. To tackle this, we must find the maximum amount of headroom required for mbufs in order to ensure that no adverse shrinkage of the data area will happen. Therefore, we performed a experiment in which ?12.3 million packets from an actual campus trace were sent to a server and then calculated the distribution of the dynamic mbufs' headroom sizes. The median of the distribution is 256 B; 95% of the values are less than 512 B; and the maximum needed headroom size is 832 B.</p><p>Examining this distribution, we set the default headroom size to 832 B to ensure that the maximum desired data area is available -but this is at the cost of extra memory usage. Note that extra memory usage does not affect performance (e.g., does not increase TLB misses), as memory allocation is done by using hugepages. The distribution of dynamic headroom size might vary on different micro-architectures. However, differences in the distribution and memory wastage is not a big concern, as it can be eliminated by handling the mbuf allocation at the application level (e.g., in FastClick <ref type="bibr" target="#b2">[3]</ref>). For instance, an application can allocate one large mempool containing mbufs. Then, it can sort mbufs across multiple mempools, each of which is dedicated to one CPU core, based on their LLC slice mappings. However, we implemented CacheDirector in DPDK as an application-agnostic solution.</p><p>Ensuring the appropriate headroom size. Since an mbuf can be used by multiple cores, CacheDirector must ensure that the headroom size is set to the appropriate value so that the first 64 B of the data segment is mapped to the appropriate LLC slice for the CPU core that will be fetching a packet from the NIC. Therefore, at run time CacheDirector sets the actual headroom size just before giving the address to the NIC for DMA-ing * packets. We implemented this as a part of user-space NIC drivers in DPDK. For example, when CPU core 5 wants to fetch packet(s) from a NIC, the NIC driver calculates the headroom such that the data segment of the mbuf(s) is in slice 5. It is worth noting that this step is eliminated when mbufs are sorted at the application level.</p><p>Mitigating calculation overhead. To avoid unnecessary run time overhead, we calculate the headroom needed to place the data segment of each mbuf into specific LLC slices during DPDK's initialization phase. These values are saved in the userdata part (i.e., udata64) of the mbuf structure (metadata), see Fig. <ref type="figure" target="#fig_13">10</ref>. Later, the NIC driver sets the actual headroom size based on the CPU core that will be fetching a packet from the NIC by using these saved values. For example, when CPU core 2 wants to fetch data from the NIC, the NIC driver looks into the userdata part of each mbuf and sets its headroom according to the pre-calculated value for slice 2. It is worth mentioning that we save the number of cache lines instead of actual headroom size and since 832 (the maximum required headroom size) is 13 cache lines, 4 bits is sufficient for each core. Therefore, our solution would be scalable for up to 16 cores on one CPU, as udata64 is 64 bits in size. * Direct Memory Access</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we demonstrate CacheDirector's effectiveness by evaluating the performance of DPDK with/without CacheDirector functionality for two different types of applications in NFV systems.</p><p>Testbed. In our testbed, we use a simple desktop machine as a plain orchestration service (aka pos) for deploying, configuring, and running experiments as well as collecting and analyzing the data (see <ref type="bibr">Fig 11)</ref>. In addition, we have connected two identical servers, one as load generator (aka LoadGen) and another one as Device under Test (aka DuT) which is running a Virtualized Networking Function (VNF). These two machines have dual Intel Xeon E5-2667 v3 processors (see ?2. NFV. To see the impact of CacheDirector on NFV service chains, we evaluate the performance of Metron <ref type="bibr" target="#b34">[33]</ref>, a state-of-the-art platform for NFV, in the presence of CacheDirector. We implemented two different applications, a simple forwarding and a stateful service chain, using Metron's extension of FastClick <ref type="bibr" target="#b2">[3]</ref>. In our experiments, we use an actual campus trace ? , in which 26.9% of frames are smaller than 100 B; 11.8% are between 100 &amp; 500 B; and the remaining frames are more than 500 B. These different traffic classes were used together with two different rates as shown in Table <ref type="table" target="#tab_5">2</ref>. Furthermore, we evaluate CacheDirector while the applications are running on different numbers of cores (i.e., from 1 to 8 CPU cores).</p><p>Measurement Method. For measuring end-to-end latency, we follow the black box approach explained in <ref type="bibr" target="#b19">[19]</ref>, where data is collected on the egress/ingress port of the LoadGen to measure throughput and latency. To do so, the LoadGen ? Same trace that was used in <ref type="bibr" target="#b34">[33]</ref>.  <ref type="figure" target="#fig_15">11</ref>), i.e., traffic sent from one port of LoadGen is received by the other port without any additional processing. By doing so, we are able to measure and characterize the effect of the link latency and extra overheads of the LoadGen, such as queuing and timestamping cost. From this point on, we refer to this portion of the end-to-end latency as "loopback" latency. We measure this latency for all configurations shown in Table <ref type="table" target="#tab_5">2</ref> and we removed the minimum value of the loopback latency from the end-to-end latency in most of the measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simple Forwarding</head><p>The simple forwarding application swaps the sending and receiving MAC addresses of the incoming packets and sends them back to LoadGen. This application assesses the impact of CacheDirector on stateless or low processing network functions. We ran this application for different numbers of cores and different sets of traffic. Here we discuss only the results for two sets of traffic while using 8 cores on one CPU socket: (i) five thousand 64 B packets generated by the FastClick RatedSource module and (ii) mixed-size packets from the real trace at 100 Gbps (see Table <ref type="table" target="#tab_5">2</ref> for details). All other traffic sets (except those related to only 1500 B packets) show the same behavior, but with different latency values -because of different packet sizes and consequently different queuing time at the DuT. The results regarding 1500 B packets will be discussed in ?8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">64 B Packets at low rate</head><p>CacheDirector only affects the processing time of packets at the DuT and consequently the queuing delay on that side. Therefore, to minimize the queuing effect and to see the pure impact of CacheDirector we send five thousand 64 B packets at a low rate (i.e., 1000 pps). Fig. <ref type="figure" target="#fig_17">12</ref> shows the variation of the higher percentiles of end-to-end latency for 50 such runs. This figure shows that CacheDirector reduces the higher percentile latencies by around ?20% which is equal to 1 ?s improvement per packet on the DuT side. It is important to note that even improvements of 1 ?s must not be ignored since 1 ?s is equal to 3200 CPU cycles for a processor running at 3.2 GHz, which can be utilized to process packets instead of stalling. This becomes even more critical at 100 Gbps links, as a server has only 5.12 ns (i.e., ?17 cycles) to process a 64 B packets before receiving a new packet.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Mixed-size Packets at 100 Gbps</head><p>To assess CacheDirector's impact at gigabit per second link speeds, we send packets from the campus trace with mixedsize packets at 100 Gbps. Fig. <ref type="figure" target="#fig_19">13</ref> shows the results of 50 runs, in which we use Receive Side Scaling (RSS) <ref type="bibr" target="#b26">[26]</ref> to distribute packets among 8 cores. The improvement in tail latencies for mixed-size packets at this rate is even greater than for 64 B packets. The top row of Table <ref type="table" target="#tab_6">3</ref> shows the measured throughput for this experiment. The ?76 Gbps limit for the forwarding application is due to the Mellanox NIC's limitation for packets smaller than 512 B <ref type="bibr" target="#b81">[79]</ref> and other architectural limitations such as PCIe <ref type="bibr" target="#b51">[50]</ref> and DDIO * .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stateful Service Chain</head><p>To show the practicality and benefits of slice-aware memory management, we ran Metron <ref type="bibr" target="#b34">[33]</ref> with and without CacheDirector to evaluate the performance of a stateful NFV service chain built from three network functions: a router, a Network Address Port Translation (NAPT), and Load Balancer (LB) using flow-based Round-Robin policy. For the router we followed Metron's approach, in which the routing table of the router with 3120 entires is offloaded to the Mellanox NIC by using FlowDirector technology <ref type="bibr" target="#b29">[29]</ref>, while the remainder of the router's functionalities are handled in software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Mixed-size Packets at 100 Gbps</head><p>For this evaluation, packets were generated using the campus trace and the results from 50 runs are shown in Fig. <ref type="figure" target="#fig_21">14</ref> (and earlier in Fig. <ref type="figure" target="#fig_1">1</ref>). The second row of Table <ref type="table" target="#tab_6">3</ref> shows the throughput for this experiment. Since the service chain is more memory-intensive compared to the simple forwarding  application, the gain becomes more tangible for Router-NAPT-LB. Note that using FlowDirector changes the trend in latency improvements (compare Fig. <ref type="figure" target="#fig_19">13</ref> and Fig. <ref type="figure" target="#fig_21">14</ref>). The improvements are increasing for RSS, i.e., the improvement for the 99 t h percentile is higher than for the 90 t h percentile. However, the improvements for FlowDirector behaves in the opposite way (i.e., the performance gain is decreasing). We observed that FlowDirector reduces contention in each slice by performing better load balancing compared to RSS for the campus trace that was used. Moreover, we believe that the reason for this behavior may also be related to DDIO's 10% limit <ref type="bibr" target="#b68">[67]</ref> and the slice imbalance (see ?8) incurred by RSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Tail Latency vs. Throughput</head><p>To see the impact of CacheDirector on the not fully-loaded system, we measured the performance of Metron with and without CacheDirector for different loads. Fig. <ref type="figure" target="#fig_22">15</ref> illustrates the data points and fitted curves for this experiment. The fitted curves are defined as piecewise functions, wherein the lower (Throughput &lt; 37 Gbps) and higher (Throughput ? 37 Gbps) parts of data points are fitted to linear and quadratic functions, respectively. The results show that our technique slightly shifts the knee of the tail latency vs. throughput curve, which means CacheDirector would still be beneficial while the system experiences a moderate load (i.e., around 50 Gbps) and before tail latency starts growing dramatically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summary</head><p>In this section, we showed that using CacheDirector brings slice-aware memory management to packet processing. Doing so can reduce the average latency by up to ?6% (14 ?s) and more importantly tail latencies (90-99 t h percentiles) by up to ?21.5% (119 ?s) for NFV systems. By doing so, we improved the performance of a highly tuned NFV platform that works at the speed of the underlying networking hardware. The reasons for this improvement are as follows:</p><p>CacheDirector places the packet header into the appropriate LLC slice. As a result, any time the CPU requires the packet header when it is not present in the L1 and L2 caches but available in LLC, the CPU stalls for fewer cycles waiting for the packet header to be brought into the lower cache levels; therefore, the CPU can process packets faster, which results in more frequent fetching of enqueued packets. Hence, the queuing delay is reduced. CacheDirector offers NFV service providers a tangible gain as they can utilize their system's capacity more efficiently, while providing a more predictable response-time to their customers and reducing their SLO violations due to reduced tail latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Porting to Newer CPU Architectures</head><p>Slice-aware memory management is architecture dependent and finding the mapping requires using the uncore performance monitoring unit. However, this unit is likely to be available in most of the current and future Intel processors. In addition, being architecture dependant is a typical requirement for achieving high performance, as any code optimization routinely results in processor dependent code. For instance, any high-quality compiler is aware of the instruction pipeline's details such as depth, cache sizes, and shadow registers, which might change for different versions of micro-architectures.</p><p>We have run most of our experiments on the Haswell architecture, but to prove the portability and feasibility of our solution on newer architectures, we adjusted our code to be compatible with the Skylake architecture. Two doctoral students accomplished this task in two days. Compared to Haswell, there are some important changes in Skylake, some of which affect the cache hierarchy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b79">78]</ref>: Firstly, the size of L2 cache is quadrupled to 1 MB (extended L2 by adding 768 kB cache on the outside of the core) and the size of LLC slices is reduced to 1.375 MB. This can be interpreted as some parts of the shared LLC becoming private to each CPU core. Secondly, the ring-based interconnect is replaced by a mesh interconnect. Additionally, the number of slices is not necessarily equal to the number of cores. There are three layouts for CPUs, which have either 10, 18, or 28 slices. Our CPU (Intel Xeon Gold 6134) has 8 cores and 18 slices. Finally, the connection between L2 &amp; LLC is changed to a "noninclusive" one and LLC acts like a victim cache for L2, hence cache lines will be loaded directly into L2 without being loaded into LLC. When a cache line is evicted from L2, it will be moved to LLC if it is expected to be reused. Later, the cache line can be re-read from LLC to L2, while still remaining in LLC. Despite the shift toward non-inclusiveness, it is important to note that this does not affect DDIO, thus packets are still loaded in LLC, rather than L2 <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b68">67]</ref>. Therefore, CacheDirector is still expected to be beneficial, but with lower improvements -as the size of L2 has been increased. Fig. <ref type="figure" target="#fig_24">16</ref> shows the access time differences from core 0 to different slices for the mentioned Skylake CPU, as measured by the same approach discussed in ( ?2.2) through polling without knowing the hash function. The results have some correlation with those measured for Haswell (see Fig. <ref type="figure" target="#fig_18">5a</ref>). The access time difference is again present. However, as the number of slices is more than the number of cores, there are multiple preferable slices for each core, as shown in Table <ref type="table" target="#tab_7">4</ref>.</p><p>Our proposed memory management scheme is still expected to be effective when the working dataset is bigger than L2 and smaller than a LLC slice. Furthermore, porting our code to a newer architecture provided us with the opportunity to study slice isolation enabled by slice-aware memory management and comparing it with way isolation   introduced by Cache Allocation Technology (CAT) <ref type="bibr" target="#b52">[51]</ref>, which will be discussed in the next section.</p><p>7 Slice-aware Cache Isolation vs. CAT Intel recently introduced a technology called CAT, which provides greater control over LLC to address concerns regarding unfair usage of LLC. CAT enables cache isolation and prioritization of applications by allocating different cache ways to different applications. By doing so, the noisy neighbor effect can be mitigated to some extent, as allocating a limited number of ways solves the problem of overutilization by an application. However, the effective LLC bandwidth still remains a bottleneck as the noisy neighbor might access LLC more frequently.</p><p>Slice-aware memory management can be used to provide cache isolation, or cache partitioning, by allocating different slices rather than cache ways. To compare this approach with CAT, we designed an experiment in which we have two simple applications similar to that discussed in ( ?3) * . One application acts as a noisy neighbor and we measure the execution time of the other application in different scenarios. Fig. <ref type="figure" target="#fig_25">17</ref> shows these results.</p><p>NoCAT describes the scenario where both noisy neighbor and our application use normal memory allocation when CAT is disabled, i.e., both use all available LLC ways (11 ways). * We allocate 2 MB, which corresponds to three-fourths of the size of each slice plus the size of L2 in Intel Xeon Gold 6134. 2W Isolated describes a scenario in which the main application only uses two ways ( <ref type="formula">2</ref>11 ? 18% of LLC) and the rest of the ways are used by the noisy neighbor.</p><p>Slice-0 Isolated describes a scenario in which the main application uses slice 0 ( 1 18 ? 5% of LLC). The noisy neighbor is still present and it pollutes all LLC slices except slice 0. It is important to note that we only isolate the application's working set, thus isolating the code section (instructions and local variables) is not considered in our experiment. However, it would be possible to realize full slice isolation through an abstraction layer (e.g., slice-aware hypervisor) or future H/W support.</p><p>Comparing the results of these scenarios, we conclude that slice-aware memory management performs ?11% better than CAT. Consequently, systems that are not equipped with CAT can use slice-aware memory management, which can provide them the same functionality, but at the cost of memory fragmentation. Moreover, even CATenabled systems can benefit from the slice-aware memory management, as it will result in better performance. We believe that these results could motivate vendors to consider extending CAT by making it possible to isolate slices rather than just ways. However, this might require a more thorough evaluation of CAT and slice isolation, which can be done by comparing the performance of known benchmarks (e.g., SPEC CPU benchmarks) for both techniques. Additionally, slice isolation can also be employed in hypervisors (e.g., KVM) to allocate different LLC slices to different virtual machines. These remain as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>This section elaborates limitations and other aspects of CacheDirector and slice-aware memory management.</p><p>Development effort. Slice-aware memory management is not as complex as it may sound. We developed a library (600 lines of code) which can be used by any application to realize slice-awareness. Implementing CacheDirector in DPDK required less than 200 additional lines of code * .</p><p>Suggestions for new CPU architectural features. To use slice-aware memory management and CacheDirector for other processors, one must first determine the mapping between different physical addresses and LLC slices. Therefore, the hash function of the processor should be known or the processor should be equipped with a uncore performance monitoring unit (such as CBo or CHA). Moreover, Intel and other vendors might consider introducing a new processor mode in which the hash function is known, the granularity of chunks are increased (e.g., 4 kB pages), or is even programmable. Considering the need for hardware changes in the future data centers <ref type="bibr" target="#b58">[57]</ref>, we hope this paper will encourage hardware vendors to adopt one or more of these alternatives. NIC driver support. CacheDirector is implemented in DPDK for the MLX5 driver and does not presently support Vectorized PMD. However, we are planning to extend CacheDirector to support additional NICs.</p><p>Noisy neighbor effect. Since LLC is shared among the different cores, having a noisy neighbor degrades performance. In CacheDirector we force part of our data to a single LLC slice, hence the degradation may be more visible than when we are not slice-aware. The noisy neighbor effect is not limited to another application running on a different core. For instance, when DuT is receiving packets with a size of 1500 B, DDIO technology loads the whole packet (of ?24 cache lines) into different LLC slices, hence previously enqueued packets can be evicted from the LLC when LoadGen sends at 100 Gbps, despite the fact that packets were sent to the LLC by DDIO. This can also happen without CacheDirector, but the probability of eviction in LLC when using CacheDirector is proportional to 1 Number of LLC slice which is greater than the normal case, i.e., 1 (Number of LLC slice) 2 . In practice, one can use multiple slices for memory allocation as ?2.2 showed that LLC access times are bimodal, which can result in a lower probability of LLC eviction.</p><p>Dealing with data larger than 64 B. Since the purpose of the current hash function in Intel's Xeon processors is to increase LLC bandwidth by uniformly distributing * The source code is available at: https://github.com/aliireza/slice-aware LLC requests among different slices, the mapping between physical memory and slices changes for almost every cache line (64 B), making it difficult to apply the same technique to certain applications. However, it would still be possible to map larger data to the appropriate LLC slice(s) by using a linked-list and scattering the data. Evaluating these techniques will remains as our future work.</p><p>The impact of H/W prefetching. Current H/W prefetchers are designed for contiguous memory allocation schemes, wherein L2 prefetchers such as L2 Hardware Prefetcher and L2 Adjacent Cache Line Prefetcher prefetch only the next cache lines into the L2 cache <ref type="bibr" target="#b75">[74]</ref>. Therefore, using sliceaware memory management might not be always beneficial for some applications which have a contiguous memory access pattern. However, there are many applications which have non-contiguous access patterns (e.g., NFV service chains and key-value stores) or some that do not benefit from it <ref type="bibr" target="#b33">[32]</ref>. Introducing programmable H/W prefetchers in general purpose processors could make slice-aware memory management even more efficient.</p><p>Extra consideration for slice-awareness. Employing sliceaware memory management requires some consideration, as it might cause performance degradation. In short, sliceaware memory management partitions LLC similar to CAT but with a granularity of a slice, which means an application is limited to a smaller portion of LLC, but with faster access, i.e., lower latency. In addition, slice-aware memory management works based on physical address, which can limit the available memory space (similar to page coloring). Therefore, developers should be careful not to create a slice imbalance. It is also important to note that the most appropriate LLC slice is not always the one with the lowest access latency. For instance, multi-threaded applications that have shared data among multiple cores should find a compromise placement and then use the LLC slice(s) which are beneficial for all cores. Additionally, some applications might be affected by thread-migration policies in the operating system. This can be handled by limiting applications to specific cores (e.g., using cgroups-cpusets) or monitoring/migration data (similar to the H/W features suggested by <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b46">45]</ref>). Furthermore, applications which only use slice-aware memory management for the "hot" data due to their very large working set should employ monitoring/migration techniques to deal with variability of hot data. Taking into account these considerations, there might be additional applications beyond NFV (e.g., slab coloring and compiler/linker optimization), which can benefit from slice-aware memory management. The proper evaluation for generality of our proposed memory management scheme remains as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>This section discuss other efforts relevant to our work.</p><p>Non-Uniform Cache Architecture. Increasing the size of cache leads to non-uniform cache architectures (NUCA), in which different cache portions are accessible with different access latency based on their distance to specific CPU cores. NUCA has been addressed in the literature in different contexts. Several works have proposed hardware-based strategies -mainly by introducing modifications to the CPU architecture -such as data migration, data placement, and data replication <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" target="#b78">77]</ref>. Some other works focus on software-based strategies, such as data layout optimization <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b89">87]</ref> or compiler optimization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">31]</ref>, to exploit NUCA characteristics to improve performance. However, these works mostly overlook the cache organization of currently available CPUs. Additionally, they are based on assumptions, some of which are not valid for current CPU micro-architectures (e.g., ignoring addressing schemes used in current Intel CPUs), and they are based on simulation. To the best of our knowledge, our work is the first to exploit the NUCA characteristics of the latest Intel CPUs to improve the performance of applications.</p><p>Intel LLC Complex Addressing. Others have tried to reverse engineer the Intel LLC Complex Addressing hash function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b86">84]</ref>. These efforts were mainly conducted by the researchers in the security community who discussed how understanding this addressing makes different classes of attacks (e.g., sandbox and Prime+Probe) practical. To the best of our knowledge, our work is the only one that takes advantage of knowledge of this addressing to improve the system's performance. We are also the only work to perform precise measurements to evaluate the access time to different LLC slices and show the potential gain that slice-aware memory management enables.</p><p>Cache-aware Memory Management. Others have addressed cache-aware memory allocation and memory management with the goal of delivering predictable cache behavior and improving system performance <ref type="bibr" target="#b47">[46]</ref>. Many of these works (e.g., <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b67">66]</ref>) proposed software techniques for cache-aware memory allocation. These works mainly suffer from being limited to a traditional physical addressing scheme where the cache is physically addressed and/or based on application profiling without considering Intel's LLC Complex Addressing. In contrast, other works (e.g., <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b87">85]</ref>) extended traditional page coloring to be applicable to Intel's multi-core architectures that involve a hash-based LLC addressing scheme. However, these works will not be as effective as before on newer architectures (e.g., Haswell and Skylake), as the mapping between LLC slices and physical addresses changes at a finer granularity than 4kpages. Furthermore, there are a series of works that proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b77">76]</ref> or exploited <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b83">[81]</ref><ref type="bibr" target="#b84">[82]</ref><ref type="bibr" target="#b85">[83]</ref> hardware-based cache partitioning to better use the LLC in order to improve performance. To the best of our knowledge, none of these works considered LLC slice-aware memory management, or slice-aware cache partitioning, and we are the only work that takes advantage of knowledge of Intel's LLC Complex Addressing for memory management and allocation.</p><p>Fast Packet Processing. NFV is a transition toward deploying network functions on general purpose hardware as opposed to using specialized physical boxes. To achieve high throughput and low latency with commodity hardware, NFV applications mostly employ user-space packet processing frameworks to eliminate the costly traditional kernel-based network stack <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b57">56]</ref>. In addition to user-space I/O frameworks, there have also been efforts to optimize the kernel network stack <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b60">59]</ref>. Additionally, several efforts have been made to improve NFV application performance when running over user-space I/O frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b88">86]</ref>. Furthermore, there are a limited number of works that employed CAT to mitigate noisy neighbor effect and improve NFV performance <ref type="bibr" target="#b71">[70,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b74">73]</ref>. Our work can be seen as complementary to these works since none of them considered LLC slice-aware memory management to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have proposed slice-aware memory management scheme which exploits the latency differences in accessing different LLC slices, aka NUCA. We also proposed CacheDirector, a network I/O solution which utilizes slice-aware memory management. CacheDirector puts the packet's header in the slice closest to the CPU core that is going to process the packet. By doing so, CacheDirector increases efficiency and performance, while reducing tail latencies over the state-of-the-art. Moreover, we investigated other potential use cases for our proposed memory management scheme, such as cache isolation and potential improvements in keyvalue stores. This work conveys a message to the research community that with a little extra attention to memory management and by taking advantage of the LLC's Complex Addressing, it is possible to boost application performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Speedup achieved for a stateful service chain (Router-NAPT-LB) at high-percentiles and mean by using CacheDirector while running at 100 Gbps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of the memory hierarchy in an Intel Xeon Processor E5 v3 (Haswell) with N cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Physical address mapping within cache hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Access time to different LLC slices from core 0 in Xeon-E5-2667 v3 (Haswell).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Average speedup achieved by core 0 (Xeon-E5-2667 v3) in access time for slice-aware memory management compared to normal memory allocation. The average execution times for 10000 read and write scenarios for normal memory allocation are 2262.38 ms and 5772.35 ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Average Operations Per Second (OPS) of the system for different array sizes while using 8 cores on a CPU with Haswell architecture. For slice-aware, each core is allocating the array using the memory mapped to the closest LLC slice. The array elements has been read/written randomly with uniform distribution generated by uniform_int_distribution class in C++11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FixedFigure 9 .</head><label>9</label><figDesc>Figure 9. Simplified memory management in DPDK: the size of the mbuf struct is equal to two cache lines (i.e., 128 B) and the headroom size is fixed (default value: 128 B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. CacheDirector changes to the mbuf structure.</figDesc><graphic url="image-124.png" coords="8,145.39,121.41,57.03,61.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>2), 128 GB of RAM, and a Mellanox ConnectX-4 MT27700 card ? . The LoadGen has a dual port Mellanox NIC. In all of the experiments on DuT, hyperthreading is disabled, one CPU socket (including 8 CPU cores) on which we run experiments is isolated. The OS is Ubuntu 16.04.4 with Linux kernel v4.4.0-104. In order to implement the CacheDirector functionality in DPDK, we extended DPDK v18.05 and we disabled vectorized PMD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Experiment setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. End-to-end latency without loopback latency for 64 B packets sent at the rate of 1000 pps. At each percentile, the left box refers to DPDK and the right one DPDK with CacheDirector. The minimum loopback latency is 9 ?s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>( a )</head><label>a</label><figDesc>End-to-end latency without loopback latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. End-to-end latency and improvement for a simple forwarding application running on 8 cores with mixed-size packets at 100 Gbps with RSS. The minimum loopback latency is 495 ?s. The values shows the median of 50 runs. Error bars represent 1 st and 3 r d quartiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>CDF of end-to-end latency without loopback latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. CDF of end-to-end latency without loopback latency and improvement for a stateful service chain (Router-NAPT-LB) running on 8 cores while sending mixed-size packets at the rate of 100 Gbps with HW offloading using FlowDirector. The minimum loopback latency is 495 ?s. The values show the median of 50 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Tail latency (99 t h percentile) vs. Throughput for a stateful service chain (Router-NAPT-LB) running on 8 cores while sending mixed-size packets at different rates with HW offloading using FlowDirector. The values of tail latency include loopback cost. The data points show the median of 50 runs. The solid lines represent the fitted curves to the measurement points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Access time to different slices from core 0 in Xeon-Gold-6134 (Skylake).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Average execution time for the main application in different scenarios for read and write operations. "W" refers to ways. Cross hatch and solid patterns represents read and write operations, respectively. Numbers show the speedup achieved by slice isolation in comparison with way isolation, i.e., CAT. The measurements were run on a Xeon-Gold-6134 (Skylake) processor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>L1 Tag L1 Index Offset L1 Tag L1 Index Offset L2 Tag L2 Index Offset L2 Tag L2 Index Offset L3 Tag L3 Index Offset L3 Tag L3 Index Offset Physical Address bits 0 63 Physical Address bits 0 63</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Intel Xeon-E5-2667 v3 -Cache Specification.</figDesc><table><row><cell>Cache Level</cell><cell>Size</cell><cell>#Ways</cell><cell>#Sets</cell><cell>Index-bits[range]</cell></row><row><cell>LLC-Slice</cell><cell>2.5 MB</cell><cell>20</cell><cell>2048</cell><cell>16-6</cell></row><row><cell>L2</cell><cell>256 kB</cell><cell>8</cell><cell>512</cell><cell>14-6</cell></row><row><cell>L1</cell><cell>32 kB</cell><cell>8</cell><cell>64</cell><cell>11-6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Intel's LLC Complex Addressing maps almost every cache line (64 B) to a different LLC slice. Consequently, it is impossible to send large packets to the appropriate LLC slice without packet fragmentation. To deal with this challenge, CacheDirector ensures that at least the first 64 B of packets, containing the packet's header, are mapped to the appropriate LLC slice by introducing dynamic headroom to the mbufs. As there are some applications which might access a different part of the packet more frequently (e.g., Virtual Extensible LAN and Deep Packet Inspection), CacheDirector can be configured to map any other 64 B portion of the packet to the appropriate LLC slice.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>The traffic classes and rates used in the experiments. Low rate traffic ("L") was generated at 1000 packets per second (pps) and high rate traffic ('H") at ?4 Mega pps. CacheDirector only affects the processing time of packets at the DuT and consequently the queuing delay on that side. To assess the delays not due to the DuT, we run a loopback experiment in which two ports of LoadGen were interconnected back to back (P1 and P2 in Fig.</figDesc><table><row><cell>Packet Size (B)</cell><cell>64</cell><cell>512 1024 1500</cell><cell>Mixed</cell></row><row><cell>Rates</cell><cell cols="3">L, H L, H L, H L, H 5-100 Gbps</cell></row><row><cell cols="4">writes a timestamp in each packet's payload and sends</cell></row><row><cell cols="4">the packet to the DuT which is running a VNF. After</cell></row><row><cell cols="4">processing the packets, DuT sends the packets back to the</cell></row><row><cell cols="4">LoadGen. Upon receiving each packet, the LoadGen reads the</cell></row><row><cell cols="4">saved timestamp inside each packet's payload and calculates</cell></row><row><cell cols="4">throughput and the end-to-end latency for each packet.</cell></row><row><cell cols="4">This latency is composed of three parts: queuing delay &amp;</cell></row><row><cell cols="4">processing time at LoadGen; link delay; and queuing delay</cell></row><row><cell cols="3">&amp; processing time at DuT.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Throughput while sending mixed-size packets at the rate of 100 Gbps + Average Improvement. The default number of ways is 2, which is equal to 10% in our CPU that has 20 ways<ref type="bibr" target="#b68">[67]</ref>.</figDesc><table><row><cell>Scenario</cell><cell>T hr ou?hput (Gbps)</cell><cell>Impr ovement (Mbps)</cell></row><row><cell>Simple Forwarding</cell><cell>76.58</cell><cell>31.17</cell></row><row><cell>Router-NAPT-LB</cell><cell>75.94</cell><cell>27.31</cell></row><row><cell>(FlowDirector with H/W offloading)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note><p>* DDIO uses a limited number of ways in LLC for I/O.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Preferable slices for each core in Intel Xeon Gold 6134. C i and S j represents i t h core and j t h slice, respectively. S 8 S 12 S 10 S 14 S 3 S 15 Secondary slices S 2 , S 6 S 1 S 11 S 13 S 7 , S 9 S 16 S 5 S 17</figDesc><table><row><cell>Core</cell><cell>C 0</cell><cell>C 1 C 2 C 3</cell><cell>C 4</cell><cell>C 5 C 6 C 7</cell></row><row><cell>Primary slice</cell><cell>S 0</cell><cell>S 4</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>?  Intel Xeon processor Scalable family is equipped with a different monitoring unit called Caching and Home Agent (CHA).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank our shepherd, <rs type="person">Adam Belay</rs>, and EuroSys reviewers for their insightful comments and suggestions. Furthermore, we are grateful to <rs type="person">Georgios Katsikas &amp; Tom Barbette</rs> for helping us prepare the testbed. This work was partially supported by the <rs type="funder">Wallenberg AI, Autonomous Systems and Software Program (WASP)</rs> funded by the <rs type="funder">Knut and Alice Wallenberg Foundation</rs>. The work was also funded by the <rs type="funder">Swedish Foundation for Strategic research (SSF)</rs>. This project has received funding from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement No <rs type="grantNumber">770889</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jcp7cNT">
					<idno type="grant-number">770889</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Systematic Reverse Engineering of Cache Slice Selection in Intel Processors</title>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Irazoqui Apecechea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Eisenbarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berk</forename><surname>Sunar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACR Cryptology ePrint Archive</title>
		<imprint>
			<biblScope unit="page">690</biblScope>
			<date type="published" when="2015">2015. 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Workload Analysis of a Large-scale Key-value Store</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
		<idno type="DOI">10.1145/2254756.2254766</idno>
		<ptr target="https://doi.org/10.1145/2254756.2254766" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS &apos;12)</title>
		<meeting>the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Userspace Packet Processing</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Soldani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Mathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS &apos;15)</title>
		<meeting>the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS &apos;15)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attack of the Killer Microseconds</title>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3015146</idno>
		<ptr target="https://doi.org/10.1145/3015146" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2017-03">2017. March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring the relationship between architectures and management policies in the design of NUCA-based chip multicore systems</title>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bartolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierfrancesco</forename><surname>Foglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosimo</forename><surname>Antonio Prete</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2017.06.001</idno>
		<ptr target="https://doi.org/10.1016/j.future.2017.06.001" />
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="481" to="501" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jigsaw: Scalable software-defined caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2013.6618818</idno>
		<ptr target="https://doi.org/10.1109/PACT.2013.6618818" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 22nd International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic Transformations for Communication-Minimized Parallelization and Locality Optimization in the Polyhedral Model</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compiler Construction</title>
		<editor>
			<persName><forename type="first">Laurie</forename><surname>Hendren</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="132" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenBox: Enabling Innovation in Middlebox Applications</title>
		<author>
			<persName><forename type="first">Anat</forename><surname>Bremler-Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Harchol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hay</surname></persName>
		</author>
		<idno type="DOI">10.1145/2785989.2785992</idno>
		<ptr target="https://doi.org/10.1145/2785989.2785992" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGCOMM Workshop on Hot Topics in Middleboxes and Network Function Virtualization (HotMiddlebox &apos;15)</title>
		<meeting>the 2015 ACM SIGCOMM Workshop on Hot Topics in Middleboxes and Network Function Virtualization (HotMiddlebox &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">100G CWDM4 SMF optical interconnects for facebook data centers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakravarty</surname></persName>
			<affiliation>
				<orgName type="collaboration">CLEO</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schmidtke</surname></persName>
			<affiliation>
				<orgName type="collaboration">CLEO</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giridharan</surname></persName>
			<affiliation>
				<orgName type="collaboration">CLEO</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
			<affiliation>
				<orgName type="collaboration">CLEO</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zeng</surname></persName>
			<affiliation>
				<orgName type="collaboration">CLEO</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conference on Lasers and Electro-Optics</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distance associativity for high-performance energy-efficient non-uniform cache architectures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2003.1253183</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2003.1253183" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 36th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>36th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
	<note type="report_type">MICRO-36</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contention-aware application mapping for Network-on-Chip communication architectures</title>
		<author>
			<persName><forename type="first">Chen-Ling</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.2008.4751856</idno>
		<ptr target="https://doi.org/10.1109/ICCD.2008.4751856" />
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Computer Design</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 architectures optimization reference manual</title>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" />
		<imprint>
			<date type="published" when="2016-05">2016. 2016. May 2018</date>
			<publisher>Intel Coorporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intel Announces Skylake-X: Bringing 18-Core HCC Silicon to Consumers for</title>
		<ptr target="https://bit.ly/2WpZZYx" />
		<imprint>
			<date type="published" when="1999-05">2017. 1999. May 2017</date>
			<publisher>Ian Cutress</publisher>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://www.anandtech.com/show/11550/the-intel-skylakex-review-core-i9-7900x-i7-7820x-and-i7-7800x-tested/" />
		<title level="m">The Intel Skylake-X Review: Core i9 7900X, i7 7820X and i7 7800X Tested</title>
		<imprint>
			<publisher>Ian Cutress</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Online</title>
		<imprint>
			<date type="published" when="2017-06">June 2017. 2019-01-10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online</title>
		<ptr target="https://dpdk.org" />
	</analytic>
	<monogr>
		<title level="m">Data Plane Development Kit (DPDK)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RouteBricks: Exploiting Parallelism to Scale Software Routers</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dobrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Egi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Knies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Manesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<idno type="DOI">10.1145/1629575.1629578</idno>
		<ptr target="https://doi.org/10.1145/1629575.1629578" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles (SOSP &apos;09)</title>
		<meeting>the ACM SIGOPS 22Nd Symposium on Operating Systems Principles (SOSP &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">KPart: A Hybrid Cache Partitioning-Sharing Technique for Commodity Multicores</title>
		<author>
			<persName><forename type="first">N</forename><surname>El-Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2018.00019</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2018.00019" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="104" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://www.iovisor.org/technology/xdp" />
		<title level="m">eXpress Data Path (XDP)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-performance packet processing and measurements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gallenm?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wohlfart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Scheitle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carle</surname></persName>
		</author>
		<idno type="DOI">10.1109/COMSNETS.2018.8328173</idno>
		<ptr target="https://doi.org/10.1109/COMSNETS.2018.8328173" />
	</analytic>
	<monogr>
		<title level="m">2018 10th International Conference on Communication Systems Networks (COMSNETS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quickly Generating Billion-record Synthetic Databases</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Baclawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1145/191839.191886</idno>
		<ptr target="https://doi.org/10.1145/191839.191886" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data (SIGMOD &apos;94)</title>
		<meeting>the 1994 ACM SIGMOD International Conference on Management of Data (SIGMOD &apos;94)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PacketShader: A GPU-accelerated Software Router</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
		<idno type="DOI">10.1145/1851275.1851207</idno>
		<ptr target="https://doi.org/10.1145/1851275.1851207" />
	</analytic>
	<monogr>
		<title level="j">SIGCOMM Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2010-08">2010. Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MegaPipe: A New Programming Interface for Scalable Network I/O</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi12/technical-sessions/presentation/han" />
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12). USENIX</title>
		<meeting><address><addrLine>Hollywood, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="135" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reactive NUCA: Near-optimal Block Placement and Replication in Distributed Caches</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<idno type="DOI">10.1145/1555754.1555779</idno>
		<ptr target="https://doi.org/10.1145/1555754.1555779" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA &apos;09)</title>
		<meeting>the 36th Annual International Symposium on Computer Architecture (ISCA &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CAMA: A Predictable Cache-Aware Memory Allocator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Haupenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reineke</surname></persName>
		</author>
		<idno type="DOI">10.1109/ECRTS.2011.11</idno>
		<ptr target="https://doi.org/10.1109/ECRTS.2011.11" />
	</analytic>
	<monogr>
		<title level="m">2011 23rd Euromicro Conference on Real-Time Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">How to translate virtual to physical addresses through</title>
		<ptr target="http://fivelinesofcode.blogspot.se/2014/03/how-to-translate-virtual-to-physical.html" />
		<imprint>
			<date type="published" when="2014-05">2014. 2014. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ted</forename><surname>Hudek</surname></persName>
		</author>
		<ptr target="https://docs.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to-receive-side-scaling" />
		<title level="m">Introduction to Receive Side Scaling</title>
		<imprint>
			<date type="published" when="2004">2017. 20 04 2017</date>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical Timing Side Channel Attacks against Kernel Space ASLR</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holz</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2013.23</idno>
		<ptr target="https://doi.org/10.1109/SP.2013.23" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="191" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/processors/xeon/scalable/xeon-scalable-uncore-performance-monitoring-manual.html" />
		<title level="m">Intel Xeon Processor Scalable Memory Family Uncore Performance Monitoring</title>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2017-07">2017. July 2017</date>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Online</title>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-ethernet-flow-director.pdf" />
		<imprint>
			<date type="published" when="2014">2014. 2014. 2018-05-22</date>
			<publisher>Intel Ethernet Flow Director and Memcached Performance</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Loop Transformation Algorithm Based on Explicit Data Layout Representation for Optimizing Locality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">F</forename><surname>Prins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanne</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Languages and Compilers for Parallel Computing</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Sehr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pen-Chung</forename><surname>Yew</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Heidelberg</forename><surname>Springer Berlin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="34" to="50" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neighborhoodaware data locality optimization for NoC-based multicores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yemliha</surname></persName>
		</author>
		<idno type="DOI">10.1109/CGO.2011.5764687</idno>
		<ptr target="https://doi.org/10.1109/CGO.2011.5764687" />
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">To Hardware Prefetch or Not to Prefetch?: A Virtualized Environment Study and Core Binding Approach</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1145/2451116.2451155</idno>
		<ptr target="https://doi.org/10.1145/2451116.2451155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;13)</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="357" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metron: NFV Service Chains at the True Speed of the Underlying Hardware</title>
		<author>
			<persName><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Katsikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Kosti?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Steinert</surname></persName>
		</author>
		<author>
			<persName><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<ptr target="https://www.usenix.org/system/files/conference/nsdi18/nsdi18-katsikas.pdf" />
	</analytic>
	<monogr>
		<title level="m">15th USENIX Conference on Networked Systems Design and Implementation (NSDI 18) (NSDI&apos;18). USENIX Association</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SNF: synthesizing high performance NFV service chains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Katsikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Enguehard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Ku?niar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Kosti?</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.98</idno>
		<ptr target="https://doi.org/10.7717/peerj-cs.98" />
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2016-11">2016. Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An Adaptive, Non-uniform Cache Structure for Wire-delay Dominated On-chip Caches</title>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<idno type="DOI">10.1145/635506.605420</idno>
		<ptr target="https://doi.org/10.1145/635506.605420" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2002-10">2002. Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Coordinated Approach for Practical OS-Level Cache Management in Multi-core Real-Time Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kandhalu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ECRTS.2013.19</idno>
		<ptr target="https://doi.org/10.1109/ECRTS.2013.19" />
	</analytic>
	<monogr>
		<title level="m">2013 25th Euromicro Conference on Real-Time Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MICA: A Holistic Approach to Fast In-Memory Key-Value Storage</title>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/lim" />
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingda</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2008.4658653</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2008.4658653" />
	</analytic>
	<monogr>
		<title level="m">2008 IEEE 14th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Last-Level Cache Side-Channel Attacks are Practical</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2015.43</idno>
		<ptr target="https://doi.org/10.1109/SP.2015.43" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="605" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking Memory Management in Modern Operating System: Horizontal, Vertical or Random?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2015.2462813</idno>
		<ptr target="https://doi.org/10.1109/TC.2015.2462813" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1921" to="1935" />
			<date type="published" when="2016-06">2016. June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data Layout Transformation for Enhancing Data Locality on NUCA Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ngai</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2009.36</idno>
		<ptr target="https://doi.org/10.1109/PACT.2009.36" />
	</analytic>
	<monogr>
		<title level="m">2009 18th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="348" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reverse Engineering Intel Last-Level Cache Complex Addressing Using Performance Counters</title>
		<author>
			<persName><forename type="first">Cl?mentine</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Scouarnec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Heen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lien</forename><surname>Francillon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-26362-5_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-26362-5_3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on Research in Attacks, Intrusions, and Defenses</title>
		<meeting>the 18th International Symposium on Research in Attacks, Intrusions, and Defenses<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">9404</biblScope>
			<biblScope unit="page" from="48" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reflections on the Memory Wall</title>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<idno type="DOI">10.1145/977091.977115</idno>
		<ptr target="https://doi.org/10.1145/977091.977115" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Computing Frontiers (CF &apos;04)</title>
		<meeting>the 1st Conference on Computing Frontiers (CF &apos;04)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">162</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online</title>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<ptr target="http://www.mellanox.com/related-docs/prod_software/MLNX_DPDK_Quick_Start_Guide_v16.11_1.5.pdf" />
	</analytic>
	<monogr>
		<title level="m">Mellanox DPDK -Quick Start Guide</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ESP-NUCA: A lowcost adaptive Non-Uniform Cache Architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Merino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Puente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gregorio</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2010.5416641</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2010.5416641" />
	</analytic>
	<monogr>
		<title level="m">HPCA -16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Survey of Techniques for Cache Partitioning in Multicore Processors</title>
		<author>
			<persName><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3062394</idno>
		<ptr target="https://doi.org/10.1145/3062394" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Drilling Down Into The Xeon Skyalke Architecture</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Prickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename></persName>
		</author>
		<ptr target="https://www.nextplatform.com/2017/08/04/drilling-xeon-skylake-architecture/" />
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Mulnix</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview" />
		<title level="m">Intel Xeon Processor Scalable Family Technical Overview</title>
		<imprint>
			<date type="published" when="2017-09">2017. Sep 2017</date>
			<biblScope unit="page" from="2018" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">What is the potential impact of PAUSE</title>
		<author>
			<persName><surname>Netapp</surname></persName>
		</author>
		<ptr target="https://ntap.com/2RpAx1Q" />
		<imprint>
			<date type="published" when="2017-11">2017. Nov 2017</date>
			<publisher>Online</publisher>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding PCIe Performance for End Host Networking</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Fernando Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>L?pez-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1145/3230543.3230560</idno>
		<ptr target="https://doi.org/10.1145/3230543.3230560" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;18)</title>
		<meeting>the 2018 Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Khang</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology" />
		<title level="m">Introduction to Cache Allocation Technology in the Intel Xeon Processor E5 v4 Family</title>
		<imprint>
			<date type="published" when="2016-02">2016. Feb 2016</date>
			<biblScope unit="page" from="2018" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openonload</surname></persName>
		</author>
		<ptr target="http://www.openonload.org" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Intel Data Direct I/O Technology Overview</title>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/data-direct-i-o-technology-overview-paper.pdf" />
		<imprint>
			<date type="published" when="2012">2012. 2012. 2018-05-22</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Online</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Paoloni</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf" />
		<imprint>
			<date type="published" when="2010-05">2010. 2010. May 2018</date>
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250662.1250709</idno>
		<ptr target="https://doi.org/10.1145/1250662.1250709" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture (ISCA &apos;07)</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture (ISCA &apos;07)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="381" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Novel Framework for Fast Packet I/O</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc12/technical-sessions/presentation/rizzo" />
	</analytic>
	<monogr>
		<title level="m">2012 USENIX Annual Technical Conference (USENIX ATC 12). USENIX Association</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Software-Defined &quot;Hardware&quot; Infrastructures: A Survey on Enabling Technologies and Open Research Directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roozbeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wuhib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Padala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Yadhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosti?</surname></persName>
		</author>
		<idno type="DOI">10.1109/COMST.2018.2834731</idno>
		<ptr target="https://doi.org/10.1109/COMST.2018.2834731" />
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys Tutorials</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2454" to="2485" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Rupp</surname></persName>
		</author>
		<ptr target="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/" />
		<title level="m">42 Years of Microprocessor Trend Data</title>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2018-02-15">2018. 15 February 2018</date>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beyond Softnet</title>
		<author>
			<persName><forename type="first">Jamal</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kuznetsov</surname></persName>
		</author>
		<ptr target="http://www.usenix.org/publications/library/proceedings/als01/full_papers/jamal/jamal.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Linux Showcase</title>
		<meeting>the 5th Annual Linux Showcase<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A Software Cache Partitioning System for Hash-Based Caches</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Scolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><forename type="middle">Basilio</forename><surname>Bartolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Domenico</forename><surname>Santambrogio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018113</idno>
		<ptr target="https://doi.org/10.1145/3018113" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2016-12">2016. Dec. 2016</date>
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Seaborn</surname></persName>
		</author>
		<idno>Online; accessed 2018-05-09</idno>
		<ptr target="http://lackingrhoticity.blogspot.se/2015/04/l3-cache-mapping-on-sandy-bridge-cpus.html" />
		<title level="m">L3 cache mapping on Sandy Bridge CPUs</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Design and Implementation of a Consolidated Middlebox Architecture</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Vyas Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Egi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2228298.2228331" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI&apos;12). USENIX Association</title>
		<meeting>the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI&apos;12). USENIX Association<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="24" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Application Clustering Policies to Address System Fairness with Intel&apos;s Cache Allocation Technology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Selfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2017.19</idno>
		<ptr target="https://doi.org/10.1109/PACT.2017.19" />
	</analytic>
	<monogr>
		<title level="m">2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Application clustering policies to address system fairness with Intel&apos;s cache allocation technology</title>
		<author>
			<persName><forename type="first">Selfa</forename><surname>Vicent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">E</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><surname>Gomez</surname></persName>
		</author>
		<idno type="DOI">10.1109/pact.2017.19</idno>
		<ptr target="http://dx.doi.org/10.1109/pact.2017.19" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Parallel Architectures and Compilation Techniques (PACT). IEEE</title>
		<meeting>the IEEE International Conference on Parallel Architectures and Compilation Techniques (PACT). IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Farewell My Shared LLC! A Case for Private Die-Stacked DRAM Caches for Servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2018.00052</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2018.00052" />
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="559" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reducing Cache Misses Using Hardware and Software Page Placement</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1145/305138.305189</idno>
		<ptr target="https://doi.org/10.1145/305138.305189" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Supercomputing (ICS &apos;99)</title>
		<meeting>the 13th International Conference on Supercomputing (ICS &apos;99)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Hardware-Level Performance Analysis of Platform I</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://dpdkprcsummit2018.sched.com/event/EsPa/hardware-level-performance-analysis-of-platform-io." />
		<imprint>
			<date type="published" when="2018-06">2018. June 2018</date>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">NFP: Enabling Network Function Parallelism in NFV</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3098822.3098826</idno>
		<ptr target="https://doi.org/10.1145/3098822.3098826" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;17)</title>
		<meeting>the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Understanding write-through, write-around and write-back caching</title>
		<author>
			<persName><forename type="first">Shahriar</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<ptr target="https://bit.ly/2CUMaIE" />
		<imprint>
			<date type="published" when="2017-08-20">2017. 20 August 2017</date>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
	<note>with Python</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">ResQ: Enabling SLOs in Network Function Virtualization</title>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Amin Tootoonchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Shenker</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/nsdi18/presentation/tootoonchian" />
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). USENIX Association</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="283" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Shane</forename><surname>Tully</surname></persName>
		</author>
		<ptr target="http://shanetully.com/2014/12/translating-virtual-addresses-to-physcial-addresses-in-user-space/" />
		<title level="m">Translating Virtual Addresses to Physical Addresses in User Space</title>
		<imprint>
			<date type="published" when="2014-05">2014. 2014. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Cache Allocation Technology: A Telco&apos;s NFV Noisy Neighbor Experiments</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Veitch</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/cache-allocation-technology-telco-nfv-noisy-neighbor-experiments" />
		<imprint>
			<date type="published" when="2017-08">2017. Aug 2017</date>
			<publisher>Online</publisher>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Performance evaluation of cache allocation technology for NFV noisy neighbor mitigation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Curley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kantecki</surname></persName>
		</author>
		<idno type="DOI">10.1109/NETSOFT.2017.8004214</idno>
		<ptr target="https://doi.org/10.1109/NETSOFT.2017.8004214" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Network Softwarization (NetSoft)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Disclosure of h/w prefetcher control on some intel processors</title>
		<author>
			<persName><surname>Vish Viswanathan</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/disclosure-of-hw-prefetcher-control-onsome-intel-processors" />
		<imprint>
			<date type="published" when="2014-05">2014. 2014. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Futility Scaling: High-Associativity Cache Partitioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.46</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.46" />
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">SWAP: Effective Fine-Grain Management of Shared Last-Level Caches with Minimum Hardware Support</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2017.65</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2017.65" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cache Access Fairness in 3D Mesh-Based NUCA</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2862633</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2862633" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="42984" to="42996" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><surname>Skylake</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)" />
		<imprint>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
	<note type="report_type">server) -Microarchitectures -Intel</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Mellanox NIC&apos;s Performance Report with DPDK 17.05</title>
		<ptr target="http://fast.dpdk.org/doc/perf/DPDK_17_05_Mellanox_NIC_performance_report.pdf" />
	</analytic>
	<monogr>
		<title level="m">Mellanox Technologies</title>
		<imprint>
			<date type="published" when="2017-05">2017. 2017. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Henry</forename><surname>Wong</surname></persName>
		</author>
		<ptr target="http://blog.stuffedcow.net/2013/01/ivb-cache-replacement." />
		<title level="m">Intel Ivy Bridge Cache Replacement Policy</title>
		<imprint>
			<date type="published" when="2013-01">2013. jan 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">DCAPS: Dynamic Cache Allocation with Partial Sharing</title>
		<author>
			<persName><forename type="first">Yaocheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3190508.3190511</idno>
		<ptr target="https://doi.org/10.1145/3190508.3190511" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference (EuroSys &apos;18)</title>
		<meeting>the Thirteenth EuroSys Conference (EuroSys &apos;18)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">dCat: Dynamic Cache Management for Efficient, Performance-sensitive Infrastructure-as-a-service</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthick</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3190508.3190555</idno>
		<ptr target="https://doi.org/10.1145/3190508.3190555" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference (EuroSys &apos;18)</title>
		<meeting>the Thirteenth EuroSys Conference (EuroSys &apos;18)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Dynamic Cache Management Using CAT Virtualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/RTAS.2017.15</idno>
		<ptr target="https://doi.org/10.1109/RTAS.2017.15" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Mapping the Intel Last-Level Cache</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruby</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
		<ptr target="https://eprint.iacr.org/2015/905" />
	</analytic>
	<monogr>
		<title level="j">Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="2015">2015. 2015/905. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">COLORIS: A dynamic cache partitioning system using page coloring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/2628071.2628104</idno>
		<ptr target="https://doi.org/10.1145/2628071.2628104" />
	</analytic>
	<monogr>
		<title level="m">2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">OpenNetVM: A Platform for High Performance Network Service Chains</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lopreiato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Gregoire Todeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Wood</surname></persName>
		</author>
		<ptr target="https://doi.org/2940147.2940155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Workshop on Hot Topics in Middleboxes and Network Function Virtualization (HotMIddlebox &apos;16)</title>
		<meeting>the 2016 Workshop on Hot Topics in Middleboxes and Network Function Virtualization (HotMIddlebox &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A data layout optimization framework for NUCA-based multicores</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
