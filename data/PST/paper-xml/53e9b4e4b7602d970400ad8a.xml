<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Don&apos;t be SCAREd: Use SCalable Automatic REpairing with Maximal Likelihood and Bounded Changes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Yakout</surname></persName>
							<email>myakout@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laure</forename><surname>Berti-Équille</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institut de Recherche pour le Développement</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
							<email>aelmagarmid@qf.org.qa</email>
							<affiliation key="aff2">
								<address>
									<country>Qatar Computing Research Institute</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Don&apos;t be SCAREd: Use SCalable Automatic REpairing with Maximal Likelihood and Bounded Changes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45EB9F0F529EBF0BDFA3B656979A5F65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>2</term>
					<term>8 [Database Applications]: Data Mining; H</term>
					<term>4 [Information Systems Applications]: Miscellaneous data cleaning, inconsistent data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to identify errors and, when possible, correct them. However, these approaches have several limitations including the scalability and quality of the values to be used in replacement of the errors. In this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data given the data distribution, which can be modeled using statistical machine learning techniques. This is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value modification. We develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database. We propose SCARE (SCalable Automatic REpairing), a systematic scalable framework that follows our approach. SCARE relies on a robust mechanism for horizontal data partitioning and a combination of machine learning techniques to predict the set of possible updates. Due to data partitioning, several updates can be predicted for a single record based on local views on each data partition. Therefore, we propose a mechanism to combine the local predictions and obtain accurate final predictions. Finally, we experimentally demonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets in comparison to recent data cleaning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data quality experts estimate that erroneous data can cost a business as much as 10 to 20% of its total system implementation budget <ref type="bibr" target="#b7">[7]</ref>. They agree that as much as 40 to 50% of a project budget might be spent correcting data errors in time-consuming, labor-intensive and tedious processes. The proliferation of data also heightens the relevance of data cleaning and makes the prob-lem more challenging: more sources and larger amounts of data imply larger variety and intrication of the data quality problems and higher complexity for maintaining the quality of the data in a cost-effective way. As a result, various computational procedures for data cleaning have been proposed by the database community to (semi-)automatically identify errors and, when possible, correct them.</p><p>Most existing solutions to repair dirty databases by value modification follow constraint-based repairing approaches <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18]</ref>, which search for minimal change of the database to satisfy a predefined set of constraints. While a variety of constraints (e.g., integrity constraints, conditional functional and inclusion dependencies) can detect the presence of errors, they are recognized to fall short of guiding to correct the errors; and worse, may introduce new errors when repairing the data <ref type="bibr" target="#b10">[10]</ref>. Moreover, despite the research conducted on integrity constraints to ensure the quality of the data, in practice, databases often contain a significant amount of non-trivial errors. These errors, both syntactic and semantic, are generally subtle mistakes which are difficult or even impossible to detect and express using the general types of constraints available in modern DBMSs <ref type="bibr" target="#b20">[20]</ref>. This highlights the need for different techniques to clean dirty databases.</p><p>In this paper, we address the issues on scalability and accuracy of replacement values by leveraging Machine Learning (ML) techniques for predicting better quality updates to repair dirty databases. Our proposed approach is complementary to existing efforts that leverage reference data <ref type="bibr" target="#b10">[10]</ref> and user's interaction <ref type="bibr" target="#b22">[22]</ref>.</p><p>Statistical ML techniques (e.g., decision tree, Bayesian networks) can capture dependencies, correlations, and outliers from datasets based on various analytic, predictive or computational models <ref type="bibr" target="#b23">[23]</ref>. Existing efforts in data cleaning using ML techniques mainly focused on data imputation (e.g., <ref type="bibr" target="#b20">[20]</ref>) and deduplication (e.g., <ref type="bibr" target="#b6">[6]</ref>). To the best of our knowledge, our work is the first approach to consider ML techniques for repairing databases by value modification.</p><p>Involving ML techniques for repairing erroneous data is not straightforward and it raises four major challenges: (1) Several attribute values (of the same record) may be dirty. Therefore, the process is not as simple as predicting values for a single erroneous attribute. This requires accurate modeling of correlations between the database attributes, which assumes that a subset is dirty and its complement is reliable. (2) A ML technique can predict an update for each tuple in the database; and the question is how to distinguish the predictions that should be applied. Therefore, a measure to quantify the quality of the predicted updates is required. (3) An over-fitting problem may occur when modeling a database with a large variety of dependencies that may hold locally for data subsets but do not hold globally. (4) Finally, the process of learning a model from a very large database is expensive, and the prediction model itself may not fit in the main memory. Despite the existence of scalable ML techniques for large datasets, they are either model dependent (i.e., limited to specific models, for example SVM <ref type="bibr" target="#b21">[21]</ref>) or data dependent (e.g., limited to specific types of datasets such as scientific data and documents repository). What is more, scalability is also an issue for constraint-based repairing approaches <ref type="bibr" target="#b8">[8]</ref>.</p><p>Such limitations motivate the need for effective and scalable methods to accurately predict cleaning updates with statistical guarantees. Precisely in this paper, our contributions can be summarized as follows:</p><p>• We formalize a novel data repairing approach that maximizes the likelihood of the data given the underline data distribution, which can be modeled using statistical ML techniques.</p><p>The objective is to apply selected database updates that (i) will best preserve the relationships among the data values; and (ii) will introduce a small amount of changes. This approach enables a variety of ML techniques to be involved for the purpose of accurately repairing dirty databases by value modification. This way we eliminate the necessity to predefine database constraints, which requires the added expense of experts involvement. In contrast to the constraintbased data repair approaches, which find the minimum number of changes to satisfy a set of constraints, our likelihoodbased repair approach finds the bounded amount of changes to maximize the data likelihood.</p><p>• One of the challenges is that multiple attributes values may be considered dirty. Therefore, we introduce a technique to provide predictions for multiple attributes at a time, while taking into account two types of dependencies: (i) the dependency between the identified clean attributes and dirty attributes; as well as, (ii) the dependency among the dirty attributes themselves. We present our technique by introducing the probabilistic principles which it relies upon.</p><p>• We propose SCARE (SCalable Automatic REpairing), a systematic scalable framework for repairing erroneous values that follows our approach and, more importantly, it is scalable for very large datasets. SCARE has a robust mechanism for horizontal data partitioning to ensure the scalability and enable parallel processing of data blocks; various ML methods are applied to each data block to model attributes values correlations and provide "local" predictions. We then provide a novel mechanism to combine the local predictions from several data partitions. The mechanism computes the validity of the predictions for the individual ML models and takes into account the models' reliability in terms of minimizing the risk of wrong predictions, as well as, the significance of partitions' sizes used in the learning stage. Finally, given several local predictions for repairing a tuple, we incorporate these predictions into a graph optimization problem, which captures the associations between the predicted values across the partitions and obtain more accurate, final tuple repair predictions.</p><p>• We present an extensive experimental evaluation to demonstrate the effectiveness, efficiency, and scalability of our approach on very large real-world datasets.</p><p>The rest of the paper is organized as follows: Section 2 defines the problem and introduces the notion of maximal likelihood repair. Section 3 presents our solutions for modeling dependencies and predicting accurate replacement values. Section 4 presents SCARE, our scalable solution to repair the data. We demonstrate the validity of our approach and experimental results in terms of efficiency and scalability in Section 5. We discuss related work in Section 6 and conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DEFINITION AND SOLU-TION APPROACH</head><p>In this section, we formalize our maximal likelihood repair problem and introduce our solution approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>We consider a database instance D over a relation schema R with A denoting its set of attributes. The domain of an attribute A ∈ A is denoted by dom(A).</p><p>In the relation R, a set F = {E1, . . . , EK } ∈ A represents the flexible attributes, which are allowed to be modified (in order to substitute the possibly erroneous values), and the other attributes R = A -F = {C1, . . . CL} are called reliable with correct values. Hence, a database tuple t has two parts: the reliable part (t[R] = t[C1, . . . CL]), and the flexible part (t[F ] = t[E1, . . . EK ]). For short we refer to t[R] and t[F ] as r and f , respectively (i.e., t = rf ). Note that the detection of the erroneous records is not the scope of our paper. We assume that it is possible to identify a subset Dc ⊂ D of clean (or correct) tuples and De = D -Dc represents the remaining possibly dirty tuples. This distinction does not have to be accurate in specifying the dirty records, but it should be accurate in specifying the clean records. Our objective is to learn from the correct tuples in Dc to predict accurate replacement values for the possibly dirty tuples in De.</p><p>There are various techniques to distinguish Dc as it is always possible to use reference data and existing statistical techniques (e.g., <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23]</ref>), as well as, database constraints (if available) to provide a score Pe(t) ∈ [0..1] for each database tuple t for being erroneous. Applying a conservative threshold on the scores of each tuple Pe(t), we can select high quality records to be used for training.</p><p>Example 1: Consider the example relation in Figure <ref type="figure">1</ref> with a sample of 8 tuples about some personal information: Name, Institution, area code AC, telephone number Tel, in addition to address information: City, State and Zip. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Illustrative example</head><p>This data is a result of integrating professional contact information and lookup address database. Due to the integration process, we know that some of the address attributes (City, State and Zip) may contain errors. Therefore, we call the address attributes flexible attributes. After the integration process, we could separate high quality records by consulting other reference data or verifying some widely known relationships among the attributes. For this example, tuples t5, . . . , t8 ∈ Dc are identified as correct ones, while we are not sure about tuples t1, . . . , t4 ∈ De.</p><p>We introduce the data repair likelihood given the data distribution as a technique to guide the selection of the updates to repair the dirty tuples. Our approach is different from the maximum likelihood approach to learn a ML model parameter. In that case, the model's parameters and the captured distribution changes according to the observed data. In our case instead, we learn initially the data distribution from the set of clean tuples; and then, we change the dirty tuples carefully to maximize the data agreement with the learnt distribution. Our hypothesis is that the more the update will make the data follows the underline data distribution with least cost, the more likely the update to be correct.</p><p>Note that the tuples usually come to exist in the database independently (e.g., new customer records are inserted in the database independent from other customers information); errors in the tuples can be detected because of the existing values correlations among the database attributes that hold across the tuples. Such values correlations can be learnt using a ML model; and hence, the correctness probability of the tuples is conditionally independent given the learnt model. Consequently, the likelihood of the database D is the product of the tuples' probabilities given a probability distribution for the tuples in the database. Given the identified clean subset of the database Dc, we can model the probability distribution P (R, F ). Then, the likelihood of the possibly erroneous subset De can be written (as log likelihood):</p><formula xml:id="formula_0">L(De|Dc) = ∑ t∈De log P (t | Dc) = ∑ t=rf ∈De log P (f | r) (1)</formula><p>where we use P (t | Dc) = P (f | r), which we discuss in Section 3.</p><p>Assuming for a given tuple t = rf a ML technique predicted f ′ instead of f . We say that the update u is predicted to replace f by f ′ . Applying u to the database will change the likelihood of the data; we call the amount of increase in the data likelihood given the data distribution as the likelihood benefit of u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 1. Likelihood benefit of an update u (l(u)):</head><p>Given a database D = Dc ∪ De, t = rf ∈ De and an update u to replace f by f ′ , the likelihood benefit of u is the increase in the database likelihood given the data distribution learnt from Dc, or (L(D u e |Dc) -L(De|Dc)), where D u e refers to De when the update u is applied. Using Eq. 1 we obtain:</p><formula xml:id="formula_1">l(u) = log P (f ′ | r) -log P (f | r).</formula><p>(</p><p>We also define the cost of an update as follows:</p><p>DEFINITION 2. Cost of an update u (c(u)): For a given database tuple t = rf and an update u to replace f by f ′ , the cost of u is the distance between f and f ′ ,</p><formula xml:id="formula_3">c(u) = ∑ E∈F dE(f [E], f ′ [E])<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">dE(f [E], f ′ [E]</formula><p>) is a distance function for the value domain of attribute E that returns a score between 0 and Our objective is to modify the data to maximize its likelihood; however, and similar to existing repairing approaches, we need to be conservative in modifying the data. Therefore, we bound the amount of changes introduced to the database by a parameter δ. Hence, the problem becomes: given an allowed amount of changes δ, how do we best select the cleaning updates from all the predicted updates? This is a constrained maximization problem where the objective is to find the updates that maximizes the likelihood value under the constraint of a bounded amount of database changes, δ. We call this problem the "Maximal Likelihood Repair". DEFINITION 3. Maximal Likelihood Repair: Given a scalar δ and a database D = De ∪ Dc. The Maximal Likelihood Repair problem is to find another database instance</p><formula xml:id="formula_5">D ′ = D ′ e ∪ Dc, such that L(D ′ e | Dc) is maximum subject to the constraint Dist(D, D ′ ) ≤ δ.</formula><p>where Dist is a distance function between the two database instances D and D ′ before and after the repairing and it can be defined as</p><formula xml:id="formula_6">Dist(D, D ′ ) = ∑ ∀t∈D,A∈A dA(t[A], t ′ [A]</formula><p>), where t ′ ∈ D ′ is the repaired tuple corresponding to tuple t ∈ D.</p><p>Note that δ can take values from 0 (i.e., no changes to the data is allowed) to |De| × |F | (i.e., change all flexible attributes of the dirty tuples). Regarding δ estimation, it is possible to use the score Pe(t), which estimates the erroneousness of tuple t, to estimate δ = ϵ ∑ t∈De Pe(t), where ϵ ∈ [0..1]. The idea is that a possibly erroneous tuple is expected to be modified according to its score of being erroneous. ϵ can be chosen close to zero to be more conservative to the amount of introduced changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solution Approach</head><p>For each tuple t = rf , we obtain the prediction f ′ that represents an update u to t. We compute the likelihood benefit and cost of u. Finally, we need to find the subset of updates that maximizes the overall likelihood subject to the constraint that the total cost is not more than δ, i.e., Dist(D, D ′ ) ≤ δ.</p><p>Formally, given a set U of updates and, for each update u, we compute l(u) and c(u) using Eq. 2 and 3, respectively. Our goal is to find the set of updates U ′ ⊆ U, such that:</p><formula xml:id="formula_7">∑ ∀u∈U ′ l(u) is maximum subject to: ∑ ∀u∈U ′ c(u) ≤ δ. (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>This is typically a 0/1 knapsack problem setting, which implies that the maximal likelihood repair problem is NP-complete.</p><p>Heuristic and quality measure: To solve the above problem, we use the famous heuristic to solve the 0/1 knapsack problem by processing the updates in decreasing order of the ratio l(u) c(u) . This heuristic suggests that the "correctness measure" of an update u is the ratio of the update's likelihood benefit to the cost of applying the update to the database (i.e., the higher the likelihood benefit with small cost, the more likely the update to be correct). Empirically, this gives good predictions for the updates as we will illustrate in our experiments. Example 2: In Figure <ref type="figure">1</ref>, assume that two updates were predicted to the database. u1 updates t3 such that f ′ 3 ={"Chicago", "IL", "60614"} and u2 updates t4 such that f ′ 4 ={"WLafayette", "IN", "47907"}. Assume also that both of l(u1), l(u2) have the same likelihood benefit. In this case, u2 will encounter lower cost in updating one character in both the Zip and the City attributes (update the Zip from"47906" to "47907" and the City from "Lafayette" to "WLafayette"), while u1 will cost updating 4 characters in the Zip from "61801" to "60614". Hence, for δ ≤ 2 characters, only u2 will be applied to the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODELING DEPENDENCIES AND PREDICTING UPDATES</head><p>The key challenge when considering data repair using the data distribution is that multiple attributes values may be dirty. In the case when a single attribute is erroneous, the problem is to model the conditional probability distribution of the erroneous attribute given the other attributes; and hence, a single classification model can be used to obtain the predicted values for the erroneous attribute. However, it is mostly the case that a set of attributes have low quality values and not a single attribute. Therefore, we need to model the probability distribution of the subset of dirty attributes given the other attributes that have reliable values (i.e., most likely to be correct) to achieve a better prediction of the replacement values.</p><p>Example 3: In Figure <ref type="figure">1</ref> assuming we know that only the City attribute contains some errors. This is the simple case because a ML model can be trained by the database tuples considering the City as the label to be predicted. However in practice, more than one attribute can be dirty at the same time, for example, all the address attributes. In this case, we need a ML technique to model the distribution of the combination (City, State, Zip)-taking into account their possible inter-dependencies-given existing reliable values of attributes, e.g., (Name, Institution, AC, Tel). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Dependencies</head><formula xml:id="formula_9">Let SR = dom(C1) × dom(C2) • • • × dom(CL)</formula><formula xml:id="formula_10">PE i (ei|r) = ∑ f ∈S F |f [E i ]=e i P (f | r)</formula><p>Note that the posterior probability distribution P (F | r) provides the means to analyze the dependencies among the flexible attributes. The distribution informs about the probability of each combination of values for the flexible attributes ⟨e1, . . . , eK ⟩, where e1 ∈ dom(E1), . . . , eK ∈ dom(EK ).</p><p>Given a database tuple t = rf , the conditional probability of each combination of the flexible attribute values f can be computed using the product rule:</p><formula xml:id="formula_11">P (f | r) = P (f [E1] | r) K ∏ i=2 P (f [Ei] | r, f [E1 . . . Ei-1]). (5)</formula><p>Note that we assume a particular order in the dependencies among the flexible attributes {E1, . . . , EK }. To obtain this order, we leverage an existing technique <ref type="bibr" target="#b14">[14]</ref> to construct a dependency network for the database attributes. The dependency network is a graph with the database attributes as the vertices; and there is a directed edge from Ai to Aj if the analysis determined that Aj depends on Ai. In our case, there will be two sets of vertices; the reliable set R and flexible set F . The first flexible attribute E1 in the order is the one that has the maximum number of reliable attributes as its parents in the graph. Then subsequently, the next attribute in order i is the one with maximum number of parents that are either reliable attributes or flexible attributes with an assigned order. In our experiments, we followed this procedure by analyzing a sample of the database to determine the dependency order of the flexible attributes. Another alternative method to compute the conditional probability P (f | r) without considering any particular order of the flexible attributes is to use Gibbs sampling <ref type="bibr" target="#b13">[13]</ref>; however, it is very expensive to be applied to even moderate size databases. Please refer to <ref type="bibr" target="#b14">[14]</ref> for further details.</p><p>In Section 3.2, we introduce an efficient way to obtain the predictions f ′ that is desirable for our cleaning approach and compute the conditional probabilities P (f | r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Predicting Updates</head><p>We use a ML model M (as predictor) to model the above joint distribution in Eq. 5. The model M is a mapping SR → SF that assigns (or predicts) the flexible attributes values f ′ for a database tuple t = rf given the values r of the reliable attributes R. The prediction takes the form:</p><formula xml:id="formula_12">M(r) = ⟨M1(r), . . . , MK (r)⟩ = f ′ .</formula><p>To estimate the joint distribution of the flexible attribute values, P (f | r) in Eq. 5, we learn K classification models Mi(•) on the input space SR × dom(E1) × • • • × dom(Ei-1), (i.e., using all the</p><formula xml:id="formula_13">Algorithm 1 GetPredictions(Classification Model Mi, ⟨r, f [E1], . . . f [Ei-1]⟩ input tuple ri, Probability P , Database Tuple t = rf ) 1: if (i &gt; K) then 2: f ′ = ri -r 3: AllPredictions = AllPredictions ∪{(f ′ , P )} 4: return 5: end if 6: fE i = Mi(ri) 7: rs = ⟨ri, fE i ⟩ 8: Ps = P × P (fE i | ri) 9: GetPredictions(Mi+1, rs, Ps, t) 10: if fE i ̸ = t[Ei] then 11:</formula><p>r ′ s = ⟨ri, t[Ei]⟩ {Adding the original Ei's value to the next input} 12:</p><formula xml:id="formula_14">P ′ s = P × P (t[Ei] | ri) 13:</formula><p>GetPredictions(Mi+1, r ′ s , P ′ s , t) {Predicting attribute Ei's value} 14: end if reliable attributes and the flexible attributes up to attribute Ei).</p><formula xml:id="formula_15">Mi : SR × dom(E1) × • • • × dom(Ei-1) → dom(Ei)</formula><p>We assume that Mi is a probabilistic classifier (e.g., Naïve Bayesian) that will be trained using Dc and produce a probability distribution over the values of the flexible attribute Ei given ⟨r,</p><formula xml:id="formula_16">f [E1], . . . , f [Ei-1]⟩.</formula><p>One efficient greedy way to approximate the optimal prediction f ′ is to proceed as follows: given a tuple t = rf , the classifier M1 is used to predict the value of attribute E1 (i.e., f ′ [E1]) given r. Then, M2 predicts the value for attribute E2 given r and f ′ [E1] as input. Proceeding in this way, Mi predicts the value of attribute Ei given r and</p><formula xml:id="formula_17">f ′ [E1] . . . f ′ [Ei-1]</formula><p>. This approach can be considered as searching greedily for a path in a tree that has the possible values of f ′ ∈ SF at the leaves. We call this tree as the flexible attributes values search tree. Needless to say this approach does not guarantee finding the prediction f ′ with the highest probability.</p><p>For a tuple t, to find a better prediction that is desired for our cleaning approach, we follow the conservative assumption in updating the database by considering and preferring the original attributes values in the tuple. Based on this assumption the best prediction will be among these explored tree paths, which involve the original values of the tuple t. Hence, we can compute, in addition to the greedy path, additional paths that assume that the original values in the tuple are the supposed predictions. The algorithm to compute a set of predictions for the flexible attributes F for a given tuple t is described in Algorithm 1, GetPredictions. Basically, GetPredictions proceeds recursively in the flexible attributes values search tree. At each node tree level i, two branches are considered when the prediction of attribute Ei is different from its original value in the tuple, otherwise, a single branch is considered. The initial call to Algorithm 1 to get predictions for tuple t = rf is GetPredictions(M0, r, 1.0, t = rf ).</p><p>In Algorithm 1, Line 1 checks if we reached the prediction of the last flexible attribute EK ; and in this case, we add the flexible part f ′ of the obtained final tuple s to AllPredictions list. In Line 6, we predict the value fE i of attribute Ei. Lines 7 and 8 compose the new input rs by adding fE i to ri and compute the prediction probability so far, Ps. We then proceed recursively to get the prediction for the next flexible attribute E (i+1) . The lines 11-13 are executed if the predicted value fE i is different from the original value t <ref type="bibr">[Ei]</ref>. In this case, we compose another input r ′ s using the original value t[Ei] and compute the prediction probability so far using P (t[Ei] | ri) from the model Mi, then finally, proceed recursively to get a prediction for E (i+1) .</p><p>Example 4: Consider the example relation in Figure <ref type="figure">1</ref>. Assume that tuple t4 was marked as erroneous and we want to obtain predictions for its flexible attributes. In GetPredictions initially ri is the reliable attributes values {"C. Clifton", "Purdue Univ.", "765", "494-6005"}. In Line 6 the classifier M0, which was trained using only the set of reliable attributes to predict the first flexible attribute City, provides the prediction to be "WLafayette". Then rs is composed to be the input to the next classifier M1, which was trained by the reliable attributes and the first flexible attribute City to predict the second flexible attribute State, rs = {"C. Clifton", "Purdue Univ.", "765", "494-6005", "WLafayette"}. Since the predicted City is different from the one in the table, we compose another input to the classifier M1 with the original City value, r ′ s = {"C. Clifton", "Purdue Univ.", "765", "494-6005", "Lafayette"}. We obtain the prediction for the State given the two inputs rs and r ′ s and proceed recursively until we used M3 to predict the Zip, and we finally extract f ′ from each ri in Line 2 to end up with the list of AllPredictions.</p><p>GetPredictions produces, for a given tuple t, at most 2 K predictions with their probabilities; however, in practice, the number of predictions are far less than 2 K . We select the prediction f ′ with the best benefit-cost ratio, i.e., the update u that replaces f with f ′ and results in the highest l(u) c(u) . Note that we need to compute l(u) for only f ′ with P (f ′ | r) being greater than P (f | r), the probability of the original values, otherwise the likelihood benefit of the predicted update will be negative. Note also that P (f | r) is included as well in the output of GetPredictions.</p><p>In Section 4, we present the method to scale up the maximal likelihood repair problem and get the predicted updates u along with their likelihood benefit l(u). The cost c(u) is straight forward to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SCALING UP THE MAXIMAL LIKELI-HOOD REPAIRING APPROACH</head><p>One of the key challenges in repairing dirty databases is the scalability <ref type="bibr" target="#b8">[8]</ref>. In our case, the scalability issue is mainly due to learning a set of classification models to predict the flexible attributes values. The learning process in most ML techniques is known to be at least quadratic in the database size and the model itself may not fit in main memory. Indeed, there are efforts on learning from large scale datasets (e.g., scalable learning using SVM <ref type="bibr" target="#b21">[21]</ref>). However, there is no much efforts in learning from large databases with most of attributes are string attributes and the number of correlations is large because of the large domain size of each database attribute.</p><p>In this section, we present a model-independent method to learn and predict updates to the database that is based on horizontally partitioning the database. Each database tuple will be a member of several partitions (or blocks). Each partition b is processed to provide predictions to the erroneous tuples t ∈ b depending on the database distribution learnt from block b (i.e., local predictions). Finally, we present a novel mechanism to combine the local predictions from the different partitions and determine more accurate final predictions.</p><p>This method is in the flavor of learning ensemble models <ref type="bibr" target="#b5">[5]</ref> or committee-based approaches, where the task is to predict a single class attribute by partitioning the dataset into several smaller partitions; then a model is trained by each data partition. For a given tuple, each model provide a prediction on the class attribute, and the final prediction is the one with the highest aggregated prediction probability. But, in our case, we want to predict the values of multiple flexible attributes together; and we are not limited to predict a single attribute value. Hence for a given tuple, we obtain a prediction (a combination f ′ of the flexible attribute values) from each data partition. We then propose a technique to combine the models' predictions into a graph optimization problem to find the final prediction for the flexible attributes. Our main insight is that For the update u to change f to f ′ , compute the likelihood measure l(u) if f ̸ = f ′ . 13: end for the final (combinations of) predicted values are those which would maximize the associations among the predicted values across the partitions. Our mechanism to collect and incorporate the predicted updates takes into account the reliability of the learnt classification models themselves to minimize the risk of the predicted updates.</p><p>After obtaining the final predicted values with their likelihood benefit l(u), we use them into the maximal likelihood repair problem (Eq. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Process Overview</head><p>Algorithm 2 illustrates the main steps of the SCARE process to get the predicted updates along with their likelihood benefit. The primary input to the framework is a database instance D. The second input is a set of database partitioning functions (or criteria) H = {h1, . . . , hJ }.</p><p>There are two main phases for SCARE: (1) Updates generation phase (lines 1-8), and (2) Tuple repair selection phase (lines 9-13).</p><p>In Phase 1 (Line 1), each function hj ∈ H will partition D into blocks {b1j, b2j, . . . }. Then, the loop in lines 2-8 processes each block bij as follows: (i) Learn the set of classifiers Mij from the identified clean tuples in bij (lines 3); (ii) Use Mij to predict the flexible attributes values for the possibly erroneous tuples in bij using Algorithm 1 (lines 4-7). For each tuple, the prediction is considered a candidate tuple repair and it is stored in a temporary repair storage, denoted as RS. Since each tuple will be a member of several data partitions, we will end up with a set of candidate tuple repairs for each possibly erroneous tuple. The details of the repair generation is provided in Section 4.2.</p><p>Phase 2 (lines 9-13) loops on each tuple t ∈ De and retrieves all its candidate tuple repairs from the repair storage RS, then uses Algorithm 3 SelectTupleRepair to get the final tuple repair (update) with its estimated likelihood benefit. The details of the repair selection algorithm is provided in Section 4.3. Note that each iteration in Phase 1 does not depend on other iterations (similarly for the iterations of Phase 2). Hence, SCARE can be efficiently parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Repair Generation Phase</head><p>In this phase, the data is partitioned, as we will explain shortly, for two main benefits: (i) scale for large data by enabling independent processing for each partition; and (ii) more accurate and efficient learning of the classification models for the prediction task. The first benefit is obvious and the second benefit is obtained because of the following: When we train a classification model for prediction, ideally, we need the model to provide high prediction accuracy capturing all the possible dependencies from the data (we call it a model with global view). All the statistically significant dependencies are considered as the model's search space. How-ever, if the space contains a lot of weak dependencies, most likely, the model will not be able to capture them. But if it does, the global view will not be accurate enough for prediction because of the model overfitting. Partitioning the database helps to capture local correlations that are significant within subsets of the database and that require a different degree of "zooming" to be recognized. Each of the partition functions h ∈ H provides the search space partitioned according to the criteria shared by the tuples within the same block. If we train models on multiple blocks, we will have models with several local views (or specialized models, sometimes called experts <ref type="bibr" target="#b16">[16]</ref>) for portions of the search space. Combining these local views, will result in a better prediction accuracy.</p><p>Partitioning the database: Each partition function or criterion h(•) maps each tuple to one of a set of partitions. Multiple criteria H = {h1, . . . , hJ } is used to partition the database in different ways. Each tuple t is mapped to a set of partitions, i.e., H(t) = ∪ ∀ j hj(t).</p><p>A simple way to choose the partition criteria is Random (i.e., randomly partition the data many times). Another way to choose the criteria is Blocking, where partitions are constructed under the assumption that similar tuples will fit in the same block or inversely, tuples across different blocks are less likely to be similar. Many techniques for blocking have been introduced for the efficient detection of the duplicate records (refer to <ref type="bibr" target="#b6">[6]</ref> for a survey).</p><p>It is worth mentioning that increasing the number of partition functions will result in a more accurate final prediction, because the variance in the predictions decreases as we increase the number of ways (partition functions) to partition the data. We found that partitioning the data using different blocking techniques provided more accurate predictions with less number of partition functions in comparison with the random partitioning.</p><p>Example 5:</p><p>Consider again the relation in Figure <ref type="figure">1</ref>. In this example, one may partition the database based on the Institution attribute (as a partition function) to get the tuples partitioned as follows: {t1, t7}, {t2, t8}, {t3}, {t4, t5, t6}. The result of the learning process from these data partitions will be expert models based on the person's institution. Another function may use the AC or a combination of attributes. Partition functions can be designed based on a signature based scheme or clustering as we elaborate in the experimental section.</p><p>Reliability measure and risk minimization: In order to be conservative in considering the predictions from each block bij and its model Mij, we propose a mechanism to measure the reliability of a model and adapt the obtained prediction probability accordingly to support or detract the model's predictions.</p><p>Two major components help us judging the reliability of a model Mij: (i) the model quality, which is classically quantified by its loss</p><formula xml:id="formula_18">L(Mij) = 1 |b ij | ∑ t∈b ij ,t∈Dc,E∈F dE(f [E], f ′ ij [E]</formula><p>), where |bij| is the number of tuples in partition bij, E is one of the flexible attributes F , dE is a distance function for the domain of attribute E and f ′ ij is the prediction of Model Mij on the flexible attributes F for the tuple t ∈ bij and t ∈ Dc; (ii) the second component is the size of the block: the smaller the block is, the less reliable the predictions will be. Hence, the reliability of model Mij can be written as:</p><formula xml:id="formula_19">Re(Mij) = |bij| |D| (1 -L(Mij)) . (<label>6</label></formula><formula xml:id="formula_20">)</formula><p>Finally, the prediction probabilities obtained from model Mij are scaled to be:  Example 6: Consider tuple t4 in Figure <ref type="figure">1</ref> and the flexible attributes are City and State, Zip. Assume that we used 5 partition functions and hence t4 was a member of 5 partitions, consequently, we obtain 5 possible candidate tuple repairs. The table in Figure <ref type="figure" target="#fig_0">2</ref> illustrates the candidate tuple repairs of t4, RS(t4), with the corresponding prediction probabilities obtained from each partition.</p><formula xml:id="formula_21">Pij(f ′ | r) = Pij(f ′ | r) × Re(Mij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tuple Repair Selection Phase</head><p>Once the candidate tuple repairs are generated, we need a repair selection strategy to pick the best one among the candidate set. One suggestion for a selection strategy can be the majority voting. For a tuple t, the majority voting can be done by selecting the most voted value from the partitions on each attribute Ei individually.</p><p>Majority Voting (MV): The majority voting strategy implies the assumption that each attribute was predicted independently from the others. For a tuple t = rf , we predict the combination of the flexible attributes f ′ together. Thus, the independence assumption of the attributes is not valid. Therefore, we propose a mechanism to vote for a final combination of the flexible attributes that takes into account the dependencies between the predicted values obtained from each partition.</p><p>Example 7: Consider the candidate tuple repairs of t4 in Figure <ref type="figure" target="#fig_0">2</ref>. Note that if we use the majority voting while using the prediction probability as the voter's certainty, the final prediction would be {"Lafayette", "IN", "47906"}. This solution does not take into account the dependencies between the predicted values within the same tuple repair. For example, there is a stronger association between "47907" and "IN" than between "47906" and "IN". This relationship is reflected on their corresponding prediction probabilities. The values "47907" and "IN" were predicted in f ′ 1 , f ′ 4 with probabilities 0.7 and 0.8, while "47906" and "IN" were predicted in f ′ 2 , f ′ 3 and their probabilities are smaller, 0.4 and 0.6. The same applies for the relationship between "WLafayette" and "IN", which have a stronger relationship than "Lafayette" and "IN". A more desired prediction will be {"WLafayette", "IN", "47907"}.</p><p>For a given database tuple t = rf , our goal is to find the final combination</p><formula xml:id="formula_22">f ′ * = ⟨e * 1 , . . . , e * K ⟩ such that ∑ b ij ,t∈b ij P (f ′ * | r) is maximum.</formula><p>This requires the computation of the probability of each possible combination of the flexible attribute in each data block. Instead, we can search for the values that can maximize all the pairwise joint probabilities. In principle, if we maximize the pairwise association between the predicted values, then this implies maximizing the full association between the predicted values. Hence, the final update is the one that would maximize the prediction probabilities for each pair of attribute values. We formalize this problem as follows:  Step by step demonstration for the SelectTupleRepair algorithm. At each iteration, the vertex with minimum weighted degree is removed as long as it is not the only vertex in its corresponding vertex set.</p><p>corresponding prediction probabilities P (f ′ j | r)), the tuple repair selection problem for tuple t is to find f ′ * = ⟨e * 1 , . . . , e * K ⟩ such that the following sum is maximum</p><formula xml:id="formula_23">∑ ∀ e * i , e * k , i̸ =k ∑ ∀ f ′ ∈RS(t),e * i =f ′ [E i ], e * k =f ′ [E k ] p(f ′ | r).</formula><p>Solving the Tuple Repair Selection Problem: To find a solution, we map this problem to a graph optimization problem for finding the K-heaviest subgraph (KHS) <ref type="bibr" target="#b2">[2]</ref> in a K-partite graph (KPG). The key idea is to process each database tuple t individually and use its set of candidate tuple repairs, RS(t), to construct a graph, where each vertex is an attribute value, and an edge is added between a pair of vertices iff the corresponding values co-occur in a prediction f ′ ∈RS(t). The edges will have a weight derived from the obtained prediction probabilities. It is worth noting that this strategy is applied for each tuple on separate, therefore, this phase can be efficiently parallelized.</p><p>Finding KHS in KPG: The K-heaviest subgraph (KHS) problem is an NP optimization problem <ref type="bibr" target="#b2">[2]</ref>. In an instance of the KHS problem, we are given a graph G = (VG, EG), where VG is the set of vertices of size n, EG is the set of edges with non-negative weights (Wwv denotes the weight on the edge between vertices w, v), and a positive integer K &lt; n.</p><p>The goal is to find</p><formula xml:id="formula_24">V ′ ⊂ VG, |V ′ | = K, where ∑ (w,v)∈E G ∩(V ′ ×V ′ )</formula><p>Wwv is maximum. In other words, the goal is to find a K-vertex subgraph with the maximum weight.</p><p>A graph G = (VG, EG) is said to be K-partite if we can divide VG into K subsets {V1, . . . , VK }, such that two vertices in the same subset can not be adjacent. We call KHS in KPG problem, the problem of finding KHS in a K-partite graph such that the subgraph contains a vertex from each partite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 5. The KHS in KPG problem.</head><p>Given a Kpartite graph G = (V1, . . . , VK , EG), find</p><formula xml:id="formula_25">V ′ = {v1, . . . , vK } such that v k ∈ V k and ∑ (v i ,v j )∈E G ∩(V ′ ×V ′ ) Wv i v j is maximum. LEMMA 1. The KHS in KPG is NP-Complete.</formula><p>PROOF. This is a proof sketch. It is straight forward to see that we can reduce the problem of finding K-Clique (Clique of size K) in K-partite graph to the KHS in KPG problem. The problem of K-Clique in K-partite graph G is NP-complete by reduction from the problem of (n -K)-vertex cover in the complement K-partite graph G ′ , which is NP-Complete (see <ref type="bibr" target="#b15">[15]</ref> for details). The repair selection problem can be mapped to the KHS in KPG problem using the following steps: }. Note that we have a set of vertices for each attribute E k (i.e., partite).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adding edges: Add an edge between vertices v, w when</head><p>their corresponding values co-occur in a candidate tuple repair. Note that v, w can not belong to the same vertex set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Assign edge weights:</head><p>For an edge between v, w, the weight is computed as follows: Let f (v,w) = {f ′ j |f ′ j contains both v, w}, i.e., the set of predictions that contain both the values v, w.</p><formula xml:id="formula_26">Wvw = ∑ f ′ j ∈f (v,w) Pij(f ′ j | r)</formula><p>where Pij(f ′ j | r) is the prediction probability of f ′ j obtained from partition bij.</p><p>The graph construction requires a single scan over the predictions RS</p><formula xml:id="formula_27">(t)= {f ′ 1 , . . . , f ′ |H(t)| }, hence, it is of O(K |H|).</formula><p>The number of vertices is the number of distinct values in the candidate tuple repairs.</p><p>Example 8: Figure <ref type="figure" target="#fig_2">3</ref>(a) shows the constructed 3-partite graph from the predictions in Figure <ref type="figure" target="#fig_0">2</ref> for tuple t4 in the original relation of Figure <ref type="figure">1</ref>. For each attribute, there is a vertex set (or partite), e.g., the corresponding set of the Zip attribute contains {"47906", "47907"}. In the graph, we replaced the actual attributes values by a character abbreviation to have a more compact graph as follows: {"6" → "47906", "7" → "47907", "L" → "Lafayette", "W" → "Wlafayette", "F" → "lafytte", "N" → "IN", "I" → "IL"}.</p><p>Note that there is an edge between "W" and "N" with edge weight of 1.1 (= 0.4 + 0.7). This is because "WLafayette" and "IN" co-occur twice in f ′ 1 and f ′ 3 and their probabilities are 0.7 and 0.4 respectively. Also, there is an edge between "I" and "6" with weight of 0.5, because "IL" and "47906" co-occur once in f ′ 5 with probability 0.5. Similarly, the rest of the graph is constructed.</p><p>Finally, finding the KHS in the constructed KPG is a solution to the tuple repair selection problem. The underlying idea is that the resulting K-subgraph G ′ (V ′ , E ′ ) will contain exactly a single vertex from each vertex set. This corresponds to selecting a value for each flexible attribute. Moreover, the weight of the selected subgraph corresponds to the result of maximizing the summation in Definition 4.</p><p>Computing the likelihood benefit: For a tuple t = rf , the solution of the KHS in KPG problem is the final prediction f ′ for the flexible attributes. The final prediction probability of f ′ is computed from the solution graph G ′ (V ′ , E ′ ) by</p><formula xml:id="formula_28">P (f ′ | r) = 1 |E ′ | ∑ evw ∈E ′ 1 |f (v,w) | ∑ f ′ j ∈f (v,w) Pij(f ′ j | r).</formula><p>The inner summation averages the probability of each pair of attribute values (i.e., each edge in G ′ ) in the final prediction f ′ . The outer summation averages the probability over all the edges in the final graph G ′ . The prediction probability of the original values in the flexible attribute f is computed following the ensemble method by averaging the obtained probability from each partition, i.e., P (f</p><formula xml:id="formula_29">| r) = 1 |H| ∑ b ij ,t∈b ij Pij(f | r).</formula><p>Finally, for the update u changing f into f ′ , we can compute the likelihood benefit l(u) using Equation <ref type="formula" target="#formula_2">2</ref>.</p><p>Example 9: Consider the constructed initial graph in Figure <ref type="figure" target="#fig_2">3(a)</ref>. Assuming that the solution for KHS in KPG is the subgraph {"W", "N", "7"} shown in Figure <ref type="figure" target="#fig_2">3(e)</ref>. Now, we have an update u to change the original values in tuple t4 in Figure <ref type="figure">1</ref> from f ={"Lafayette", "IN", "47906"} to f ′ ={"WLafayette", "IN", "47907"}. From the RS(t4) in Figure <ref type="figure" target="#fig_0">2</ref>, we get P (f | r) = avg{0.6, 0.5, 0.3, 0.6, 0.5} = 0.5. For</p><formula xml:id="formula_30">P (f ′ | r) = 1 3 [ 1 2 (0.7 + 0.4) + 1 2 (0.7 + 0.8) + 0.7 ] = 0.66.</formula><p>Finally, we can use Eq. 2 to compute l(u) = 0.12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Approximate Solution for Tuple Repair Selection</head><p>For the general problem of finding the KHS, many approximate algorithms were introduced (e.g., <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b12">12]</ref>). For instance, in <ref type="bibr" target="#b1">[1]</ref> the authors model the problem as a quadratic 0/1 program and apply random sampling and randomized rounding techniques resulting in a polynomial-time approximation scheme, and in <ref type="bibr" target="#b12">[12]</ref>, the algorithm is based on semi-definite programming relaxation.</p><p>If K is very small, then the optimal solution can be found by enumeration. For the case where K is not very small, we provide here an approximate solution that is inspired by the greedy heuristic discussed in <ref type="bibr" target="#b2">[2]</ref>. For the general case graph problem, the heuristic repeatedly deletes a vertex with the least weighted degree from the current graph until K vertices are left. The vertex weighted degree is the sum of weights on the edges attached to it.</p><p>In the following, we follow the same heuristic for the case of K-partite graph. However, we iteratively remove the vertex with least weighted degree as long as it is not the only vertex left in the partite, otherwise, we find the next least weighted degree vertex. The algorithm is a greedy 2-approximation following the analysis discussed in <ref type="bibr" target="#b2">[2]</ref>.</p><formula xml:id="formula_31">Algorithm 3 SelectTupleRepair(G(V, E) graph, S = {S1, . . . , SK }) 1: while ∃S ∈ S s.t. |S| &gt; 1 do 2: v =GetMinWeightedDegreeVertex(G, S) 3: If v = null Then break; 4:</formula><p>for all vertex w ∈ V s.t. ewv ∈ E do 5:</p><p>Remove ewv from G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Weighted_Degree(w) -= Wwv 7:</p><p>end for 8:</p><p>Remove v from its corresponding set S. 9: end while Algorithm 3 shows the main steps to find the final tuple repair. There are two inputs to the algorithm: (i) the constructed graph G(VG, EG) from the predictions; and (ii) the sets of vertices S = {S1, . . . , SK }, where each S k represents the predicted values for attribute E k . We store for each vertex v its current weighted degree in Weighted_Degree(v)= ∑ ∀evw ∈E G Wvw, which is the sum of the edges weights that are incident to v.</p><p>The algorithm proceeds iteratively in the loop illustrated in lines 1-9. The loop stops when a solution is found, where there is only one vertex in each vertex set, i.e., |S| = 1 ∀S ∈ S. In each loop iteration, we start (Line 2) by finding the vertex v that has the minimum weighted degree using the Algorithm GetMinWeightedDegreeVertex(G, S). Then, we remove all the edges incident to v and update the WeightedDegree(w) by and sub-tracting Wwv, where w was connected to v by the removed edge ewv (Lines 4-7). Finally, vertex v is removed from G and from its corresponding vertex set in Line 8.</p><p>GetMinWeightedDegreeVertex goes through the vertex sets that has more than one vertex and returns the vertex that has the minimum weighted degree.</p><p>Analysis: Algorithm 3 requires: First, visiting all n vertices to remove them except for K ones. For each vertex, each set S ∈ S of the K sets is visited to get its minimum vertex according to the weighted degree. This requires O(nK log |S|), where n ≈ O(K|H|) and |S|'s worst case is O(|H|). Hence, visiting the vertices is of O(K<ref type="foot" target="#foot_1">2</ref> |H| log |H|). Second, removing the vertices requires visiting their edges, O(|EG|), which has a worst case of O(K 2 |H|). Then, the overall complexity of Algorithm 3 is</p><formula xml:id="formula_32">O(K 2 |H| log |H|).</formula><p>Example 10: The SelectTupleRepair algorithm is illustrated step-by-step in Figure <ref type="figure" target="#fig_2">3</ref>. The algorithm looks for the vertex with the least weighted degree to be removed. The first vertex is "I", which has a weighted degree equal 1.0 = 0.5 + 0.5, corresponding to the two incident edges in "I". This leaves the vertex set of the State attribute with only one vertex, "N". Therefore, we do not consider removing the vertex "N" in further iterations of the algorithm. The next vertex to remove is "F" to get Figure <ref type="figure" target="#fig_2">3(c)</ref>, and so on.</p><p>Finally, we got the final solution in Figure <ref type="figure" target="#fig_2">3</ref>(e), which corresponds to a subgraph with 3 vertices -there is a vertex from each initial partite. This graph is the heaviest subgraph of size 3 (i.e., the sum of the edges weight is the maximum), where each vertex belongs to a different partite. It is worth mentioning that the final graph does not have to be fully connected. Thus, the final prediction is {"WLafayette", "IN", "47907"}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we evaluate our data repair approach; specifically, the objectives of the experiments are as follows: (1) Quality evaluation of SCARE and the notion of maximal likelihood repair in comparison with the constraint-based repairing approaches of <ref type="bibr" target="#b3">[3]</ref> and <ref type="bibr" target="#b11">[11]</ref>, the single model approach, and majority voting; <ref type="bibr" target="#b2">(2)</ref> Comparison between SCARE and ERACER <ref type="bibr" target="#b20">[20]</ref> for missing values prediction; (3) Study of SCARE parameters varying the number of changes, SCARE iterations and partition functions; (4) Assessment of the scalability of SCARE.</p><p>Datasets: In our evaluations, we use three datasets: (i) Dataset 1 is a real-world dataset obtained by integrating (anonymized) emergency room visits from 74 hospitals. This is the dataset we use for most of the quality experiments. Since such data is coming from several sources, a myriad of data quality issues arose due to the different health care information systems used by these hospitals and the different operators responsible for entering the data. We selected a subset of the available patient attributes, namely Patient ID, Age, Sex, Classification, Complaint, HospitalName, StreetAddress, City, Zip, State and VisitDate. This is in addition to the Longitude and Latitude of the address information. (ii) Dataset 2 is the US Census Data (1990) Dataset<ref type="foot" target="#foot_0">1</ref> containing about 2 M tuples. It has been used only in the scalability experiments. (iii) Dataset 3 is the Intel Lab Data (ILD 2 ) used to evaluate SCARE for predicting missing values and compare it to ERACER <ref type="bibr" target="#b20">[20]</ref>, as a recent system that relies on relational learning for predicting missing data in relational databases.</p><p>Setup: To evaluate the quality of the repairing techniques in the first comparative study, we manually cleaned the dirty Dataset 1: we used addresses web sites and repositories, external reference  data sources, and visual inspection to obtain the clean dataset considered as the Golden Standard. We selected a set of attributes to be the flexible ones where we injected an increasing percentage of errors introduced in multiple attributes of the records. The flexible attributes of Dataset 1 that we considered for error injection and repairing were (City, Zip, HospitalName, Longitude and Latitude). Then, we compared the clean version of Dataset 1 (Golden Standard) with the repaired dataset outputs of each method (including ours).</p><p>Parameters: In our evaluations, we study several parameters that we list here with their assigned default values: (1) e: the percentage of the erroneous tuples in the dataset (default 30%), (2) d: the dataset size (default 10,000 tuples), (3) δ: the maximum amount of changes, as a fraction of d, the dataset size that SCARE is allowed to update (default 0.1 or 10% of d). (4) I: the number of iterations to run SCARE (default 1). ( <ref type="formula">5</ref>) |H|: the number of partition functions (default 5).</p><p>Regarding the partition functions using blocking, we repeat the following process |H| times: we randomly sample from the dataset a small number of tuples to be clustered in |D| n b clusters, where n b is the average number of tuples per partition. Then, each tuple is assigned to the closest cluster as its corresponding partition name. This process allows for having different blocking functions due to the random sample of tuples used in the clustering step of each iteration. The tuples that have been assigned to the same partition have common or similar features due to their assignment to the closest cluster. In all our quality experiments, we use blocking as the technique to partition the dataset. Another simple way to partition the dataset is to use random partitioning functions. In this case, given n b , we assign each tuple to a partition name bij, where i is a random number from {1, . . . , |D| n b }, and j = 0 initially. This process is repeated |H| times while incrementing j each time.</p><p>All the experiments were conducted on a server running Linux with 32 GB RAM and 2 processors each with 3 GHz speed. We use MySQL to store and query the tuples. For the probabilistic classifiers, we use the Naïve Bayesian, specifically, we use the NBC WEKA implementation 3 with the default parameters settings. Java is used to implement our approach and we use Java Threads to benefit from a multiprocessor environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quality Evaluation: Comparison with Constraint Based Approaches</head><p>In the following experiments, we use the standard precision and recall to measure the quality of the applied updates for string attributes.</p><p>The precision is defined as the ratio of the number of values that have been correctly updated to the total number of values that were updated, while the recall is defined as the ratio of the number of 3 NBC WEKA available at http://www.cs.waikato.ac.nz/ml/weka values that have been correctly updated to the number of incorrect values in the entire database. For numerical attributes, we use the mean absolute error (MAE):</p><formula xml:id="formula_33">1 N ∑ d i=1 |vi</formula><p>-ai| where vi is the suggested value by SCARE, ai is the actual value of the original data. We can compute these values since we know the ground truth for Dataset 1.</p><p>We report the quality results for four approaches:</p><p>• KHSinKPG: This is SCARE with the described tuple repair selection strategy as described in Section 4.3.</p><p>• MV: In this approach, SCARE uses directly the majority voting to select an attribute value from the candidate tuple repairs. We include this approach in the evaluation to compare our tuple repair selection strategy to the straight forward majority voting strategy.</p><p>• SM: The single model approach, where the whole dataset is considered as a single partition. Afterward, the likelihood benefit and cost are computed to select the best updates. This approach is included to show the advantages of combining several models with local views rather than using a single model with global view on the data.</p><p>• ConstRepair: One of the recent constraint-based repair approaches. We implemented (to the best of our understanding) the technique described in <ref type="bibr" target="#b3">[3]</ref>, which uses CFDs as constraints to find dirty tuples and derive repairs. We manually compiled a list of CFDs during the process of cleaning Dataset 1. Moreover, we implemented a CFD discovery algorithm <ref type="bibr">[9]</ref> to be used as input to the repairing algorithm.</p><p>To have a high quality rules and be fair to this approach, we discovered CFDs from the original "correct" data, which can not be the case in a realistic situation as we usually start with dirty data. We specified the rules support threshold to be 1%. Moreover, we assigned the attributes values correctness score following the technique described in <ref type="bibr" target="#b3">[3]</ref>. The implementation to discover the CFDs is very time-consuming, therefore, this approach does not show up in all of our plots.</p><p>• MasterRpr: This is a recent data repair approach described in <ref type="bibr" target="#b11">[11]</ref> that rely on a master clean data in addition to Matching Dependency (MD) rules as well as CFD rules. Since we split the database into Dc and De, we can use Dc as the master data. Also, we derived some MDs from the CFDs. More information on the MDs and how it can be used to clean the data is found in <ref type="bibr" target="#b11">[11]</ref>.</p><p>We use Dataset 1 and report in Figure <ref type="figure" target="#fig_1">4</ref> the precision and recall results for the applied updates while changing e, the percentage of tuples with erroneous values, from 10 to 50%. Generally, the approaches that maximize the data likelihood substantially outperform the constraint-based approach for the precision. Moreover, for the recall the likelihood approaches outperform the constraintbased approach when the errors is up to 30%, afterwards, the recall is comparable for all the approaches when errors is more than 30%.</p><p>SCARE with KHSinKP G shows the highest precision. The precision increases using the three likelihood-based approaches with the increase in the amount of errors, but the recall is decreasing. For the Longitude and Latitude attributes, SCAREbased approaches (KHSinKP G and M V ) corrected these attributes with error rate between 1% and 5%.</p><p>For the SCARE-based approaches, the precision increases because we use a fixed δ. When the amount of errors in the data (noted e) is small (10 to 20%), SCARE-based approaches modify more data that allow for less accurate updates resulting in less precision and relatively high recall. As e increases, the data needs more updates to correct it, however, SCARE applies fewer, yet more accurate updates, and hence the precision increases, but the recall  decreases (Figure <ref type="figure" target="#fig_1">4</ref>(b)). The KHSinKP G approach outperforms M V approach, because KHSinKP G takes into account further associations between the predicted values across the data partitions. These associations are ignored in the M V approach. Both SCAREbased approaches that rely on data partitioning (KHSinKP G and M V ) show a comparable, and sometimes even better, accuracy than that of the SM approach.</p><p>The ConstRepair and M asterRpr have been outperformed by all the likelihood-based approaches. The ConstRepair relies on the heuristics of finding values replacement that are close to the original data without considering any information on the data distributions and relationships. Moreover, the discovered CFDs that were used in ConstRepair were not exhaustive enough to cover all the problems in the data and this explains the low recall in addition to the low precision. The M asterRpr shows an improved performance over the ConstRepair because, in addition to the CFDs, the M asterRpr relies on a master clean data which helps in selecting the right replacement values through the MD rules. Therefore, both the precision and recall of M asterRpr are better than those for the ConstRepair. But, the precision and recall are still low because, similarly with ConstRepair, the discovered CFDs and MDs are not exhaustive enough to cover the data problems. In contrast, SCARE relies on statistical ML techniques, which are able to cope with variable and multiple problems in the data and still rely on the data distributions to accurately predict the replacement values.</p><p>The recall of all the repairing approaches is in the range of 30 to 65%. However, for the likelihood-based approaches, the recall can be improved by running the approach again over the resulting database instance. We illustrate this improvement later in the experiment of Figure <ref type="figure">7</ref>. But, the ConstRepair approach achieves about 35% recall that can not be further improved given a fixed set of constraints.</p><p>To conclude, in comparison to the constraint-based repairing approaches, which demonstrated both low precision and recall, our likelihood based approach demonstrated accurate updates with high precision. Moreover, partitioning the data and combining the different predictions across the partitions provide more accurate predictions, because partitioning allows to learn data relationships at different granularity levels (local and global).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SCARE vs ERACER to predict missing values</head><p>In this experiment, we use Dataset 3, which includes a number of measurements taken from 54 sensors once every 31 seconds. It contains only numerical attributes. We include this dataset to evaluate SCARE for predicting missing values and compare it to ER-ACER. We used the same dataset with the same introduced errors as reported in ERACER evaluation of <ref type="bibr" target="#b20">[20]</ref>. Thus, we were able to compare the effectiveness results of ERACER based on the same dataset. The efficiency of ERACER was not demonstrated in <ref type="bibr" target="#b20">[20]</ref> and it requires domain expertise unlike our approach. ERACER is a recent machine learning technique for data cleaning; it is specifically dedicated to replace missing values but it can not be used for data repair by value modification. ERACER leverages domain expert knowledge about the dataset to design a Bayesian network. Then, ERACER uses an underlying relational database design to store all constructed model's parameters.</p><p>In Figure <ref type="figure" target="#fig_7">5</ref>, we evaluate SCARE for predicting the missing values in comparison with ERACER. Here, we do not use δ as SCARE is not updating an existing database value. Instead, we consider all the predictions obtained from SCARE that fill a missing value.</p><p>Figure <ref type="figure" target="#fig_7">5</ref>(a) reports the mean absolute error (MAE) while errors in the dataset are in the form of missing values. Figure <ref type="figure" target="#fig_7">5</ref>(b) reports also the MAE, while errors are in the form of corrupting values, e.g., adding random values. More details on how this data was corrupted is provided in <ref type="bibr" target="#b20">[20]</ref>.</p><p>There are two major numerical attributes in the Dataset 3: humidity and temperature. In Figure <ref type="figure" target="#fig_7">5</ref>(a), both SCARE and ERACER predict the missing values for the humidity and temperature with almost the same low error percentage (2 to 4 %); and also in Figure <ref type="figure" target="#fig_7">5</ref>(a), they both behave similarly when increasing the percentage of corrupted data, and in this case, the error percentage in prediction is between 3 and 7 %.</p><p>These numbers show that both techniques provide high accuracy predictions. However, SCARE does not require the expensive domain expert to design a Bayesian network as for ERACER. For this experiment, SCARE used the Naïve Bayesian for the statistical models. The Naïve Bayesian did well when plugged into SCARE in comparison to the Bayesian network which has to be carefully designed for ERACER. This is thanks to the partitioning technique used in SCARE, which enables several local views of the data that are then combined at the end to obtain the most reliable global view for accurate predictions. Moreover, SCARE can benefit from carefully designed learning techniques, like ERACER, and plug it in the learning step to get more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameters Study</head><p>Quality vs. the amount of changes δ: In this experiment, we study the effect of bounding δ, the number of changes fixed by the user on the repair quality. We report in Figure <ref type="figure" target="#fig_8">6</ref> the resulting precision and recall when δ |D| changes from 1% to 10% (e.g., if δ |D| = 5%, then SCARE can change up to 10% of the tuples by replacing a selected attribute value v by v ′ with distance d(v, v ′ ) = 0.5). Generally, low values of δ guarantee high precision and inversely. Increasing δ gives more chance to SCARE to modify the data. Hence, the recall increases as we increase δ, but updates with a lower confidence could be made. This justifies the decrease in the precision when we increase δ.</p><p>The conclusion from this experiment is that small δ guarantees high precision at the cost of the recall. However, the recall can be improved by further SCARE iterations over the data as we will see in the next experiment.</p><p>Quality vs. the number of SCARE iterations: This experiment shows the effect on the quality if we repeatedly execute SCARE over the dataset I times, I = {1, . . . , 5}. After each iteration, the repaired tuples are considered members of the clean subset of the database, which is used in training the ML models. We report the obtained precision and recall in Figure <ref type="figure">7</ref>. For all the SCARE-based approaches, the recall substantially improves from about 35% to close to 70% as we increase the number of iterations, while, the precision slightly decreases from the 90's to the 80's %. This indicates that the overall quality improves as we run more SCARE iterations. The KHSinKP G outperforms the other approaches in terms of both precision and recall.</p><p>In each iteration, SCARE tries to repair the data to maximize the data likelihood given the learnt classification models subject to a constant amount of changes, δ. In the first few iterations, the discovered updates have higher correctness measure (i.e., ratio of the likelihood benefit to the cost of the update, l(u) c(u) ) than those that are discovered later. Therefore, the applied updates in the first iterations have higher confidence, and hence, the precision starts high and decreases in later iterations. The recall increases faster than the decrease in precision, and therefore, the overall quality is improving. The main reason is that most of the applied updates are correct in the first few iterations, so the obtained database instances after each iteration are of higher quality to be learnt and modeled for predictions in the later iterations of SCARE. A stopping criteria for the iterations can be computed from the obtained overall likelihood benefit of the updates. If the benefit is not significant, then it is better to stop SCARE. In another setting, the user can be involved interactively (e.g., <ref type="bibr" target="#b22">[22]</ref>) to inspect very small number of the least beneficial updates after each iteration. If the least beneficial updates are correct, then most of the updates are correct according to the data distribution.</p><p>Quality vs. the partition functions: Here, we study the use of different partition functions (blocking techniques), as well as, the effect of the number of partition functions |H| on the quality of SCARE on all the datasets. In Figure <ref type="figure" target="#fig_11">8</ref>(a), we report the repair precision for Dataset 1 when we use four different blocking techniques as we increase the number of partition functions. One of the blocking techniques follows the clustering approach as described earlier in this section. The remaining three blocking techniques are signature based blocking, where we compute local sensitive hashing (LSH) for different attributes (Zip, City, StreetAddress) with different random seeds.</p><p>For each dataset, we note that as we increase the number of functions, the performances of all the blocking techniques improve. They all converge to obtain similar accuracy. Increasing |H| will increase the chance that a tuple belongs to a larger number of partitions. SCARE learns a model from a local view (i.e., from one partition) and predicts the values for the attributes of the considered tuple. Consequently, a larger number of candidate tuple repairs is proposed when increasing the number of partitions and the variance of the predictions decreases. The predictions become more independent from the blocking technique being used. The repair selection strategy combines the predictions of the local view models, and this increases the chance to obtain more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SCARE Scalability</head><p>The scalability is one of the main advantages provided by SCARE, this is in addition to the quality of the updates demonstrated in the previous experiments. In the following, we assess the scalability of SCARE to large datasets.</p><p>In Figure <ref type="figure" target="#fig_11">8</ref>(b), we report the scalability of SCARE on Dataset 2. Figure <ref type="figure">7</ref>: Using SCARE in an iterative way helps improving the recall and the overall quality of the updates. The decrease in the precision is small compared to the increase in the recall, achieving an overall high quality improvement demonstrated by the f-measure.</p><p>We report as well the fraction of time for each of the two phases of SCARE. The reported time includes the time for learning the classification models. SCARE scales linearly, because of its systematic process of handling each data partition. Note also that SCARE finished the processing of a 1 M tuples in less than 6 minutes. Phase 1, the updates generation, takes 80-85 % of the time because of the process of learning models and obtaining predictions.</p><p>In Figure <ref type="figure" target="#fig_11">8</ref>(c), we report the overall time taken by SCARE to handle datasets from Dataset 1 with different size from 5,000 to 50,000 tuples. We use two different partition techniques: Random and Blocking. In general, SCARE still scales linearly with the dataset size. Moreover, it is noted that partitioning the data by blocking makes SCARE more efficient. The reason is that blocking makes a data partition containing less diversity of the domain values. This results in a faster processing to train the classification models for prediction.</p><p>Usually, the process of statistical modeling and prediction tasks is quadratic, but this is not the case with SCARE because of the robust mechanism of partitioning the data and the strategy used to combine multiple predictions.</p><p>Finally, we mentioned that SCARE can be efficiently parallelized. In our implementation we used Java Threads and in Figure <ref type="figure" target="#fig_11">8(d)</ref>, we demonstrate the speedup we obtain as we increase the number of processors. Note that SCARE achieves a linear speedup, which supports our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Improving data quality has been the focus of a large body of research for decades. Our work is closely related to two research areas: i) constraint-based data repair; and ii) statistical machine learning techniques for data cleaning.</p><p>Constraint-based data repair: In this approach, usually we have a database instance that is inconsistent with a predefined set of constraints. The objective is to find another database instance that minimally differs from the original one and that is consistent with the constraints. For example in <ref type="bibr" target="#b3">[3]</ref>, conditional FDs (CFDs) are used as database constraints. The repairing technique is based on a cost-based greedy heuristic to decide upon a repair of errors. The authors use equivalent classes to group the attributes values that are equivalent in obtaining a final consistent database instance. The work in <ref type="bibr" target="#b18">[18]</ref> uses FDs to map the repairing problem to hyper-graph optimization problem, where a heuristic vertex cover algorithm can help finding the minimal number of attributes values to modify in order to find a database consistent with the FDs. The main drawback of these approaches is that the data should be covered by a set of constraints that have been specified or validated by domain experts, which may be an expensive manual process and may not be affordable for all data domains. Moreover, the constraints usually fall short to correctly identify the right fixes <ref type="bibr" target="#b10">[10]</ref>.  The work in <ref type="bibr" target="#b11">[11]</ref> presents an entropy based technique to provide a guarantee on the applied cleaning updates. For SCARE, since the data is partitioned and the predictions are aggregated, it is not clear how the entropy measures can be aggregated. Instead, we rely on the updates likelihood benefit-to-cost ratio as a measure for the updates quality. Our experiments demonstrate overall high precision in updating the database.</p><p>Statistical machine learning techniques for data cleaning: Data cleaning using ML techniques mainly focused on deduplication (refer to <ref type="bibr" target="#b6">[6]</ref> for survey), data imputation (e.g., <ref type="bibr" target="#b20">[20]</ref>) and errors detecting (e.g., <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b17">17]</ref>). For instance, in <ref type="bibr" target="#b20">[20]</ref>, data imputation is based on relational learning to learn the characteristics of the attributes relationships in a relational database. Then, the learnt model is used to infer the missing values. This technique requires a priori knowledge about the relationships between the attributes to construct the appropriate Bayesian network. Most of similar techniques for data imputation are limited to numerical or categorical attributes. SCARE does not have such limitation. Our probabilistic setting is related to the efforts on predicting multiple class labels <ref type="bibr" target="#b4">[4]</ref> (i.e., multiple values for a single attribute). However, our task is to predict a single value from multiple attributes predictions.</p><p>The main challenges for ML techniques are mainly (i) the scalability for large databases to be modeled considering all existing significant data correlations; and (ii) the accuracy of the replacement values prediction due to the fact that existing methods usually capture either local or global data relationships and do not combine both views whereas SCARE approach does. Nevertheless, the usefulness of ML techniques can be leveraged inside the learning step of SCARE and help further improving the accuracy of the predicted updates to the data. Note that our approach is not limited to a specific (probabilistic) classifier. Moreover, it can also take advantage of DB constraints and FDs when they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we propose SCARE, a robust and scalable approach for accurately repairing erroneous data values. Our solution offers several advantages over previous methods for data repairing using database constraints: the accuracy of the replacement values and the scalability of the method are guaranteed; the cost of the repair is bounded by the amount of changes that the user is willing to tolerate; no constraint or editing rule is needed since SCARE analyzes the data, learns the correlations from the correct data and take advantages of them for predicting accurate replacement values. Finally, as shown in our experiments, SCARE outperforms existing methods on large databases with no limitation on the type of data (i.e., string, numeric, ordinal, categorical) or on the type of errors (i.e., missing, incomplete, outlying, or inconsistent values). Our current work is to extend SCARE to enable duplicate elimination while repairing errors both at the value and tuple levels.</p><p>Future work is to efficiently and interactively involve the user in a visual repairing scenario so that the selection of the candidate replacement values as well as the entire database repairing process can be user-approved in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENT</head><p>This research is based in part upon work supported by QCRI and by NSF under Grant Number IIS 0916614.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Generated predictions for tuple repairs with their corresponding prediction probabilities for tuple t4 in Figure 1. P (f ′ | r) is the prediction probability of the repairing update f ′ , and P (f | r) is the probability of the original values in t. The space required for RS is of O(|D| × |H|).Example 6: Consider tuple t4 in Figure1and the flexible attributes are City and State, Zip. Assume that we used 5 partition functions and hence t4 was a member of 5 partitions, consequently, we obtain 5 possible candidate tuple repairs. The table in Figure2illustrates the candidate tuple repairs of t4, RS(t4), with the corresponding prediction probabilities obtained from each partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>DEFINITION 4 .</head><label>4</label><figDesc>The Tuple Repair Selection Problem: Given a set of predicted combination for the flexible attributes RS(t)= {f ′ 1 , . . . , f ′ |H| } for database tuple t = rf along with the prediction probabilities of each combination, (i.e., for f ′ j ∈RS(t), we have the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:Step by step demonstration for the SelectTupleRepair algorithm. At each iteration, the vertex with minimum weighted degree is removed as long as it is not the only vertex in its corresponding vertex set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Mapping the Tuple Repair Selection Problem to the KHS in KPG problem (KHSKPG): Given a set of predictions RS(t)= {f ′ 1 , . . . , f ′ |H| } for the flexible attributes of a tuple t = rf , where f ′ j = ⟨e (j) 1 , . . . , e (j) K ⟩ and K = |F | is the number of flexible attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Quality vs. the percentage of errors: SCARE maintains high precision by making the best use of δ, the allowed amount of changes in Dataset 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison between SCARE and ERACER to predict missing values. Generally, both SCARE and ERACER show high accuracy in predicting the missing values. SCARE uses in this experiment Naïve Bayesian model, while ERACER leverages domain knowledge interpreted in carefully designed Bayesian Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: δ controls the amount of changes to apply to the database: small δ guarantees high precision at the cost of the recall and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The effect of the blocking techniques and SCARE scalability experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 2 SCARE(D dirty database, H = {h1, . . . , hJ } DB partitioning functions) 1: Given H, partition D into blocks bij.</figDesc><table><row><cell cols="2">2: for all block bij do</cell></row><row><cell>3:</cell><cell>Learn the models Mij.</cell></row><row><cell>4:</cell><cell>for all tuple t = rf ∈ bij ∧ t ∈ De do</cell></row><row><cell>5:</cell><cell>Use Mij to predict f ′ j and get Pij(f ′ j | r) and Pij(f | r)</cell></row><row><cell>6:</cell><cell>Store f ′ j , Pij(f ′ j | r) and Pij(f | r) in RS. {store in the</cell></row><row><cell></cell><cell>Repair Storage.}</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell cols="2">8: end for</cell></row><row><cell cols="2">9: for all tuple t = xy ∈ D do</cell></row><row><cell>10:</cell><cell>RS(t) = the candidate tuple repairs for t in RS.</cell></row><row><cell>11:</cell><cell>f ′ =SelectFinalPrediction(RS(t))</cell></row><row><cell>12:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). Aggregating Suggestions: As mentioned earlier, a tuple t = rf will be a member of |H(t)| data partitions. From each partition, we get a candidate tuple repair for t, which are then stored in the storage RS with the following schema: {t_id, partition, E1, ..., EK , P (f ′ |r), P (f |r)}, where t_id is the original tuple identifier, partition is the partition name, E k ∈ F ,</figDesc><table><row><cell>RS(t 4 )</cell><cell>City</cell><cell cols="2">State Zip P(f '|r ) P(f |r )</cell></row><row><cell cols="3">f' 1 WLafayette IN 47907 0.7</cell><cell>0.6</cell></row><row><cell>f' 2</cell><cell>lafytte</cell><cell>IN 47906 0.6</cell><cell>0.5</cell></row><row><cell cols="3">f' 3 WLafayette IN 47906 0.4</cell><cell>0.3</cell></row><row><cell cols="2">f' 4 Lafayette</cell><cell>IN 47907 0.8</cell><cell>0.6</cell></row><row><cell cols="2">f' 5 Lafayette</cell><cell>IL 47906 0.5</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>1 .</head><label>1</label><figDesc>Building vertex sets for each attribute E k : For each attribute E k , create a vertex v for each distinct value in {e</figDesc><table><row><cell>(1) k , . . . , e</cell><cell>(|H|) k</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://db.csail.mit.edu/labdata/labdata.html.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polynomial time approximation schemes for dense instances of np-hard problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedily finding a dense subgraph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Asahiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tokuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Algorithms</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving data quality: consistency and accuracy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayes optimal multilabel classification via probabilistic classifier chains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hullermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MCS workshop</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Information quality applied: best practices for improving business information, processes and systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>English</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependencies revisited for improving data quality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discovering conditional functional dependencies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards certain fixes with editing rules and master data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction between record matching and data repairing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the densest k-subgraph problem</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neurocomputing: foundations of research. chapter Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>German</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency networks for inference, collaborative filtering, and data visualization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rounthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient bounds for the stable set, vertex cover and set packing problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Discrete Applied Mathematics</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Correlation-based detection of attribute outliers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DASFAA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On approximating optimum repairs for functional dependency violations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient approximation algorithms for repairing inconsistent databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopatenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eracer: a database approach for statistical inference and data cleaning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Yakout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guided data repair</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Class noise vs. attribute noise: a quantitative study of their impacts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
