<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-05">5 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
							<email>qzeng@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinfeng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Cleland-Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<email>mjiang2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-05">5 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467308</idno>
					<idno type="arXiv">arXiv:2106.02974v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Taxonomy Completion, Concept Generation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic construction of a taxonomy supports many applications in e-commerce, web search, and question answering. Existing taxonomy expansion or completion methods assume that new concepts have been accurately extracted and their embedding vectors learned from the text corpus. However, one critical and fundamental challenge in fixing the incompleteness of taxonomies is the incompleteness of the extracted concepts, especially for those whose names have multiple words and consequently low frequency in the corpus. To resolve the limitations of extraction-based methods, we propose GenTaxo to enhance taxonomy completion by identifying positions in existing taxonomies that need new concepts and then generating appropriate concept names. Instead of relying on the corpus for concept embeddings, GenTaxo learns the contextual embeddings from their surrounding graph-based and language-based relational information, and leverages the corpus for pre-training a concept name generator. Experimental results demonstrate that GenTaxo improves the completeness of taxonomies over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Taxonomies have been widely used to enhance the performance of many applications such as question answering <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> and personalized recommendation <ref type="bibr" target="#b9">[10]</ref>. With the influx of new content in evolving applications, it is necessary to curate these taxonomies to include emergent concepts; however, manual curation is laborintensive and time-consuming. To this end, many recent studies aim to automatically expand or complete an existing taxonomy. For Frequency of concept in more than 30 million abstracts from the PubMed database The MeSH Taxonomy:</p><p>(Medical Subject Headings, produced by National Library of Medicine) Figure <ref type="figure">1</ref>: A great number of concepts that are desired to be in the taxonomies (e.g., MeSH) are very rare in even large-scale text corpus (e.g., 30 million PubMed abstracts). So it is hard to extract these concepts or learn their embedding vectors. Concept names need to be generated rather than extracted to fix the incompleteness of taxonomies.</p><p>example, given a new concept, Shen et al. measured the likelihood of each existing concept in the taxonomy being its hypernym and then added it as a new leaf node <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr">Manzoor et al.</ref> extended the measurement to be taxonomic relatedness with implicit relational semantics <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr">Zhang et al.</ref> predicted the position of the new concept considering hypernyms and hyponyms <ref type="bibr" target="#b39">[40]</ref>. In all of these cases, the distance between concepts was measured using their embeddings learned from some text corpus, with the underlying assumption that new concepts could be extracted accurately and found frequently in the corpus.</p><p>We argue that such an assumption is inappropriate in real-world taxonomies based on the frequency of concepts in Medical Subject Headings (MeSH), a widely-used taxonomy of approximately 30,000 terms that is updated annually and manually, in a large-scale public text corpus of 30 million paper abstracts (about 6 billion tokens) from the PubMed database. We observe that many concepts that have multiple words appear fewer than 100 times in the corpus (as depicted by the red outlined nodes in Figure <ref type="figure">1</ref>) and around half of the terms cannot be found in the corpus (see Table <ref type="table" target="#tab_1">1</ref> in <ref type="bibr">Section 4)</ref>. Concept extraction tools <ref type="bibr" target="#b37">[38]</ref> often fail to find them at the top of a list of over half a million concept candidates; and there is insufficient data to learn their embedding vectors. The incompleteness of concepts is a critical challenge in taxonomy completion, and has not yet been properly studied. Figure <ref type="figure">2</ref>: Existing methods extracted a concept candidate list from the corpus and assumed that the top concepts (high frequency or quality) could be added into existing taxonomies. However, frequent concepts may not be qualified and many qualified concepts are very rare. We find that though the concept names are not frequent, their tokens are. Our idea is to generate concept names at candidate positions with relational contexts to complete taxonomies. Our solution Gen-Taxo is pre-trained on the corpus and learns to generate the names token by token on the existing taxonomy.</p><p>Despite the low frequency of many multi-gram concepts in a text corpus, the frequency of individual words is naturally much higher. Inspired by recent advances in text generation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>, we propose a new task, "taxonomy generation", that identifies whether a new concept fits in a candidate position within an existing taxonomy, and if it does fit, generates the concept name token by token.</p><p>The key challenge lies in the lack of information for accurately generating the names of new concepts when their full names do not (frequently) appear in the text corpus. To address this challenge, our framework for enhancing taxonomy completion, called GenTaxo, has the following novel design features (see Figure <ref type="figure">2</ref> and 3):</p><p>First, GenTaxo has an encoder-decoder scheme that learns to generate any concept in the existing taxonomy in a self-supervised manner. Suppose an existing concept 𝑣 is masked. Two types of encoders are leveraged to fuse both sentence-based and graph-based representations of the masked position learned from the relational contexts. One is a sequence encoder that learns the last hidden states of a group of sentences that describe the relations such as "𝑣 𝑝 is a class of" and "𝑣 𝑐 is a subclass of", when 𝑣 𝑝 and 𝑣 𝑐 are parent and child concepts of the position, respectively. The other is a graph encoder that aggregates information from two-hop neighborhoods in the top-down subgraph (with 𝑣 at the bottom) and bottom-up subgraph (with 𝑣 at the top). The fused representations are fed into a GRU-based decoder to generate 𝑣's name token by token with a special token [EOS] at the end. With the context fusion, the decoding process can be considered as the completion of a group of sentences (e.g., "𝑣 𝑝 is a class of" and "𝑣 𝑐 is a subclass of") with the same ending term while simultaneously creating a hyponym/hypernym node in the top-down/bottom-up subgraphs.</p><p>Second, GenTaxo is pre-trained on a large-scale corpus to predict tokens in concept names. The pre-training task is very similar to the popular Mask Token Prediction task <ref type="bibr" target="#b5">[6]</ref> except that the masked token must be a token of a concept that appears in the existing taxonomy and is found in a sentence of the corpus.</p><p>Third, GenTaxo performs the task of candidate position classification simultaneously with concept name generation. It has a binary classifier that uses the final state of the generated concept name to predict whether the concept is needed at the position. We adopt negative sampling to create "invalid" candidate positions in the existing taxonomy. The classifier is attached after the name generation (not before it) because the quality of the generated name indicates the need for a new concept at the position.</p><p>Furthermore, we develop GenTaxo++ to enhance extractionbased methods, when a set of new concepts, though incomplete, needs to be added to existing taxonomies (as described in existing studies). In GenTaxo++, we apply GenTaxo to generate concept names in order to expand the set of new concepts. We then use the extraction-based methods with the expanded set to improve the concept/taxonomy completeness.</p><p>The main contributions of this work are summarized as follows:</p><p>• We propose a new taxonomy completion task that identifies valid positions to add new concepts in existing taxonomies and generates their names token by token. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>Traditionally, the input of the taxonomy completion task includes two parts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>: <ref type="bibr" target="#b0">(1)</ref> an existing taxonomy T 0 = (V 0 , E 0 ) and (2) a set of new concepts C that have been either manually given or accurately extracted from text corpus D. The overall goal is completing the existing taxonomy T 0 into a larger one T = (V 0 ∪ C, E ′</p><p>). We argue that existing technologies, which defined the task as described above, could fail when concepts cannot be extracted due to their low frequency, or cannot be found in the corpus. We aim to mitigate this problem by generating the absent concepts to achieve better taxonomy completion performance. Suppose 𝑣 = (𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑇 𝑣 ), where 𝑡 𝑖 ∈ W is the 𝑖-th token in the name of concept 𝑣, 𝑇 𝑣 is the number of 𝑣's tokens (length of 𝑣), W is the token vocabulary which includes words and punctuation marks (e.g., comma). Definition 2.1 (Taxonomy). We follow the definition of taxonomy in <ref type="bibr" target="#b39">[40]</ref>. A taxonomy T = (V, E) is a directed acyclic graph where each node 𝑣 ∈ V represents a concept (i.e., a word or a phrase) and each directed edge 〈𝑢, 𝑣〉 ∈ E implies a relation between a parent-child pair such as "is a type of" or "is a class of". We expect the taxonomy to follow a hierarchical structure where concept 𝑢 is the most specific concept that is more general than concept 𝑣. Note that a taxonomy node may have multiple parents.</p><p>In most taxonomies, the parent-child relation can be specified as a hypernymy relation between concept 𝑢 and 𝑣, where 𝑢 is the hypernym (parent) and 𝑣 is the hyponym (child). We use the terms "hypernym" and "parent", "hyponym" and "child" interchangeably throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.2 (Candidate Position). Given two sets of concepts</head><formula xml:id="formula_0">V 𝑝 , V 𝑐 ⊂ V 0 , if 𝑣 𝑐 is</formula><p>one of the descendants of 𝑣 𝑝 in the existing taxonomy for any pair of concepts 𝑣 𝑝 ∈ V 𝑝 and 𝑣 𝑐 ∈ V 𝑐 , then a candidate position acts as a placeholder for a new concept 𝑣. It becomes a valid position when 𝑣 satisfies (1) 𝑣 𝑝 is a parent of 𝑣 and (2) 𝑣 𝑐 is a child of 𝑣. When it is a valid position, we add edges 〈𝑣 𝑝 , 𝑣〉 and 〈𝑣, 𝑣 𝑐 〉 and delete redundant edges to update E ′ .</p><p>We reduce the task of generating concept names for taxonomy completion as the problem below: Given text corpus D and a candidate position on an existing taxonomy T 0 , predict whether the position is valid, and if yes, generate the name of the concept 𝑣 from the token vocabulary W (extracted from D) to fill in the position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENTAXO: GENERATE CONCEPT NAMES</head><p>Overall architecture. Figure <ref type="figure" target="#fig_2">3</ref> presents the architecture of Gen-Taxo. The goal is to learn from an existing taxonomy to identify valid candidate positions and generate concept names at those positions. Given a taxonomy, it determines valid candidate positions by masking an existing concept in the taxonomy; it also determines invalid candidate positions using negative sampling strategies which will be discussed in Section 3.2.</p><p>Here we focus on the valid positions and masked concepts. As shown in the left bottom side of Figure <ref type="figure" target="#fig_2">3</ref>, suppose the term "bacterial outer membrane proteins" is masked. GenTaxo adopts an encoderdecoder scheme in which the encoders represent the taxonomic relations in forms of sentences and subgraphs (see the middle part of the figure) and the decoders perform two tasks to achieve the goal (see the right-hand side of the figure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoders: Representing Taxonomic Relations</head><p>Here we introduce how to represent the taxonomic relations into sentences and subgraphs, and use sequence and graph encoders to generate and fuse the relational representations.</p><p>3.1.1 Representing Taxonomic Relations as Sentences. Given a taxonomy T 0 = (V 0 , E 0 ) and a candidate position for masked concept 𝑣, we focus on three types of taxonomic relations between the masked concept 𝑣 and some concepts 𝑢 ∈ V 0 :</p><p>(1) Parent or ancestor. Suppose there is a sequence of nodes (𝑣 1 , 𝑣 2 , . . . , 𝑣 𝑙 ) where 𝑣 1 = 𝑢, 𝑣 𝑙 = 𝑣, and 〈𝑣 𝑖 , 𝑣 𝑖+1 〉 ∈ E 0 for any 𝑖 = 1, . . . , 𝑙 − 1. In other words, there is a path from 𝑢 to 𝑣 in T 0 . When 𝑙 = 1, 𝑢 is a parent of 𝑣; when 𝑙 ≥ 2, 𝑢 is an ancestor. We denote 𝑢 = (𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑇 𝑢 ), where 𝑇 𝑢 is the length of concept 𝑢 and {𝑡 𝑖 } 𝑇 𝑢 𝑖=1 are its tokens. Then we create a sentence with the template below:</p><p>"𝑡 1 𝑡 2 . . . 𝑡 𝑇 𝑢 is a class of [MASK]" And we denote the set of parent or ancestor nodes of 𝑣 (i.e., all possible 𝑢 as described above) by V 𝑝 (𝑣).</p><p>(2) Child or descendant. Similar as above expect 𝑣 1 = 𝑣 and 𝑣 𝑙 = 𝑢, we have a path from 𝑣 to 𝑢. When 𝑙 = 1, 𝑢 is a child of 𝑣; when 𝑙 ≥ 2, 𝑢 is a descendant. Then we create a sentence: "𝑡 1 𝑡 2 . . . 𝑡 𝑇 𝑢 is a subclass of [MASK]" We denote the set of child or descendant nodes of 𝑣 by V 𝑐 (𝑣).</p><p>(3) Sibling. If there is a concept 𝑝 ∈ V 0 and two edges 〈𝑝, 𝑣〉 and 〈𝑝, 𝑢〉, then 𝑢 is a sibling node of 𝑣. We create a sentence:</p><formula xml:id="formula_1">"𝑡 1 𝑡 2 . . . 𝑡 𝑇 𝑢 is a sibling of [MASK]"</formula><p>Given the candidate position for concept 𝑣, we can collect a set of sentences from its relational contexts, denoted by S(𝑣). We apply neural network models that capture sequential patterns in the sentences and create the hidden states at the masked position. The hidden states are vectors that encode the related concept node 𝑢 and the relational phrase to indicate the masked concept 𝑣. The hidden states support the tasks of concept name generation and candidate position classification. We provide three options of sentence encoders: BiGRU, Transformer, and SciBERT <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Representing</head><p>Taxonomic Relations as Subgraphs. The relations between the masked concept 𝑣 and its surrounding nodes can be represented as two types of subgraphs:</p><p>(1) Top-down subgraph: It consists of all parent and ancestor nodes of 𝑣, denoted by 𝐺 down (𝑣) = {V down (𝑣), E down (𝑣)}, where</p><formula xml:id="formula_2">V down (𝑣) = V 𝑝 (𝑣) and E down (𝑣) = {〈𝑣 𝑖 , 𝑣 𝑗 〉 ∈ E | 𝑣 𝑖 , 𝑣 𝑗 ∈ V down (𝑣)}.</formula><p>The role of 𝑣 is the very specific concept of any other concepts in this subgraph. So, the vector representations of this masked position should be aggregated in a top-down direction. The aggregation indicates the relationship of being from more general to more specific. (2) Bottom-up subgraph: Similarly, it consists of all child and descendant nodes of 𝑣, denoted by</p><formula xml:id="formula_3">𝐺 up (𝑣) = {V up (𝑣), E up (𝑣)}, where V up (𝑣) = V 𝑐 (𝑣) and E up (𝑣) = {〈𝑣 𝑖 , 𝑣 𝑗 〉 ∈ E | 𝑣 𝑖 , 𝑣 𝑗 ∈ V up (𝑣)}.</formula><p>The representations of this masked position should be aggregated in a bottom-up direction. The aggregation indicates the relationship of being from specific to general.</p><p>Graph encoders: We adopt two independent graph neural networks (GNNs) to encode the relational contexts in 𝐺 down (𝑣) and 𝐺 up (𝑣) separately. Given a subgraph 𝐺, GNN learns the graphbased hidden state of every node on the final layer through the graph structure, while we will use that of the node 𝑣 for next steps.</p><p>𝑣 was specifically denoted for the masked concept node. In this paragraph, we denote any node on 𝐺 by 𝑣 for convenience. We initialize the vector representations of 𝑣 randomly, denoted by v (0) . Then, the 𝐾-layered GNN runs the following functions to generate the representations of 𝑣 on the 𝑘-th layer (𝑘 = 1, . . . , 𝐾):</p><formula xml:id="formula_4">a (𝑘−1) = Aggregate 𝑘 ( (u (𝑘−1) ) : ∀𝑢 ∈ N (𝑣) ∪ 𝐺 ), v (𝑘) = Combine 𝑘 (v (𝑘−1) , a (𝑘−1) ),</formula><p>where N (𝑣) is the set of neighbors of 𝑣 in 𝐺.</p><p>There are a variety of choices for Aggregate 𝑘 (•) and Combine 𝑘 (•). For example, Aggregate 𝑘 (•) can be mean pooling, max pooling, graph attention, and concatenation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. One popular choice for</p><formula xml:id="formula_5">Combine 𝑘 (•) is graph convolution: v (𝑘) = 𝜎 W (𝑘) (v (𝑘−1) ⊕ a (𝑘−1) ) ,</formula><p>where W (𝑘) is the weight matrix for linear transformation on the 𝑘-th layer and 𝜎 (•) is a nonlinear function.</p><p>For the masked concept 𝑣, we finally return the graph-based hidden state v 𝐺 at K-th layer, i.e., v 𝐺 = v (𝐾) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Representations Fusion.</head><p>As aforementioned, we have a set of sentence-based hidden states {h(𝑠)} 𝑠 ∈S (𝑣) from the sentence encoder; also, we have two graph-based hidden states v 𝐺 down and v 𝐺 up from the graph encoder. In this section, we present how to fuse these relational representations for decoding concept names.</p><p>Fusing sentence-based hidden states: We use the self-attention mechanism to fuse the hidden states with a weight matrix W seq :</p><formula xml:id="formula_6">h seq = ∑︁ 𝑠 ∈S (𝑣) 𝑤 (𝑠)•h(𝑠), where 𝑤 (𝑠) = exp(𝜎 (W seq h(𝑠))) 𝑠 ′ ∈S (𝑣) exp(𝜎 (W seq h(𝑠 ′ )))</formula><p>.</p><p>Fusing graph-based hidden states: We adopt a learnable weighted sum to fuse v 𝐺 down and v 𝐺 up with weight matrices W down and W up :</p><formula xml:id="formula_7">v graph = 𝛽 • v 𝐺 down + (1 − 𝛽) • v 𝐺 up , where 𝛽 = exp(𝜎 (W down •v 𝐺 down )) exp(𝜎 (W down •v 𝐺 down ))+exp(𝜎 (W up •v 𝐺up )) .</formula><p>Fusing the fused sentence-and graph-based hidden states: Given a masked concept 𝑣, there are a variety of strategies to fuse h seq and v graph : v fuse = Fuse(h seq , v graph ), such as mean pooling, max pooling, attention, and concatenation. Take concatenation as an example: Fuse(a, b) = a ⊕ b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoders: Identifying Valid Positions and Generating Concept Names</head><p>Given a masked position 𝑣, we now have the fused representations of its relational contexts v fuse from the above encoders. We perform two tasks jointly to complete the taxonomy: one is to identify whether the candidate position is valid or not; the other is to generate the name of concept for the masked position.</p><p>3.2.1 Task 1: Candidate Position Classification. Given a candidate position, this task predicts whether it is valid, i.e., a concept is needed. If the position has a masked concept in the existing taxonomy, it is a valid position; otherwise, this invalid position is produced by negative sampling. We use a three-layer feed forward neural network (FFNN) to estimate the probability of being a valid position with the fused representations: 𝑝 valid (𝑣) = FFNN(v fuse ).</p><p>The loss term on the particular position 𝑣 is based on cross entropy:</p><formula xml:id="formula_8">𝐿 1 (Θ; 𝑣) = −(𝑦 𝑣 • log(𝑝 valid (𝑣)) + (1 − 𝑦 𝑣 ) • log(1 − 𝑝 valid (𝑣))),</formula><p>where 𝑦 𝑣 = 1 when 𝑣 is valid as observed; otherwise, 𝑦 𝑣 = 0.</p><p>Negative sampling: Suppose a valid position is sampled by masking an existing concept 𝑣 ∈ V 0 , whose set of parent/ancestor nodes is denoted by V 𝑝 (𝑣) and set of child/descendant nodes is denoted by V 𝑐 (𝑣). We create 𝑟 neg negative samples (i.e., invalid positions) by replacing one concept in either V 𝑝 (𝑣) or V 𝑐 (𝑣) by a random concept in V neg (𝑣) = V 0 \ (V 𝑝 (𝑣) ∪ V 𝑐 (𝑣) ∪ {𝑣 }). We will investigate the effect of negative sampling ratio 𝑟 neg in experiments.</p><p>3.2.2 Task 2: Concept Name Generation. We use a Gated Recurrent Unit (GRU)-based decoder to generate the name of concept token by token which is a variable-length sequence 𝑣 = (𝑣 1 , 𝑣 2 , . . . , 𝑣 𝑇 𝑣 ). As shown at the right of Figure <ref type="figure" target="#fig_2">3</ref>, we add a special token [EOS] to indicate the concept generation is finished.</p><p>We initialize the hidden state of the decoder as v 0 = h seq . Then the conditional language model works as below:</p><formula xml:id="formula_9">v 𝑡 = GRU(𝑣 𝑡 −1 , v 𝑡 −1 ⊕ v graph ), 𝑝 (𝑣 𝑡 |𝑣 𝑡 −1 , . . . , 𝑣 1 , v fuse ) = Readout(𝑣 𝑡 −1 , v 𝑡 , v fuse ),</formula><p>where Readout(•) is a nonlinear multi-layer function that predicts the probability of token 𝑣 𝑡 . Then this task can be regarded as a sequential multi-class classification problem. The loss term is:</p><formula xml:id="formula_10">𝐿 2 (Θ; 𝑣) = − log(𝑝 (𝑣 |v fuse )) = − 𝑇 𝑣 ∑︁ 𝑡 =1 log(𝑝 (𝑣 𝑡 |𝑣 &lt;𝑡 , v fuse )),</formula><p>where 𝑣 is the target sequence (i.e., the concept name) and v fuse is the fused relational representations of the masked position.</p><p>Pre-training: To perform well in Task 2, a model needs the ability of predicting tokens in a concept's name. So, we pre-train the model with the task of predicting masked concept's tokens (MCT) in sentences of text corpus D. We find all the sentences in D that contain at least one concept in the existing taxonomy T 0 . Given a sentence 𝑋 = (𝑥 1 , 𝑥 2 , . . . , 𝑥 𝑛 ) where (𝑥 𝑠 , . . . , 𝑥 𝑒 ) = 𝑣 = (𝑣 1 , . . . , 𝑣 𝑇 𝑣 ) is an existing concept. Here a token 𝑥 𝑚 is masked (𝑠 ≤ 𝑚 ≤ 𝑒) and predicted using all past and future tokens. The loss function is:</p><formula xml:id="formula_11">𝐿 MCT (Θ; 𝑥 𝑚 ) = − log(𝑝 (𝑥 𝑚 |𝑥 1 , . . . , 𝑥 𝑚−1 , 𝑥 𝑚+1 , . . . , 𝑥 𝑛 )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint Training.</head><p>The joint loss function is a weighted sum of the loss terms of the above two tasks:</p><formula xml:id="formula_12">𝐿 = ∑︁ 𝑣 ∈V 0 (𝐿 1 (Θ; 𝑣) + 𝜆𝐿 2 (Θ; 𝑣)) + 𝑟 neg ∑︁ 𝑖=1 E 𝑣 neg ∼V neg (𝑣) 𝐿 1 (Θ; 𝑣 neg ),</formula><p>where 𝜆 is introduced as a hyper-parameter to control the importance of Task 2 concept name generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GenTaxo++: Enhancing Extraction-based Methods with GenTaxo</head><p>While GenTaxo is designed to replace the process of extracting new concepts by concept generation, GenTaxo++ is an alternative solution when the set of new concepts is given and of high quality.</p><p>GenTaxo++ can use any extraction-based method <ref type="bibr">[16, 22-24, 31, 36, 39, 40]</ref> as the main framework and iteratively expand the set of new concepts using concept generation (i.e., GenTaxo) to continuously improve the taxonomy completeness. We choose TaxoExpan as the extraction-based method GenTaxo++ <ref type="bibr" target="#b23">[24]</ref>.</p><p>The details of GenTaxo++ are as follows. We start with an existing taxonomy 𝑇 0 = (V 0 , E 0 ) and a given set of new concepts 𝐶 0 . </p><formula xml:id="formula_13">𝑇 ′ 𝑖−1 = (V ′ 𝑖−1 , E ′ 𝑖−1 ) ← GenTaxo(𝑇 𝑖−1 ), 𝐶 𝑖 = (𝐶 𝑖−1 ∪ V ′ 𝑖−1 ) \ V 𝑖−1 , 𝑇 𝑖 ← ExtractionMethod(𝑇 𝑖−1 , 𝐶 𝑖 ).</formula><p>The iterative procedure terminates when 𝐶 𝑖 == ∅. In this procedure, we have two hyperparameters:</p><p>(1) Concept quality threshold 𝜏: GenTaxo predicts the probability of being a valid position 𝑝 valid (𝑣) which can be considered as the quality of the generated concept 𝑣. We have a constraint on adding generated concepts to the set: 𝑝 valid (𝑣) ≥ 𝜏, for any 𝑣 ∈ 𝐶 𝑖 . When 𝜏 is bigger, the process is more cautious: fewer concepts are added each iteration. (2) Maximum number of iterations 𝑚𝑎𝑥 iter : An earlier stop is more cautious but may cause the issue of low recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS</head><p>In this work, we propose GenTaxo and GenTaxo++ to complete taxonomies through concept generation. We conduct experiments to answer the following research questions (RQs):</p><p>• RQ1: Do the proposed approaches perform better than existing methods on taxonomy completion? • RQ2: Given valid positions in an existing taxonomy and a corresponding large text corpus, which method produce more accurate concept names, the proposed concept generation or existing extraction-and-filling methods? • RQ3: How do the components and hyperparametersa impact the performance of GenTaxo?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Table <ref type="table" target="#tab_1">1</ref> presents the statistics of six taxonomies from two different domains we use to evaluate the taxonomy completion methods:</p><p>• MAG-CS <ref type="bibr" target="#b25">[26]</ref>: The Microsoft Academic Graph (MAG) taxonomy has over 660 thousand scientific concepts and more than 700 thousand taxonomic relations. We follow the processing in TaxoExpan <ref type="bibr" target="#b23">[24]</ref> to select a partial taxonomy under the "computer science" (CS) node.</p><p>• OSConcepts <ref type="bibr" target="#b20">[21]</ref>: It is a taxonomy manually crated in a popular textbook "Operating System Concepts" for OS courses. • DroneTaxo:<ref type="foot" target="#foot_0">1</ref> is a human-curated hierarchical ontology on unmanned aerial vehicle (UAV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MeSH:</head><p>Acc Acc-Uni Acc-Multi HiExpan <ref type="bibr" target="#b24">[25]</ref> 9.89 18.28 8.49 TaxoExpan <ref type="bibr" target="#b23">[24]</ref> 16.32 • MeSH <ref type="bibr" target="#b13">[14]</ref>: It is a taxonomy of medical and biological terms suggested by the National Library of Medicine (NLM). • SemEval-Sci and SemEval-Env <ref type="bibr" target="#b4">[5]</ref>: They are from the shared task of taxonomy construction in SemEval2016. We select two scientific-domain taxonomies ("science" and "environment"). Both datasets have been used in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>We use two different corpora for the two different domains of data: (1) DBLP corpus has about 156K paper abstracts from the computer science bibliography website; (2) PubMed corpus has around 30M abstracts on MEDLINE. We observe that on the two largest taxonomies, around a half of concept names and a much higher percentage of unique tokens can found at least once in the corpus, which indicates a chance of generating rare concept names token by token. The smaller taxonomies show similar patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Methods</head><p>We randomly divide the set of concepts of each taxonomy into training, validation, and test sets at a ratio of 3:1:1. We build "existing" taxonomies with the training sets following the same method in <ref type="bibr" target="#b39">[40]</ref>. To answer RQ1, we use Precision, Recall, and F1 score to evaluate the completeness of taxonomy. The Precision is calculated by dividing the number of correctly inserted concepts by the number of total inserted concepts, and Recall is calculated by dividing the the number of correct inserted concepts by the number of total concepts. For RQ2, we use accuracy (Acc) to evaluate the quality of generated concepts. For IE models, we evaluate what percent of concepts in taxonomy can be correctly extracted/generated when a position is given. We use Uni-grams (Acc-Uni), and Accuracy on Multi-grams (Acc-Multi) for scenarios where dataset contains only Uni-grams and multi-gram concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>This work proposes the first method that generates concepts for taxonomy completion. Therefore, We compare GenTaxo and Gen-Taxo++ with state-of-the-art extraction-based methods below:</p><p>• HiExpan <ref type="bibr" target="#b24">[25]</ref> uses textual patterns and distributional similarities to capture the "isA" relations and then organize the extracted concept pairs into a DAG as the output taxonomy. • TaxoExpan <ref type="bibr" target="#b23">[24]</ref> adopts GNNs to encode the positional information and uses a linear layer to identify whether the candidate concept is the parent of the query concept. • Graph2Taxo <ref type="bibr" target="#b21">[22]</ref> uses cross-domain graph structure and constraint-based DAG learning for taxonomy construction. • ARBORIST <ref type="bibr" target="#b15">[16]</ref> is the state-of-the-art method for taxonomy expansion. It aims for taxonomies with heterogeneous edge semantics and adopts a large-margin ranking loss to guaranteed an upper-bound on the shortest-path distance between the predicted parents and actual parents. • TMN <ref type="bibr" target="#b39">[40]</ref> is the state-of-the-art method for taxonomy completion. It uses a triplet matching network to match a query concept with (hypernym, hyponym)-pairs.</p><p>For RQ1, the extraction-based baselines as well as GenTaxo++ are offered the concepts from the test set if they can be extracted from the text corpus but NOT to the pure generation-based GenTaxo. For RQ2, given a valid position in an existing taxonomy, it is considered as accurate if a baseline can extract the desired concept from text corpus and assign it to that position or if GenTaxo can generate the concept's name correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RQ1: Taxonomy Completion</head><p>Table <ref type="table" target="#tab_3">3</ref> presents the results on taxonomy completion. We have three main observations. First, TaxoExpan and STEAM are either the best or the second best among all the baselines on the six datasets. When TaxoExpan is better, the gap between the two methods in terms of F1 score is no bigger than 0.7% (15.69 vs 16.39 on MeSH); when STEAM is better, its F1 score is at least 2.2% higher than Tax-oExpan (e.g., 28.60 vs 25.92 on MAG-CS). So, STEAM is generally a stronger baseline. This is because the sequential model that encodes the mini-paths from the root node to the leaf node learns useful information. The sequence encoders in our GenTaxo learn such information from the template-based sentences. STEAM loses to TaxoExpan on MeSH and OSConcepts due to the over-smoothing issue of too long mini-paths. We also find that ARBORIST and TMN do not perform better than STEAM. This indicates that GNN-based encoder in STEAM captures more structural information (e.g., sibling relations) than ARBORIST's shortest-path distance and TMN's scoring function based on hypernym-hyponym pairs. Second, on the largest two taxonomies MAG-CS and MeSH, Gen-Taxo outperforms the best extraction-based methods STEAM and TaxoExpan in terms of recall and F1 score. This is because with the module of concept generation, GenTaxo can produce more desired concepts beyond those that are extracted from the corpus. Moreover, GenTaxo uses the fused representations of relational contexts. Compared with STEAM, GenTaxo encodes both top-down and bottom-up subgraphs. When TaxoExpan considers one-hop egonets of a query concept, GenTaxo aggregates multi-hop information into the fused hidden states.</p><p>Third, on the smaller taxonomies GenTaxo performs extremely bad due to the insufficient amount of training data (i.e., fewer than 600 concepts). Note that we assumed the set of new concepts was given for all the extraction-based methods, while GenTaxo was not given it and had to rely on name generation to "create" the set. In a fair setting -when we allow GenTaxo to use the set of new concepts, then we have -GenTaxo++ performs consistently better than all the baselines. This is because it takes advantages of both the concept generation and extraction-and-fill methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RQ2: Concept Name Generation</head><p>Table <ref type="table" target="#tab_2">2</ref> presents the results on concept name generation/extraction: Given a valid position on an existing taxonomy, evaluate the accuracy of the concept names that are (1) extracted and filled by baselines or (2) generated by GenTaxo. Our observations are:</p><p>First, among all baselines, STEAM achieves the highest on multigram concepts, and TaxoExpan achieves the highest accuracy on uni-gram concepts. This is because the mini-path from root to leaf may encode the naming system for a multi-gram leaf node. Second, the accuracy scores of TaxoExpan, STEAM, ARBORIST, and TMN are not significantly different from each other (within 1.03%). This indicates that these extraction-based methods are unfortunately limited by the presence of concepts in corpus.</p><p>Third, compared GenTaxo (the last two lines in the table) with STEAM, we find GenTaxo achieves 9.7% higher accuracy. This is because GenTaxo can assemble frequent tokens into infrequent or even unseen concepts.</p><p>Then, do negative samples help learn to generate concept names? In Figure <ref type="figure" target="#fig_3">4</ref>, we show the accuracy of GenTaxo given different values of negative sampling ratio 𝑟 neg .</p><p>First, We observe that GenTaxo performs consistently better than the strongest baseline STEAM in this case. And the overall accuracy achieves the highest when 𝑟 neg = 0.15. From our point of view, the negative samples accelerate the convergence at early stage by providing a better gradient descending direction for loss function. However, too many negative samples would weaken the signal from positive examples, making it difficult for the model to learn knowledge from them.</p><p>Second, we find that the uni-gram and multi-gram concepts have different kinds of sensitivity to the ratio but comply to the same trend. Generally uni-grams have higher accuracy because generating fewer tokens is naturally an easier task; however, they take a smaller percentage of the data. So the overall performance is closer to that on multi-grams. And our GenTaxo focuses on generating new multi-gram concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RQ3: Ablation Studies</head><p>In this section, we perform ablation studies to investigate two important designs of GenTaxo: (3. contexts. We use MeSH in these studies, so based on Table <ref type="table" target="#tab_3">3</ref>, the F1 score of GenTaxo is 19.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.1</head><p>Sequence encoders for sentence-based relational contexts. We consider three types of encoders: GRU, Transformer, and SciBERT, pre-trained by a general masked language model (MLM) task on massive scientific corpora. We use our proposed task (Masked Concept's Tokens) to pre-train GRU/Transformer and fine-tune SciB-ERT. We add sentences that describe 1-hop relations (i.e., parent or child), sibling relations, 2-hops relations, and 3-hops relations step by step. Table <ref type="table">4</ref> presents the results of all the combinations. Our observations are as follows.</p><p>First, the pre-training process on related corpus is useful for generating a concept's name at a candidate position in an existing taxonomy. The pre-training task is predicting a token in an existing concept, strongly relevant with the target task. We find that pretraining improves the performance of the three sequence encoders.</p><p>Second, surprisingly we find that GRU performs slightly better than Transformer and SciBERT (19.03 vs 18.53 and 18.87). The reason may be that the sentence templates that represent relational contexts of a masked position always place two concepts in a relation at the beginning or end of the sentence. Because GRU encodes the sentences from both left-to-right and right-to-left directions, it probably represents the contexts better than the attention mechanisms in Transformer and SciBERT. SciBERT is pre-trained on MLM and performs better than Transformer.</p><p>Third, it is not having all relations in 3-hops neighborhood of the masked concept that generates the highest F1 score. On all the three types of sequence encoders, we find that the best collection of constructed sentences are those that describe 1-hop relations (i.e., parent and child) and sibling relations which is a typical relationship of two hops. Because 1-hop ancestor (parent), 2-hop ancestor (grandparent), and 3-hop ancestor are all the "class" of the masked concept, if sentences were created for all these relations, sequence encoders could not distinguish the levels of specificity of the concepts. Similarly, it is good to represent only the closest type of descendant relationships (i.e., child) as "subclass" of the masked concept. And sibling relations are very useful for generating concept names. For example, in Figure <ref type="figure">2</ref>, "nucleic acid denaturation" and "nucleic acid renaturation" have similar naming patterns when they are sibling concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.2</head><p>Graph encoders for subgraph-based relational contexts. We consider four types of GNN-based encoders: GAT <ref type="bibr" target="#b29">[30]</ref>, GCN <ref type="bibr" target="#b11">[12]</ref>, GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, and GraphTransformer <ref type="bibr" target="#b12">[13]</ref>. We add 1-hop, 2-hop, and 3-hop relations step by step in constructing the top-down and bottom-up subgraphs. The construction forms either undirected or directed subgraphs in the information aggregation GNN algorithms. Table <ref type="table" target="#tab_4">5</ref> presents the results. Our observations are as follows.</p><p>First, we find that encoding directed subgraphs can achieved a better performance than encoding undirected subgraphs for all the four types of graph encoders. This is because the directed subgraph can represent asymmetric relations. For example, it can distinguish parent-child and child-parent relations. In directed subgraphs, the edges always point from parent to child while such information is missing in undirected graphs.</p><p>Second, the best graph encoder is GraphTransformer and the second best is Graph Attention Network (GAT). They both have the attention mechanism which plays a significant role in aggregating information from top-down and bottom-up subgraphs for generating the name of concept. GraphTransformer adopts the Transformer architecture (of all attention mechanism) that can capture the contextual information better and show stronger ability of generalization than GAT.</p><p>Third, we find that all the types of graph encoders perform the best with 2-hops subgraphs. The reason may be that the GNN-based architectures cannot effectively aggregate multi-hop information. In other words, they suffer from the issue of over smoothing when they use to encode information from 3-hops neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK 6.1 Taxonomy Construction</head><p>Many methods used a two-step scheme: (1) extracted hypernymhyponym pairs from corpus, then (2) organized the extracted relations into hierarchical structures. Pattern-based <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> and embedding-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> were widely used in the first step. The second step was often considered as graph optimization and solved by maximum spanning tree <ref type="bibr" target="#b2">[3]</ref>, optimal branching <ref type="bibr" target="#b28">[29]</ref>, and minimum-cost flow <ref type="bibr" target="#b7">[8]</ref>. Mao et al. used reinforcement learning to organize the hypernym-hyponym pairs by optimizing a holistic tree metric as a reward function over the training taxonomies <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Taxonomy Expansion</head><p>These methods aimed at collecting emergent terms and placing them at appropriate positions in an existing taxonomy. Aly et al. adopted hyperbolic embedding to expand and refine an existing taxonomy <ref type="bibr" target="#b1">[2]</ref>. Shen et al. <ref type="bibr" target="#b24">[25]</ref> and Vedula et al. <ref type="bibr" target="#b27">[28]</ref> applied semantic patterns to determine the position of the new terms. Fauceglia et al. used a hybrid method to combine lexico-syntactic patterns, semantic web, and neural networks <ref type="bibr" target="#b6">[7]</ref>. Manzoor et al. proposed a joint-learning framework to simultaneously learn latent representations for concepts and semantic relations <ref type="bibr" target="#b15">[16]</ref>. Shen et al. proposed a position-enhanced graph neural network to encode the relative position of each term <ref type="bibr" target="#b23">[24]</ref>. Yu et al. applied a mini-path-based classifier instead of hypernym attachment <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Keypharse Generation</head><p>This is the most relevant task with the proposed concept name generation in taxonomies. Meng et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> applied Seq2Seq to generate keyphrases from scientific articles. Ahmad et al. proposed a joint learning method to select, extract, and generate keyphrases <ref type="bibr" target="#b0">[1]</ref>. Our approaches combine textual and taxonomic information to generate the concept names accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we proposed GenTaxo to enhance taxonomy completion by identifying the positions in existing taxonomies that need new concepts and generating the concept names. It learned position embeddings from both graph-based and language-based relational contexts. Experimental results demonstrated that Gen-Taxo improves the completeness of real-world taxonomies over extraction-based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>photochemical processes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of GenTaxo. Suppose the concept "bacterial outer membrane proteins" is masked. Given the masked position in the taxonomy, the model learns to generate the name of the concept. It has two types of encoders to represent the relational contexts of the position. Sequence encoders learn a group of sentences that describe the relations with the masked concept. Graph encoders learn the masked concept's embedding by aggregating information from its top-down and bottom-up subgraphs. The fused sentence-and graph-based hidden states are used to predict whether the masked position is valid (Task 1). They are fed into GRU decoders to generate the concept's name (Task 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of negative sampling ratio 𝑟 neg on the quality of concept name generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of six taxonomy data sets.During the 𝑖-th iteration, we first use GenTaxo to generate a set of new concepts, and then expand the set of new concepts and use the extraction-based method to update the taxonomy (𝑖 ≥ 1):</figDesc><table><row><cell></cell><cell>#Concepts</cell><cell cols="3">#Tokens #Edges Depth</cell></row><row><cell cols="4">Computer Science domains (Corpus: DBLP)</cell><cell></cell></row><row><cell>MAG-CS</cell><cell>29,484</cell><cell cols="2">16,398 46,144</cell><cell>6</cell></row><row><cell cols="3">(found in corpus) 18,338 (62.2%) 13,914 (84.9%)</cell><cell></cell><cell></cell></row><row><cell>OSConcepts</cell><cell>984</cell><cell>967</cell><cell>1,041</cell><cell>4</cell></row><row><cell>DroneTaxo</cell><cell>263</cell><cell>247</cell><cell>528</cell><cell>4</cell></row><row><cell cols="4">Biology/Biomedicine domains: (Corpus: PubMed)</cell><cell></cell></row><row><cell>MeSH</cell><cell>29,758</cell><cell cols="2">22,367 40,186</cell><cell>15</cell></row><row><cell cols="3">(found in corpus) 14,164 (47.6%) 22,193 (99.2%)</cell><cell></cell><cell></cell></row><row><cell>SemEval-Sci</cell><cell>429</cell><cell>573</cell><cell>452</cell><cell>8</cell></row><row><cell>SemEval-Env</cell><cell>261</cell><cell>317</cell><cell>261</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluating the quality of generated concepts (RQ2).</figDesc><table><row><cell></cell><cell></cell><cell>28.35</cell><cell>14.31</cell></row><row><cell>Graph2Taxo [22]</cell><cell>10.35</cell><cell>19.02</cell><cell>8.90</cell></row><row><cell>STEAM [36]</cell><cell>17.04</cell><cell>27.61</cell><cell>15.26</cell></row><row><cell>ARBORIST [16]</cell><cell>16.01</cell><cell>27.91</cell><cell>14.19</cell></row><row><cell>TMN [40]</cell><cell>16.53</cell><cell>27.52</cell><cell>14.85</cell></row><row><cell>GenTaxo (𝑟 neg = 0)</cell><cell>26.72</cell><cell>28.13</cell><cell>26.29</cell></row><row><cell cols="2">GenTaxo (𝑟 neg = 0.15) 26.93</cell><cell>31.34</cell><cell>26.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>• STEAM<ref type="bibr" target="#b35">[36]</ref> models the mini-path information to capture global structure information to expand the taxonomy. Performance on taxonomy completion: Bold for the highest among all. Underlined for the best baseline. (RQ1)</figDesc><table><row><cell>Largest</cell><cell></cell><cell>MAG-CS</cell><cell></cell><cell></cell><cell>MeSH</cell><cell></cell></row><row><cell>two:</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>HiExpan</cell><cell>19.61</cell><cell cols="3">8.23 11.59 17.77</cell><cell>5.66</cell><cell>8.59</cell></row><row><cell>TaxoExpan</cell><cell cols="6">36.19 20.20 25.92 26.87 11.79 16.39</cell></row><row><cell>Graph2Taxo</cell><cell cols="6">23.43 12.97 16.70 26.13 10.35 14.83</cell></row><row><cell>STEAM</cell><cell cols="6">36.73 23.42 28.60 26.05 11.23 15.69</cell></row><row><cell>ARBORIST</cell><cell cols="6">29.72 15.90 20.72 26.19 10.76 15.25</cell></row><row><cell>TMN</cell><cell cols="4">28.82 23.09 25.64 23.73</cell><cell cols="2">9.84 13.91</cell></row><row><cell>GenTaxo</cell><cell cols="6">36.15 28.19 31.67 21.47 17.10 19.03</cell></row><row><cell>GenTaxo++</cell><cell cols="6">36.24 28.68 32.01 22.61 17.66 19.83</cell></row><row><cell>Computer</cell><cell cols="2">OSConcepts</cell><cell></cell><cell cols="2">DroneTaxo</cell><cell></cell></row><row><cell>Science:</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>HiExpan</cell><cell cols="6">21.77 13.71 16.82 43.24 30.77 35.95</cell></row><row><cell>TaxoExpan</cell><cell cols="6">30.43 21.32 25.07 60.98 48.08 53.77</cell></row><row><cell>Graph2Taxo</cell><cell cols="6">22.88 13.71 17.15 44.90 23.31 30.69</cell></row><row><cell>STEAM</cell><cell cols="6">30.71 19.79 24.07 58.33 53.85 56.00</cell></row><row><cell>ARBORIST</cell><cell cols="6">31.09 18.78 23.42 52.38 42.31 46.81</cell></row><row><cell>TMN</cell><cell cols="6">30.65 19.29 23.68 47.72 40.38 43.74</cell></row><row><cell>GenTaxo</cell><cell cols="4">18.32 12.18 14.63 11.63</cell><cell cols="2">9.62 10.53</cell></row><row><cell>GenTaxo++</cell><cell cols="6">30.18 25.89 27.87 65.96 59.62 62.63</cell></row><row><cell>Biology/</cell><cell cols="2">SemEval-Sci</cell><cell></cell><cell cols="3">SemEval-Env</cell></row><row><cell>Biomedicine:</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>HiExpan</cell><cell cols="4">14.63 10.34 12.12 15.79</cell><cell cols="2">8.11 10.72</cell></row><row><cell>TaxoExpan</cell><cell cols="6">24.14 29.17 26.42 23.07 16.22 19.05</cell></row><row><cell>Graph2Taxo</cell><cell cols="6">26.19 18.96 21.99 21.05 10.81 14.28</cell></row><row><cell>STEAM</cell><cell cols="6">35.56 27.58 31.07 46.43 35.13 39.99</cell></row><row><cell>ARBORIST</cell><cell cols="6">41.93 22.41 29.21 46.15 32.43 38.09</cell></row><row><cell>TMN</cell><cell cols="6">34.15 24.14 28.29 37.93 29.73 33.33</cell></row><row><cell>GenTaxo</cell><cell>11.43</cell><cell>6.90</cell><cell cols="4">8.61 16.13 13.51 14.70</cell></row><row><cell>GenTaxo++</cell><cell cols="6">38.78 32.76 35.52 48.28 37.84 42.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on graph encoders for subgraph contexts on the taxonomy. (U/D is for undirected/directed subgraph settings in GNN-based aggregation.)</figDesc><table><row><cell>MeSH:</cell><cell cols="2">PT 1 hop</cell><cell cols="2">+ 2 hops</cell><cell>+ 3 hops</cell></row><row><cell>(F1 score)</cell><cell></cell><cell></cell><cell cols="2">Sibling Grand-p/c</cell><cell></cell></row><row><cell>GRU</cell><cell>×</cell><cell>18.19</cell><cell>18.29</cell><cell>18.92</cell><cell>17.41</cell></row><row><cell></cell><cell>✓</cell><cell>18.35</cell><cell>19.03</cell><cell>18.40</cell><cell>17.49</cell></row><row><cell cols="2">Transformer ×</cell><cell>17.89</cell><cell>18.13</cell><cell>17.97</cell><cell>17.04</cell></row><row><cell>[27]</cell><cell>✓</cell><cell>18.02</cell><cell>18.53</cell><cell>18.19</cell><cell>17.07</cell></row><row><cell>SciBERT</cell><cell>×</cell><cell>18.05</cell><cell>18.16</cell><cell>18.12</cell><cell>17.29</cell></row><row><cell>[4]</cell><cell>✓</cell><cell>18.11</cell><cell>18.87</cell><cell>18.23</cell><cell>17.41</cell></row><row><cell cols="6">Table 4: Ablation study on sequence encoders for sentence-</cell></row><row><cell cols="6">based relational contexts. (PT is for Pre-training. Grand-p/c</cell></row><row><cell cols="4">is for grandparent/grandchild.)</cell><cell></cell><cell></cell></row><row><cell cols="2">MeSH: (F1 score)</cell><cell cols="4">U/D 1 hop + 2 hops + 3 hops</cell></row><row><cell>GAT [30]</cell><cell></cell><cell>U</cell><cell>18.17</cell><cell>18.43</cell><cell>17.11</cell></row><row><cell></cell><cell></cell><cell>D</cell><cell>18.29</cell><cell>18.94</cell><cell>17.39</cell></row><row><cell>GCN [12]</cell><cell></cell><cell>U</cell><cell>18.10</cell><cell>18.27</cell><cell>17.09</cell></row><row><cell></cell><cell></cell><cell>D</cell><cell>18.21</cell><cell>18.35</cell><cell>17.15</cell></row><row><cell>GraphSAGE [9]</cell><cell></cell><cell>U</cell><cell>18.12</cell><cell>18.36</cell><cell>17.05</cell></row><row><cell></cell><cell></cell><cell>D</cell><cell>18.19</cell><cell>18.66</cell><cell>17.20</cell></row><row><cell cols="3">GraphTransformer [13] U</cell><cell>18.22</cell><cell>18.79</cell><cell>17.13</cell></row><row><cell></cell><cell></cell><cell>D</cell><cell>18.35</cell><cell>19.03</cell><cell>17.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the methods of fusing sentencebased and graph-based relational representations.</figDesc><table><row><cell>1) Sequence encoders for sentence-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.dronetology.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research was supported by National Science Foundation award CCF-1901059.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Sentence encoders</head><p>The descriptions of three sentence encoders we use are as below.</p><p>(1) BiGRU: Given a sentence 𝑠 ∈ S(𝑣), we denote 𝑛 as the length of 𝑠 which is also the position index of  <ref type="bibr" target="#b3">[4]</ref> that leverages unsupervised pretraining on a large corpus of scientific publications (from semanticscholar.org) to improve performance on scientific NLP tasks such as sequence tagging, sentence classification, and dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baselines Implementation</head><p>HiExpan, TaxoExpan, STEAM, and ARBORIST are used for taxonomy expansion. Given a subgraph which includes the ancestor, parent, and sibling nodes as the seed taxonomy, the final output is a taxonomy expanded by the target concepts. For taxonomy completion methods, we set the threshold for deciding whether to add the target node to the specified position of taxonomy as 0.5. For Graph2Taxo, we input the graph-based context within two hops from the target concept in order to construct the taxonomy structure. To evaluate Graph2Taxo, we check the parent and child nodes of the target concept. When both sets matched the ground truth, the prediction result is marked as correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameter Settings</head><p>We use stochastic gradient descent (SGD) with momentum as our optimizer. We applied a scheduler with "warm restarts" that reduces the learning rate from 0.25 to 0.05 over the course of 5 epochs as a cyclical regiment. Models are trained for 50 epochs based on the validation loss. All the dropout layers used in our model had a default ratio of 0.3. The number of dimensions of hidden states is 200 for sequence encoders and 100 for graph encoders, respectively. We search for the best loss weight 𝜆 in {0.1, 0.2, 0.5, 1, 2, 5} and set it as 2. We set 𝑟 neg = 0.15, 𝜏 = 0.8, and max iter = 2 as default. Their effects will be investigated in the experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soomin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01739</idno>
		<title level="m">Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4811" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">SciBERT: A pretrained language model for scientific text</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international workshop on semantic evaluation</title>
				<meeting>the 10th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1081" to="1091" />
		</imprint>
	</monogr>
	<note>semeval-2016</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Md Faisal Mahbub Chowdhury, and Nandana Mihindukulasooriya</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Rodolfo Fauceglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Automatic taxonomy induction and expansion</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Taxonomy induction using hypernym subsequences</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taxonomy-aware multi-hop reasoning networks for sequential recommendation</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaole</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="573" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Role of&quot; Condition&quot; A Novel Scientific Knowledge Graph Representation and Construction Model</title>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1634" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Medical subject headings (MeSH)</title>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning term embeddings for taxonomic relation identification using dynamic weighting neural network</title>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See</forename><forename type="middle">Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expanding Taxonomies with Implicit Edge Semantics</title>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhananjay</forename><surname>Shrouty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2044" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-End Reinforcement Learning for Automatic Taxonomy Induction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2462" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10229</idno>
		<title level="m">An Empirical Study on Neural Keyphrase Generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Keyphrase Generation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PATTY: A taxonomy of relational patterns with semantic types</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Operating system concepts</title>
		<author>
			<persName><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><surname>Silberschatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taxonomy construction of unseen domains via graph-based cross-domain knowledge transfer</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2198" to="2208" />
		</imprint>
	</monogr>
	<note>Md Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, and Alfio Gliozzo</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nettaxo: Automated topic taxonomy construction from text-rich network</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1908" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TaxoExpan: self-supervised taxonomy expansion with positionenhanced graph neural network</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="486" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hiexpan: Task-guided taxonomy construction by hierarchical tree expansion</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the web conference</title>
				<meeting>the web conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enriching taxonomies with functional domain knowledge</title>
		<author>
			<persName><forename type="first">Nikhita</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Patrick K Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Ajwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ontolearn reloaded: A graph-based algorithm for taxonomy induction</title>
		<author>
			<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="665" to="707" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enquire One&apos;s Parent and Child Before Decision: Fully Exploit Hierarchical Structure for Self-Supervised Taxonomy Expansion</title>
		<author>
			<persName><forename type="first">Suyuchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficiently answering technical questions-a knowledge graph approach</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Technical Question Answering across Tasks and Domains</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchi</forename><surname>Mahindru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinem</forename><surname>Guven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
				<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04389</idno>
		<title level="m">A Survey of Knowledge-Enhanced Text Generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1026" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faceted hierarchy: A new graph type to organize scientific concepts and a construction method</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing</title>
				<meeting>the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>TextGraphs-13</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2701" to="2709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Taxonomy Completion via Triplet Matching Network</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
