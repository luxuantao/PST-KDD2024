<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel k-means, Spectral Clustering and Normalized Cuts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
							<email>inderjit@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiang</forename><surname>Guan</surname></persName>
							<email>yguan@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Kulis</surname></persName>
							<email>kulis@cs.utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel k-means, Spectral Clustering and Normalized Cuts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6E7E9FF5617ADDAD4951CD972715923</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Information Search and Retrieval; I.5.3 [Pattern Recognition]: Clustering Algorithms</term>
					<term>Theory Spectral Clustering</term>
					<term>Kernel k -means</term>
					<term>Graph Partitioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kernel k -means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k -means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead to a novel weighted kernel k -means algorithm that monotonically decreases the normalized cut. This has important implications: a) eigenvector-based algorithms, which can be computationally prohibitive, are not essential for minimizing normalized cuts, b) various techniques, such as local search and acceleration schemes, may be used to improve the quality as well as speed of kernel k -means. Finally, we present results on several interesting data sets, including diametrical clustering of large geneexpression matrices and a handwriting recognition data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Clustering has received a significant amount of attention in the last few years as one of the fundamental problems in data mining. k -means is one of the most popular clustering algorithms. Recent research has generalized the algorithm in many ways; for example, similar algorithms for clustering can be obtained using arbitrary Bregman divergences as the distortion measure <ref type="bibr" target="#b2">[2]</ref>. Other advances include using local search to improve the clustering results <ref type="bibr" target="#b5">[5]</ref> and using the triangle inequality to speed up the computation <ref type="bibr" target="#b4">[4]</ref>.</p><p>A major drawback to k -means is that it cannot separate clusters that are non-linearly separable in input space. Two recent approaches have emerged for tackling such a problem. One is kernel k -means, where, before clustering, points are mapped to a higher-dimensional feature space using a nonlinear function, and then kernel k -means partitions the points by linear separators in the new space. The other approach is spectral clustering algorithms, which use the eigenvectors of an affinity matrix to obtain a clustering of the data. A popular objective function used in spectral clustering is to minimize the normalized cut <ref type="bibr" target="#b12">[12]</ref>.</p><p>On the surface, kernel k -means and spectral clustering appear to be completely different approaches. In this paper we first unite these two forms of clustering under a single framework. By generalizing the k -means objective function to use both weights and kernels, we show how the two approaches to clustering are related. Specifically, we can rewrite the weighted kernel k -means objective function as a trace maximization problem whose relaxation can be solved with eigenvectors. The result shows how a particular kernel and weight scheme is connected to the spectral algorithm of Ng, Jordan, and Weiss <ref type="bibr" target="#b10">[10]</ref>. However, the advantage to our approach is that we can generalize the clustering algorithm to use arbitrary kernels and weights.</p><p>Further, we show that by choosing the weights in particular ways, the weighted kernel k -means objective function is identical to the normalized cut. Thus far, only eigenvectorbased algorithms have been employed to minimize normalized cuts in spectral clustering and image segmentation. However, software to compute eigenvectors of large sparse matrices (often based on the Lanczos algorithm) can have substantial computational overheads, especially when a large number of eigenvectors are to be computed. In such situations, our equivalence has an important implication: we can use k -means-like iterative algorithms for directly minimizing the normalized-cut of a graph.</p><p>We show the usefulness of our approach to the application of clustering gene expression data by applying a quadratic kernel (squared correlation) to obtain anti-correlated gene clusters and we illustrate the scalability of our algorithms in terms of computation time by applying it to a large handwriting recognition data set.</p><p>A word about notation. Capital letters such as A, X, Y </p><formula xml:id="formula_0">Polynomial Kernel κ(a, b) = (a • b + c) d Gaussian Kernel κ(a, b) = exp(-||a -b|| 2 /2σ 2 ) Sigmoid Kernel κ(a, b) = tanh(c(a • b) + θ)</formula><formula xml:id="formula_1">||X|| F = ( P i,j X 2 ij ) 1/2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE ESSENTIALS</head><p>In this section, we summarize the seemingly different approaches of weighted kernel k -means and spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weighted Kernel k-means</head><p>The k -means clustering algorithm can be enhanced by the use of a kernel function; by using an appropriate nonlinear mapping from the original (input) space to a higherdimensional feature space, one can extract clusters that are non-linearly separable in input space. Furthermore, we can generalize the kernel k-means algorithm by introducing a weight for each point a, denoted by w(a). As we shall see later, this generalization is powerful and encompasses the normalized cut of a graph.</p><p>Let us denote clusters by πj, and a partitioning of points as {π j } k j=1 . Using the non-linear function φ, the objective function of weighted kernel k-means is defined as:</p><formula xml:id="formula_2">D({πj} k j=1 ) = k X j=1 X a∈π j w(a) φ(a) -mj 2<label>(1)</label></formula><p>where mj =</p><formula xml:id="formula_3">P b∈π j w(b)φ(b) P b∈π j w(b)</formula><p>.</p><p>Note that m j is the "best" cluster representative since</p><formula xml:id="formula_4">mj = argmin z X a∈π j w(a) φ(a) -z 2 .</formula><p>The Euclidean distance from φ(a) to center m j is given by ŕ ŕ ŕ ŕ ŕ ŕ ŕ ŕ φ(a) -</p><formula xml:id="formula_5">P b∈π j w(b)φ(b) P b∈π j w(b) ŕ ŕ ŕ ŕ ŕ ŕ ŕ ŕ 2 = φ(a) • φ(a) - 2 P b∈π j w(b)φ(a) • φ(b) P b∈π j w(b) + P b,c∈π j w(b)w(c)φ(b) • φ(c) ( P b∈π j w(b)) 2 . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>The dot products φ(a)•φ(b) are computed using kernel function κ (see Table <ref type="table" target="#tab_0">1</ref> for examples of popular kernel functions), and are contained in the kernel matrix K. All computation is in the form of such inner products, hence we can replace all inner products by entries of the kernel matrix. The weighted kernel k-means algorithm (Algorithm 1) shares many properties of standard k-means; for example, the objective function value defined in (1) monotonically decreases with each iteration.</p><p>Assuming we are able to store the whole affinity matrix in main memory, we can analyze the time complexity of Algorithm 1. It is clear that the bottleneck is Step 3, i.e., the computation of distances. The first term in <ref type="bibr" target="#b2">(2)</ref>, φ(a) • φ(a),  need not be computed since it is a constant for a and thus does not affect the assignment of a to clusters. The second term is calculated once per data point, and costs O(n) each time it is computed, leading to a cost of O(n 2 ) per iteration.</p><p>For the third term, notice that</p><formula xml:id="formula_7">P b,c∈π j w(b)w(c)φ(b)•φ(c) ( P b∈π j w(b)) 2</formula><p>is fixed for cluster j, so in each iteration it is computed once and stored. Thus the complexity is O(n 2 ) scalar operations per iteration. Initially, we must compute the kernel matrix K, which usually takes time O(n 2 m), where m is the dimension of the original points. If the total number of iterations is τ , then the time complexity of Algorithm 1 is O(n 2 (τ + m)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral clustering</head><p>Spectral clustering has emerged recently as a popular clustering method that uses eigenvectors of a matrix derived from the data. Several algorithms have been proposed in the literature <ref type="bibr">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12]</ref>, each using the eigenvectors in slightly different ways. In this paper, we will focus on the normalized cut spectral algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Normalized Cuts</head><p>In <ref type="bibr" target="#b13">[13]</ref>, the authors consider the k -way normalized cut problem. We are given a graph G = (V, E, A), where V is the set of vertices, E is the set of edges connecting vertices, and A is an edge affinity matrix, assumed to be nonnegative and symmetric. Suppose A, B ⊆ V, we define</p><formula xml:id="formula_8">links(A, B) = X i∈A,j∈B A(i, j).</formula><p>Then the normalized linkratio of A, B is:</p><formula xml:id="formula_9">normlinkratio(A, B) = links(A, B) links(A, V) .</formula><p>The k -way normalized cut problem is to minimize the links that escape a cluster relative to the total "weight" of the cluster. For a k -way partitioning of the vertices, we are interested in solving the following problem:</p><formula xml:id="formula_10">minimize 1 k k X j=1 normlinkratio(Vj, V \ Vj).</formula><p>The authors of <ref type="bibr" target="#b13">[13]</ref> obtain the following spectral relaxation to this problem: let D be the diagonal matrix whose (i, i) entry is the sum of the entries of row i in matrix A. Then the normalized cut criterion is equivalent to the following trace maximization problem:</p><formula xml:id="formula_11">maximize 1 k trace(Z T AZ),</formula><p>where Z = X(X T DX) -1/2 , and X is an n × k indicator matrix for the partitions. Note that Z T DZ = I k . Letting Z = D 1/2 Z and relaxing the constraint that X is an indicator matrix results in the following problem: maximize the trace of ZT D -1/2 AD -1/2 Z, where the constraints on Z are relaxed such that ZT Z = I k . A well-known solution to this problem is obtained by setting the matrix Z to be the top k eigenvectors of the matrix D -1/2 AD -1/2 . These eigenvectors are then used to compute a discrete partitioning of the points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE SPECTRAL CONNECTION</head><p>At first glance, weighted kernel k -means and normalized cuts using spectral clustering appear to be quite different. After all, spectral clustering uses eigenvectors to help determine the partitions, whereas eigenvectors do not appear to figure in kernel k -means. However, we saw that the normalized cut problem can be expressed as a trace maximization problem, and in this section, we show how we can express weighted kernel k -means as a trace maximization problem as well. This will show how to connect the two methods of clustering.</p><p>For ease in presentation, let us denote the "distortion" of a cluster π j to be d(π j ) = P a∈π j w(a) φ(a) -m j 2 . Then we have that D({π j } k j=1 ) = P k j=1 d(π j ). Moreover, let us denote, for a cluster π j , the sum of the w weights of the points in π j to be s j ; in other words, s j = P a∈π j w(a). Finally, let us denote W to be the diagonal matrix of all the w weights, and Wj to be the diagonal matrix of the weights in π j . Then we can rewrite the mean vector m j as</p><formula xml:id="formula_12">mj = Φj W j e sj ,</formula><p>where Φ j is the matrix of points associated with cluster π j (after the φ mapping), i.e., Φ = [φ(a 1 , φ(a 2 ), . . . , φ(a n )], and e is the vector of all ones of appropriate size. We can rewrite the distortion of cluster πj to be:</p><formula xml:id="formula_13">d(πj) = X a∈π j w(a) φ(a) -mj 2 = X a∈π j w(a) φ(a) -Φ j W j e s j 2 = (Φ j -Φ j W j ee T s j )W 1/2 j 2 F = (Φ j W 1/2 j (I - W 1/2 j ee T W 1/2 j s j ) 2 F .</formula><p>Using the fact that trace(AA T ) = trace(A T A) = A 2 F , and noting that I -</p><formula xml:id="formula_14">W 1/2 j ee T W 1/2 j s j</formula><p>= P is an orthogonal projection, i.e. P 2 = P since s j = e T W j e, we get that</p><formula xml:id="formula_15">d(πj) = trace ţ ΦjW 1/2 j ţ I - W 1/2 j ee T W 1/2 j s j ű 2 W 1/2 j Φ T j ű = trace ţ Φ j W 1/2 j ţ I - W 1/2 j ee T W 1/2 j s j ű W 1/2 j Φ T j ű = trace(W 1/2 j Φ T j Φ j W 1/2 j ) - e T W j √ s j Φ T j Φ j W j e √ s j .</formula><p>If we represent the full matrix of points as Φ = [Φ1, Φ2, . . . , Φ k ], then we have that</p><formula xml:id="formula_16">D({π j } k j=1 ) = trace(W 1/2 Φ T ΦW 1/2 )-trace(Y T W 1/2 Φ T ΦW 1/2 Y ), where Y = 2 6 6 6 6 6 4 W 1/2 1 e √ s 1 W 1/2 2 e √ s 2 • • • W 1/2 k e √ s k 3 7 7 7 7 7 5</formula><p>.</p><formula xml:id="formula_17">Note that Y is an n × k orthonormal matrix, i.e., Y T Y = I.</formula><p>Since trace(ΦW Φ T ) is a constant, the minimization of the objective function in ( <ref type="formula" target="#formula_2">1</ref>) is equivalent to the maximization of trace(Y T W 1/2 Φ T ΦW 1/2 Y ). The matrix Φ T Φ is simply the kernel matrix K of the data, so we can rewrite it as the maximization of trace(Y T W 1/2 KW 1/2 Y ).</p><p>A standard result in linear algebra <ref type="bibr" target="#b8">[8]</ref> provides a global solution to a relaxed version of this problem. By allowing Y to be an arbitrary orthonormal matrix, we can obtain an optimal Y by taking the top k eigenvectors of W 1/2 KW 1/2 . Similarly, the sum of the top k eigenvalues of W 1/2 KW 1/2 gives the optimal trace value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLICATIONS</head><p>The previous two sections show that the seemingly unrelated graph cut problem and weighted kernel k -means problem can both be written as trace maximization problems. This hints at a connection between these problems. We now make this connection precise, and discuss its implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Normalized Cuts using Weighted Kernel k-means</head><p>As discussed in Section 2.2.1, the normalized cut problem can be recast as a trace maximization problem, where we attempt to maximize trace( ZT D -1/2 AD -1/2 Z). A simple calculation reveals that Z is analogous to the matrix Y from the previous section.</p><p>We now show a direct relationship between the trace maximizations of the normalized cut and kernel k -means problems. Consider weighted kernel k -means with W = D and K = D -1 AD -1 . The trace maximization of weighted kernel k -means is then trace(Y T D -1/2 AD -1/2 Y ), which is equivalent to the trace maximization for normalized cut. If the affinity matrix K is positive definite, we can use the weighted kernel k -means procedure described in Algorithm 1 in order to minimize the normalized cut (positive definiteness allows us to factor K into Φ T Φ, and allows us to prove convergence). Indeed, any starting partition can potentially be improved by Algorithm 1.</p><p>One advantage to our use of an iterative algorithm for these graph problems is that we can use different improve-ment methods, such as local search, to increase the quality of the results. In situations where eigenvector computation is difficult, for example, when the affinity matrix is large and sparse, and many eigenvectors are desired, our iterative algorithm is particularly useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Kernel k-means using Eigenvectors</head><p>The reformulation of the kernel k -means objective function allows us to solve a relaxed problem using the eigenvectors of the matrix W 1/2 KW 1/2 . This yields a spectral approach to minimizing the objective function: we first compute the top k eigenvectors of the matrix W 1/2 KW 1/2 . This maps the original points to a lower-dimensional space. Once postprocessing is performed and a discrete clustering solution has been attained, one can treat the resulting partitioning as a good initialization to kernel k -means on the full data set. This two-layer approach -first running spectral clustering to get an initial partitioning and then refining the partitioning by running kernel k -means on the partitioning -typically results in a robust partitioning of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interpreting NJW As Kernel k-means</head><p>The results from the previous sections give us novel ways to interpret the spectral algorithm of Ng, Jordan, and Weiss <ref type="bibr" target="#b10">[10]</ref>. Their algorithm first computes the kernel matrix K, where the kernel that is used is the Gaussian Kernel. They compute a diagonal matrix D such that the diagonal entries of D are the sums of the rows of K. Then they compute the eigenvectors of the matrix D -1/2 KD -1/2 , and form a discrete clustering using these eigenvectors.</p><p>Hence, we see that the NJW algorithm can be viewed as either a spectral relaxation to the weighted kernel k -means objective function or as a normalized cut problem. The connection to normalized cuts is clear: we view the affinity matrix K in the spectral algorithm as defining the edge weights of a graph, and their algorithm attempts to minimize the normalized cut in this graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SCALABILITY ISSUES</head><p>In this section, we discuss methods for scaling the kernel k -means algorithm to large data sets.</p><p>To speed up the distance computation in our weighted kernel k-means algorithm, we can adapt the pruning procedure used in <ref type="bibr" target="#b4">[4]</ref>. The idea behind the acceleration scheme is that we can use the triangle inequality to avoid unnecessary computation. We compute the distances between corresponding new and old centers, m n j -m o j for all j, and store the information in a k × k matrix D. Similarly, we keep a k × n matrix L that contains lower bound for the distance from each point to each center. The distance from a point to its cluster center is exact in L. After the centers are updated, we estimate the lower bound from each point a to new cluster center, say m n j , to be the difference between the lower bound from a to m o j and m n j -m o j . We actually compute the distance from a to m n j only if the estimation is smaller than distance from a to its cluster center. Figure <ref type="figure" target="#fig_4">3</ref> shows significant computation savings due to this estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL RESULTS</head><p>We now provide experimental results to validate the usefulness of the results presented in the previous sections. We first illustrate "diametric clustering" of genes with degree-2 polynomial kernel k-means. Then, with the handwriting recognition data set, we show that using eigenvectors to initialize kernel k-means gives better initial and final objective function values and better clustering results. Thus the theoretical connection between spectral clustering and kernel kmeans helps in obtaining higher quality results. Finally, we show that our distance estimation techniques save a considerable amount of computation time, verifying the scalability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data sets</head><p>The human fibroblast gene expression has 517 genes across 12 conditions and the yeast dataset of Rosetta Inpharmatics has 5246 genes across 300 conditions. They are used and preprocessed as in <ref type="bibr" target="#b6">[6]</ref>.</p><p>The Pendigits is downloaded from UCI machine learning repository (ftp://ftp.ics.uci.edu/ pub/machine-learningdatabases/pendigits), which contains (x, y) coordinates of hand-written digits. This dataset contains 7494 training digits and 3498 testing digits. Each digit is represented as a vector in 16-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation details</head><p>Our kernel k-means algorithm is implemented in C++ and all experiments are done on a PC (Linux, two AMD 1.19GHz processors, 1GB main memory). In our implementation, we store the kernel matrix in main memory. All the plots are generated using Matlab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diametrical Clustering of Gene Expression Data</head><p>In gene expression clustering, identifying anti-correlated relationship among genes is important, as it has been observed that genes whose expression patterns are strongly anti-correlated may also be functionally similar. We show that degree-2 polynomial kernel k-means can identify anticorrelated genes as was done in <ref type="bibr" target="#b6">[6]</ref>. We cluster human fibroblast genes into 5 clusters. Then for each cluster Ai, i = 1, ..., 5, we compute the dot product of each gene vector and the leading eigenvector of A i A T i and plot genes across experiments in red or blue depending on whether the dot product value is positive or negative. The first 3 plots of Figure <ref type="figure" target="#fig_2">1</ref> show some sample results. Then we cluster 5246 yeast genes into 40 clusters. This took approximately 4.5 minutes, including forming the kernel matrix and clustering. The last 3 plots in Figure <ref type="figure" target="#fig_2">1</ref> correspond to one cluster of yeast genes. We magnify three parts of one cluster plot across 300 experiments in order to show the details. From the plots we see that degree-2 polynomial kernel k-means captures the anti-correlation similar to those captured by the diametric clustering algorithm. For example, cluster 2 of the human fibroblast dataset includes a number of genes involved in inter-cellular signaling, inflammation, angiogenesis and re-epithelialization, such as IL1beta, thrombomodulin, IL8, etc., corresponding to Figure <ref type="figure" target="#fig_4">3d</ref> in <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Handwriting Recognition Data Set</head><p>Since the eigenvectors of the kernel matrix are the optimizers of a relaxed trace maximization problem, using the output of spectral clustering to initialize kernel k-means can often produce better clustering results than pure random initialization. To illustrate this, we run sigmoid kernel k-means on the Pendigits.tes dataset 10 times with random initialization, then average the initial and final objective function values. Then we run sigmoid kernel k-means 10 times again,     but initialized with the output of the spectral clustering algorithm. We also compute the average of the initial and final objective function values. For this dataset, since we know the underlying class labels we can evaluate clustering results by forming a confusion matrix, where entry (i, j), n (j) i gives the number of points in cluster i and class j. From such a confusion matrix, we compute normalized mutual information (NMI) as</p><formula xml:id="formula_18">2 P k l=1 P c h=1 n (h) l n log ţ n (h) l n P k i=1 n (h) i P c i=1 n (i) l ű H(π) + H(ζ) ,</formula><p>where c is the number of classes,</p><formula xml:id="formula_19">H(π) = - P k i=1 n i n log ą n i n ć ,</formula><p>and</p><formula xml:id="formula_20">H(ζ) = - P c j=1 n (j)</formula><p>n log ş n (j) n ť . High NMI value indicates that the clustering and true class labels match well. In Table <ref type="table">2</ref>, we compare the average initial and final objective function values as well as the average normalized mutual information values of 10 runs each with random initialization and spectral initialization. Clearly, using spectral initialization can improve clustering quality. Figure <ref type="figure">2</ref> shows two sample runs of sigmoid kernel k-means clustering, one with random initialization and the other with spectral initialization.</p><p>The bottleneck in Algorithm 1 is the computation of Euclidean distances in kernel space. In order to avoid unnecessary computation, we incorporate the triangle inequality estimation mentioned in Section 5 into our kernel k-means software. Figure <ref type="figure" target="#fig_4">3</ref> shows the considerable savings in the number of Euclidean distance computations as the iteration count increases in a typical run of Algorithm 1 on the whole  Pendigits dataset, which contains 10992 digits. Without using the estimation, in every iteration nk distance calculations, 109920 in this case, need to be performed. However, after incorporating the estimation, we save considerable computation; for example, in the ninth iteration only 621 distances need to be computed. Figure <ref type="figure" target="#fig_1">4</ref> gives the computation time taken to cluster 10992 digits into a varied number of clusters. Times for both the original clustering algorithm and the one with distance estimation are shown. The distance estimation technique yields more savings in computation time as the number of clusters grows.</p><p>Figure <ref type="figure" target="#fig_8">5</ref> shows the objective function values of kernel kmeans and the corresponding normalized cut values at each iteration of Algorithm 1 on the human fibroblast gene data. Again, a degree-2 polynomial kernel is used and 5 diametrical clusters are generated. We see that cut values decrease monotonically along with the objective function value of the kernel k-means algorithm. As can be seen, the difference between the corresponding cut value and objective function value at each iteration is a constant, which is equal to k -trace(D -1 A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Our work has been largely inspired by recent results on spectral clustering and relaxation methods presented in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b14">14]</ref>. In <ref type="bibr" target="#b14">[14]</ref>, the authors show that the traditional k -means objective function can be recast as a trace maximization problem of the Gram matrix for the original data. We generalize their work to the case when non-linear kernels are used, plus we treat the weighted version of the kernel k-means algorithm, which allows us to encompass spectral algorithms that use normalized cuts.</p><p>Although focusing on a different issue, <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b1">1]</ref> also discusses a relation to normalized cuts, as well as a method for finding a good discrete clustering from the eigenvector matrix. In <ref type="bibr" target="#b1">[1]</ref>, they hint at a way to run an iterative algorithm for normalized cuts but their algorithm considers the factorization of a semi-definite matrix K such that K = GG T , which takes O(n 3 ) time, and thus is computationally worse than our kernel k-means approach.</p><p>The notion of using a kernel to enhance the k -means objective function was first described in <ref type="bibr" target="#b11">[11]</ref>. Kernel-based learning methods have appeared in a number of other areas, especially in the area of Support Vector Machines <ref type="bibr" target="#b3">[3]</ref>.</p><p>In <ref type="bibr" target="#b7">[7]</ref>, the objective function was recast as a trace maxi-mization, but they developed an EM-style algorithm to solve the kernel k -means problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have presented a theoretical connection between weighted kernel k -means and spectral clustering. We show that the weighted kernel k -means formulation is very general, and that the normalized cut objective can be recast as a special case of the weighted kernel k -means objective function. We also show that, given weights and a kernel matrix, it is possible to derive a spectral algorithm that solves a relaxed version of the objective function. We also provide new interpretations of the spectral algorithm of Ng, Jordan, and Weiss, while generalizing them to use other kernels, such as the degree-2 kernel for diametric clustering.</p><p>In future work, we would like to incorporate alternate objectives, such as ratio cut and ratio association, into our framework. So far, we have assumed that the affinity matrix is positive definite. In the future, we would like to be able to handle indefinite matrices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 2 .</head><label>12</label><figDesc>Weighted Kernel k-means. Weighted Kernel kmeans(K, k, w, C 1 , ..., C k ) Input: K: kernel matrix, k: number of clusters, w: weights for each point Output: C 1 , ...., C k : partitioning of the points 1. Initialize the k clusters: C Set t = 0. 3. For each point a, find its new cluster index as j * (a) = argmin j φ(a) -mj 2 , using (2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 .</head><label>4</label><figDesc>Compute the updated clusters as C t+1 j = {a : j * (a) = j}. 5. If not converged, set t = t + 1 and go to Step 3; Otherwise, stop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: In each figure are plotted the mean expression profiles of two opposed clusters obtained on the human fibroblast dataset (first 3 plots) and the Rosetta dataset (last 3 plots). The clustering algorithm used is degree 2 polynomial kernel k-means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Table 2 :</head><label>22</label><figDesc>Figure 2: Two sample runs of sigmoid kernel kmeans clustering of Pendigits.tes dataset; a = 0.0045 and b = 0.11 are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Computation savings due to triangle inequality estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Computation time required to cluster the whole Pendigits dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>) and objective function (red) valuesObjective function value Cuts value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Objective function value of kernel k -means and normalized cut values monotonically decrease in Algorithm 1. Corresponding objective function value and cut value at each iteration differ by a constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of kernel functionsand Φ denote matrices; lower-case bold letters such as a, b denote column vectors; script letters such as A, B, V, E represent sets; ||a|| denotes the L 2 norm of a vector; and ||X||F denotes the Frobenius norm of a matrix, and is given by</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This research was supported by NSF CAREER Award No. ACI-0093404, Texas Advanced Research Program grant 003658-0431-2001 and NSF ITR Award No. IIS-0325116.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS-16</title>
		<meeting>of NIPS-16</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering with Bregman divergence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of SIAM Data Mining conference</title>
		<meeting>eeding of SIAM Data Mining conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="234" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Introduction to Support Vector Machines: And Other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient clustering of very large document collections</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining for Scientific and Engineering Applications</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="357" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative clustering of high dimensional text data augmented by local search</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2002 IEEE International Conference on Data Mining</title>
		<meeting>The 2002 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diametrical clustering for identifying anti-correlated gene clusters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Roshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1612" to="1619" />
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mercer kernel based clustering in feature space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On clusteringsgood, bad, and spectral</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 41st Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS-14</title>
		<meeting>of NIPS-14</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiclass spectral clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral relaxation for k -means clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
