<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-18">18 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University Yuxiao Dong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond Jing Zhang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Remin University Hongxia Yang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group Ming Ding</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University Kuansan Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Redmond Jie Tang *</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-18">18 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403168</idno>
					<idno type="arXiv">arXiv:2006.09963v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Data mining</term>
					<term>Social networks</term>
					<term>• Computing methodologies → Unsupervised learning</term>
					<term>Neural networks</term>
					<term>Learning latent representations graph representation learning, graph neural network, pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning has emerged as a powerful technique for real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, graph classification, and link prediction. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph, which is usually non-transferable to out-of-domain data. Inspired by recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) 1 -an unsupervised graph representation learning framework -to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph-level instance discrimination in and across networks and leverage contrastive learning to empower the model to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its taskspecific trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Hypothesis. Representative graph structural patterns are universal and transferable across networks.</p><p>Over the past two decades, the main focus of network science research has been on discovering and abstracting the universal structural properties underlying different networks. For example, Barabasi and Albert show that several types of networks, e.g., World Wide Web, social, and biological networks, have the scale-free property, i.e., all of their degree distributions follow a power law <ref type="bibr" target="#b0">[1]</ref>. Leskovec et al. <ref type="bibr" target="#b27">[28]</ref> discover that a wide range of real graphs satisfy the densification and shrinking laws. Other common patterns across networks include small world <ref type="bibr" target="#b57">[58]</ref>, motif distribution <ref type="bibr" target="#b30">[31]</ref>, community organization <ref type="bibr" target="#b33">[34]</ref>, and core-periphery structure <ref type="bibr" target="#b5">[6]</ref>, validating our hypothesis at the conceptual level.</p><p>In the past few years, however, the paradigm of graph learning has been shifted from structural pattern discovery to graph representation learning <ref type="bibr">[11, 14, 25, 39-41, 48, 60]</ref>, motivated by recent advances in deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, graph representation learning converts the vertices, edges or subgraphs of a graph into low-dimensional embeddings such that vital structural information of the graph is preserved. The learned embeddings from the input graph can be then fed into standard machine learning models for downstream tasks on the same graph.</p><p>However, most representation learning work on graphs has thus far focused on learning representations for one single graph or a fixed set of graphs and very limited work can be transferred to outof-domain data and tasks. Essentially, those representation learning models aim to learn network-specific structural patterns dedicated for each dataset. For example, the DeepWalk embedding model <ref type="bibr" target="#b38">[39]</ref> learned on the Facebook social graph cannot be applied to other graphs. In view of <ref type="bibr" target="#b0">(1)</ref> this limitation of graph representation learning and (2) the prior arts on common structural pattern discovery, a natural question arises here: can we universally learn transferable representative graph embeddings from networks?</p><p>The similar question has also been asked and pursued in natural language processing <ref type="bibr" target="#b9">[10]</ref>, computer vision <ref type="bibr" target="#b16">[17]</ref>, and other domains. To date, the most powerful solution is to pre-train a representation learning model from a large dataset, that is, self-supervised representation learning. The idea of pre-training is to use the pre-trained model as a good initialization for fine-tuning over (different) tasks on unseen datasets. For example, BERT <ref type="bibr" target="#b9">[10]</ref> designs language model pre-training tasks to learn a Transformer encoder <ref type="bibr" target="#b52">[53]</ref> from a large corpus. The pre-trained Transformer encoder is then adapted to various NLP tasks <ref type="bibr" target="#b55">[56]</ref> by fine-tuning.</p><p>Presented Work. Inspired by this and the existence of universal graph structural patterns, we propose to study the potential of pretraining representation learning models for graphs. Ideally, given a (diverse) set of input graphs, such as the Facebook social graph and the DBLP co-author graph, we aim to pre-train a representation learning model from them with a self-supervised task, and then finetune it on different graphs with different graph learning tasks, such as node classification on the US-Airport graph. The critical question for graph pre-training here is: how to design the pre-training task such that the universal structural patterns in and across networks can be captured and further transferred?</p><p>In this work, we present the Graph Contrastive Coding (GCC) model to learn structural representations across graphs. Conceptually, we leverage the idea of contrastive learning <ref type="bibr" target="#b58">[59]</ref> to design GCC's pre-training task as instance discrimination. Its basic idea is to sample instances from input graphs, treat each of them as a distinct class of its own, and learn to encode and discriminate between these instances. Specifically, there are three questions to answer for GCC such that it can learn the transferable structural patterns: <ref type="bibr" target="#b0">(1)</ref> what are the instances? <ref type="bibr" target="#b1">(2)</ref> what are the discrimination rules? And (3) how to encode the instances?</p><p>In GCC's pre-training stage, we propose to distinguish vertices according to their local structures (Cf. Figure <ref type="figure" target="#fig_0">1</ref>). For each vertex, we sample subgraphs from its multi-hop ego network as instances. GCC aims to distinguish between subgraphs sampled from a certain vertex and subgraphs sampled from other vertices. Finally, for each subgraph, we use a graph neural network (specifically, the GIN model <ref type="bibr" target="#b59">[60]</ref>) as the graph encoder to map the underlying structural patterns to latent representations. As GCC does not assume vertices and subgraphs come from the same graph, the graph encoder is forced to capture universal patterns across different input graphs. Given the pre-trained GCC model, we apply it to unseen graphs for various downstream tasks. GCC is able to measure the structural similarity between two vertices from different domains, e.g., a user from Facebook friendship network and a researcher from DBLP co-authorship network, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To the best of our knowledge, very limited work exists in the field of structural graph representation pre-training to date. A very recent one is to design strategies for pre-training GNNs on labeled graphs with node attributes for specific domains (molecular graphs) <ref type="bibr" target="#b18">[19]</ref>. Another recent work is InfoGraph <ref type="bibr" target="#b45">[46]</ref>, which focuses on learning domain-specific graph-level representations, especially for graph classification tasks. The third related work is by Hu et al. <ref type="bibr" target="#b19">[20]</ref>, who define several graph learning tasks, such as predicting centrality scores, to pre-train a GCN <ref type="bibr" target="#b24">[25]</ref> model on synthetic graphs.</p><p>We conduct extensive experiments to demonstrate the performance and transferability of GCC. We pre-train the GCC model on a collection of diverse types of graphs and apply the pre-trained model to three downstream graph learning tasks on ten new graph datasets. The results suggest that the GCC model achieves competitive or better results to the state-of-the-art task-specific graph representation learning models that are trained from scratch. For example, for node classification on the US-Airport network, GCC pre-trained on the Facebook, IMDB, and DBLP graphs outperforms GraphWave <ref type="bibr" target="#b11">[12]</ref>, ProNE <ref type="bibr" target="#b64">[65]</ref> and Struc2vec <ref type="bibr" target="#b42">[43]</ref> which are trained directly on the US-Airport graph, empirically demonstrating our hypothesis at the beginning.</p><p>To summarize, our work makes the following four contributions: • We formalize the problem of structural graph representation pretraining across multiple graphs and identify its design challenges.</p><p>• We design the pre-training task as subgraph-level instance discrimination to capture universal and transferable structural patterns from multiple input graphs.</p><p>• We present the Graph Contrastive Coding (GCC) framework to learn structural graph representations, which leverages contrastive learning to guide the pre-training.</p><p>• We conduct extensive experiments to demonstrate that for out-ofdomain tasks, GCC can offer comparable or superior performance over dedicated graph-specific models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work of vertex similarity, contrastive learning and graph pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vertex Similarity</head><p>Quantifying similarity of vertices in networks/graphs has been extensively studied in the past years. The goal of vertex similarity is to answer questions <ref type="bibr" target="#b25">[26]</ref> like "How similar are these two vertices?" or "Which other vertices are most similar to these vertices?" The definition of similarity can be different in different situations. We briefly review the following three types of vertex similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighborhood similarity</head><p>The basic assumption of neighborhood similarity, a.k.a., proximity, is that vertices closely connected should be considered similar. Early neighborhood similarity measures include Jaccard similarity (counting common neighbors), RWR similarity <ref type="bibr" target="#b35">[36]</ref> and SimRank <ref type="bibr" target="#b20">[21]</ref>, etc. Most recently developed network embedding algorithms, such as LINE <ref type="bibr" target="#b47">[48]</ref>, DeepWalk <ref type="bibr" target="#b38">[39]</ref>, node2vec <ref type="bibr" target="#b13">[14]</ref>, also follow the neighborhood similarity assumption.</p><p>Structural similarity Different from neighborhood similarity which measures similarity by connectivity, structural similarity doesn't even assume vertices are connected. The basic assumption of structural similarity is that vertices with similar local structures should be considered similar. There are two lines of research about modeling structural similarity. The first line defines representative patterns based on domain knowledge. Examples include vertex degree, structural diversity <ref type="bibr" target="#b51">[52]</ref>, structural hole <ref type="bibr" target="#b6">[7]</ref>, k-core <ref type="bibr" target="#b1">[2]</ref>, motif <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, etc. Consequently, models of this genre, such as Struc2vec <ref type="bibr" target="#b42">[43]</ref> and RolX <ref type="bibr" target="#b17">[18]</ref>, usually involve explicit featurization. The second line of research leverages the spectral graph theory to model structural similarity. A recent example is Graph-Wave <ref type="bibr" target="#b11">[12]</ref>. In this work, we focus on structural similarity. Unlike the above two genres, we adopt contrastive learning and graph neural networks to learn structural similarity from data.</p><p>Attribute similarity Real world graph data always come with rich attributes, such as text in citation networks, demographic information in social networks, and chemical features in molecular graphs.</p><p>Recent graph neural networks models, such as GCN <ref type="bibr" target="#b24">[25]</ref>, GAT <ref type="bibr" target="#b53">[54]</ref>, GraphSAGE <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b62">63]</ref> and MPNN <ref type="bibr" target="#b12">[13]</ref>, leverage additional attributes as side information or supervised signals to learn representations which are further used to measure vertex similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning</head><p>Contrastive learning is a natural choice to capture similarity from data. In natural language processing, Word2vec <ref type="bibr" target="#b29">[30]</ref> model uses cooccurring words and negative sampling to learn word embeddings.</p><p>In computer vision, a large collection of work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b58">59</ref>] learns self-supervised image representation by minimizing the distance between two views of the same image. In this work, we adopt the InfoNCE loss from Oord et al. <ref type="bibr" target="#b34">[35]</ref> and instance discrimination task from Wu et al. <ref type="bibr" target="#b58">[59]</ref>, as discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Pre-training</head><p>Skip-gram based model Early attempts to pre-train graph representations are skip-gram based network embedding models inspired by Word2vec <ref type="bibr" target="#b29">[30]</ref>, such as LINE <ref type="bibr" target="#b47">[48]</ref>, DeepWalk <ref type="bibr" target="#b38">[39]</ref>, node2vec <ref type="bibr" target="#b13">[14]</ref>, and metapath2vec <ref type="bibr" target="#b10">[11]</ref>. Most of them follow the neighborhood similarity assumption, as discussed in section 2.1. The representations learned by the above methods are tied up with graphs used to train the models, and can not handle out-of-sample problems. Our Graph Contrastive Coding (GCC) differs from these methods in two aspects. First, GCC focuses on structural similarity, which is orthogonal to neighborhood similarity. Second, GCC can be transferred across graphs, even to graphs never seen during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training graph neural networks</head><p>There are several recent efforts to bring ideas from language pre-training <ref type="bibr" target="#b9">[10]</ref> to pre-training</p><formula xml:id="formula_0">Graph 𝑥 " Graph 𝑥 # $ 𝒒 𝒌 𝟎 , 𝒌 𝟏 , 𝒌 𝟐 Similarity Contrastive Loss Graph 𝑥 # + Graph 𝑥 # , Graph Encoder 𝑓 # Graph Encoder 𝑓 " Figure 2: A Running Example of GCC. Left: examples of two</formula><p>2-ego networks with the red and blue vertices as the egos. Middle: a similar pair (x q , x k 0 ) is randomly augmented from the red ego network, and two negative subgraphs, x k 1 and x k 2 , are randomly augmented from another ego network -the blue one. Right: these subgraph instances are encoded by graph neural networks f q and f k , after which the contrastive loss in Equation 1 encourages a higher similarity score between positive pairs than negative ones. Note that the two ego networks are not required to come from the same graph.</p><p>graph neural networks (GNN). For example, Hu et al. <ref type="bibr" target="#b18">[19]</ref> pretrain GNN on labeled graphs, especially molecular graphs, where each vertex (atom) has an atom type (such as C, N, O), and each edge (chemical bond) has a bond type (such as the single bond and double bond). The pre-training task is to recover atom types and chemical bond types in masked molecular graphs. Another related work is by Hu et al. <ref type="bibr" target="#b19">[20]</ref>, which defines several graph learning tasks to pre-train a GCN <ref type="bibr" target="#b24">[25]</ref>. Our GCC framework differs from the above methods in two aspects. First, GCC is for general unlabeled graphs, especially social and information networks. Second, GCC does not involve explicit featurization and pre-defined graph learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH REPRESENTATION PRE-TRAINING</head><p>In this section, we formalize the structural graph representation pretraining problem. To address it, we present the Graph Contrastive Coding (GCC) framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Graph Pre-Training Problem</head><p>Conceptually, given a collection of graphs from various domains, we aim to pre-train a model to capture structural patterns across these graphs in an unsupervised manner. The model should be able to benefit various graph learning tasks on different graphs. The underlying assumption is that there exist common and transferable structural patterns across different graphs such as the motif, k-core, and structural hole, as evident in network science literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>. One illustrative scenario is that we pre-train a model on Facebook, IMDB, and DBLP graphs with self-supervision, and apply it on the US-Airport network for node classification. Formally, the structural graph representation pre-training problem is to learn a function f that maps a vertex to a low-dimensional feature vector, such that f has the following two properties:</p><p>• First, structural similarity, it maps vertices with similar local network topologies close to each other in the vector space;</p><p>• Second, transferability, it is compatible with vertices and graphs unseen during pre-training.</p><p>As such, the embedding function f can be adopted in various graph learning tasks, such as social role prediction, node classification, and graph classification. Note that the focus of this work is on structural representation learning without node attributes and node labels, making it completely different from the problem setting in graph neural network research. In addition, the goal is to pre-train a structural representation model and apply it to unseen graphs, differing from traditional network embedding <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> and recent attempts on pre-training graph neural networks with attributed graphs as input and applying them within a specific domain <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Contrastive Coding Overview</head><p>To pre-train structural representations on graphs, we present the Graph Contrastive Coding (GCC) model. Given a set of graphs, our goal is to pre-train a universal graph neural network encoder to capture the structural patterns behind these graphs. To achieve this, we need to design proper self-supervised tasks and learning objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref> and NLP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>, we propose to use instance discrimination <ref type="bibr" target="#b58">[59]</ref> as our pre-training task, and InfoNCE <ref type="bibr" target="#b34">[35]</ref> as our learning objective. Our choice of pre-training task and learning objective treats each instance as a distinct class of its own and learns to discriminate between these instances <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>. The promise is that it can output instance representations that are capable of capturing the similarities between instances. From a dictionary look-up perspective, given an encoded query q and a dictionary of K + 1 encoded keys {k 0 , • • • , k K }, contrastive learning looks up a single key (denoted by k + ) that q matches in the dictionary. In this work, we adopt InfoNCE <ref type="bibr" target="#b34">[35]</ref> such that:</p><formula xml:id="formula_1">L = − log exp q ⊤ k + /τ K i =0 exp (q ⊤ k i /τ )<label>(1)</label></formula><p>where τ is the temperature hyper-parameter. f q and f k are two graph neural networks that encode the query instance x q and each key instance x k to d-dimensional representations, denoted by q = f q (x q ) and k = f k (x k ).</p><p>To instantiate each component in GCC, we need to answer the following three questions: • Q1: How to define instances in graphs?</p><p>• Q2: How to define (dis) similar instance pairs in and across graphs, i.e., for a query x q , which key x k is the matched one?</p><p>• Q3: What are the proper graph encoders f q and f k ?</p><p>It is worth noting that in our problem setting, x q and x k 's are not assumed to be from the same graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GCC Design</head><p>In this part, we present the design strategies for the GCC model by correspondingly answering the aforementioned questions.</p><p>Q1: Design contrastive instances in graphs. The success of contrastive learning framework largely relies on the definition of the data instance. It is straightforward for CV and NLP tasks to define an instance as an image or a sentence. However, such ideas cannot be directly extended to graph data, as instances in graphs are not clearly defined. Moreover, our pre-training focus is purely on structural representations without additional input features/attributes. This leaves the natural choice of a single vertex as an instance infeasible, as it is not applicable to discriminate between two vertices. To address this issue, we propose to extend one single vertex to its local structure. Specially, for a certain vertex v, we define an instance to be its r -ego network: Definition 3.1. A r -ego network. Let G = (V , E) be a graph, where V denotes the set of vertices and E ⊆ V × V denotes the set of edges <ref type="foot" target="#foot_0">2</ref> . For a vertex v, its r -neighbors are defined as S v = {u : d(u, v) ≤ r } where d(u, v) is the shortest path distance between u and v in the graph G. The r -ego network of vertex v, denoted by G v , is the sub-graph induced by S v .</p><p>The left panel of Figure <ref type="figure">2</ref> shows two examples of 2-ego networks. GCC treats each r -ego network as a distinct class of its own and encourages the model to distinguish similar instances from dissimilar instances. Next, we introduce how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type="bibr" target="#b16">[17]</ref>, two random data augmentations (e.g., random crop, random resize, random color jitering, random flip, etc) of the same image are treated as a similar instance pair. In GCC, we consider two random data augmentations of the same r -ego network as a similar instance pair and define the data augmentation as graph sampling <ref type="bibr" target="#b26">[27]</ref>. Graph sampling is a technique to derive representative subgraph samples from the original graph. Suppose we would like to augment vertex v's r -ego network (G v ), the graph sampling for GCC follows the three steps-random walks with restart (RWR) <ref type="bibr" target="#b50">[51]</ref>, subgraph induction, and anonymization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>(1) Random walk with restart. We start a random walk on G from the ego vertex v. The walk iteratively travels to its neighborhood with the probability proportional to the edge weight.</p><p>In addition, at each step, with a positive probability the walk returns back to the starting vertex v.</p><p>(2) Subgraph induction. The random walk with restart collects a subset of vertices surrounding v, denoted by S v . The sub-graph G v induced by S v is then regarded as an augmented version of the r -ego network G v . This step is also known as the Induced Subgraph Random Walk Sampling (ISRW).</p><p>(3) Anonymization We then anonymize the sampled graph G v by re-labeling its vertices to be {1, 2, • • • , | S v |}, in arbitrary order <ref type="foot" target="#foot_1">3</ref> .</p><p>We repeat the aforementioned procedure twice to create two data augmentations, which form a similar instance pair (x q , x k + ). If two subgraphs are augmented from different r -ego networks, we treat them as a dissimilar instance pair (x q , x k ) with k k + . It is worth noting that all the above graph operations -random walk with restart, subgraph induction, and anonymization -are available in the DGL package <ref type="bibr" target="#b56">[57]</ref>.</p><p>Discussion on graph sampling. In random walk with restart sampling, the restart probability controls the radius of ego-network (i.e., r ) which GCC conducts data augmentation on. In this work, we follow Qiu et al. <ref type="bibr" target="#b41">[42]</ref> to use 0.8 as the restart probability. The proposed GCC framework is flexible to other graph sampling algorithms, such as neighborhood sampling <ref type="bibr" target="#b15">[16]</ref> and forest fire <ref type="bibr" target="#b26">[27]</ref>.</p><p>Discussion on anonymization. Now we discuss the intuition behind the anonymization step in the above data augmentation procedure. This step is designed to keep the underlying structural patterns and hide the exact vertex indices. This design avoids learning a trivial solution to instance discrimination, i.e., simply checking whether vertex indices of two sampled graphs match. Moreover, it facilitates the transfer of the learned model across different graphs as such a model is not associated with a particular vertex set.</p><p>Q3: Define graph encoders. Given two sampled subgraphs x q and x k , GCC encodes them via two graph neural network encoders f q and f k , respectively. Technically, any graph neural networks <ref type="bibr" target="#b3">[4]</ref> can be used here as the encoder, and the GCC model is not sensitive to different choices. In practice, we adopt the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b59">[60]</ref>, a state-of-the-art graph neural network model, as our graph encoder. Recall that we focus on structural representation pre-training while most GNN models require vertex features/attributes as input. To bridge the gap, we propose to leverage the graph structure of each sampled subgraph to initialize vertex features. Specifically, we define the generalized positional embedding as follows: Definition 3.2. Generalized positional embedding. For each subgraph, its generalized positional embedding is defined to be the top eigenvectors of its normalized graph Laplacian. Formally, suppose one subgraph has adjacency matrix A and degree matrix D, we conduct eigen-decomposition on its normalized graph Laplacian s.t. I −D −1/2 AD −1/2 = U ΛU ⊤ , where the top eigenvectors in U <ref type="bibr" target="#b54">[55]</ref> are defined as generalized positional embedding.</p><p>The generalized positional embedding is inspired by the Transformer model in NLP <ref type="bibr" target="#b52">[53]</ref>, where the sine and cosine functions of different frequencies are used to define the positional embeddings in word sequences. Such a definition is deeply connected with graph Laplacian as follows.</p><p>Fact. The Laplacian of path graph has eigenvectors:</p><formula xml:id="formula_2">u k (i) = cos (πki/n − πk/2n), for 1 ≤ k ≤ n, 1 ≤ i ≤ n.</formula><p>Here n is the number of vertices in the path graph, and u k (i) is the entry at i-th row and k-the column of U , i.e.,</p><formula xml:id="formula_3">U = u 1 • • • u n .</formula><p>The above fact shows that the positional embedding in sequence models can be viewed as Laplacian eigenvectors of path graphs. This inspires us to generalize the positional embedding from path graphs to arbitrary graphs. The reason for using the normalized graph Laplacian rather than the unnormalized version is that path graph is a regular graph (i.e., with constant degrees) while real-world graphs are often irregular and have skewed degree distributions. In addition to the generalized positional embedding, we also add the one-hot encoding of vertex degrees <ref type="bibr" target="#b59">[60]</ref> and the binary indicator of the ego vertex <ref type="bibr" target="#b41">[42]</ref> as vertex features. After encoded by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type="bibr" target="#b16">[17]</ref>.</p><p>A running example. We illustrate a running example of GCC in Figure <ref type="figure">2</ref>. For simplicity, we set the dictionary size to be 3, i.e., K = 2. GCC first randomly augment two subgraphs x q and x k 0 from a 2-ego network on the left panel of Figure <ref type="figure">2</ref>. Meanwhile, another two subgraphs, x k 1 and x k 2 , are generated from a noise distributionin this example, they are randomly augmented from another 2-ego network on the left panel of Figure <ref type="figure">2</ref>. Then the two graph encoders, f q and f k , map the query and the three keys to low-dimensional vectorsq and {k 0 , k 1 , k 2 }. Finally, the contrastive loss in Eq. 1 encourages the model to recognize (x q , x k 0 ) as a similar instance pair and distinguish them from dissimilar instances, i.e., {x k 1 , x k 2 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GCC Learning</head><p>In contrastive learning, it is required to maintain the K-size dictionary and encoders. Ideally, in Eq. 1, the dictionary should cover as many instances as possible, making K extremely large. However, due to the computational constraints, we usually design and adopt economical strategies to effectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type="bibr" target="#b16">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E samples mini-batches of instances and considers samples in the same mini-batch as the dictionary. The objective in Eq. 1 is then optimized with respect to parameters of both f q and f k , both of which can accept gradient updates by backpropagation consistently. The main drawback of E2E is that the dictionary size is constrained by the batch size.</p><p>MoCo is designed to increase the dictionary size without additional backpropagation costs. Concretely, MoCo maintains a queue of samples from preceding mini-batches. During optimization, MoCo only updates the parameters of f q (denoted by θ q ) by backpropagation. The parameters of f k (denoted by θ k ) are not updated by gradient descent. He et al. <ref type="bibr" target="#b16">[17]</ref> propose a momentum-based update rule for θ k . More formally, MoCo updates θ k by θ k ← mθ k + (1 − m)θ q , where m ∈ [0, 1) is a momentum hyper-parameter. The above momentum update rule gradually propagates the update in θ q to θ k , making θ k evolve smoothly and consistently. In summary, MoCo achieves a larger dictionary size at the expense of dictionary consistency, i.e., the key representations in the dictionary are encoded by a smoothly-varying key encoder.</p><p>In addition to E2E and MoCo, there are other contrastive learning mechanisms to maintain the dictionary, such as memory bank <ref type="bibr" target="#b58">[59]</ref>. Recently, He et al. <ref type="bibr" target="#b16">[17]</ref> show that MoCo is a more effective option than memory bank in computer vision tasks. Therefore, we mainly focus on E2E and MoCo for GCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GCC Fine-Tuning</head><p>Downstream tasks. Downstream tasks in graph learning generally fall into two categories -graph-level and node-level, where the target is to predict labels of graphs or nodes, respectively. For graph-level tasks, the input graph itself can be encoded by GCC to achieve the representation. For node-level tasks, the node representation can be defined by encoding its r -ego networks (or subgraphs augmented from its r -ego network). In either case, the encoded representations are then fed into downstream tasks to predict taskspecific outputs.</p><p>Freezing vs. full fine-tuning. GCC offers two fine-tuning strategies for downstream tasks -freezing mode and full fine-tuning mode. In the freezing mode, we freeze the parameters of the pretrained graph encoder f q and treat it as a static feature extractor, then the classifiers catering for specific downstream tasks are trained on top of the extracted features. In the full fine-tuning mode, the graph encoder f q is trained end-to-end together with the classifier on a downstream task. More implementation details about fine-tuning are available in Section 4.2.</p><p>GCC as a local algorithm. As a graph algorithm, GCC belongs to the local algorithm category <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>, in which the algorithms only involve local explorations of the input (large-scale) network, since GCC explores local structures by random walk based graph sampling. Such property enables GCC to scale to large-scale graph learning tasks and to be friendly to the distributed computing setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate GCC on three graph learning tasks -node classification, graph classification, and similarity search, which have been commonly used to benchmark graph learning algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. We first introduce the self-supervised pre-training settings in Section 4.1, and then report GCC transfer learning results on three graph learning tasks in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-Training</head><p>Datasets. Our self-supervised pre-training is performed on six graph datasets, which can be categorized into two groups -academic graphs and social graphs. As for academic graphs, we collect the Academia dataset from NetRep <ref type="bibr" target="#b43">[44]</ref> as well as two DBLP datasets from SNAP <ref type="bibr" target="#b61">[62]</ref> and NetRep <ref type="bibr" target="#b43">[44]</ref>, respectively. As for social graphs, we collect Facebook and IMDB datasets from NetRep <ref type="bibr" target="#b43">[44]</ref>, as well as a LiveJournal dataset from SNAP <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table" target="#tab_0">1</ref> presents the detailed statistics of datasets for pre-training.</p><p>Pre-training settings. We train for 75,000 steps and use Adam <ref type="bibr" target="#b23">[24]</ref> for optimization with learning rate of 0.005, β 1 = 0.9, β 2 = 0.999, ϵ = 1 × 10 −8 , weight decay of 1e-4, learning rate warmup over the first 7, 500 steps, and linear decay of the learning rate after 7, 500 steps. Gradient norm clipping is applied with range [−1, 1]. For MoCo, we use mini-batch size of 32, dictionary size of 16, 384, and momentum m of 0.999. For E2E, we use mini-batch size of 1, 024. For both MoCo and E2E, the temperature τ is set as 0.07, and we adopt GIN <ref type="bibr" target="#b59">[60]</ref> with 5 layers and 64 hidden units each layer as our encoders. Detailed hyper-parameters can be found in Table <ref type="table">6</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Task Evaluation</head><p>In this section, we apply GCC to three graph learning tasks including node classification, graph classification, and similarity search. As prerequisites, we discuss the two fine-tuning strategies of GCC as well as the baselines we compare with.</p><p>Fine-tuning. As we discussed in Section 3.5, we adopt two finetuning strategies for GCC. We select logistic regression or SVM from the scikit-learn <ref type="bibr" target="#b37">[38]</ref> package as the linear classifier for the freezing strategy <ref type="foot" target="#foot_2">4</ref> . As for the full fine-tuning strategy, we use Adam optimizer with learning rate 0.005, learning rate warmup over the first 3 epochs, and linear learning rate decay after 3 epochs.</p><p>Baselines. Baselines can be categorized into two categories. In the first category, the baseline models learn vertex/graph representations from unlabeled graph data and then feed them into logistic regression or SVM. Examples include DGK <ref type="bibr" target="#b60">[61]</ref>, Struc2vec <ref type="bibr" target="#b42">[43]</ref>, GraphWave <ref type="bibr" target="#b11">[12]</ref>, graph2vec <ref type="bibr" target="#b32">[33]</ref> and InfoGraph <ref type="bibr" target="#b45">[46]</ref>. GCC with freezing setting belongs to this category. In the second category, the models are optimized in an end-to-end supervised manner. Examples include DGCNN <ref type="bibr" target="#b66">[67]</ref> and GIN <ref type="bibr" target="#b59">[60]</ref>. GCC with the full fine-tuning setting belongs to this category. For a fair comparison, we fix the representation dimension of all models to be 64 except graph2vec and InfoGraph <ref type="foot" target="#foot_3">5</ref> . The details of baselines will be discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Node Classification.</head><p>Setup. Node classification task is to predict unknown node labels in a partially labeled network. To evaluate GCC, we sample a subgraph centered at each vertex and apply GCC on it. Then the obtained representation is fed into an output layer to predict the node label. As for datasets, we adopt US-Airport <ref type="bibr" target="#b42">[43]</ref> and H-index <ref type="bibr" target="#b63">[64]</ref>. US-Airport consists of the airline activity data among 1,190 airports. The 4 classes indicate different activity levels of the airports. Hindex is a co-authorship graph extracted from OAG <ref type="bibr" target="#b63">[64]</ref>. The labels indicate whether the h-index of the author is above or below the median.</p><p>Experimental results. We compare GCC with ProNE <ref type="bibr" target="#b64">[65]</ref>, Graph-Wave <ref type="bibr" target="#b11">[12]</ref>, and Struc2vec <ref type="bibr" target="#b42">[43]</ref>. Table <ref type="table" target="#tab_1">2</ref> represents the results. It is worth noting that, under the freezing setting, the graph encoder in GCC is not trained on either US-Airport or H-Index dataset, which other baselines use as training data. This places GCC at a disadvantage. However, GCC (MoCo, freeze) performs competitively to Struc2vec in US-Airport, and achieves the best performance in H-index where Struc2vec cannot finish in one day. Moreover, GCC can be further boosted by fully fine-tuning on the target US-Airport or H-Index domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Graph Classification.</head><p>Setup. We use five datasets from Yanardag and Vishwanathan <ref type="bibr" target="#b60">[61]</ref> -COLLAB, IMDB-BINARY, IMDB-MULTI, REDDITBINARY and REDDIT-MULTI5K, which are widely benchmarked in recent graph classification models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b66">67]</ref>. Each dataset is a set of graphs where each graph is associated with a label. To evaluate GCC on this task, we use raw input graphs as the input of GCC. Then the encoded graph-level representation is fed into a classification layer to predict the label of the graph. We compare GCC with several recent developed graph classification models, including Deep Graph Kernel (DGK) <ref type="bibr" target="#b60">[61]</ref>, graph2vec <ref type="bibr" target="#b32">[33]</ref>, InfoGraph <ref type="bibr" target="#b45">[46]</ref>, DGCNN <ref type="bibr" target="#b66">[67]</ref> and GIN <ref type="bibr" target="#b59">[60]</ref>. Among these baselines, DGK, graph2vec   and InfoGraph belong to the first category, while DGCNN and GIN belong to the second category.</p><p>Experimental Results. Table <ref type="table" target="#tab_2">3</ref> shows the comparison. In the first category, GCC (MoCo, freeze) performs competitively to InfoGraph in IMDB-B and IMDB-M, while achieves the best performance in other datasets. Again, we want to emphasize that DGK, graph2vec and InfoGraph all need to be pre-trained on target domain graphs, but GCC only relies on the graphs listed in Table <ref type="table" target="#tab_0">1</ref> for pre-training.</p><p>In the second category, we compare GCC with DGCNN and GIN. GCC achieves better performance than DGCNN and comparable performance to GIN. GIN is a recently proposed SOTA model for graph classification. We follow the instructions in the paper <ref type="bibr" target="#b59">[60]</ref> to train GIN and report the detailed results in  Setup. We adopt the co-author dataset from Zhang et al. <ref type="bibr" target="#b65">[66]</ref>, which are the conference co-author graphs of KDD, ICDM, SIGIR, CIKM, SIGMOD, and ICDE. The problem of top-k similarity search is defined as follows. Given two graphs G 1 and G 2 , for example KDD and ICDM co-author graphs, we want to find the most similar vertex v from G 1 for each vertex u in G 2 . In this dataset, the ground truth is defined to be authors publish in both conferences. Note that similarity search is an unsupervised task, so we evaluate GCC without fine-tuning. Especially, we first extract two subgraphs centered at u and v by random walk with restart graph sampling. After encoding them by GCC, we measure the similarity score between u and v to be the inner product of their representations. Finally, by sorting the above scores, we use HITS@10 (top-10 accuracy) to measure the performance of different methods. We compare GCC with RolX <ref type="bibr" target="#b17">[18]</ref>, Panther++ <ref type="bibr" target="#b65">[66]</ref> and GraphWave <ref type="bibr" target="#b11">[12]</ref>. We also provide random guess results for reference.</p><p>Experimental Results. Table <ref type="table" target="#tab_4">4</ref> presents the performance of different methods on top-k similarity search task in three co-author networks. We can see that, compared with Panther++ <ref type="bibr" target="#b65">[66]</ref> and GraphWave <ref type="bibr" target="#b11">[12]</ref> which are trained in place on co-author graphs, simply applying pre-trained GCC can be competitive.</p><p>Overall, we show that a graph neural network encoder pre-trained on several popular graph datasets can be directly adapted to new graph datasets and unseen graph learning tasks. More importantly, compared with models trained from scratch, the reused model achieves competitive and sometimes better performance. This demonstrates the transferability of graph structural patterns and the effectiveness of our GCC framework in capturing these patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Effect of pre-training. It is still not clear if GCC's good performance is due to pre-training or the expression power of its GIN <ref type="bibr" target="#b59">[60]</ref> encoder. To answer this question, we fully fine-tune GCC with its GIN encoder randomly initialized, which is equivalent to train a GIN encoder from scratch. We name this model GCC (rand), as shown  in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>. In all datasets except IMDB-B, GCC (MoCo) outperforms its randomly initialized counterpart, showing that pretraining always provides a better start point for fine-tuning than random initialization. For IMDB-B, we attribute it to the domain shift between pre-training data and down-stream tasks.</p><p>Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type="bibr" target="#b16">[17]</ref>, and a larger dictionary size K always helps. We also observe such trends, as shown in Figure <ref type="figure" target="#fig_1">3</ref>. However, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type="bibr" target="#b16">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K = 1024) by small margins in terms of accuracy -1.0 absolute gain in US-Airport and 0.8 absolute gain in COLLAB. However, training MoCo is much more economical than training E2E. E2E (K=1024) takes 5 days and 16 hours, while MoCo (K=16384) only needs 9 hours. Detailed training time can be found in Table <ref type="table" target="#tab_6">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type="bibr" target="#b16">[17]</ref>, momentum m plays a subtle role in learning high-quality representations. Table <ref type="table" target="#tab_6">5</ref> shows accuracy with different momentum values on US-Airport and COL-LAB datasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type="bibr" target="#b16">[17]</ref>, showing that building a consistent dictionary is important for MoCo. However, in COLLAB, it seems that a larger momentum value brings better performance. Moreover, we do not observe the "training loss oscillation" reported in <ref type="bibr" target="#b16">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the accuracy is much worse.</p><p>Pre-training datasets. We ablate the number of datasets used for pre-training. To avoid enumerating a combinatorial space, we pre-train with first several datasets in Table <ref type="table" target="#tab_0">1</ref>, and report the 10-fold validation accuracy scores on US-Airport and COLLAB, respectively. For example, when using one dataset for pre-training, we select Academia; when using two, we choose Academia and DBLP (SNAP); and so on. We present ordinary least squares (OLS) estimates of the relationship between the number of datasets and the model performance. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, we can observe a trend towards higher accuracy when using more datasets for pre-training. On average, adding one more dataset leads to 0.43 and 0.81 accuracy (%) gain on US-Airport and COLLAB, respectively.<ref type="foot" target="#foot_4">6</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we study graph representation learning with the goal of characterizing and transferring structural features in social and information networks. We present Graph Contrastive Coding (GCC), which is a graph-based contrastive learning framework to learn structural representations and similarity from data. The pre-trained model achieves competitive performance to its supervised trained-from-scratch counterparts in three graph learning tasks on ten graph datasets. In the future, we plan to benchmark more graph learning tasks and more graph datasets. We also would like to explore applications of GCC on graphs in other domains, such as protein-protein association networks <ref type="bibr" target="#b46">[47]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of GCC. In this example, GCC aims to measure the structural similarity between a user from the Facebook friendship network and a researcher from the DBLP co-authorship network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of contrastive loss mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>COLLAB: y = 0.8065x + 74.4737</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation study on pre-training datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets for pre-training, sorted by number of vertices.</figDesc><table><row><cell cols="4">Dataset Academia DBLP (SNAP) DBLP (NetRep)</cell><cell>IMDB</cell><cell cols="2">Facebook LiveJournal</cell></row><row><cell>|V |</cell><cell>137,969</cell><cell>317,080</cell><cell>540,486</cell><cell>896,305</cell><cell>3,097,165</cell><cell>4,843,953</cell></row><row><cell>|E |</cell><cell>739,384</cell><cell>2,099,732</cell><cell cols="3">30,491,458 7,564,894 47,334,788</cell><cell>85,691,368</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Node classification.</figDesc><table><row><cell>Datasets</cell><cell cols="2">US-Airport H-index</cell></row><row><cell>|V |</cell><cell>1,190</cell><cell>5,000</cell></row><row><cell>|E |</cell><cell>13,599</cell><cell>44,020</cell></row><row><cell>ProNE</cell><cell>62.3</cell><cell>69.1</cell></row><row><cell>GraphWave</cell><cell>60.2</cell><cell>70.3</cell></row><row><cell>Struc2vec</cell><cell cols="2">66.2 &gt; 1 Day</cell></row><row><cell>GCC (E2E, freeze)</cell><cell>64.8</cell><cell>78.3</cell></row><row><cell>GCC (MoCo, freeze)</cell><cell>65.6</cell><cell>75.2</cell></row><row><cell>GCC (rand, full)</cell><cell>64.2</cell><cell>76.9</cell></row><row><cell>GCC (E2E, full)</cell><cell>68.3</cell><cell>80.5</cell></row><row><cell>GCC (MoCo, full)</cell><cell>67.2</cell><cell>80.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Graph classification.</figDesc><table><row><cell>Datasets</cell><cell cols="5">IMDB-B IMDB-M COLLAB RDT-B RDT-M</cell></row><row><cell># graphs</cell><cell>1,000</cell><cell>1,500</cell><cell cols="3">5,000 2,000 5,000</cell></row><row><cell># classes</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>5</cell></row><row><cell>Avg. # nodes</cell><cell>19.8</cell><cell>13.0</cell><cell cols="3">74.5 429.6 508.5</cell></row><row><cell>DGK</cell><cell>67.0</cell><cell>44.6</cell><cell>73.1</cell><cell>78.0</cell><cell>41.3</cell></row><row><cell>graph2vec</cell><cell>71.1</cell><cell>50.4</cell><cell>-</cell><cell>75.8</cell><cell>47.9</cell></row><row><cell>InfoGraph</cell><cell>73.0</cell><cell>49.7</cell><cell>-</cell><cell>82.5</cell><cell>53.5</cell></row><row><cell>GCC (E2E, freeze)</cell><cell>71.7</cell><cell>49.3</cell><cell>74.7</cell><cell>87.5</cell><cell>52.6</cell></row><row><cell>GCC (MoCo, freeze)</cell><cell>72.0</cell><cell>49.4</cell><cell>78.9</cell><cell>89.8</cell><cell>53.7</cell></row><row><cell>DGCNN</cell><cell>70.0</cell><cell>47.8</cell><cell>73.7</cell><cell>-</cell><cell>-</cell></row><row><cell>GIN</cell><cell>75.6</cell><cell>51.5</cell><cell>80.2</cell><cell>89.4</cell><cell>54.5</cell></row><row><cell>GCC (rand, full)</cell><cell>75.6</cell><cell>50.9</cell><cell>79.4</cell><cell>87.8</cell><cell>52.1</cell></row><row><cell>GCC (E2E, full)</cell><cell>70.8</cell><cell>48.5</cell><cell>79.0</cell><cell>86.4</cell><cell>47.4</cell></row><row><cell>GCC (MoCo, full)</cell><cell>73.8</cell><cell>50.3</cell><cell>81.1</cell><cell>87.6</cell><cell>53.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>in the Appendix.</cell></row><row><cell>We can see that, in each dataset, the best performance of GIN is</cell></row><row><cell>achieved by different hyper-parameters. And by varying hyper-</cell></row></table><note>parameters, GIN's performance could be sensitive. However, GCC on all datasets shares the same pre-training/fine-tuning hyperparameters, showing its robustness on graph classification.4.2.3 Top-k Similarity Search.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Top-k similarity search (k = 20, 40).</figDesc><table><row><cell></cell><cell cols="2">KDD-ICDM</cell><cell cols="2">SIGIR-CIKM</cell><cell cols="2">SIGMOD-ICDE</cell></row><row><cell>|V |</cell><cell>2,867</cell><cell>2,607</cell><cell>2,851</cell><cell>3,548</cell><cell>2,616</cell><cell>2,559</cell></row><row><cell>|E |</cell><cell>7,637</cell><cell>4,774</cell><cell>6,354</cell><cell>7,076</cell><cell>8,304</cell><cell>6,668</cell></row><row><cell># groud truth</cell><cell></cell><cell>697</cell><cell></cell><cell>874</cell><cell></cell><cell>898</cell></row><row><cell>k</cell><cell>20</cell><cell>40</cell><cell>20</cell><cell>40</cell><cell>20</cell><cell>40</cell></row><row><cell>Random</cell><cell>0.0198</cell><cell>0.0566</cell><cell>0.0223</cell><cell>0.0447</cell><cell>0.0221</cell><cell>0.0521</cell></row><row><cell>RolX</cell><cell>0.0779</cell><cell>0.1288</cell><cell>0.0548</cell><cell>0.0984</cell><cell>0.0776</cell><cell>0.1309</cell></row><row><cell>Panther++</cell><cell>0.0892</cell><cell cols="2">0.1558 0.0782</cell><cell>0.1185</cell><cell>0.0921</cell><cell>0.1320</cell></row><row><cell>GraphWave</cell><cell cols="2">0.0846 0.1693</cell><cell>0.0549</cell><cell cols="3">0.0995 0.0947 0.1470</cell></row><row><cell>GCC (E2E)</cell><cell>0.1047</cell><cell>0.1564</cell><cell cols="2">0.0549 0.1247</cell><cell>0.0835</cell><cell>0.1336</cell></row><row><cell>GCC (MoCo)</cell><cell>0.0904</cell><cell>0.1521</cell><cell>0.0652</cell><cell>0.1178</cell><cell>0.0846</cell><cell>0.1425</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Momentum ablation.</figDesc><table><row><cell>momentum m</cell><cell>0</cell><cell cols="3">0.9 0.99 0.999 0.9999</cell></row><row><cell>US-Airport</cell><cell cols="2">62.3 63.2 63.7</cell><cell>65.6</cell><cell>61.5</cell></row><row><cell>COLLAB</cell><cell cols="2">76.6 75.1 77.4</cell><cell>78.9</cell><cell>79.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In this work, we consider undirected edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Vertex order doesn't matter because most of graph neural networks are invariant to permutations of their inputs<ref type="bibr" target="#b3">[4]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">In node classification tasks, we follow Struc2vec to use logistic regression. For graph classification tasks, we follow DGK<ref type="bibr" target="#b60">[61]</ref> and GIN<ref type="bibr" target="#b59">[60]</ref> to use SVM<ref type="bibr" target="#b7">[8]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We allow them to use their preferred dimension size in their papers: graph2vec uses 1024 and InfoGraph uses 512.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">The effect on US-Airport is positive, but statistically insignificant (p-value = 0.231), while the effect on COLLAB is positive and significant (p-value ≪ 0.001).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The work is supported by the National Key R&amp;D Program of China (2018YFB1402600), NSFC for Distinguished Young Scholar (61825602), and NSFC (61836013).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>The detailed hyper-parameters are listed in Table <ref type="table">6</ref>. Training times of GCC variants are listed in Figure <ref type="figure">5</ref>. 7 The training time of GCC (E2E) grows sharply with the dictionary size K while GCC (MoCo) roughly remains the same, which indicates that MoCo is more economical and easy to scale with larger dictionary size. 7 The table shows the elapsed real time for pre-training, which might be affected by other programs running on the server. GraphWave <ref type="bibr" target="#b11">[12]</ref> We download the authors' official source code and keep all the training settings as the same. The implementation requires a networkx graph and time points as input. We convert our dataset to the networkx format, and use automatic selection of the range of scales provided by the authors. We set the output embedding dimension to 64.</p><p>Code: https://github.com/snap-stanford/graphwave/.</p><p>Struc2vec <ref type="bibr" target="#b42">[43]</ref> We download the authors' official source code and use default hyper-parameters provided by the authors: We find the method hard to scale on the H-index datasets although we set the number of workers to 48, compared to 4 by default. We keep the code running for 24 hours on the H-index datasets and it failed to finish. We observed that the sampling strategy in Struc2vec takes up most of the time, as illustrated in the original paper.</p><p>Code: https://github.com/leoribeiro/struc2vec.</p><p>ProNE <ref type="bibr" target="#b64">[65]</ref> We download the authors' official code and keep hyper-parameters as the same: (1) step = 10; (2) θ = 0.5; (3) µ = 0.2. The dimension size is set to 64. Code: https://github.com/THUDM/ProNE/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Graph Classification.</head><p>DGK <ref type="bibr" target="#b60">[61]</ref>, graph2vec <ref type="bibr" target="#b32">[33]</ref>, InfoGraph <ref type="bibr" target="#b45">[46]</ref>, DGCNN <ref type="bibr" target="#b66">[67]</ref> We adopt the reported results in these papers. Our experimental setting is exactly the same except for the dimension size. Note that graph2vec uses 1024 and InfoGraph uses 512 as the dimension size.</p><p>Following GIN, we use 64.</p><p>GIN <ref type="bibr" target="#b59">[60]</ref> We use the official code released by <ref type="bibr" target="#b59">[60]</ref> and follow exactly the procedure described in their paper: the hyper-parameters tuned for each dataset are: (1) the number of hidden units ∈ {16, 32} for bioinformatics graphs and 64 for social graphs; (2) the batch size ∈ {32, 128}; (3) the dropout ratio ∈ {0, 0.5} after the dense layer; (4) the number of epochs, i.e., a single epoch with the best cross-validation accuracy averaged over the 10 folds was selected. We report the obtained results in Table <ref type="table">7</ref>.</p><p>Code: https://github.com/weihua916/powerful-gnns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Top-k Similarity Search.</head><p>Random, RolX <ref type="bibr" target="#b17">[18]</ref>, Panther++ <ref type="bibr" target="#b65">[66]</ref> We obtainÂăthe experimental results for these baselines from Zhang et al. <ref type="bibr" target="#b65">[66]</ref>.</p><p>Code: https://github.com/yuikns/panther/.</p><p>GraphWave <ref type="bibr" target="#b11">[12]</ref> Embeddings computed by the GraphWave method also have the ability to generalize across graphs. The authors evaluated on synthetic graphs in their paper which are not publicly available. To compare with GraphWave on the co-author datasets, we compute GraphWave embeddings given two graphs G 1 and G 2 and follow the same procedure mentioned in section 4.2.2 to compute the HITS@10 (top-10 accuracy) score. Code: https://github.com/snap-stanford/graphwave/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Datasets</head><p>A.3.1 Node Classification Datasets.</p><p>US-Airport 8 We obtain the US-Airport dataset directly from Ribeiro et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>H-index 9 We create the H-index dataset, a co-authorship graph extracted from OAG <ref type="bibr" target="#b63">[64]</ref>. Since the original OAG co-authorship graph has millions of nodes, it is too large as a node classification benchmark. Therefore, we implemented the following procedure to extract smaller subgraphs from OAG:</p><p>(1) Select an initial vertex set V s in OAG;</p><p>(2) Run breadth first search (BFS) from V s until N nodes are visited;</p><p>(3) Return the sub-graph induced by the visited N nodes.</p><p>We set N = 5, 000, and randomly select 20 nodes from top 200 nodes with largest degree as he initial vertex set in step <ref type="bibr" target="#b0">(1)</ref>.</p><p>A.3.2 Graph Classification Datasets. 10  We download COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI5K from Benchmark Data Sets for Graph Kernels <ref type="bibr" target="#b22">[23]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale networks fingerprinting and visualization using the k-core decomposition</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Alvarez-Hamelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Dall'asta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: membership, growth, and evolution</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;06</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Models of core/periphery structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">G</forename><surname>Borgatti</surname></persName>
		</author>
		<author>
			<persName><surname>Everett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="375" to="395" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Structural holes: The social structure of competition</title>
		<author>
			<persName><forename type="first">Burt</forename><surname>Ronald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Harvard university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ICLR &apos;19</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;17. JMLR. org</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;06</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR &apos;19</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Pre-Training of Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 Workshop: Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SimRank: a measure of structural-context similarity</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;02</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07675</idno>
		<title level="m">GraLSP: Graph Neural Networks with Local Structural Patterns</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Benchmark Data Sets for Graph Kernels</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>ICLR &apos;15</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ICLR &apos;17</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vertex similarity in networks</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Leicht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Holme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26120</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;06</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;05</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reconstructing markov processes from independent and anonymous experiments</title>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="108" to="122" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuven</forename><surname>Levitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbal</forename><surname>Ayzenshtat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Superfamilies of evolved and designed networks</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="1538" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">graph2vec: Learning distributed representations of graphs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic multimedia cross-modal correlation discovery</title>
		<author>
			<persName><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung-Jeong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="653" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10">2011. Oct (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno>KDD &apos;14. 701-710</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Netsmf: Large-scale network embedding as sparse matrix factorization</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1509" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A scalable permutation approach reveals replication and preservation patterns of network modules in large datasets</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Scott C Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><forename type="middle">G</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">E</forename><surname>Fearnley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gad</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><surname>Inouye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICLR &apos;19</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The STRING database in 2017: quality-controlled proteinprotein association networks, made broadly accessible</title>
		<author>
			<persName><forename type="first">Damian</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nadezhda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="page">937</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable algorithms for data and network analysis</title>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast random walk with restart and its applications</title>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;06</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structural diversity in social contagion</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="5962" to="5966" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR &apos;18</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ICLR &apos;19</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Collective dynamics of small-world networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">H</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR &apos;19</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">OAG: Toward Linking Large-Scale Heterogeneous Entity Graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ProNE: fast and scalable network representation learning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Panther: Fast top-k similarity search on large networks</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Top-k Similarity Search Datasets. 11 We obtain the paired conference co-author datasets</title>
		<idno>A.3.3</idno>
	</analytic>
	<monogr>
		<title level="m">KDD-ICDM, SIGIR-CIKM, SIGMOD-ICDE</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
