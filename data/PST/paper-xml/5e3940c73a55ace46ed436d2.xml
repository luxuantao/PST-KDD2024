<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Knowledge Graphs: Representation, Acquisition and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-02">2 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">A Survey on Knowledge Graphs: Representation, Acquisition and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-02">2 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.00388v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge graph</term>
					<term>representation learning</term>
					<term>knowledge graph completion</term>
					<term>relation extraction</term>
					<term>reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review on knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference and logical rule reasoning are reviewed. We further explore several emerging topics including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I NCORPORATING human knowledge is one of the research directions of artificial intelligence (AI). Knowledge representation and reasoning, inspired by human's problem solving, is to represent knowledge for intelligent systems to gain the ability to solve complex tasks. Recently, knowledge graphs as a form of structured human knowledge have drawn great research attention from both the academia and the industry. A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions. Entities can be real-world objects and abstract concepts, relationships represent the relation between entities, and semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning. Property graphs or attributed graphs are widely used, in which nodes and relations have properties or attributes.</p><p>The term of knowledge graph is synonymous with knowledge base with a minor difference. A knowledge graph can be viewed as a graph when considering its graph structure. When it involves formal semantics, it can be taken as a knowledge base for interpretation and inference over facts. Examples of knowledge base and knowledge graph are illustrated in Fig. <ref type="figure" target="#fig_6">1</ref>. Knowledge can be expressed in a factual triple in the form of (head, relation, tail) or (subject, predicate, object) under</p><p>• S. Ji is with Aalto University, Finland and The University of Queensland, Australia. E-mail: shaoxiong.ji@aalto.fi • S.</p><p>Pan is with Monash University, Australia. Email: shirui.pan@monash.edu • E. Cambria is with Nanyang Technological University, Singapore. Email: cambria@ntu.edu.sg • P.</p><p>Marttinen is with Aalto University, Finland. Email: pekka.marttinen@aalto.fi • P. S. Yu is with University of Illinois at Chicago, USA. Email: psyu@uic.edu • S. Pan is the corresponding author.</p><p>the resource description framework (RDF), for example, (Albert Einstein, WinnerOf, Nobel Prize). It can also be represented as a directed graph with nodes as entities and edges as relations. For simplicity and following the trend of research community, this paper uses the terms knowledge graph and knowledge base interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Entities and relations in knowledge graph</head><p>Fig. <ref type="figure" target="#fig_6">1</ref>: An example of knowledge base and knowledge graph Recent advances in knowledge-graph-based research focus on knowledge representation learning (KRL) or knowledge graph embedding (KGE) by mapping entities and relations into low-dimensional vectors while capturing their semantic meanings. Specific knowledge acquisition tasks include knowledge graph completion (KGC), triple classification, entity recognition, and relation extraction. Knowledgeaware models benefit from the integration of heterogeneous information, rich ontologies and semantics for knowledge representation, and multi-lingual knowledge. Thus, many real-world applications such as recommendation systems and question answering have been brought about prosperity with the ability of commonsense understanding and reasoning. Some real-world products, for example, Microsoft's Satori and Google's Knowledge Graph, have shown a strong capacity to provide more efficient services.</p><p>The remainder of this survey is organized as follows: first, an overview of knowledge graphs including history, notations, definitions and categorization is given in Section 2; then, we discuss KRL in Section 3 from four scopes; next, our review goes to tasks of knowledge acquisition and temporal knowledge graphs in Section 4 and Section 5; downstream applications are introduced in Section 6; finally, we discuss future research directions, together with a conclusion in the end. Other information, including KRL model training and a collection of knowledge graph datasets and open-source implementations can be found in the appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW 2.1 A Brief History of Knowledge Bases</head><p>Knowledge representation has experienced a long-period history of development in the fields of logic and AI. The idea of graphical knowledge representation firstly dated back to 1956 as the concept of semantic net proposed by Richens <ref type="bibr" target="#b0">[1]</ref>, while the symbolic logic knowledge can go back to the General Problem Solver <ref type="bibr" target="#b1">[2]</ref> in 1959. The knowledge base is firstly used with knowledge-based systems for reasoning and problem solving. MYCIN <ref type="bibr" target="#b2">[3]</ref> is one of the most famous rule-based expert systems for medical diagnosis with a knowledge base of about 600 rules. Later, the community of human knowledge representation saw the development of frame-based language, rule-based, and hybrid representations. Approximately at the end of this period, the Cyc project 1 began, aiming at assembling human knowledge. Resource description framework (RDF) 2 and Web Ontology Language (OWL) 3 were released in turn, and became important standards of the Semantic Web 4 . Then, many open knowledge bases or ontologies were published such as WordNet, DBpedia, YAGO, and Freebase. Stokman and Vries <ref type="bibr" target="#b3">[4]</ref> proposed a modern idea of structure knowledge in a graph in 1988. However, it was in 2012 that the concept of knowledge graph gained great popularity since its first launch by Google's search engine 5 , where the knowledge fusion framework called Knowledge Vault <ref type="bibr" target="#b4">[5]</ref> was proposed to build large-scale knowledge graphs. A brief road map of knowledge base history is illustrated in Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definitions and Notations</head><p>Most efforts have been made to give a definition by describing general semantic representation or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type="bibr" target="#b5">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W öß <ref type="bibr" target="#b6">[7]</ref> analyzed several existing definitions and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a definition as a multi-relational graph in Definition 2. Following previous literature, we define a knowledge graph as G = {E, R, F}, where E, R and F are sets of entities, relations and facts, respectively. A fact is denoted as a triple (h, r, t) ∈ F. Definition 1 (Ehrlinger and W öß <ref type="bibr" target="#b6">[7]</ref>). A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type="bibr" target="#b7">[8]</ref>). A knowledge graph is a multirelational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively. Specific notations and their descriptions are listed in Table <ref type="table" target="#tab_0">1</ref>. Details of several mathematical operations are explained in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Categorization of Research on Knowledge Graph</head><p>This survey provides a comprehensive literature review on the research of knowledge graphs, namely KRL, knowledge acquisition, and a wide range of downstream knowledgeaware applications, where many recent advanced deep learning techniques are integrated. The overall categorization of the research is illustrated in Fig. <ref type="figure">2</ref>.</p><p>1. http://cyc.com 2. Released as W3C recommendation in 1999 available at http://w3. org/TR/1999/REC-rdf-syntax-19990222.</p><p>3. http://w3.org/TR/owl-guide 4. http://w3.org/standards/semanticweb 5. http://blog.google/products/search/ introducing-knowledge-graph-things-not Knowledge Representation Learning is a critical research issue of knowledge graph which paves a way for many knowledge acquisition tasks and downstream applications. We categorize KRL into four aspects of representation space, scoring function, encoding models and auxiliary information, providing a clear workflow for developing a KRL model. Specific ingredients include: 1) representation space in which the relations and entities are represented; 2) scoring function for measuring the plausibility of factual triples; 3) encoding models for representing and learning relational interactions; 4) auxiliary information to be incorporated into the embedding methods.</p><p>Representation learning includes point-wise space, manifold, complex vector space, Gaussian distribution, and discrete space. Scoring metrics are generally divided into distance-based and similarity matching based scoring functions. Current research focuses on encoding models including linear/bilinear models, factorization, and neural networks. Auxiliary information considers textual, visual and type information. Fig. <ref type="figure">2</ref>: Categorization of research on knowledge graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Related Surveys</head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type="bibr" target="#b8">[9]</ref>, knowledge graph refinement <ref type="bibr" target="#b5">[6]</ref>, Chinese knowledge graph construction <ref type="bibr" target="#b9">[10]</ref>, KGE <ref type="bibr" target="#b7">[8]</ref> or KRL <ref type="bibr" target="#b10">[11]</ref>. The latter two surveys are more related to our work. Lin et al. <ref type="bibr" target="#b10">[11]</ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type="bibr" target="#b7">[8]</ref> categorized KRL according to scoring functions, and specifically focused on the type of information utilized in KRL. It provides a general view of current research only from the perspective of scoring metric. Our survey goes deeper to the flow of KRL, and provides a full-scaled view from four folds including representation space, scoring function, encoding models, and auxiliary information. Besides, our paper provides a comprehensive review on knowledge acquisition and knowledge-aware applications with several emerging topics such as knowledge-graph-based reasoning and few-shot learning discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KNOWLEDGE REPRESENTATION LEARNING</head><p>KRL is also known as KGE, multi-relation learning, and statistical relational learning in the literature. This section reviews recent advances on distributed representation learning with rich semantic information of entities and relations form four scopes including representation space (representing entities and relations, Section 3.1), scoring function (measuring the plausibility of facts, Section 3.2), encoding models (modeling the semantic interaction of facts, Section 3.3), and auxiliary information (utilizing external information, Section 3.4). We further provide a summary in Section 3.5. The training strategies for KRL models are reviewed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Space</head><p>The key issue of representation learning is to learn lowdimensional distributed embedding of entities and relations. Current literature mainly uses real-valued point-wise space (Fig. <ref type="figure">3a</ref>) including vector, matrix and tensor space, while other kinds of space such as complex vector space (Fig. <ref type="figure">3b</ref>), Gaussian space (Fig. <ref type="figure">3c</ref>), and manifold (Fig. <ref type="figure">3d</ref>) are utilized as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Point-Wise Space</head><p>Point-wise Euclidean space is widely applied for representing entities and relations, projecting relation embedding in vector or matrix space, or capturing relational interactions. TransE <ref type="bibr" target="#b11">[12]</ref> represents entities and relations in d-dimension vector space, i.e., h, t, r ∈ R d , and makes embeddings follow the translational principle h + r ≈ t. To tackle this problem of insufficiency of a single space for both entities and relations, TransR <ref type="bibr" target="#b12">[13]</ref> then further introduces separated spaces for entities and relations. The authors projected entities (h, t ∈ R k ) into relation (r ∈ R d ) space by a projection matrix M r ∈ R k×d . NTN <ref type="bibr" target="#b13">[14]</ref> models entities across multiple dimensions by a bilinear tensor neural layer. The relational interaction between head and tail h T Mt is captured as a tensor denoted as M ∈ R d×d×k . Many other translational models such as TransH <ref type="bibr" target="#b14">[15]</ref> also use similar representation space, while semantic matching models use plain vector space (e.g., HolE <ref type="bibr" target="#b15">[16]</ref>) and relational projection matrix (e.g., ANALOGY <ref type="bibr" target="#b16">[17]</ref>). Principles of these translational and semantic matching models are introduced in Section 3.2.1 and 3.2.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Complex Vector Space</head><p>Instead of using a real-valued space, entities and relations are represented in a complex space, where h, t, r ∈ C d . Take head entity as an example, h has a real part Re(h) and an imaginary part Im(h), i.e., h = Re(h) + i Im(h). Com-plEx <ref type="bibr" target="#b17">[18]</ref> firstly introduces complex vector space shown in Fig. <ref type="figure">3b</ref> which can capture both symmetric and antisymmetric relations. Hermitian dot product is used to do composition for relation, head and the conjugate of tail. Inspired by Euler's identity e iθ = cos θ + i sin θ, RotatE <ref type="bibr" target="#b18">[19]</ref> proposes a rotational model taking relation as a rotation from head entity to tail entity in complex space as t = h • r where • denotes the element-wise Hadmard product. QuatE <ref type="bibr" target="#b19">[20]</ref> extends the complex-valued space into hypercomplex h, t, r ∈ H d by a quaternion Q = a + bi + cj + dk with three imaginary components, where the quaternion inner product, i.e., the Hamilton product h ⊗ r, is used as compositional operator for head entity and relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Gaussian Distribution</head><p>Inspired by Gaussian word embedding, the density-based embedding model KG2E <ref type="bibr" target="#b20">[21]</ref> introduces Gaussian distribution to deal with the (un)certainties of entities and relations. The authors embedded entities and relations into multi-dimensional Gaussian distribution H ∼ N (µ h , Σ h ) and T ∼ N (µ t , Σ t ). The mean vector u indicates entities and relations' position, and the covariance matrix Σ models their (un)certainties. Following the translational principle, the probability distribution of entity transformation H − T is denoted as P e ∼ N (µ h − µ t , Σ h + Σ t ). Similarly, TransG <ref type="bibr" target="#b21">[22]</ref> represents entities with Gaussian distributions, while it draws a mixture of Gaussian distribution for relation embedding, where the m-th component translation vector of relation r is denoted as u</p><formula xml:id="formula_0">r,m = t − h ∼ N u t − u h , σ 2 h + σ 2 t E .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Manifold and Group</head><p>This section reviews knowledge representation in manifold space, Lie group and dihedral group. A manifold is a topological space which could be defined as a set of points with neighborhoods by the set theory, while the group is algebraic structures defined in abstract algebra. Previous point-wise modeling is an ill-posed algebraic system where the number of scoring equations is far more than the number of entities and relations. And embeddings are restricted in an overstrict geometric form even in some methods with subspace projection. To tackle these issues, ManifoldE <ref type="bibr" target="#b22">[23]</ref> extends point-wise embedding into manifold-based embedding. The authors introduced two settings of manifoldbased embedding, i.e., Sphere and Hyperplane. An example of a sphere is shown in Fig. <ref type="figure">3d</ref>. For the sphere setting, Reproducing Kernel Hilbert Space is used to represent the manifold function, i.e.,</p><formula xml:id="formula_1">M(h, r, t) = ϕ(h) + ϕ(r) − ϕ(t) 2 = K(h, h) + K(t, t) + K(r, r) − 2K(h, t) − 2K(r, t) + 2K(r, h),<label>(1)</label></formula><p>where ϕ maps the original space to the Hilbert space, and K is the kernel function. Another "hyperplane" setting is introduced to enhance the model with intersected embeddings, i.e., M(h, r, t) = (h + r head ) (t + r tail ) .</p><p>(2)</p><p>TorusE <ref type="bibr" target="#b23">[24]</ref> solves the regularization problem of TransE via embedding in an n-dimensional torus space which is a compact Lie group. With the projection from vector space into torus space defined as π : R n → T n , x → [x], entities and relations are denoted as [h], [r], [t] ∈ T n . Similar to TransE, it also learns embeddings following the relational translation in torus space, i.e., [h] + [r] ≈ [t]. Recently, DihEdral <ref type="bibr" target="#b24">[25]</ref> proposes dihedral symmetry group preserving a 2-dimensional polygon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring Function</head><p>The scoring function is used to measure the plausibility of facts, which is also referred to the energy function in the energy-based learning framework. Energy-based learning aims to learn the energy function E θ (x) parameterized by θ taking x as input, and to make sure positive samples have higher scores than negative samples. In this paper, the term of scoring function is adopted for unification. There are two typical types of scoring functions, i.e., distancebased (Fig. <ref type="figure">4a</ref>) and similarity-based (Fig. <ref type="figure">4b</ref>) functions, to measure the plausibility of a fact. Distance-based scoring function measures the plausibility of facts by calculating the distance between entities, where addictive translation with relations as h + r ≈ t is widely used. Semantic similarity based scoring measures the plausibility of facts by semantic matching, which usually adopts multiplicative formulation, i.e., h M r ≈ t , to transform head entity near the tail in the representation space.  <ref type="bibr" target="#b11">[12]</ref> and DistMult <ref type="bibr" target="#b25">[26]</ref> as examples.</p><formula xml:id="formula_2">r 2 R d M r 2 R d⇥d c M r 2 R d⇥d⇥k (a) Point-wise space Im(u) u = a + bi b 2 R d a 2 R d u 2 C d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Distance-based Scoring Function</head><p>An intuitional distance-based approach is to calculate the Euclidean distance between the relational projection of entities. Structural Embedding (SE) <ref type="bibr" target="#b26">[27]</ref> uses two projection matrices and L 1 distance to learn structural embedding as</p><formula xml:id="formula_3">fr(h, t) = Mr,1h − Mr,2t L 1 .<label>(3)</label></formula><p>A more intensively used principle is the translation-based scoring function that aims to learn embeddings by representing relations as translations from head to tail entities. Bordes et al. <ref type="bibr" target="#b11">[12]</ref> proposed TransE by assuming that the added embedding of h + r should be close to the embedding of t with the scoring function is defined under L 1 or L 2 constraints as</p><formula xml:id="formula_4">fr(h, t) = h + r − t L 1 /L 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Since that, many variants and extensions of TransE have been proposed. For example, TransH <ref type="bibr" target="#b14">[15]</ref> projects entities and relations into a hyperplane as</p><formula xml:id="formula_6">fr(h, t) = − h − w r hwr + r − t − w r twr 2 2 ,<label>(5)</label></formula><p>TransR <ref type="bibr" target="#b12">[13]</ref> introduces separate projection spaces for entities and relations as</p><formula xml:id="formula_7">f r (h, t) = − M r h + r − M r t 2 2 ,<label>(6)</label></formula><p>and TransD <ref type="bibr" target="#b27">[28]</ref> constructs dynamic mapping matrices M rh = r p h p + I and M rt = r p t p + I by the projection vectors h p , t p , r p ∈ R n , with the scoring function as</p><formula xml:id="formula_8">fr(h, t) = − rph p + I h + r − rpt p + I t 2 2 . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>By replacing Euclidean distance, TransA <ref type="bibr" target="#b28">[29]</ref> uses Mahalanobis distance to enable more adaptive metric learning, with the scoring function defined as</p><formula xml:id="formula_10">fr(h, t) = (|h + r − t|) Wr(|h + r − t|).<label>(8)</label></formula><p>Previous methods used additive score functions, TransF <ref type="bibr" target="#b29">[30]</ref> relaxes the strict translation and uses dot product as f r (h, t) = (h + r) t. To balance the constraints on head and tail, a flexible translation scoring function is further defined as</p><formula xml:id="formula_11">fr(h, t) = (h + r) t + h (t − r).<label>(9)</label></formula><p>Recently, ITransF <ref type="bibr" target="#b30">[31]</ref> enables hidden concepts discovery and statistical strength transferring by learning associations between relations and concepts via sparse attention vectors. TransAt <ref type="bibr" target="#b31">[32]</ref> integrates relation attention mechanism with translational embedding, and TransMS <ref type="bibr" target="#b32">[33]</ref> transmits multidirectional semantics with nonlinear functions and linear bias vectors, with the scoring function as</p><formula xml:id="formula_12">fr(h, t) = − tanh(t • r) • h + r − tanh(h • r) • t + α • (h • t) 1/2 . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>KG2E <ref type="bibr" target="#b20">[21]</ref> in Gaussian space and ManifoldE <ref type="bibr" target="#b22">[23]</ref> with manifold also use the translational distance-based scoring function. KG2E uses two scoring methods, i.e, asymmetric KL-divergence as</p><formula xml:id="formula_14">fr(h, t) = x∈R ke N (x; µ r , Σr) log N (x; µ e , Σe) N (x; µ r , Σr) dx,<label>(11)</label></formula><p>and symmetric expected likelihood as</p><formula xml:id="formula_15">fr(h, t) = log x∈R ke N (x; µ e , Σe) N (x; µ r , Σr) dx. (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>While the scoring function of ManifoldE is defined as</p><formula xml:id="formula_17">fr(h, t) = M(h, r, t) − D 2 r 2 , (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where M is the manifold function, and D r is a relationspecific manifold parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Semantic Matching</head><p>Another direction is to calculate the semantic similarity. SME <ref type="bibr" target="#b33">[34]</ref> proposes to semantically match separate combinations of entity-relation pairs of (h, r) and (r, t). Its scoring function is defined with two versions of matching blockslinear and bilinear block, i.e.,</p><formula xml:id="formula_19">fr(h, t) = g left (h, r) g right (r, t). (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>The linear matching block is defined as g left (h, t) = M l,1 h + M l,2 r + b l , and the bilinear form is g left (h, r) = (M l,1 h) • (M l,2 r)+b l . By restricting relation matrix M r to be diagonal for multi-relational representation learning, DistMult <ref type="bibr" target="#b25">[26]</ref> proposes a simplified bilinear formulation defined as</p><formula xml:id="formula_21">fr(h, t) = h diag(Mr)t.<label>(15)</label></formula><p>To capture rich interactions in relational data and compute efficiently, HolE <ref type="bibr" target="#b15">[16]</ref> introduces circular correlation of embedding, which can be interpreted as compressed tensor product, to learn compositional representations. By semantically matching circular correlation with the relation embedding, the scoring function of HolE is defined as</p><formula xml:id="formula_22">fr(h, t) = r (h t). (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>By defining a perturbed holographic compositional operator as p(a, b; c) = (c • a) b, where c is a fixed vector, the expanded holographic embedding model HolEx <ref type="bibr" target="#b34">[35]</ref> interpolates the HolE and full tensor product method. Given l vectors c 0 , • • • , c l−1 , the rank-l semantic matching metric of HolEx is defined as</p><formula xml:id="formula_24">fr(h, t) = l j=0 p (h, r; cj) • t. (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>It can be viewed as linear concatenation of perturbed HolE. Focusing on multi-relational inference, ANALOGY <ref type="bibr" target="#b16">[17]</ref> models analogical structures of relational data. It's scoring function is defined as</p><formula xml:id="formula_26">fr(h, t) = h Mrt,<label>(18)</label></formula><p>with relation matrix constrained to be normal matrices in linear mapping, i.e., M r M r = M r M r for analogical inference. Crossover interactions are introduced by CrossE <ref type="bibr" target="#b35">[36]</ref> with an interaction matrix C ∈ R nr×d to simulate the bi-directional interaction between entity and relation. The relation specific interaction is obtained by looking up interaction matrix as c r = x r C. By combining the interactive representations and matching with tail embedding, the scoring function is defined as</p><formula xml:id="formula_27">f (h, r, t) = σ tanh (cr • h + cr • h • r + b) t .<label>(19)</label></formula><p>The semantic matching principle can be encoded by neural networks further discussed in Sec. 3.3. Aforementioned two methods in Sec. 3.1.4 with group representation also follow the semantic matching principle. The scoring function of TorusE <ref type="bibr" target="#b23">[24]</ref> is defined as:</p><formula xml:id="formula_28">min (x,y)∈([h]+[r])×[t]</formula><p>x − y i. <ref type="bibr" target="#b19">(20)</ref> By modeling 2L relations as group elements, the scoring function of DihEdral <ref type="bibr" target="#b24">[25]</ref> is defined as the summation of components:</p><formula xml:id="formula_29">fr(h, t) = h Rt = L l=1 h (l) R (l) t (l) ,<label>(21)</label></formula><p>where the relation matrix R is defined in block diagonal form for R (l) ∈ D K , and entities are embedded in real-valued space for h (l) and t (l) ∈ R 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Models</head><p>This section introduces models that encode the interactions of entities and relations through specific model architectures, including linear/bilinear models, factorization models, and neural networks. Linear models formulate relations as a linear/bilinear mapping by projecting head entities into a representation space close to tail entities. Factorization aims to decompose relational data into low-rank matrices for representation learning. Neural networks encode relational data with non-linear neural activation and more complex network structures. Several neural models are illustrated in Fig. <ref type="figure" target="#fig_1">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Linear/Bilinear Models</head><p>Linear/bilinear models encode interactions of entities and relations by applying linear operation as:</p><formula xml:id="formula_30">gr (h, t) = M T r h t ,<label>(22)</label></formula><p>or bilinear transformation operations as Eq. 18. Canonical methods with linear/bilinear encoding include SE <ref type="bibr" target="#b26">[27]</ref>, SME <ref type="bibr" target="#b33">[34]</ref>, DistMult <ref type="bibr" target="#b25">[26]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and ANALOGY <ref type="bibr" target="#b16">[17]</ref>.</p><p>For TransE <ref type="bibr" target="#b11">[12]</ref> with L2 regularization, the scoring function can be expanded to the form with only linear transformation with one-dimensional vectors, i.e.,</p><formula xml:id="formula_31">h + r − t 2 2 = 2r T (h − t) − 2h T t + r 2 2 + h 2 2 + t 2 2 .<label>(23)</label></formula><p>Wang et al. <ref type="bibr" target="#b39">[40]</ref> studied various bilinear models and evaluated their expressiveness and connections by introducing the concepts of universality and consistency. The authors further showed that the ensembles of multiple linear models can improve the prediction performance through experiments. Recently, to solve the independence embedding issue of entity vectors in canonical Polyadia decomposition, SimplE <ref type="bibr" target="#b40">[41]</ref> introduces the inverse of relations and calculates the average canonical Polyadia score of (h, r, t) and (t, r −1 , h) as</p><formula xml:id="formula_32">fr(h, t) = 1 2 h • rt + t • r t , (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>where r is the embedding of inversion relation. More bilinear models are proposed from a factorization perspective discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Factorization Models</head><p>Factorization methods formulated KRL models as threeway tensor X decomposition. A general principle of tensor factorization can be denoted as X hrt ≈ h M r t, with the composition function following the semantic matching pattern. Nickel et al. <ref type="bibr" target="#b41">[42]</ref> proposed the three-way rank-r factorization RESCAL over each relational slice of knowledge graph tensor. For k-th relation of m relations, the k-th slice of X is factorized as</p><formula xml:id="formula_34">X k ≈ AR k A T . (<label>25</label></formula><formula xml:id="formula_35">)</formula><p>The authors further extended it to handle attributes of entities efficiently <ref type="bibr" target="#b42">[43]</ref>. Jenatton et al. <ref type="bibr" target="#b43">[44]</ref> then proposed a bilinear structured latent factor model (LFM), which extends RESCAL by decomposing R k = d i=1 α k i u i v i . By introducing threeway Tucker tensor decomposition, TuckER <ref type="bibr" target="#b44">[45]</ref> learns embedding by outputting a core tensor and embedding vectors of entities and relations. Its scoring function is defined as  <ref type="bibr" target="#b4">[5]</ref> and (b) CNN <ref type="bibr" target="#b36">[37]</ref> input triples into dense layer and convolution operation to learn semantic representation, (c) GCN <ref type="bibr" target="#b37">[38]</ref> acts as encoder of knowledge graphs to produce entity and relation embeddings. (d) RSN <ref type="bibr" target="#b38">[39]</ref> encodes entity-relation sequences and skips relations discriminatively.</p><formula xml:id="formula_36">fr (h, t) = W ×1 h ×2 r ×3 t,<label>(26)</label></formula><p>where W ∈ R de×dr×de is the core tensor of Tucker decomposition and × n denotes the tensor product along the n-th mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Neural Networks</head><p>Neural networks for encoding semantic matching have yielded remarkable predictive performance in recent studies.</p><p>Encoding models with linear/bilinear blocks can also be modeled using neural networks, for example, SME <ref type="bibr" target="#b33">[34]</ref>.</p><p>Representative neural models include multi-layer perceptron (MLP) <ref type="bibr" target="#b4">[5]</ref>, neural tensor network (NTN) <ref type="bibr" target="#b13">[14]</ref>, and neural association model (NAM) <ref type="bibr" target="#b45">[46]</ref>. Generally, they take entities and/or relations into deep neural networks and compute a semantic matching score. MLP <ref type="bibr" target="#b4">[5]</ref> (Fig. <ref type="figure" target="#fig_1">5a</ref>) encodes entities and relations together into a fully-connected layer, and uses a second layer with sigmoid activation for scoring a triple as</p><formula xml:id="formula_37">fr(h, t) = σ(w σ(W[h, r, t])),<label>(27)</label></formula><p>where W ∈ R n×3d is the weight matrix and [h, r, t] is a concatenation of three vectors. NTN <ref type="bibr" target="#b13">[14]</ref> takes entity embeddings as input associated with a relational tensor and outputs predictive score in as</p><formula xml:id="formula_38">fr(h, t) = r σ(h T Mt + Mr,1h + Mr,2t + br),<label>(28)</label></formula><p>where b r ∈ R k is bias for relation r, M r,1 and M r,2 are relation-specific weight matrices. It can be regarded as a combination of MLPs and bilinear models. NAM <ref type="bibr" target="#b45">[46]</ref> associates the hidden encoding with the embedding of tail entity, and proposes the relational-modulated neural network (RMNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Convolutional Neural Networks</head><p>CNNs are utilized for learning deep expressive features. ConvE <ref type="bibr" target="#b46">[47]</ref> uses 2D convolution over embeddings and multiple layers of nonlinear features to model the interactions between entities and relations by reshaping head entity and relation into 2D matrix, i.e., M h ∈ R dw×d h and M r ∈ R dw×d h for d = d w ×d h . Its scoring function is defined as</p><formula xml:id="formula_39">fr (h, t) = σ (vec (σ ([M h ; Mr] * ω)) W) t, (<label>29</label></formula><formula xml:id="formula_40">)</formula><p>where ω is the convolutional filters and vec is the vectorization operation reshaping a tensor into a vector. ConvE can express semantic information by non-linear feature learning through multiple layers. ConvKB <ref type="bibr" target="#b36">[37]</ref> adopts CNNs for encoding the concatenation of entities and relations without reshaping (Fig. <ref type="figure" target="#fig_1">5b</ref>). Its scoring function is defined as</p><formula xml:id="formula_41">fr(h, t) = concat (σ ([h, r, t] * ω)) • w. (<label>30</label></formula><formula xml:id="formula_42">)</formula><p>The concatenation of a set for feature maps generated by convolution increases the learning ability of latent features.</p><p>Compared with ConvE which captures the local relationships, ConvKB keeps the transitional characteristic and shows better experimental performance. HypER <ref type="bibr" target="#b47">[48]</ref> utilizes hypernetwork H for 1D relation-specific convolutional filter generation to achieve multi-task knowledge sharing, and meanwhile simplifies 2D ConvE. It can also be interpreted as a tensor factorization model when taking hypernetwork and weight matrix as tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Recurrent Neural Networks</head><p>Aforementioned MLP-and CNN-based models learn triplelevel representation. To capture long-term relational dependency in knowledge graphs, recurrent networks are utilized. Gardner et al. <ref type="bibr" target="#b48">[49]</ref> and Neelakantan et al. <ref type="bibr" target="#b49">[50]</ref> propose RNNbased model over relation path to learn vector representation without and with entity information, respectively. RSN <ref type="bibr" target="#b38">[39]</ref> (Fig. <ref type="figure" target="#fig_1">5d</ref>) designs a recurrent skip mechanism to enhance semantic representation learning by distinguishing relations and entities. The relational path as (x 1 , x 2 , . . . , x T ) with entities and relations in an alternating order is generated by random walk, and it is further used to calculate recurrent hidden state</p><formula xml:id="formula_43">h t = tanh (W h h t−1 + W x x t + b).</formula><p>The skipping operation is conducted as</p><formula xml:id="formula_44">h t = ht xt ∈ E S1ht + S2xt−1 xt ∈ R ,<label>(31)</label></formula><p>where S 1 and S 2 are weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.6">Transformers</head><p>Transformer-based models have boosted contextualized text representation learning. To utilize contextual information in knowledge graphs, CoKE <ref type="bibr" target="#b50">[51]</ref> employs transformers to encode edges and path sequences. Similarly, KG-BERT <ref type="bibr" target="#b51">[52]</ref> borrows the idea form language model pre-training and takes Bidirectional Encoder Representations from Transformer (BERT) as encoder for entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.7">Graph Neural Networks</head><p>GNNs are introduced for learning connectivity structure under an encoder-decoder framework. R-GCN <ref type="bibr" target="#b52">[53]</ref> proposes relation-specific transformation to model the directed nature of knowledge graphs. Its forward propagation is defined as</p><formula xml:id="formula_45">x (l+1) i = σ   r∈R j∈N r i 1 ci,r W (l) r x (l) j + W (l) 0 x (l) i   ,<label>(32)</label></formula><p>where</p><formula xml:id="formula_46">x (l) i ∈ R d (l)</formula><p>is the hidden state of the i-th entity in l-th layer, N r i is a neighbor set of i-th entity within relation r ∈ R, W (l) r and W (l) 0 are the learnable parameter matrices, and c i,r is normalization such as c i,r = |N r i |. Here, the GCN <ref type="bibr" target="#b53">[54]</ref> acts as a graph encoder. To enable specific tasks, an encoder model still needs to be developed and integrated into the R-GCN framework. R-GCN takes the neighborhood of each entity equally. SACN <ref type="bibr" target="#b37">[38]</ref> introduces weighted GCN (Fig. <ref type="figure" target="#fig_1">5c</ref>), defining the strength of two adjacent nodes with the same relation type, to capture the structural information in knowledge graphs by utilizing node structure, node attributes, and relation types. The decoder module called Conv-TransE adopts ConvE model as semantic matching metric and preserves the translational property. By aligning the convolutional outputs of entity and relation embeddings with C kernels to be M (h, r) ∈ R C×d , its scoring function is defined as</p><formula xml:id="formula_47">fr(h, t) = g (vec (M (h, r)) W ) t.<label>(33)</label></formula><p>Nathani et al. <ref type="bibr" target="#b54">[55]</ref> introduced graph attention networks with multi-head attention as encoder to capture multi-hop neighborhood features by inputing the concatenation of entity and relation embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Embedding with Auxiliary Information</head><p>To facilitate more effective knowledge representation, multimodal embedding incorporates external information such as text descriptions, type constraints, relational paths, and visual information, with a knowledge graph itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Textual Description</head><p>Entities in knowledge graphs have textual descriptions denoted as D =&lt; w 1 , w 2 , . . . , w n &gt;, providing supplementary semantic information. The challenge of KRL with textual description is to embed both structured knowledge and unstructured textual information in the same space. Wang et al. <ref type="bibr" target="#b55">[56]</ref> proposed two alignment models for aligning entity space and word space by introducing entity names and Wikipedia anchors. DKRL <ref type="bibr" target="#b56">[57]</ref> extends TransE <ref type="bibr" target="#b11">[12]</ref> to learn representation directly from entity descriptions by a convolutional encoder. SSP <ref type="bibr" target="#b57">[58]</ref> models the strong correlations between triples and textual descriptions by projecting them in a semantic subspace. Joint loss function is widely applied when incorporating KGE with textual description. Wang et al. <ref type="bibr" target="#b55">[56]</ref> used a three-component loss</p><formula xml:id="formula_48">L = L K + L T + L A of knowledge model L K , text model L T</formula><p>and alignment model L A . SSP <ref type="bibr" target="#b57">[58]</ref> uses a two-component objective function L = L embed + µL topic of embeddingspecific loss L embed and topic-specific loss L topic within textual description, traded off by a parameter µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Type Information</head><p>Entities are represented with hierarchical classes or types, and consequently, relations with semantic types. SSE <ref type="bibr" target="#b58">[59]</ref> incorporates semantic categories of entities to embed entities belonging to the same category smoothly in semantic space. TKRL <ref type="bibr" target="#b59">[60]</ref> proposes type encoder model for projection matrix of entities to capture type hierarchy. Noticing that some relations indicate attributes of entities, KR-EAR <ref type="bibr" target="#b60">[61]</ref> categorizes relation types into attributes and relations and modeled the correlations between entity descriptions. Zhang et al. <ref type="bibr" target="#b61">[62]</ref> extended existing embedding methods with hierarchical relation structure of relation clusters, relations and sub-relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Visual Information</head><p>Visual information (e.g., entity images) can be utilized to enrich KRL. Image-embodied IKRL <ref type="bibr" target="#b62">[63]</ref>, containing crossmodal structure-based and image-based representation, encodes images to entity space, and follows the translation principle. The cross-modal representations make sure that structure-based and image-based representations are in the same representation space. There still remains many kinds of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type="bibr" target="#b7">[8]</ref> gave a detailed review on these information. This paper discusses relation path and logical rules under the umbrella of KGC in Sec. 4.1.2 and 4.1.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>Knowledge representation learning is important in the research community of knowledge graph . This section reviews four folds of KRL with several recent methods summarized in Table <ref type="table" target="#tab_2">2</ref> and more in Appendix C. Overall, developing a novel KRL model is to answer the following four questions: 1) which representation space to choose; 2) how to measure the plausibility of triples in specific space; 3) what encoding model to modeling relational interaction; 4) whether to utilize auxiliary information.</p><p>The most popularly used representation space is Euclidean point-based space by embedding entities in vector space and modeling interactions via vector, matrix or tensor. Other representation spaces including complex vector space, Gaussian distribution, and manifold space and group are also studied. Manifold space has an advantage over pointwise Euclidean space by relaxing the point-wise embedding. Gaussian embeddings are able to express the uncertainties of entities and relations, and multiple relation semantics. Embedding in complex vector space can model different relational connectivity patterns effectively, especially the symmetry/antisymmetry pattern. The representation space plays an important role in encoding the semantic information of entities and capturing the relational properties. When developing a representation learning model, appropriate representation space should be selected and designed carefully to match the nature of encoding methods and balance the expressiveness and computational complexity. The scoring function with distance-based metric utilizes the translation principle, while the semantic matching scoring function employs compositional operators. Encoding models, especially neural networks, play a critical role in modeling interactions of entities and relations. The bilinear models also have drawn much attention, and some tensor factorization can also be regarded as this family. Other methods incorporate auxiliary information of textual description, relation/entity types, and entity images. </p><formula xml:id="formula_49">RotatE [19] h, t ∈ C d , r ∈ C d h • r − t TorusE [24] [h], [t] ∈ T n , [r] ∈ T n min (x,y)∈([h]+[r])×[t] x − y i SimplE [41] h, t ∈ R d , r, r ∈ R d 1 2 h • rt + t • r t TuckER [45] h, t ∈ R d e , r ∈ R d r W ×1 h ×2 r ×3 t ITransF [31] h, t ∈ R d , r ∈ R d α H r • D • h + r − α T r • D • t HolEx [35] h, t ∈ R d , r ∈ R d l j=0 p (h, r; cj ) • t CrossE [36] h, t ∈ R d , r ∈ R d σ σ (cr • h + cr • h • r + b) t QuatE [20] h, t ∈ H d , r ∈ H d h ⊗ r |r| • t SACN [38] h, t ∈ R d , r ∈ R d g (vec (M (h, r)) W ) t ConvKB [37] h, t ∈ R d , r ∈ R d concat (g ([h, r, t] * ω)) w ConvE [47] M h ∈ R dw ×d h , t ∈ R d σ (vec (σ ([M h ; Mr] * ω)) W) t Mr ∈ R dw ×d h DihEdral [25] h (l) , t (l) ∈ R 2 L l=1 h (l) R (l) t (l) R (l) ∈ D K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KNOWLEDGE ACQUISITION</head><p>Knowledge acquisition aims to construct knowledge graphs from unstructured text, complete an existing knowledge graph, and discover and recognize entities and relations. Well-constructed and large-scale knowledge graphs can be useful for many downstream applications and empower knowledge-aware models with the ability of commonsense reasoning, thereby paving the way for AI. The main tasks of knowledge acquisition include relation extraction, KGC, and other entity-oriented acquisition tasks such entity recognition and entity alignment. Most methods formulate KGC and relation extraction separately. These two tasks, however, can also be integrated into a unified framework. Han et al. <ref type="bibr" target="#b63">[64]</ref> proposed a joint learning framework with mutual attention for data fusion between knowledge graphs and text, which solves KGC and relation extraction from text. There are also other tasks related to knowledge acquisition such as triple classification and relation classification. In this section, three-fold knowledge acquisition techniques on KGC, entity discovery and relation extraction are reviewed thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Graph Completion</head><p>Because of the nature of incompleteness of knowledge graphs, KGC is developed to add new triples to a knowledge graph. Typical subtasks include link prediction, entity prediction and relation prediction. Here gives a task-oriented definition as Def. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. Given an incomplete knowledge graph</head><formula xml:id="formula_50">G = (E, R, F), KGC is to infer missing triples T = {(h, r, t)|(h, r, t) / ∈ F}.</formula><p>Preliminary research on KGC focused on learning lowdimensional embedding for triple prediction. In this survey, we term those methods as embedding-based methods. Most of them, however, failed to capture multi-step relationships. Thus, recent work turns to explore multi-step relation paths and incorporate logical rules, termed as relation path inference and rule-based reasoning, respectively. Triple classification as an associated task of KGC, which evaluates the correctness of a factual triple, is additionally reviewed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Embedding-based Models</head><p>Taking entity prediction as an example, embedding-based ranking methods as shown in Fig. <ref type="figure" target="#fig_2">6a</ref> firstly learn embedding vectors based on existing triples, and then replace tail entity or head entity with each entity e ∈ E to calculate scores of all the candidate entities and rank the top k entities. Aforementioned KRL methods (e.g., TransE <ref type="bibr" target="#b11">[12]</ref>, TransH <ref type="bibr" target="#b14">[15]</ref>, TransR <ref type="bibr" target="#b12">[13]</ref>, HolE <ref type="bibr" target="#b15">[16]</ref>, and R-GCN <ref type="bibr" target="#b52">[53]</ref>) and joint learning methods like DKRL <ref type="bibr" target="#b56">[57]</ref> with textual information can been used for KGC.</p><p>Unlike representing inputs and candidates in the unified embedding space, ProjE <ref type="bibr" target="#b64">[65]</ref> proposes a combined embedding by space projection of the known parts of input triples, i.e., (h, r, ?) or (?, r, t), and the candidate entities with the candidate-entity matrix W c ∈ R s×d , where s is the number of candidate entities. The embedding projection function including a neural combination layer and a output projection layer is defined as h(e, r) = g (W c σ(e ⊕ r) + b p ), where e ⊕ r = D e e + D r r + b c is the combination operator of input entity-relation pair. Previous embedding methods do not differentiate entities and relation prediction, and ProjE does not support relation prediction. Based on these observations, SENN <ref type="bibr" target="#b65">[66]</ref> distinguishes three KGC subtasks explicitly by introducing a unified neural shared embedding with adaptively weighted general loss function to learn different latent features. Existing methods rely heavily on existing connections in knowledge graphs and fail to capture the evolution of factual knowledge or entities with a few connections. ConMask <ref type="bibr" target="#b66">[67]</ref> proposes relationship-dependent content masking over the entity description to select relevant snippets of given relations, and CNN-based target fusion to complete the knowledge graph with unseen entities. It can only make prediction when query relations and entities are explicitly expressed in the text description. Previous methods are discriminative models which rely on preprepared entity pairs or text corpus. Focusing on medical domain, REM-EDY <ref type="bibr" target="#b67">[68]</ref> proposes a generative model called conditional relationship variational autoencoder for entity pair discovery from latent space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Relation Path Reasoning</head><p>Embedding learning of entities and relations has gained remarkable performance in some benchmarks, but it fails to model complex relation paths. Relation path reasoning turns to leverage path information over the graph structure. Random walk inference has been widely investigated, for example, the Path-Ranking Algorithm (PRA) <ref type="bibr" target="#b68">[69]</ref> chooses relational path under a combination of path constraints, and conducts maximum-likelihood classification. To improve path search, Gardner et al. <ref type="bibr" target="#b48">[49]</ref> introduced vector space similarity heuristics in random work by incorporating textual content, which also relieves the feature sparsity issue in PRA.</p><p>Neural multi-hop relational path modeling is also studied. Neelakantan et al. <ref type="bibr" target="#b49">[50]</ref> developed a RNN model to compose the implications of relational paths by applying compositionality recursively (in Fig. <ref type="figure" target="#fig_2">6b</ref>). Chain-of-Reasoning <ref type="bibr" target="#b69">[70]</ref>, a neural attention mechanism to enable multiple reasons, represents logical composition across all relations, entities and text. Recently, DIVA <ref type="bibr" target="#b70">[71]</ref> proposes a unified variational inference framework that takes multi-hop reasoning as two sub-steps of path-finding (a prior distribution for underlying path inference) and path-reasoning (a likelihood for link classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">RL-based Path Finding</head><p>Deep reinforcement learning (RL) is introduced for multihop reasoning by formulating path-finding between entity pairs as sequential decision making, specifically a Markov decision process (MDP). The policy-based RL agent learns to find a step of relation to extend the reasoning paths via the interaction between the knowledge graph environment, where the policy gradient is utilized for training RL agents. DeepPath <ref type="bibr" target="#b71">[72]</ref> firstly applies RL into relational path learning and develops a novel reward function to improve accuracy, path diversity, and path efficiency. It encodes states in the continuous space via a translational embedding method, and takes the relation space as its action space. Similarly, MINERVA <ref type="bibr" target="#b72">[73]</ref> takes path walking to the correct answer entity as a sequential optimization problem by maximizing the expected reward. It excludes the target answer entity and provides more capable inference. Instead of using a binary reward function, Multi-Hop <ref type="bibr" target="#b73">[74]</ref> proposes a soft reward mechanism. To enable more effective path exploration, action dropout is also adopted to mask some outgoing edges during training. M-Walk <ref type="bibr" target="#b74">[75]</ref> applies an RNN controller to capture the historical trajectory and uses the Monte Carlo Tree Search (MCTS) for effective path generation. By leveraging text corpus with the sentence bag of current entity denoted as b et , CPL <ref type="bibr" target="#b75">[76]</ref> proposes collaborative policy learning for path finding and fact extraction from text.</p><p>With source, query and current entity denoted as e s , e q and e t , and query relation denoted as r q , the MDP environment and policy networks of these methods are summarized in Table <ref type="table" target="#tab_3">3</ref>, where MINERVA, M-Walk and CPL use binary reward. For the policy networks, DeepPath uses fully-connected network, the extractor of CPL employs CNN, while the rest uses recurrent networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Rule-based Reasoning</head><p>To better make use of the symbolic nature of knowledge, another research direction of KGC is logical rule learning.</p><p>A rule is defined by the head and body in the form of head ← body. The head is an atom, i.e., a fact with variable subjects and/or objects, while the body can be a set of atoms. For example, given relations sonOf, hasChild and gender, and entities X and Y , there is a rule in the reverse form of logic programming as:</p><formula xml:id="formula_51">(Y, sonOf, X) ← (X, hasChild, Y) ∧ (Y, gender, Male) (34)</formula><p>Logical rules can been extracted by rule mining tools like AMIE <ref type="bibr" target="#b76">[77]</ref>. The recent RLvLR <ref type="bibr" target="#b77">[78]</ref> proposes a scalable rule mining approach with efficient rule searching and pruning, and uses the extracted rules for link prediction.</p><p>More research attention focuses on injecting logical rules into embeddings to improve reasoning, with joint learning or iterative training applied to incorporate first-order logic rules. For example, KALE <ref type="bibr" target="#b78">[79]</ref> proposes a unified joint model with t-norm fuzzy logical connectives defined for compatible triples and logical rules embedding. Specifically, three compositions of logical conjunction, disjunction and negation are defined to compose the truth value of complex formula. Fig. <ref type="figure" target="#fig_3">7a</ref> illustrates a simple first-order Horn clause inference. RUGE <ref type="bibr" target="#b79">[80]</ref> proposes an iterative model, where soft rules are utilized for soft label prediction from unlabeled triples and labeled triples for embedding rectification. IterE <ref type="bibr" target="#b80">[81]</ref> proposes an iterative training strategy with three components of embedding learning, axiom induction and axiom injection.</p><p>The combination of neural and symbolic models has also attracted increasing attention to do rule-based reasoning in an end-to-end manner. Neural Theorem Provers (NTP) <ref type="bibr" target="#b81">[82]</ref> learns logical rules for multi-hop reasoning which utilizes radial basis function kernel for differentiable computation on vector space. NeuralLP <ref type="bibr" target="#b82">[83]</ref> enables gradient-based optimization to be applicable in the inductive logic programming, where a neural controller system is proposed by integrating attention mechanism and auxiliary memory. pLogicNet <ref type="bibr" target="#b83">[84]</ref> proposes probabilistic logic neural networks (Fig. <ref type="figure" target="#fig_3">7b</ref>) to leverage first-order logic and learn effective embedding by combining the advantages of Markov logic networks and KRL methods, while handling the uncertainty of logic rules. ExpressGNN <ref type="bibr" target="#b84">[85]</ref> generalizes pLogicNet by tuning graph networks and embedding, and achieves more efficient logical reasoning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Meta Relational Learning</head><p>The long-tail phenomena exist in the relations of knowledge graphs. Meanwhile, the real-world scenario of knowledge is dynamic, where unseen triples are usually acquired. The new DeepPath <ref type="bibr" target="#b71">[72]</ref> Global 1 et = eq or −1 et = eq (et, eq − et)</p><p>{r} Efficiency</p><formula xml:id="formula_52">1 length(p)</formula><p>Fully-connected network (FCN)</p><formula xml:id="formula_53">Diversity − 1 |F | |F | i=1 cos (p, pi)</formula><p>MINERVA <ref type="bibr" target="#b72">[73]</ref> (et, es, rq, eq) {(et, r, v)} I {et = eq} ht = LST M (ht−1, [at−1; ot]) Multi-Hop <ref type="bibr" target="#b73">[74]</ref> (et, (es, rq)) r , e | et, r , e ∈ G γ + (1 − γ) fr q (es, e T ) ht = LST M (ht−1, at−1) M-Walk <ref type="bibr" target="#b74">[75]</ref> st−1 <ref type="bibr" target="#b75">[76]</ref> Extractor be t , et r , e (et,r ,e ) ∈ be t step-wise delayed from reasoner PCNN-ATT scenario, called as meta relational learning or few-shot relational learning, requires models to predict new relational facts with only a very few samples.</p><formula xml:id="formula_54">∪ at−1, vt, E G v t , Vv t t E G v t ∪ {STOP} I {et = eq} GRU-RNN + FCN CPL [76] Reasoner (es, rq, ht) {e G } I {et = eq} ht = LST M (ht−1, [rt, et]) CPL</formula><p>Targeting at the previous two observations, GMatching <ref type="bibr" target="#b85">[86]</ref> develops a metric based few-shot learning method with entity embeddings and local graph structures. It encodes one-hop neighbors to capture the structural information with R-GCN, and then takes the structural entity embedding for multi-step matching guided by long short-term memory (LSTM) networks to calculate the similarity scores. Meta-KGR <ref type="bibr" target="#b86">[87]</ref>, an optimization-based meta learning approach, adopts model agnostic meta learning for fast adaption and reinforcement learning for entity searching and path reasoning. Inspired by model-based and optimization-based meta learning, MetaR <ref type="bibr" target="#b87">[88]</ref> transfers relation-specific meta information from support set to query set, and archives fast adaption via loss gradient of high-order relational representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Triple Classification</head><p>Triple classification is to determine whether facts are correct in testing data, which is typically regarded as a binary classification problem. The decision rule is based on the scoring function with a specific threshold. Aforementioned embedding methods could be applied for triple classification, including translational distance-based methods like TransH <ref type="bibr" target="#b14">[15]</ref> and TransR <ref type="bibr" target="#b12">[13]</ref> and semantic matching-based methods such as NTN <ref type="bibr" target="#b13">[14]</ref>, HolE <ref type="bibr" target="#b15">[16]</ref> and ANALOGY <ref type="bibr" target="#b16">[17]</ref>.</p><p>Vanilla vector-based embedding methods failed to deal with 1-to-n relations. Recently, Dong et al. <ref type="bibr" target="#b88">[89]</ref> extended the embedding space into region-based n-dimensional balls where tail region is in head region for 1-to-n relation using fine-grained type chains, i.e., tree-structure conceptual clusterings. This relaxation of embedding to n-balls turns triple classification into a geometric containment problem, and improves the performance for entities with long type chains. However, it relies on the type chains of entities, and suffers from the scalability problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Discovery</head><p>This section distinguishes entity-based knowledge acquisition into several fractionized tasks, i.e., entity recognition, entity disambiguation, entity typing, and entity alignment. We term them as entity discovery as they all explore entityrelated knowledge under different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Entity Recognition</head><p>Entity recognition or named entity recognition (NER), when it focuses on specifically named entities, is a task that tags entities in text. Hand-crafted features such as capitalization patterns and language-specific resources like gazetteers are applied in many literatures. Recent work applies sequence-tosequence neural architectures, for example, LSTM-CNN <ref type="bibr" target="#b89">[90]</ref> for learning character-level and word-level features and encoding partial lexicon matches. Lample et al. <ref type="bibr" target="#b90">[91]</ref> proposed stacked neural architectures by stacking LSTM layers and CRF layers, i.e., LSTM-CRF (in Fig. <ref type="figure" target="#fig_4">8a</ref>) and Stack-LSTM. Recently, MGNER <ref type="bibr" target="#b91">[92]</ref> proposes an integrated framework with entity position detection in various granularities and attention-based entity classification for both nested and nonoverlapping named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Entity Typing</head><p>Entity typing includes coarse and fine-grained types, while the latter one uses a tree-structured type category and is typically regarded as multi-class and multi-label classification. To reduce label noise, PLE <ref type="bibr" target="#b92">[93]</ref> focuses on correct type identification and proposes a partial-label embedding model with a heterogenous graph for the representation of entity mentions, text features and entity types and their relationships. To tackle the increasing growth of type set and noisy labels, Ma et al. <ref type="bibr" target="#b93">[94]</ref> proposed prototype-driven label embedding with hierarchical information for zero-shot fine-grained named entity typing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Entity Disambiguation</head><p>Entity disambiguation or entity linking is a unified task which links entity mentions to the corresponding entities in a knowledge graph. For example, Einstein won Noble Prize in Physics in 1921. The entity mention of "Einstein" should be linked to the entity of Albert Einstein. The trendy end-to-end learning approaches have made efforts through representation learning of entities and mentions, for example, DSRM <ref type="bibr" target="#b94">[95]</ref> for modeling entity semantic relatedness and EDKate <ref type="bibr" target="#b95">[96]</ref> for the joint embedding of entity and text. Ganea and Hofmann <ref type="bibr" target="#b96">[97]</ref> proposed an attentive neural model over local context windows for entity embedding learning and differentiable message passing for inferring ambiguous entities. By regarding relations between entities as latent variables, Le and Titov <ref type="bibr" target="#b97">[98]</ref> developed an end-toend neural architecture with relation-wise and mention-wise normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Entity Alignment</head><p>Aforementioned tasks involve with entity discovery from text or a single knowledge graph, while entity alignment (EA) aims to fuse knowledge among heterogeneous knowledge graphs. Given E 1 and E 2 as two different entity sets of two different knowledge graphs, EA is to find an alignment set A = {(e 1 , e 2 ) ∈ E 1 × E 2 |e 1 ≡ e 2 }, where entity e 1 and entity e 2 hold an equivalence relation ≡. In practice, a small set of alignment seeds (i.e., synonymous entities appear in different knowledge graphs) is given to start the alignment process as shown in the left box of Fig. <ref type="figure" target="#fig_4">8b</ref>.</p><p>Embedding-based alignment calculates the similarity between embeddings of a pair of entities. IPTransE <ref type="bibr" target="#b98">[99]</ref> maps entities into a unified representation space under a joint embedding framework (Fig. <ref type="figure" target="#fig_4">8b</ref>) through aligned translation as e 1 + r (E1→E2) − e 2 , linear transformation as M (E1→E2) e 1 − e 2 , and parameter sharing as e 1 ≡ e 2 . To solve error accumulation in iterative alignment, BootEA <ref type="bibr" target="#b99">[100]</ref> proposes a bootstrapping approach in an incremental training manner, together with an editing technique for checking newly-labeled alignment.</p><p>Additional information of entities is also incorporated for refinement, for example, JAPE <ref type="bibr" target="#b100">[101]</ref> capturing the correlation between cross-lingual attributes, KDCoE <ref type="bibr" target="#b101">[102]</ref> embedding multi-lingual entity descriptions via co-training, MultiKE <ref type="bibr" target="#b102">[103]</ref> learning multiple views of entity name, relation and attributes, and alignment with character attribute embedding <ref type="bibr" target="#b103">[104]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation Extraction</head><p>Relation extraction is a key task to build large-scale knowledge graphs automatically by extracting unknown relational facts from plain text and adding them into knowledge graphs. Due to the lack of labeled relational data, distant supervision <ref type="bibr" target="#b104">[105]</ref>, also referred as weak supervision or self supervision, uses heuristic matching to create training data by assuming that sentences containing the same entity mentions may express the same relation under the supervision of a relational database. Mintz et al. <ref type="bibr" target="#b105">[106]</ref> adopted the distant supervision for relation classification with textual features including lexical and syntactic features, named entity tags, and conjunctive features. Traditional methods rely highly on feature engineering <ref type="bibr" target="#b105">[106]</ref>, with a recent approach exploring the inner correlation between features <ref type="bibr" target="#b106">[107]</ref>. Deep neural networks is changing the representation learning of knowledge graphs and texts. This section reviews recent advances of neural relation extraction (NRE) methods, with an overview illustrated in Fig. <ref type="figure">9</ref>.  <ref type="bibr" target="#b109">[110]</ref> applies the piecewise max pooling over the segments of convolutional representation divided by entity position. Compared with vanilla CNN <ref type="bibr" target="#b107">[108]</ref>, PCNN can more efficiently capture the structural information within entity pair. MIMLCNN <ref type="bibr" target="#b110">[111]</ref> further extends it to multilabel learning with cross-sentence max pooling for feature selection. Side information such as class ties <ref type="bibr" target="#b111">[112]</ref> and relation path <ref type="bibr" target="#b112">[113]</ref> is also utilized. RNNs are also introduced, for example, SDP-LSTM <ref type="bibr" target="#b113">[114]</ref> adopts multi-channel LSTM while utilizing the shortest dependency path between entity pair, and Miwa et al. <ref type="bibr" target="#b114">[115]</ref> stacks sequential and tree-structure LSTMs based on dependency tree. BRCNN <ref type="bibr" target="#b115">[116]</ref> combines RNN for capturing sequential dependency with CNN for representing local semantics using two-channel bidirectional LSTM and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Attention Mechanism</head><p>Many variants of attention mechanisms are combined with CNNs, for example, word-level attention to capture semantic information of words <ref type="bibr" target="#b116">[117]</ref> and selective attention over multiple instances to alleviate the impact of noisy instances <ref type="bibr" target="#b117">[118]</ref>. Other side information is also introduced for enriching semantic representation. APCNN <ref type="bibr" target="#b118">[119]</ref> introduces entity description by PCNN and sentence-level attention, while HATT <ref type="bibr" target="#b119">[120]</ref> proposes hierarchical selective attention to capture the relation hierarchy by concatenating attentive representation of each hierarchical layer. Rather than CNNbased sentence encoders, Att-BLSTM <ref type="bibr" target="#b120">[121]</ref> proposes wordlevel attention with BiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Graph Convolutional Networks</head><p>GCNs are utilized for encoding dependency tree over sentences or learning KGEs to leverage relational knowledge for sentence encoding. C-GCN <ref type="bibr" target="#b121">[122]</ref> is a contextualized GCN model over pruned dependency tree of sentences after path-centric pruning. AGGCN <ref type="bibr" target="#b122">[123]</ref> also applies GCN over dependency tree, but utilizes multi-head attention for edge selection in a soft weighting manner. Unlike previous two GCN-based models, Zhang et al., <ref type="bibr" target="#b123">[124]</ref> applied GCN for relation embedding in knowledge graph for sentence-based relation extraction. The authors further proposed a coarse-tofine knowledge-aware attention mechanism for the selection of informative instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Adversarial Training</head><p>Adversarial Training (AT) is applied to add adversarial noise to word embeddings for CNN-and RNN-based relation extraction under the MIML learning setting <ref type="bibr" target="#b124">[125]</ref>. DSGAN <ref type="bibr" target="#b125">[126]</ref> denoises distantly supervised relation extraction by learning a generator of sentence-level true positive samples and a discriminator that minimizes the probability of being true positive of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Reinforcement Learning RL has been integrated into neural relation extraction recently</head><p>by training instance selector with policy network. Qin et al. <ref type="bibr" target="#b126">[127]</ref> proposed to train policy-based RL agent of sentential relation classifier to redistribute false positive instances into negative samples to mitigate the effect of noisy data. The authors took F1 score as evaluation metric and used F1 score based performance change as the reward for policy networks. Similarly, Zeng et al. <ref type="bibr" target="#b127">[128]</ref> and Feng et al. <ref type="bibr" target="#b128">[129]</ref> proposed different reward strategies. The advantage of RL-based NRE is that the relation extractor is model-agnostic. Thus, it could be easily adapted to any neural architectures for effective relation extraction. Recently, HRL <ref type="bibr" target="#b129">[130]</ref> proposed a hierarchical policy learning framework of high-level relation detection and low-level entity extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Other Advances</head><p>Other advances of deep learning are also applied for neural relation extraction. Noticing that current NRE methods do not use very deep networks, Huang and Wang <ref type="bibr" target="#b130">[131]</ref> applied deep residual learning to noisy relation extraction and found that 9-layer CNNs have improved performance. Liu et al. <ref type="bibr" target="#b131">[132]</ref> proposed to initialize the neural model by transfer learning from entity classification. The cooperative CORD <ref type="bibr" target="#b132">[133]</ref> ensembles text corpus and knowledge graph with external logical rules by bidirectional knowledge distillation and adaptive imitation. TK-MF <ref type="bibr" target="#b133">[134]</ref> enriches sentence representation learning by matching sentences and topic words. The existence of low-frequency relations in knowledge graphs requires few-shot relation classification with unseen classes or only a few instances. Gao et al. <ref type="bibr" target="#b134">[135]</ref> proposed hybrid attention-based prototypical networks to compute prototypical relation embedding and compare its distance between the query embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary</head><p>This section reviews knowledge completion for incomplete knowledge graph and acquisition from plain text.</p><p>Knowledge graph completion completes missing links between existing entities or infers entities given entity and relation queries. Embedding-based KGC methods generally rely on triple representation learning to capture semantics, and do candidate ranking for completion. Embedding-based reasoning remains in individual relation level, and is poor at complex reasoning because it ignores the symbolical nature of knowledge graph, and lack of interpretability. Hybrid methods with symbolics and embedding incorporate rulebased reasoning, overcome the sparsity of knowledge graph to improve the quality of embedding, facilitate efficient rule injection, and induce interpretable rules. With the observation of graphical nature of knowledge graphs, path search and neural path representation learning are studied, but they suffer from connectivity deficiency when traverses over large-scale graphs. The emerging direction of meta relational learning aims to learn fast adaptation over unseen relations in low-resource settings.</p><p>Entity discovery acquires entity-oriented knowledge from text and fuses knowledge between knowledge graphs. There are several categories according to specific settings. Entity recognition is explored in a sequence-to-sequence manner, entity typing discusses noisy type labels and zero-shot typing, and entity disambiguation and alignment learn unified embeddings with iterative alignment model proposed to tackle the issue of limited number of alignment seed. But it may face the error accumulation problems if newly-aligned entities suffer from poor performance. Language-specific knowledge has increased recent years, and consequentially motivates the research on cross-lingual knowledge alignment.</p><p>Relation extraction suffers from noisy patterns under the assumption of distant supervision, especially in text corpus of different domains. Thus, it is important for weakly supervised relation extraction to mitigate the impact of noisy labeling, for example, multi-instance learning taking bags of sentences as inputs, attention mechanism <ref type="bibr" target="#b117">[118]</ref> for soft selection over instances to reduce noisy patterns, and RL-based methods formulating instance selection as hard decision. Another principle is to learn richer representation as possible. As deep neural networks can solve error propagation in traditional feature extraction methods, this field is dominated by DNN-based models as summarized in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TEMPORAL KNOWLEDGE GRAPH</head><p>Current knowledge graph research mostly focuses on static knowledge graphs where facts are not changed with time, while the temporal dynamics of a knowledge graph is less explored. However, the temporal information is of great importance because the structured knowledge only holds within a specific period, and the evolution of facts follows a time sequence. Recent research begins to take temporal information into KRL and KGC, which is termed as temporal knowledge graph in contrast to the previous static knowledge graph. Research efforts have been made for learning temporal and relational embedding simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Temporal Information Embedding</head><p>Temporal information is considered in temporal aware embedding by extending triples into temporal quadruple as (h, r, t, τ ), where τ provides additional temporal information about when the fact held. Leblay and Chekol <ref type="bibr" target="#b135">[136]</ref> investigated temporal scope prediction over time-annotated triple, and simply extended existing embedding methods, for example, TransE with the vector-based TTransE defined as  <ref type="bibr" target="#b108">[109]</ref> Multi-window convolution + max pooling position embedding PCNN <ref type="bibr" target="#b109">[110]</ref> CNN + piecewise max pooling position embedding MIMLCNN <ref type="bibr" target="#b110">[111]</ref> CNN + piecewise and cross-sentence max pooling position embedding Ye et al. <ref type="bibr" target="#b111">[112]</ref> CNN/PCNN + pairwise ranking position embedding, class ties Zeng et al. <ref type="bibr" target="#b112">[113]</ref> CNN + max pooling position embedding, relation path RNNs SDP-LSTM <ref type="bibr" target="#b113">[114]</ref> Multichannel LSTM + dropout dependency tree, POS, GR, hypernyms LSTM-RNN <ref type="bibr" target="#b114">[115]</ref> Bi-LSTM + Bi-TreeLSTM POS, dependency tree BRCNN <ref type="bibr" target="#b115">[116]</ref> Two-channel LSTM + CNN + max pooling dependency tree, POS, NER Attention Attention-CNN <ref type="bibr" target="#b116">[117]</ref> CNN + word-level attention + max pooling POS, position embedding Lin et al. <ref type="bibr" target="#b117">[118]</ref> CNN/PCNN + selective attention + max pooling position embedding Att-BLSTM <ref type="bibr" target="#b120">[121]</ref> Bi-LSTM + word-level attention position indicator APCNN <ref type="bibr" target="#b118">[119]</ref> PCNN + sentence-level attention entity descriptions HATT <ref type="bibr" target="#b119">[120]</ref> CNN/PCNN + hierarchical attention position embedding, relation hierarchy GCNs C-GCN <ref type="bibr" target="#b121">[122]</ref> LSTM + GCN + path-centric pruning dependency tree KATT <ref type="bibr" target="#b123">[124]</ref> Pre-training + GCN + CNN + attention position embedding, relation hierarchy AGGCN <ref type="bibr" target="#b122">[123]</ref> GCN + multi-head attention + dense layers dependency tree Adversarial Wu et al. <ref type="bibr" target="#b124">[125]</ref> AT + PCNN/RNN + selective attention indicator encoding DSGAN <ref type="bibr" target="#b125">[126]</ref> GAN + PCNN/CNN + attention position embedding RL Qin et al. <ref type="bibr" target="#b126">[127]</ref> Policy gradient + CNN + performance change reward position embedding Zeng et al. <ref type="bibr" target="#b127">[128]</ref> Policy gradient + CNN + +1/-1 bag-result reward position embedding Feng et al. <ref type="bibr" target="#b128">[129]</ref> Policy gradient + CNN + predictive probability reward position embedding HRL <ref type="bibr" target="#b129">[130]</ref> Hierarchical policy learning + Bi-LSTM + MLP relation indicator</p><formula xml:id="formula_55">fτ (h, r, t) = − h + r + τ − t L 1/2 . (<label>35</label></formula><formula xml:id="formula_56">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Others</head><p>ResCNN-x <ref type="bibr" target="#b130">[131]</ref> Residual convolution block + max pooling position embedding Liu et al. <ref type="bibr" target="#b131">[132]</ref> Transfer learning + sub-tree parse + attention position embedding CORD <ref type="bibr" target="#b132">[133]</ref> BiGRU + hierarchical attention + cooperative module position embedding, logic rules TK-MF <ref type="bibr" target="#b133">[134]</ref> Topic modeling + multi-head self attention position embedding, topic words HATT-Proto <ref type="bibr" target="#b134">[135]</ref> Prototypical networks + CNN + hybrid attention position embedding</p><p>Temporally scoped quadruple extends triples by adding a time scope [τ s , τ e ], where τ s and τ e stand for the beginning and ending of the valid period of a triple, and then a static subgraph G τ can be derived from the dynamic knowledge graph when given a specific timestamp τ . HyTE <ref type="bibr" target="#b136">[137]</ref> takes a time stamp as a hyperplane w τ and projects entity and relation representation as P τ (h) = h − w τ h w τ , P τ (t) = t − w τ t w τ , and P τ (r) = r − w τ r w τ . The temporally projected scoring function is calculated as</p><formula xml:id="formula_57">fτ (h, r, t) = Pτ (h) + Pτ (r) − Pτ (t) L 1 /L 2<label>(36)</label></formula><p>within the projected translation of P τ (h) + P τ (r) ≈ P τ (t). García-Durán et al. <ref type="bibr" target="#b137">[138]</ref> concatenated predicate token sequence and temporal token sequence, and used LSTM to encode the concatenated time-aware predicate sequences. The last hidden state of LSTM is taken as temporal-aware relational embedding r temp . The scoring function of extended TransE and DistMult are calculated as h + r temp − t 2 and (h • t) r T temp , respectively. By defining the context of an entity e as an aggregate set of facts containing e, Liu et al. <ref type="bibr" target="#b138">[139]</ref> proposed context selection to capture useful contexts, and measured temporal consistency with selected context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entity Dynamics</head><p>Real-world events change entities' state, and consequently, affect the corresponding relations. To improve temporal scope inference, the contextual temporal profile model <ref type="bibr" target="#b139">[140]</ref> formulates the temporal scoping problem as state change detection, and utilizes the context to learn state and state change vectors. Know-evolve <ref type="bibr" target="#b140">[141]</ref>, a deep evolutionary knowledge network, investigates the knowledge evolution phenomenon of entities and their evolved relations. A multivariate temporal point process is used to model the occurrence of facts, and a novel recurrent network is developed to learn the representation of non-linear temporal evolution. To capture the interaction between nodes, RE-NET <ref type="bibr" target="#b141">[142]</ref> models event sequences via RNN-based event encoder and neighborhood aggregator. Specifically, RNN is used to capture the temporal entity interaction, and the concurrent interactions are aggregated by the neighborhood aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Temporal Relational Dependency</head><p>There exists temporal dependencies in relational chains following the timeline, for example, wasBornIn → graduateFrom → workAt → diedIn. Jiang et al. <ref type="bibr" target="#b142">[143]</ref>, <ref type="bibr" target="#b143">[144]</ref> proposed time-aware embedding, a joint learning framework with temporal regularization, to incorporate temporal order and consistency information. The authors defined a temporal scoring function as</p><formula xml:id="formula_58">f ( r k , r l ) = r k T − r l L 1/2 , (<label>37</label></formula><formula xml:id="formula_59">)</formula><p>where T ∈ R d×d is an asymmetric matrix that encodes the temporal order of relation, for a temporal ordering relation pair r k , r l . Three temporal consistency constraints of disjointness, ordering, and spans are further applied by integer linear programming formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Temporal Logical Reasoning</head><p>Logical rules are also studied for temporal reasoning. Chekol et al. <ref type="bibr" target="#b144">[145]</ref> explored Markov logic network and probabilistic soft logic for reasoning over uncertain temporal knowledge graphs. RLvLR-Stream <ref type="bibr" target="#b77">[78]</ref> considers temporal close-path rules and learns the structure of rules from knowledge graph stream for reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">KNOWLEDGE-AWARE APPLICATIONS</head><p>Rich structured knowledge can be useful for AI applications. But how to integrate such symbolic knowledge into the computational framework of real-world applications remains a challenge. This section introduces several recent DNNbased knowledge-driven approaches with the applications on NLU, recommendation, and question answering. More miscellaneous applications such as digital health and search engine are introduced in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Natural Language Understanding</head><p>Knowledge-aware NLU enhances language representation with structured knowledge injected into a unified semantic space. Recent knowledge-driven advances utilize explicit factual knowledge and implicit language representation, with many NLU tasks explored. Chen et al. <ref type="bibr" target="#b145">[146]</ref> proposed doublegraph random walks over two knowledge graphs, i.e., a slotbased semantic knowledge graph and a word-based lexical knowledge graph, to consider inter-slot relations in spoken language understanding. Wang et al. <ref type="bibr" target="#b146">[147]</ref> augmented short text representation learning with knowledge-based conceptualization by a weighted word-concept embedding. Peng et al. <ref type="bibr" target="#b147">[148]</ref> integrated external knowledge base to build heterogeneous information graph for event categorization in short social text. Language modeling as a fundamental NLP task predicts the next word given preceding words in the given sequence. Traditional language modeling does not exploit factual knowledge with entities frequently observed in the text corpus. How to integrate knowledge into language representation has drawn increasing attention. Knowledge graph language model (KGLM) <ref type="bibr" target="#b148">[149]</ref> learns to render knowledge by selecting and copying entities. ERNIE-Tsinghua <ref type="bibr" target="#b149">[150]</ref> fuses informative entities via aggregated pre-training and random masking. BERT-MK <ref type="bibr" target="#b150">[151]</ref> encodes graph contextualized knowledge and focuses on the medical corpus. ERNIE-Baidu <ref type="bibr" target="#b151">[152]</ref> introduces named entity masking and phrase masking to integrate knowledge into language model, and is further improved by ERNIE 2.0 <ref type="bibr" target="#b152">[153]</ref> via continual multi-task learning. Rethinking about large-scale training on language model and querying over knowledge graphs, Petroni et al. <ref type="bibr" target="#b153">[154]</ref> conducted an analysis on language model and knowledge base, and found that certain factual knowledge can be acquired via pre-training language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Question Answering</head><p>knowledge-graph-based question answering (KG-QA) answers natural language questions with facts from knowledge graphs. Neural network based approaches represent questions and answers in distributed semantic space, and some also conduct symbolic knowledge injection for commonsense reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Single-fact QA</head><p>Taking knowledge graph as an external intellectual source, simple factoid QA or single-fact QA is to answer simple question involving with a single knowledge graph fact. Bordes et al. <ref type="bibr" target="#b154">[155]</ref> adapted memory network for simple question answering, taking knowledge base as external memory. Dai et al. <ref type="bibr" target="#b155">[156]</ref> proposed a conditional focused neural network equipped with focused pruning to reduce the search space. To generate natural answers in a user-friendly way, COREAQ <ref type="bibr" target="#b156">[157]</ref> introduces copying and retrieving mechanisms to generate smooth and natural responses in a seq2seq manner, where an answer is predicted from the corpus vocabulary, copied from the given question or retrieved from the knowledge graph. BAMnet <ref type="bibr" target="#b157">[158]</ref> models the two-way interaction between questions and knowledge graph with a bidirectional attention mechanism.</p><p>Although deep learning techniques are intensively applied in KG-QA, they inevitably increase the model complexity. Through evaluation on simple KG-QA with and without neural networks, Mohammed et al. <ref type="bibr" target="#b158">[159]</ref> found that sophisticated deep models such as LSTM and gated recurrent unit (GRU) with heuristics achieve the state of the art, and non-neural models also gain reasonably well performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Multi-hop Reasoning</head><p>Those neural network based methods gain improvements with the combination of neural encoder-decoder models, but to deal with complex multi-hop relation requires a more dedicated design to be capable of multi-hop commonsense reasoning. Structured knowledge provides informative commonsense observations and acts as relational inductive biases, which boosts recent on commonsense knowledge fusion between symbolic and semantic space for multi-hop reasoning. Bauer et al. <ref type="bibr" target="#b159">[160]</ref> proposed multi-hop bidirectional attention and pointer-generator decoder for effective multihop reasoning and coherent answer generation, where external commonsense knowledge is utilized by relational path selection from ConceptNet and injection with selectivelygated attention. Variational Reasoning Network (VRN) <ref type="bibr" target="#b160">[161]</ref> conducts multi-hop logic reasoning with reasoning-graph embedding, while handles the uncertainty in topic entity recognition. KagNet <ref type="bibr" target="#b161">[162]</ref> performs concept recognition to build a schema graph from ConceptNet and learns path-based relational representation via GCN, LSTM and hierarchical path-based attention. CogQA <ref type="bibr" target="#b162">[163]</ref> combines implicit extraction and explicit reasoning, and proposes a cognitive graph model based on BERT and GNN for multihop QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Recommender Systems</head><p>Recommender systems have been widely explored by collaborative filtering which makes use of users' historical information. However, it often fails to solve the sparsity issue and the cold start problem. Integrating knowledge graphs as external information enables recommendation systems to have the ability of commonsense reasoning.</p><p>By injecting knowledge-graph-based side information such as entities, relations, and attributes, many efforts work on embedding-based regularization to improve recommendation. The collaborative CKE <ref type="bibr" target="#b163">[164]</ref> jointly trains KGEs, item's textual information and visual content via translational KGE model and stacked auto-encoders. Noticing that time-sensitive and topic-sensitive news articles consist of condensed entities and common knowledge, DKN <ref type="bibr" target="#b164">[165]</ref> incorporates knowledge graph by a knowledge-aware CNN model with multi-channel word-entity-aligned textual inputs. However, DKN cannot be trained in an end-to-end manner as entity embedding need to be learned in advance. To enable end-to-end training, MKR <ref type="bibr" target="#b165">[166]</ref> associates multi-task knowledge graph representation and recommendation by sharing latent features and modeling high-order item-entity interaction. While other works consider the relational path and structure of knowledge graphs, KPRN <ref type="bibr" target="#b166">[167]</ref> regards the interaction between users and items as entity-relation path in knowledge graph and conducts preference inference over the path with LSTM to capture the sequential dependency. PGPR <ref type="bibr" target="#b167">[168]</ref> performs reinforcement policyguided path reasoning over knowledge-graph-based useritem interaction. KGAT <ref type="bibr" target="#b168">[169]</ref> applies graph attention network over the collaborative knowledge graph of entity-relation and user-item graphs to encode high-order connectivities via embedding propagation and attention-based aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE DIRECTIONS</head><p>Many efforts have been conducted to tackle the challenges of knowledge representation and its related applications. But there still remains several formidable open problems and promising future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Complex Reasoning</head><p>Numerical computing for knowledge representation and reasoning requires a continuous vector space to capture the semantic of entities and relations. While embedding-based methods have a limitation on complex logical reasoning, two directions on the relational path and symbolic logic are worthy of being further explored. Some promising methods such as recurrent relational path encoding, GNN-based message passing over knowledge graph, and reinforcement learningbased path finding and reasoning are very promising for handling complex reasoning. For the combination of logic rules and embeddings, recent works <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref> combines Markov logic networks with KGE, aiming to leveraging logic rules and handling their uncertainty. Enabling probabilistic inference for capturing the uncertainty and domain knowledge with efficiently embedding will be a noteworthy research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Unified Framework</head><p>Several knowledge graph representation learning models have been verified as equivalence, for example, Hayshi and Shimbo <ref type="bibr" target="#b169">[170]</ref> proved that HolE and ComplEx are mathematically equivalent for link prediction with a certain constraint. ANALOGY <ref type="bibr" target="#b16">[17]</ref> provides a unified view of several representative models including DistMult, ComplEx, and HolE. Wang et al. <ref type="bibr" target="#b39">[40]</ref> explored connections among several bilinear models. Chandrahas et al. <ref type="bibr" target="#b170">[171]</ref> explored the geometric understanding of additive and multiplicative KRL models. Most work formulated knowledge acquisition KGC and relation extraction separately with different models. Han et al. <ref type="bibr" target="#b63">[64]</ref> put them under the same roof and proposed a joint learning framework with mutual attention for information sharing between knowledge graph and text. A unified understanding of knowledge representation and reasoning is less explored. An investigation towards unification in a way similar to the unified framework of graph networks <ref type="bibr" target="#b171">[172]</ref>, however, will be worthy to bridge the research gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Interpretability</head><p>Interpretability of knowledge representation and injection is a key issue for knowledge acquisition and real-world applications. Preliminary efforts have been done for interpretability. ITransF <ref type="bibr" target="#b30">[31]</ref> uses sparse vectors for knowledge transferring and interprets with attention visualization. CrossE <ref type="bibr" target="#b35">[36]</ref> explores the explanation scheme of knowledge graphs by using embedding-based path searching to generate explanations for link prediction. Recent neural models, however, have limitations on transparency and interpretability, although they have gained impressive performance. Some methods combine black-box neural models and symbolic reasoning by incorporating logical rules to increase the interoperability. Interpretability can convince people to trust predictions. Thus, further work should go into interpretability and improve the reliability of predicted knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Scalability</head><p>Scalability is crucial in large-scale knowledge graphs. There is a trade-off between computational efficiency and model expressiveness, with a limited number of works applied to more than 1 million entities. Several embedding methods use simplification to reduce the computation cost, for example, simplifying tensor product with circular correlation operation <ref type="bibr" target="#b15">[16]</ref>. However, these methods still struggle with scaling to millions of entities and relations.</p><p>Probabilistic logic inference such as using Markov logic networks is computationally intensive, making it hard to be scalable to large-scale knowledge graphs. Rules in a recent neural logical model <ref type="bibr" target="#b83">[84]</ref> are generated by simple bruteforce search, making it insufficient on large-scale knowledge graphs. ExpressGNN <ref type="bibr" target="#b84">[85]</ref> attempts to use NeuralLP <ref type="bibr" target="#b82">[83]</ref> for efficient rule induction. But there still has a long way to go to deal with cumbersome deep architectures and the increasingly growing knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Knowledge Aggregation</head><p>The aggregation of global knowledge is the core of knowledge-aware applications. For example, recommendation systems use knowledge graph to model user-item interaction and text classification jointly to encode text and knowledge graph into a semantic space. Most of current knowledge aggregation methods design neural architectures such as attention mechanism and GNNs. The natural language processing community has been boosted from largescale pre-training via transformers and variants like BERT models, while a recent finding <ref type="bibr" target="#b153">[154]</ref> reveals that pre-training language model on unstructured text can actually acquire certain factual knowledge. Large-scale pre-training can be a straightforward way for injecting knowledge. However, rethinking the way of knowledge aggregation in an efficient and interpretable manner is also of significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Automatic Construction and Dynamics</head><p>Current knowledge graphs rely highly on manual construction, which is labor-intensive and expensive. The widespread applications of knowledge graphs on different cognitive intelligence fields require automatic knowledge graph construction from large-scale unstructured content. Recent research mainly works on semi-automatic construction under the supervision of existing knowledge graphs. Facing the multimodality, heterogeneity and large-scale application, automatic construction is still of great challenge.</p><p>The mainstream research focuses on static knowledge graphs, with several work on predicting temporal scope validity and learning temporal information and entity dynamics. Many facts only hold within a specific time period. Considering the temporal nature, dynamic knowledge graph can address the limitation of traditional knowledge representation and reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Knowledge graphs as the ensemble of human knowledge have attracted increasing research attention, with the recent emergence of knowledge representation learning, knowledge acquisition methods, and a wide variety of knowledge-aware applications. The paper conducts a comprehensive survey on the following four scopes: 1) knowledge graph embedding, with a full scale systematic review from embedding space, scoring metrics, encoding models, embedding with external information, and training strategies; 2) knowledge acquisition of entity discovery, relation extraction, and graph completion from three perspectives of embedding learning, relational path inference and logical rule reasoning; 3) temporal knowledge graph representation learning and completion; 4) real-world knowledge-aware applications on natural language understanding, recommendation systems, question answering and other miscellaneous applications. In addition, some useful resources of datasets and opensource libraries, and future research directions are introduced and discussed. Knowledge graph hosts a large research community and has a wide range of methodologies and applications. We conduct this survey to have a summary of current representative research efforts and trends, and expect it can facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A A BRIEF HISTORY OF KNOWLEDGE BASES</head><p>Knowledge bases experienced a development timeline as illustrated in Fig. <ref type="figure" target="#fig_5">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B MATHEMATICAL OPERATIONS</head><p>Hermitian dot product (Eq. 38) and Hamilton product (Eq. 39) are used in complex vector space (Sec. 3.1.2). Given h and t represented in complex space C d , the Hermitian dot product , :</p><formula xml:id="formula_60">C d × C d −→ C is calculated as the sesquilinear form of h, t = h T t,<label>(38)</label></formula><p>where h = Re(h) − i Im(h) is the conjugate operation over h ∈ C d . The quaternion extends complex numbers into fourdimensional hypercomplex space. With two d-dimensional quaternions defined as</p><formula xml:id="formula_61">Q 1 = a 1 + b 1 i + c 1 j + d 1 k and Q 2 = a 2 + b 2 i + c 2 j + d 2 k, the Hamilton product ⊗ : H d × H d → H d is defined as Q1 ⊗ Q2 = (a1 • a2 − b1 • b2 − c1 • c2 − d1 • d2) + (a1 • b2 + b1 • a2 + c1 • d2 − d1 • c2) i + (a1 • c2 − b1 • d2 + c1 • a2 + d1b2) j + (a1 • d2 + b1 • c2 − c1 • b2 + d1 • a2) k.<label>(39)</label></formula><p>The Hadmard product (Eq. 40) and circular correlation (Eq. 41) are utilized in semantic matching based methods (Sec. 3.2.2). Hadmard product, denoted as • or : R d ×R d → R d , is also known as element-wise product or Schur product.</p><formula xml:id="formula_62">(h • t)i = (h t)i = (h)i(t)i<label>(40)</label></formula><p>Circular correlation : R d × R d → R d is an efficient computation calcuated as:</p><formula xml:id="formula_63">[a b] k = d−1 i=0 aib (k+i) mod d .<label>(41)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C A SUMMARY OF KRL MODELS</head><p>We conduct a comprehensive summary on KRL models in Table <ref type="table" target="#tab_7">5</ref>. The representation space has an impact on the expressiveness of KRL methods to some extent. By expanding point-wise Euclidean space <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, manifold space <ref type="bibr" target="#b22">[23]</ref>, complex space <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and Gaussian distribution <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> are introduced. ManifoldE <ref type="bibr" target="#b22">[23]</ref> relaxes the real-valued point-wise space into manifold space with more expressive representation from the geometric perspective. When M(h, r, t) = h + r − t 2 2 and D r is set to be zero, the manifold collapses into a point. With the introduction of rotational Hadmard product, RotatE <ref type="bibr" target="#b18">[19]</ref> can also capture inversion and composition patterns as well as symmetry and antisymmetry. QuatE <ref type="bibr" target="#b19">[20]</ref> uses Hamilton product to capture latent inter-dependency within four-dimensional space of entities and relations, and gains more expressive rotational capability than RotatE. Group theory remains less explored to capture rich information of relations. The very recent DihEdral <ref type="bibr" target="#b24">[25]</ref> firstly introduces the finite non-Abelian group to preserve the relational properties of symmetry/skewsymmetry, inversion and composition effectively with the rotation and reflection properties in the dihedral group. Ebisu and Ichise <ref type="bibr" target="#b23">[24]</ref> summarized that the embedding space should follow three conditions, i.e., differentiability, calculation possibility, and definability of a scoring function.</p><p>Distance-based and semantic matching scoring functions consist of the foundation stones of plausibility measure in KRL. Translational distance-based methods, especially the groundbreaking TransE <ref type="bibr" target="#b11">[12]</ref>, borrowed the idea of distributed word representation learning and inspired many following approaches such as TransH <ref type="bibr" target="#b14">[15]</ref> and TransR <ref type="bibr" target="#b12">[13]</ref> which specify complex relations (1-to-N, N-to-1, and N-to-N) and the recent TransMS <ref type="bibr" target="#b32">[33]</ref> which models multi-directional semantics. As for the semantic matching side, many methods utilizes mathematical operations or compositional operators including linear matching in SME <ref type="bibr" target="#b33">[34]</ref>, bilinear mapping in DistMult <ref type="bibr" target="#b25">[26]</ref>, tensor product in NTN <ref type="bibr" target="#b13">[14]</ref>, circular correlation in HolE <ref type="bibr" target="#b15">[16]</ref> and ANALOGY <ref type="bibr" target="#b16">[17]</ref>, Hadamard product in CrossE <ref type="bibr" target="#b35">[36]</ref>, and quaternion inner product in QuatE <ref type="bibr" target="#b19">[20]</ref>.</p><p>Recent encoding models for knowledge representation have developed rapidly, and generally fall into two families of bilinear and neural networks. Linear and bilinear models use product-based functions over entities and relations, while factorization models regard knowledge graphs as three-way tensors. With the multiplicative operations, RESCAL <ref type="bibr" target="#b41">[42]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and SimplE <ref type="bibr" target="#b40">[41]</ref> also belong to the bilinear models. DistMult <ref type="bibr" target="#b25">[26]</ref> can only model symmetric relations, while its extension of ComplEx <ref type="bibr" target="#b17">[18]</ref> managed to preserve antisymmetric relations, but involves redundant computations <ref type="bibr" target="#b40">[41]</ref>. ComplEx <ref type="bibr" target="#b17">[18]</ref>, SimplE <ref type="bibr" target="#b40">[41]</ref>, and TuckER <ref type="bibr" target="#b44">[45]</ref> can guarantee full expressiveness under specific embedding dimensionality bounds. Neural network-based encoding models start from distributed representation of entities and relations, and some utilizes complex neural structures such as tensor networks <ref type="bibr" target="#b13">[14]</ref>, graph convolution networks <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b54">[55]</ref>, recurrent networks <ref type="bibr" target="#b38">[39]</ref> and transformers <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> to learn richer representation. These deep models have achieved very competitive results, but they are not transparent, and lack of interpretability. As deep learning techniques are growing prosperity and gaining extensive superiority in many tasks, the recent trend is still likely to focus on more powerful neural architectures or large-scale pre-training, while interpretable deep models remains a challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D KRL MODEL TRAINING</head><p>To train knowledge representation learning models, open world assumption (OWA) and closed world assumption (CWA) <ref type="bibr" target="#b175">[176]</ref> are considered. During training, negative sample set F is randomly generated by corrupting a golden triple set F under the OWA. Mini-batch optimization and Stochastic Gradient Descent (SGD) are carried out to minimize a certain loss function. Under the OWA, negative samples are generated with specific sampling strategies designed to reduce the number of false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Open and Closed World Assumption</head><p>The CWA assumes that unobserved facts are false. By contrast, the OWA has a relaxed assumption that unobserved ones can be either missing or false. Generally, OWA has advantage over CWA because of the incompleteness nature of knowledge graphs. RESCAL <ref type="bibr" target="#b41">[42]</ref> is a typical model trained under the CWA, while more models are formulated under the OWA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Loss Function</head><p>Several families of loss function are introduced for KRL model optimization. First, margin-based loss is optimized to learn representations that positive samples have higher scores than negative ones. Some literature also called it as pairwise ranking loss. As shown in Eq. 42 , the rank-based hinge loss maximizes the discriminative margin between a golden triple (h, r, t) and an invalid triple (h , r, t ). </p><p>here y hrt is the label of triple instance. Some methods also use other kinds of loss functions. For example, ConvE and TuckER use binary cross-entropy or the so-called Bernoulli negative log-likelihood loss function defined as:</p><formula xml:id="formula_65">− 1 Ne Ne i (yi • log (pi) + (1 − yi) • log (1 − pi)) , (<label>44</label></formula><formula xml:id="formula_66">)</formula><p>where p is the prediction and y is the ground label. And RotatE uses the form of loss function in Eq. 45.</p><p>− log σ (γ − fr(h, t))</p><formula xml:id="formula_67">− n i=1 1 k log σ fr h i , t i − γ<label>(45)</label></formula><p>For all those kinds of loss functions, specific regularization like L2 on parameters or constraints can also be applied, as well as combined with the joint learning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Negative Sampling</head><p>Facing the nature of incompleteness of knowledge graphs, several heuristics of sampling distribution are proposed to corrupt the head or tail entities. The widest applied one is uniform sampling <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref> that uniformly replaces entities. But it leads to sampling false negative labels. More effective negative sampling strategies are required to learn semantic representation and improve the predictive performance.</p><p>Considering the mapping property of relations, Bernoulli sampling <ref type="bibr" target="#b14">[15]</ref> introduces a heuristic of sampling distribution as tph tph+hpt , where tph and hpt denote the average number of tail entities per head entity and the average number of head entities per tail entity respectively. Domain sampling <ref type="bibr" target="#b30">[31]</ref> chooses corrupted samples from entities in the same domain or from the whole entity set with a relation-dependent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex vector</head><p>ComplEx <ref type="bibr" target="#b17">[18]</ref> h Recently, two adversarial sampling are further proposed. KBGAN <ref type="bibr" target="#b176">[177]</ref> introduces adversarial learning for negative sampling, where the generator uses probability-based logloss embedding models. The probability of generating negative samples p h j , r, t j | {(h i , r i , t i )} is defined as</p><formula xml:id="formula_68">, t ∈ C d r ∈ C d Re &lt; r, h, t &gt; = Re K k=1 r k h k t k RotatE [19] h, t ∈ C d r ∈ C d h • r − t QuatE [20] h, t ∈ H d r ∈ H d h ⊗ r |r| • t Manifold &amp; Group ManifoldE [23] h, t ∈ R d r ∈ R d M(h, r, t) − D 2 r 2 TorusE [24] [h], [t] ∈ T n [r] ∈ T n min (x,y)∈([h]+[r])×[t] x − y i DihEdral [25] h (l) , t (l) ∈ R 2 R (l) ∈ D K L l=1 h (l) R (l) t (l) Gaussian KG2E [21] h ∼ N (µ h , Σ h ) r ∼ N (µ r , Σr) x∈R ke N (x; µ r , Σr) log N (x;µ e ,Σe) N (x;µ r ,Σr ) dx t ∼ N (µ t , Σt) µ h , µ t ∈ R d µ r ∈ R d , Σr ∈ R d×d log x∈R ke N (x; µ e , Σe) N (x; µ r , Σr) dx Σ h , Σt ∈ R d×d TransG [22] h ∼ N µ h , σ 2 h I µ i r ∼ N µ t − µ h , σ 2 h + σ 2 t I i π i r exp − µ h +µ i r −µ t 2 2 σ 2 h +σ 2 t t ∼ N (µ t , Σt) r = i π i r µ i r ∈ R d µ h , µ t ∈ R d Translational distance TransE [12] h, t ∈ R d r ∈ R d − h + r − t 1/2 TransR [13] h, t ∈ R d r ∈ R k , Mr ∈ R k×d − Mrh + r − Mrt 2 2 TransH [15] h, t ∈ R d r, wr ∈ R d − h − w r hwr + r − t − w r twr 2 2 TransA [29] h, t ∈ R d r ∈ R d , Mr ∈ R d×d (|h + r − t|) Wr(|h + r − t|) TransF [30] h, t ∈ R d r ∈ R d (h + r) t + (t − r) h ITransF [31] h, t ∈ R d r ∈ R d α H r • D • h + r − α T r • D • t TransAt [32] h, t ∈ R d r ∈ R d Pr (σ (r h ) h) + r − Pr (σ (rt) t) TransD [28] h, t, w h wt ∈ R d r, wr ∈ R k − wrw h + I h + r − wrw t + I t 2 2 TransM [173] h, t ∈ R d r ∈ R d −θr h + r − t 1/2 TranSparse [174] h, t ∈ R d r ∈ R k , Mr (θr) ∈ R k×d − Mr (θr) h + r − Mr (θr) t 2 1/2 M 1 r θ 1 r , M 2 r θ 2 r ∈ R k×d − M 1 r θ 1 r h + r − M 2 r θ 2 r t 2 1/2 Semantic matching TATEC [175] h, t ∈ R d r ∈ R d , Mr ∈ R d×d h Mrt + h r + t r + h Dt ANALOGY [17] h, t ∈ R d Mr ∈ R d×d h Mrt CrossE [36] h, t ∈ R d r ∈ R d σ tanh (cr • h + cr • h • r + b) t SME [34] h, t ∈ R d r ∈ R d g left (h, r) g right (r, t) DistMult [26] h, t ∈ R d r ∈ R d h diag(Mr)t HolE [16] h, t ∈ R d r ∈ R d r (h t) HolEx [35] h, t ∈ R d r ∈ R d l j=0 p (h, r; cj ) • t SE [27] h, t ∈ R d M 1 r , M 2 r ∈ R d×d − M 1 r h − M 2 r t 1 SimplE [41] h, t ∈ R d r, r ∈ R d 1 2 h • rt + t • r t RESCAL [42] h, t ∈ R d Mr ∈ R d×d h Mrt LFM [44] h, t ∈ R d ur, vr ∈ R p h d i=1 α r i uiv i t TuckER [45] h, t ∈ R d e r ∈ R d r W ×1 h ×2 r ×3 t Neural Networks MLP [5] h, t ∈ R d r ∈ R d σ(w σ(W[h, r, t])) NAM [46] h, t ∈ R d r ∈ R d σ z (L) • t + B (L+1) r ConvE [47] M h ∈ R dw ×d h , t ∈ R d Mr ∈ R dw ×d h σ (vec (σ ([M h ; Mr] * ω)) W) t ConvKB [37] h, t ∈ R d r ∈ R d concat (σ ([h, r, t] * ω)) • w HypER [48] h, t ∈ R d wr ∈ R dr σ vec h * vec −1 (wrH) W t SACN [38] h, t ∈ R d r ∈ R d g (vec (M (h, r)) W ) t NTN [14] h, t ∈ R d r, br ∈ R k , M ∈ R d×d×k r σ h T Mt + Mr,1h + Mr,2t + br Mr,1, Mr,2 ∈ R k×d</formula><formula xml:id="formula_69">exp fG (h i , r, t i ) j=1 exp fG h j , r, t j ,<label>(46)</label></formula><p>where f G (h, r, t) is the scoring function of generator. Similarly, Sun et al. <ref type="bibr" target="#b18">[19]</ref> proposed self-adversarial negative sampling based on self scoring function by sampling negative triples from the distribution in Eq. 47, where α is the temperature of sampling.</p><formula xml:id="formula_70">p h j , r, t j | {(hi, ri, ti)} = exp αf h j , r, t j i exp αf (h i , r, t i )<label>(47)</label></formula><p>Negative sampling strategies are summarized in Table <ref type="table" target="#tab_8">6</ref>. Trouillon et al. <ref type="bibr" target="#b17">[18]</ref> studied the number of negative samples generated per positive training sample, and found a trade-off between accuracy and training time. Adversarial <ref type="bibr" target="#b176">[177]</ref> generator embedding exp f G (h i ,r,t i )</p><p>j=1 exp f G h j ,r,t j</p><p>Self-adversarial <ref type="bibr" target="#b18">[19]</ref> current embedding exp αf h j ,r,t j i exp αf (h i ,r,t i )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E MORE KNOWLEDGE-AWARE APPLICATIONS</head><p>There are also many other applications that utilize knowledge-driven methods. 1) Question generation focuses on generating natural language questions. Seyler et al. <ref type="bibr" target="#b177">[178]</ref> studied quiz-style knowledge question generation by generating a structured triple-pattern query over the knowledge graph while estimating how difficult the questions are. But for verbalizing the question, the authors used a templatebased method, which may have a limitation on generating more natural expression. 2) Academic search engine helps research to find relevant academic papers. Xiong et al. <ref type="bibr" target="#b178">[179]</ref> proposed explicit semantic ranking with knowledge graph embedding to help academic search better understand the meaning of query concepts. 3) Medical applications involve with domain-specific knowledge graph of medical concepts. Li et al. <ref type="bibr" target="#b179">[180]</ref> formulated medical image report generation by three steps of encoding, retrieval and paraphrasing, where medical image is encoded by the abnormality graph. 4) Mental healthcare with knowledge graph facilitates a good understanding of mental conditions and risk factors of mental disorders, and is applied to effective prevention of mental health leaded suicide. Gaurs et al. <ref type="bibr" target="#b180">[181]</ref> developed a rulebased classifier for knowledge-aware suicide risk assessment with a suicide risk severity lexicon incorporating medical knowledge bases and suicide ontology. 5) Zero-shot image classification gets benefits from knowledge graph propagation with semantic descriptions of classes. Wang et al. <ref type="bibr" target="#b181">[182]</ref> proposed a multi-layer GCN to learn zero-shot classifiers using semantic embeddings of categories and categorical relationship. 6) Text generation synthesizes and composes coherent multi-sentence texts. Koncel-Kedziorski et al. <ref type="bibr" target="#b182">[183]</ref> studied text generation for information extraction systems, and proposed a graph transforming encoder for graph-to-text generation from the knowledge graph. 7) Sentiment analysis integrated with sentiment-related concepts can better understand people's opinions and sentiments. SenticNet <ref type="bibr" target="#b183">[184]</ref> learns conceptual primitives for sentiment analysis, which can also be used as a commonsense knowledge source.</p><p>To enable sentiment-related information filtering, Sentic LSTM <ref type="bibr" target="#b184">[185]</ref> injects knowledge concepts to the vanilla LSTM, and designs a knowledge output gate for concept-level output as a complement to the token level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Dialogue Systems</head><p>QA can also be viewed as a single-turn dialogue system by generating the correct answer as response, while dialogue systems consider conversational sequences and aim to generate fluent responses to enable multi-round conversations via semantic augmentation and knowledge graph walk. Liu et al. <ref type="bibr" target="#b185">[186]</ref> encoded knowledge to augment semantic representation and generated knowledge aware response by knowledge graph retrieval and graph attention mechanism under an encoder-decoder framework. DialKG Walker <ref type="bibr" target="#b186">[187]</ref> traverses symbolic knowledge graph to learn contextual transition in dialogue, and predicts entity responses with attentive graph path decoder. Semantic parsing via formal logical representation is another direction for dialog systems. By predefining a set of base actions, Dialog-to-Action <ref type="bibr" target="#b187">[188]</ref> is an encoder-decoder approach that maps executable logical forms from utterance in conversation, to generate action sequence under the control of a grammar-guided decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F DATASETS AND LIBRARIES</head><p>In this section, we introduce and list useful resources of knowledge graph datasets and open-source libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Datasets</head><p>Many public datasets have been released. We conduct an introduction and a summary of general, domain-specific, task-specific and temporal datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 General Datasets</head><p>Datasets with general ontological knowledge include Word-Net <ref type="bibr" target="#b188">[189]</ref>, Cyc <ref type="bibr" target="#b189">[190]</ref>, DBpedia <ref type="bibr" target="#b190">[191]</ref>, YAGO <ref type="bibr" target="#b191">[192]</ref>, Freebase <ref type="bibr" target="#b192">[193]</ref>, NELL <ref type="bibr" target="#b193">[194]</ref> and Wikidata <ref type="bibr" target="#b194">[195]</ref>. It is hard to compare them within a table as their ontologies are different. Thus, only an informal comparison is illustrated in Table <ref type="table" target="#tab_9">7</ref>, where their volumes kept going after their release.</p><p>WordNet, firstly released in 1995, is a lexical database that contains about 117,000 synsets. DBpedia is a communitydriven dataset extracted from Wikipedia. It contains 103 million triples and can be enlarged when interlinked with other open datasets. To solve the problems of low coverage and low quality of single-source ontological knowledge, YAGO utilized the concept information in the category page of Wikipedia and the hierarchy information of concepts in WordNet to build a multi-source dataset with high coverage and quality. Moreover, it is extendable by other knowledge sources. It is available online with more than 10 million entities and 120 million facts currently. Freebase, a scalable knowledge base, came up for the storage of the world's knowledge in 2008. Its current number of triples is 1.9 billion. NELL is built from the Web via an intelligent agent called Never-Ending Language Learner. It has 2,810,379 beliefs with high confidence by far. Wikidata is a free structured knowledge base, which is created and maintained by human editors to facilitate the management of Wikipedia data. It is multi-lingual with 358 different language.</p><p>The aforementioned datasets are openly published and maintained by communities or research institutions. There are also some commercial datasets. The Cyc knowledge base from Cycorp contains about 1.5 million general concepts and more than 20 million general rules, with an accessible version called OpenCyc deprecated sine 2017. Google knowledge graph hosts more than 500 million entities and 3.5 billion facts and relations. Microsoft builds a probabilistic taxonomy called Probase <ref type="bibr" target="#b195">[196]</ref> with 2.7 million concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 Domain-Specific Datasets</head><p>To solve domain-specific tasks, some knowledge bases on specific domains are designed and collected. Some notable domains include life science, health care, and scientific research, covering complex domain and relations such as compounds, diseases and tissues. Examples of domainspecific knowledge graphs are ResearchSpace 6 , a cultural heritage knowledge graph; UMLS <ref type="bibr" target="#b196">[197]</ref>, a unified medical language system; GeneOntology 7 , a gene ontology resource; SNOMED CT 8 , a commercial clinical terminology; and a medical knowledge graph from Yidu Research 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.3 Task-Specific Datasets</head><p>A popular way for generating task-specific datasets is to sample subsets from large general datasets. Statistics of several datasets for tasks on knowledge graph itself are listed in Table <ref type="table">8</ref>. Notice that WN18 and FB15k suffer from test set leakage <ref type="bibr" target="#b46">[47]</ref>. For KRL with auxiliary information and other downstream knowledge-aware applications, texts and images are also collected, for example, WN18-IMG <ref type="bibr" target="#b62">[63]</ref> with sampled images and textual relation extraction dataset including SemEval 2010 dataset, NYT <ref type="bibr" target="#b197">[198]</ref> and Google-RE 10 . IsaCore <ref type="bibr" target="#b198">[199]</ref>, an analogical closure of Probase for opinion mining and sentiment analysis, is built by common knowledge base blending and multi-dimensional scaling. Recently, the FewRel dataset <ref type="bibr" target="#b199">[200]</ref> was built to evaluate the emerging few-shot relation classification task. There are also more datasets for specific tasks such as cross-lingual DBP15K <ref type="bibr" target="#b100">[101]</ref> and DWY100K <ref type="bibr" target="#b99">[100]</ref> for entity alignment, multi-view knowledge graphs of YAGO26K-906 and DB111K-174 <ref type="bibr" target="#b200">[201]</ref> with instances and ontologies.</p><p>Numerous downstream knowledge-aware applications also come up with many datasets, for example, Wiki-Facts <ref type="bibr" target="#b202">[203]</ref> for language modeling; SimpleQuestions <ref type="bibr" target="#b154">[155]</ref> and LC-QuAD <ref type="bibr" target="#b203">[204]</ref> for question answering; and Freebase Semantic Scholar <ref type="bibr" target="#b178">[179]</ref> for academic search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Open-Source Libraries</head><p>Recent research has boosted open source campaign, with several libraries listed in Table <ref type="table" target="#tab_11">9</ref>. They are AmpliGraph <ref type="bibr" target="#b204">[205]</ref> for knowledge representation learning, Grakn for integration knowledge graph with machine learning techniques, and Akutan for knowledge graph store and query. The research community has also released codes to facilitate further research. Notably, there are three useful toolkits, namely scikit-kge and OpenKE <ref type="bibr" target="#b205">[206]</ref> for knowledge graph embedding, and OpenNRE <ref type="bibr" target="#b206">[207]</ref> for relation extraction. We  <ref type="bibr" target="#b189">[190]</ref> 47,000 306,000 https://www.cyc.com/opencyc/ Cyc <ref type="bibr" target="#b189">[190]</ref> ∼250,000 ∼2,200,000 https://www.cyc.com YAGO <ref type="bibr" target="#b191">[192]</ref> 1,056,638 ∼5,000,000 http://www.mpii.mpg.de/ ∼ suchanek/yago DBpedia <ref type="bibr" target="#b190">[191]</ref> ∼1,950,000 ∼103,000,000 https://wiki.dbpedia.org/develop/datasets Freebase <ref type="bibr" target="#b192">[193]</ref> -∼125,000,000 https://developers.google.com/freebase/ NELL <ref type="bibr" target="#b193">[194]</ref> -242,453 http://rtw.ml.cmu.edu/rtw/ Wikidata <ref type="bibr" target="#b194">[195]</ref> 14  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: An illustration of knowledge representation in different spaces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Illustrations of neural encoding models. (a) MLP<ref type="bibr" target="#b4">[5]</ref> and (b) CNN<ref type="bibr" target="#b36">[37]</ref> input triples into dense layer and convolution operation to learn semantic representation, (c) GCN<ref type="bibr" target="#b37">[38]</ref> acts as encoder of knowledge graphs to produce entity and relation embeddings. (d) RSN<ref type="bibr" target="#b38">[39]</ref> encodes entity-relation sequences and skips relations discriminatively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Illustrations of embedding-based ranking and relation path reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Illustrations of logical rule learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Illustrations of several entity discovery tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: A brief history of knowledge bases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>probability p r or 1 −</head><label>1</label><figDesc>p r respectively, with the head and tail domain of relation r denoted as M H r = {h | ∃ t(h, r, t) ∈ P } and M T r = {t | ∃ h(h, r, t) ∈ P }, and induced relational set denoted as N r = {(h, r, t) ∈ P }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Notations and descriptions</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>G F (h, r, t) (h, r, t) r ∈ R, e ∈ E v ∈ V e G ∈ E G es, eq, et rq &lt; w1, . . . , wn &gt; fr(h, t) σ(•), g(•) Mr</cell><cell>A knowledge graph A set of facts A triple of head, relation and tail Embedding of head, relation and tail Relation set and entity set Vertex in vertice set Edge in edge set Source/query/current entity Query relation Text corpus Scoring function Non-linear activation function Mapping matrix</cell></row><row><cell>M L R d</cell><cell>Tensor Loss function d dimensional real-valued space</cell></row><row><cell>C d</cell><cell>d dimensional complex space</cell></row><row><cell>H d</cell><cell>d dimensional hypercomplex space</cell></row><row><cell>T d</cell><cell>d dimensional torus space</cell></row><row><cell>N (u, σ 2 I) h, t t ⊗ r h • t, h t h t concat(), [h, r] ω</cell><cell>Gaussian distribution Hermitian dot product Hamilton product Hadmard (element-wise) product Circular correlation Vectors/matrices concatenation Convolutional filters</cell></row><row><cell></cell><cell></cell></row></table><note>* Convolution operator</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>tasks are divided into three categories, i.e., KGC, relation extraction and entity discovery. The first one is for expanding existing knowledge graphs, while the other two discover new knowledge (aka relations and entities) from text. KGC falls into the following categories: embedding-based ranking, relation path reasoning, rule-based reasoning and meta relational learning. Entity discovery includes recognition, disambiguation, typing and alignment. Relation extraction models utilize attention mechanism, graph convolutional networks (GCNs), adversarial training, reinforcement learning, deep residual learning, and transfer learning.</figDesc><table><row><cell cols="6">Temporal Knowledge Graphs incorporate temporal in-formation for representation learning. This survey catego-rizes four research fields including temporal embedding, entity dynamics, temporal relational dependency, and tem-poral logical reasoning.</cell></row><row><cell cols="6">Knowledge-aware Applications include natural lan-guage understanding (NLU), question answering, recommen-dation systems, and miscellaneous real-world tasks, which inject knowledge to improve representation learning.</cell></row><row><cell cols="2">-Point-wise -Manifold</cell><cell></cell><cell></cell><cell></cell><cell>-Single-fact QA</cell></row><row><cell cols="2">-Complex -Gaussian -Discrete</cell><cell cols="2">Representation Space</cell><cell>Natural Language Understanding</cell><cell>Question Answering</cell><cell>-Multi-hop Reasoning</cell></row><row><cell>-Distance -Semantic Matching -Others</cell><cell>Scoring Function Encoding Models</cell><cell cols="2">Knowledge Representation Learning</cell><cell cols="2">Knowledge-Aware Applications</cell><cell>Dialogue Systems Recommender Systems</cell></row><row><cell>-Linear/Bilinear</cell><cell cols="2">Auxiliary Information</cell><cell></cell><cell>Knowledge</cell><cell>Others Applications</cell><cell>-Question Generation</cell></row><row><cell>-Factorization</cell><cell></cell><cell></cell><cell></cell><cell>Graph</cell><cell>-Search Engine</cell></row><row><cell>-Neural Nets</cell><cell cols="2">-Textual -Type -Visual</cell><cell></cell><cell></cell><cell>-Medical Applications</cell></row><row><cell>-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-Mental Healthcare</cell></row><row><cell>-RNN -Transformers -GCN</cell><cell cols="2">Relation Extraction Entity Discovery</cell><cell>Knowledge Acquisition</cell><cell></cell><cell>Temporal Graph Knowledge</cell><cell>-Sentiment Analysis -Zero-shot Image Classification -Text Generation</cell></row><row><cell>-Recognition -Typing</cell><cell></cell><cell cols="3">Knowledge Graph Completion</cell><cell>Temporal Embedding</cell></row><row><cell>-Disambiguation -Alignment</cell><cell>-Neural Nets -Attention</cell><cell cols="3">-Embedding-based Ranking</cell><cell>Entity Dynamics</cell></row><row><cell></cell><cell>-GCN</cell><cell cols="2">-Path-based Reasoning</cell><cell></cell></row><row><cell></cell><cell>-GAN</cell><cell cols="2">-Rule-based Reasoning</cell><cell></cell><cell>Temporal Relational Dependency</cell></row><row><cell></cell><cell>-RL</cell><cell cols="3">-Meta Relational Learning</cell></row><row><cell></cell><cell>-Others</cell><cell cols="2">-Triple Classification</cell><cell cols="2">Temporal Logical Reasoning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>A summary of recent KRL models. See more in Appendix C.</figDesc><table><row><cell>Model</cell><cell>Ent. &amp; Rel. embed.</cell><cell>Scoring Function fr(h, t)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison of RL-based path finding for knowledge graph reasoning</figDesc><table><row><cell>Method</cell><cell>State st</cell><cell>Action at</cell><cell>Reward γ</cell><cell>Policy Network</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>A summary of neural relation extraction and recent advances</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Mechanism</cell><cell>Auxiliary Information</cell></row><row><cell></cell><cell>O-CNN [108] Multi CNN</cell><cell>CNN + max pooling</cell><cell>position embedding</cell></row><row><cell>CNNs</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>A comprehensive summary of knowledge representation learning models</figDesc><table><row><cell>Category</cell><cell>Model</cell><cell>Ent. embed.</cell><cell>Rel. embed.</cell><cell>Scoring Function fr(h, t)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>A summary of negative sampling</figDesc><table><row><cell>Sampling</cell><cell>Mechanism</cell><cell cols="3">Sampling probability</cell></row><row><cell>Uniform [34] Bernoulli [15]</cell><cell>uniform distribution mapping property</cell><cell cols="2">1 n tph+hpt tph</cell><cell></cell></row><row><cell>Domain [31]</cell><cell>relation-depend domain</cell><cell>min</cell><cell>λ M T r M H r |Nr |</cell><cell>, 0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>6. https://www.researchspace.org/index.html 7. http://geneontology.org 8. http://www.snomed.org/snomed-ct/five-step-briefing 9. https://www.yiducloud.com.cn/en/academy.html 10. https://code.google.com/archive/p/ relation-extraction-corpus/ provide an online collection of knowledge graph publications, together with links to some open-source implementations of them, hosted at https://github.com/shaoxiongji/ awesome-knowledge-graph. Statistics of datasets with general knowledge when originally released</figDesc><table><row><cell>Dataset</cell><cell># entities</cell><cell># facts</cell><cell>Website</cell></row><row><cell>WordNet [189] OpenCyc</cell><cell>117,597</cell><cell>207,016</cell><cell>https://wordnet.princeton.edu</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 :</head><label>9</label><figDesc>A summary of open-source libraries</figDesc><table><row><cell>Task</cell><cell>Library</cell><cell>Language URL</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We acknowledge the Australian Research Council (ARC) Linkage Project (LP150100671), the UQ Candidate Travel Award, and the Aalto Science-IT project.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>General Grakn Python https://github.com/graknlabs/kglib General AmpliGraph TensorFlow https://github.com/Accenture/AmpliGraph Database Akutan Go https://github.com/eBay/akutan KRL OpenKE PyTorch https://github.com/thunlp/OpenKE KRL Fast-TransX C++ https://github.com/thunlp/Fast-TransX KRL scikit-kge Python https://github.com/mnick/scikit-kge RE OpenNRE PyTorch https://github.com/thunlp/OpenNRE</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Preprogramming for mechanical translation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Richens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechanical Translation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Report on a general problem solving program</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFIP congress</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computer-based medical consultations: MYCIN. Elsevier</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shortliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Structuring knowledge in a graph</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Stokman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Vries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Human-Computer Interaction</publisher>
			<biblScope unit="page" from="186" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge vault: A webscale approach to probabilistic knowledge fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic web</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="489" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards a definition of knowledge graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ehrlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Öß</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SEMANTiCS (Posters, Demos, SuCCESS)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of techniques for constructing chinese knowledge graphs and their applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3245</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge representation learning: A quantitative review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10901</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analogical inference for multirelational embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TransG: A generative model for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Orbit models for knowledge graph embedding</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1315" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TorusE: Knowledge graph embedding on a lie group</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1819" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation embedding with dihedral group in knowledge graph</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TransA: An adaptive approach for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding by flexible translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>KR</publisher>
			<biblScope unit="page" from="557" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An interpretable knowledge transfer model for knowledge base completion</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="950" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Translating embeddings for knowledge graph completion with relation attention mechanism</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4286" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TransMS: knowledge graph embedding for complex relations by multidirectional semantics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1935" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Expanding holographic embeddings for knowledge completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4491" to="4501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interaction embeddings for prediction and explanation in knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-toend structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to exploit long-term relational dependencies in knowledge graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On multi-relational link prediction with bilinear models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4227" to="4234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SimplE embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4284" to="4295" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TuckER: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning via deep learning: Neural association models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evdokimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07704</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hypernetwork knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">I</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="553" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">CoKE: Contextualized knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02168</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for knowledge graph completion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprent</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SSP: semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantically smooth knowledge graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Knowledge representation learning with entities, attributes and relations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2866" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with hierarchical relation structure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3198" to="3207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image-embodied knowledge representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3140" to="3146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural knowledge acquisition via mutual attention between knowledge graph and text</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4832" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ProjE: Embedding projection for knowledge graph completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Shared embedding based neural networks for knowledge graph completion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Open-world knowledge graph completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On the generative discovery of structured medical knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2720" to="2728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Variational knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DeepPath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3243" to="3253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">M-Walk: Learning to walk over graphs using monte carlo tree search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6786" to="6797" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Collaborative policy learning for open knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2681" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">AMIE: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">An embedding-based approach to rule learning in knowledge graphs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Iteratively learning embeddings and rules for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2366" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">End-to-end differentiable proving</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3788" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Probabilistic logic neural networks for reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="7710" to="7720" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Efficient probabilistic logic reasoning with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1980" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Adapting meta knowledge graph information for multi-hop reasoning over fewshot relations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3374" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4217" to="4226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Triple classification using regions and fine-grained entity typing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multi-grained named entity recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1430" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Entity disambiguation by knowledge and text jointly embedding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Iterative entity alignment via joint knowledge embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4258" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Bootstrapping entity alignment with knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4396" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attribute-preserving embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Cotraining embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3998" to="4004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Multi-view knowledge graph embedding for entity alignment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5429" to="5435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Entity alignment between knowledge graphs using attribute embeddings</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Trsedya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL and IJCNLP of the AFNLP</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Discovering correlations between sparse features in distant supervision for relation extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="726" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Vector Space Modeling for Natural Language Processing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Relation extraction with multi-instance multi-label convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1471" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Jointly extracting relations with class ties via effective deep ranking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1810" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Incorporating relation paths in neural relation extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1768" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for semantic relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2236" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">DSGAN: Generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2137" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Large scaled relation extraction with reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5658" to="5665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5779" to="5786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A hierarchical framework for relation extraction with reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7072" to="7079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Deep residual learning for weaklysupervised relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1803" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Neural relation extraction via inner-sentence noise reduction and transfer learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2195" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Cooperative denoising for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="426" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Relation extraction using supervision from topic knowledge of relation labels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="5024" to="5030" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Hybrid attention-based prototypical networks for noisy few-shot relation classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6407" to="6414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Deriving validity time in knowledge graph</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chekol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1771" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Hyte: Hyperplanebased temporally aware knowledge graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2001" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumančić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4816" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Context-aware temporal knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WISE</publisher>
			<biblScope unit="page" from="583" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">CTPs: Contextual temporal profiles for time scoping facts using state change detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1930" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Recurrent event network for reasoning over temporal knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR RLGM Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Towards time-aware knowledge graph completion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Encoding temporal information for time-aware link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2350" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Marrying uncertainty and time in knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chekol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pirr Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schoenfisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding</title>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="619" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Combining knowledge with deep convolutional neural networks for short text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Fine-grained event categorization with heterogeneous graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Barack&apos;s wife hillary: Using knowledge graphs for fact-aware language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Integrating graph contextualized knowledge into pre-trained language models</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00147</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">ERNIE: Enhanced representation through knowledge integration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">CFO: Conditional focused neural question answering with large-scale knowledge bases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="800" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Bidirectional attentive memory networks for question answering over knowledge bases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2913" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Strong baselines for simple question answering over knowledge graphs with and without neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Commonsense for generative multi-hop question answering tasks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4220" to="4230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6069" to="6076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">KagNet: Knowledgeaware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2829" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">DKN: Deep knowledgeaware network for news recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1835" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Multi-task feature learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2000" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Reinforcement knowledge graph reasoning for explainable recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Towards understanding the geometry of knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Effective blending of two and three-way interactions for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="434" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Deductive question-answering on relational data bases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logic and data bases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="149" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">KBGAN: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Knowledge questions from knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Seyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Explicit semantic ranking for academic search via knowledge graph embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1271" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Knowledge-driven encode, retrieve, paraphrase for medical image report generation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10122</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Knowledgeaware assessment of severity of suicide risk for early intervention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alambo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Sain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kursuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Welton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pathak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="514" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">SenticNet 5: Discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1795" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5876" to="5883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Knowledge aware conversation generation with explainable reasoning over augmented graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1782" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Dialog-to-Action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2942" to="2951" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">An introduction to the syntax and content of cyc</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deoliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledge base</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kr Ötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An upper-level ontology for the biomedical domain</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Genomics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="84" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Semantic multidimensional scaling for open-domain sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Universal representation learning of knowledge bases by jointly embedding instances and ontological concepts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1709" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on CVSC</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">LC-QuAD: A corpus for complex question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">AmpliGraph: a Library for Representation Learning on Knowledge Graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mccarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">OpenKE: An open toolkit for knowledge embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">OpenNRE: An open and extensible toolkit for neural relation extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
