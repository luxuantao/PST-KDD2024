<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="laboratory">Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
							<email>whdeng@bupt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="laboratory">Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="laboratory">Pattern Recognition and Intelligent System Laboratory</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1E231E8291CE8BD4F536FC4420A7F69</idno>
					<idno type="DOI">10.1109/TIP.2018.2868382</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2868382, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2868382, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Expression recognition</term>
					<term>Basic emotion</term>
					<term>Compound emotion</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30,000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization (EM) algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units (AUs) of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTOMATIC facial expression recognition has made sig- nificant progress in the past two decades <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b76">[78]</ref>. However, many developed frameworks have been employed strictly on the data collected under controlled laboratory settings with frontal faces, uniform illumination and posed expressions. On the other hand, a massive amount of images from different events and social gatherings in unconstrained environments have been captured by users from real world <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b58">[60]</ref>. The design of systems capable of understanding the community perception of emotional attributes and affective displays from social images is receiving increasing interest. Fortunately, the emerging deep learning techniques have advanced unconstrained expression recognition to a new state-of-the-art <ref type="bibr" target="#b29">[31]</ref>, <ref type="bibr" target="#b43">[45]</ref>. However, to automatically infer the affective state of facial images, databases that contain large-scale valid samples and can simultaneously reflect the characteristic of real-world expressions are urgently needed.</p><p>Although Internet users [1], <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b34">[36]</ref>, <ref type="bibr" target="#b67">[69]</ref> provide an abundant data source for unconstrained expressions, the complexity of emotion categories annotation has hindered the collection of large annotated databases. In particular, popular AU coding <ref type="bibr" target="#b15">[17]</ref> requires specific expertise to take months to learn and be perfected. In addition, due to cultural differences in the perception of facial emotion <ref type="bibr" target="#b16">[18]</ref>, it is difficult for psychologists to define definite prototypical AUs for each facial expression. Therefore, it is also worth to study the emotion of social images based on the judgments of a large common population by crowdsourcing <ref type="bibr" target="#b8">[10]</ref>, rather than the professional knowledge of a few experts.</p><p>Motivated by these observations, we investigate human perception and automatic recognition of unconstrained facial expressions via crowdsourcing and deep learning techniques. To this end, we have collected a large-scale, diverse, and reliably annotated facial expression database in the wild, Realworld Affective Face Database (RAF-DB)<ref type="foot" target="#foot_0">1</ref> . During annotation, 315 well-trained annotators are asked to label facial images with one of seven basic categories <ref type="bibr" target="#b14">[16]</ref>, and each image is independently annotated enough times, i.e., around 40 times in our experiment. The contributions of this paper are fourfold:</p><p>First, to enhance the readability of the label estimation, we develop an EM-based reliability estimation algorithm to evaluate the professionalism level of each labeler and then filter out the noisy labels, enabling each image to be represented reliably by a 7-dimensional emotion probability vector. By analyzing 1.2 million labels of 29,672 highly diverse facial images downloaded from the Internet, we find that real-world affective faces are naturally categorized into two types: basic expressions with single-modal distributions and compound emotions with bimodal distributions. This observation supports a recent finding in the lab-controlled condition <ref type="bibr" target="#b12">[14]</ref>. To the best of our knowledge, the real-world expression database RAF-DB is the first large-scale database to provide reliable labels of common expression perception and compound emotions in an unconstrained environment.</p><p>Second, to investigate differences between expressions captured under controlled and unconstrained conditions, we conduct a cross-database study between CK+ <ref type="bibr" target="#b44">[46]</ref> (the most pop- Images provided in RAF-DB are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions (e.g., glasses, facial hair or special gestures that hide some of the feature points), post-processing operations (e.g., various filters and special effects), etc.</p><p>ular expression benchmark database defined by psychologists) and our RAF-DB. The expression recognition results, as well as the manual AU inspections, reveal that the AUs of realworld expressions are different from and much more diverse than those of lab-controlled expressions, as illustrated in Fig. <ref type="figure">7</ref>. Due to these difficulties, as well as large variations in pose, illumination, and occlusion, traditional handcrafted features or shallow-learning-based features, which are well-established in lab-controlled datasets, fail to recognize facial expressions under unconstrained conditions. Third, to improve the CNN based expression recognition, we propose a novel deep learning based framework, Deep Locality-preserving CNN (DLP-CNN). Inspired by <ref type="bibr" target="#b24">[26]</ref>, we develop a practical backpropagation algorithm that adapts the seminal idea of local neighbors from "shallow" learning to a new deep feature learning approach by creating a locality preserving loss (LP loss) which aims to pull the locally neighboring faces of the same class together. Jointly trained with the classical softmax loss which forces different classes to stay apart, locality preserving loss drives the intra-class local clusters of each class to become compact, thus highly enhancing the discriminative power of the deeply learned features. Moreover, locally neighboring faces tend to share similar emotion intensity by using DLP-CNN, which can derive discriminative deep features with smooth emotion intensity transitions. To the best of our knowledge, this is the first attempt to use such a loss function to help to supervise the learning of CNNs, thereby achieving enhanced discriminating power compared to the up-to-date approaches and setting a new state-of-the-art for expression recognition in-the-wild.</p><p>Finally, to facilitate the translation of the research from the laboratory environment to the real world, we have defined two challenging benchmark experiments on RAF-DB: 7-class basic expression classification and 11-class compound expression classification. We also conduct extensive experiments on RAF-DB and other related databases. The comparison results show that the proposed DLP-CNN outperforms handcrafted features and other state-of-the-art CNN methods. Moreover, the activation features trained on RAF-DB can be repurposed to new databases with small-sample training data, suggesting that the DLP-CNN is a powerful tool to handle the crossculture problem on perception of emotion (POE).</p><p>This journal paper is an extended version of the conference paper <ref type="bibr" target="#b37">[39]</ref> of CVPR 2017. The new content in this paper includes a detailed discussion of existing expression image databases, a comparative facial action coding system (FACS) analysis of CK+ and RAF-DB, a shape feature learning method for the baseline of RAF-DB and a comparative study of the proposed deep learning method on other three common databases. The remainder of this paper is structured as follows.</p><p>In the next section, we briefly review related work on expression database and recognition methods. Then, we introduce the details of the construction of RAF-DB and the crossdatabase study between CK+ and RAF-DB in Section III. In Section IV, we introduce our new DLP-CNN approach in detail. Additionally, we include the experimental results of the baseline and DLP-CNN in Section V. Finally, we conclude and discuss future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We first discuss related expression image datasets and then review the generic framework for facial expression analysis. Moreover, because the deep learning technique has achieved state-of-the-art performance in the field of image processing, we investigate several existing deep learning methods that have been employed for facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expression image datasets</head><p>Developments of facial expression recognition largely rely on sufficient facial expression databases. However, due to the nature of facial expression, there is a restricted number of publicly available databases providing a sufficient number of face images labeled with accurate expression information. Table <ref type="table" target="#tab_0">I</ref> shows the summary of the existing image databases with main reference, number of samples, age range, collected environment, expression distribution, annotation method and additional information.</p><p>Several limitations among these widely used databases are common:</p><p>1. Many available databases were produced in tightly controlled environments. Subjects in them were taught to act expressions in a posed paradigm. Owing to the lack of diversity of subjects and conditions, the current recognition systems tested on these facial expression databases have reached near-perfect performance, which hinders the progress of expression recognition in the wild. 2. Images captured in real-life scenarios often present complex, compound or even ambiguous emotions rather than simple and prototypical emotions. However, the majority of the current databases include only six basic categories (surprise, fear, disgust, happiness, sadness and anger) or fewer. We then focus on discussing image databases with spontaneous expressions. SFEW 2.0 <ref type="bibr" target="#b10">[12]</ref> collected images from movies using keyframe extraction method and was introduced in the EmotiW 2015 Challenge. The database covers unconstrained facial expressions, varied head poses, a large age range, occlusions, varied focus and different resolutions of faces. However, it contains only 1,635 images labeled by two independent labelers. FER-2013 <ref type="bibr" target="#b20">[22]</ref> contains 35,887 images collected and automatically labeled by Google image search API. Cropped images are provided in 48×48 low resolution and converted to grayscale. Unfortunately, FER-2013 does not provide information about facial landmark location and the images are difficult to register well at the provided resolution and quality.</p><p>BP4D-Spontaneous <ref type="bibr" target="#b78">[80]</ref> contains abundant images with high resolution from 41 subjects displaying a range of spontaneous expressions elicited through eight tasks. One highlight of BP4D is that it captured images using a 3D dynamic face capturing system. However, the database organization were lab-controlled, and all the subjects in this dataset are young adults. AM-FED <ref type="bibr" target="#b49">[51]</ref> contains 242 facial videos from the real world. Spontaneous facial expressions were captured from subjects under different recording conditions while they were watching Super Bowl commercials. The database was annotated for the presence of 14 FACS action units. However, without specific emotion labels, it is more suited for researches on AUs.</p><p>EmotioNet <ref type="bibr" target="#b2">[4]</ref> is a large-scale database with one million facial expression images collected from the Internet. Most samples were annotated by an automatic AU detection algorithm, and the remaining 10% were manually annotated with AUs. EmotioNet contains 6 basic expressions and also 17 compound expressions; however, the emotion categories are judged based on AU label and not manually annotated.</p><p>AffectNet <ref type="bibr" target="#b51">[53]</ref> contains more than one million images obtained from the Internet by querying different search engines using emotion related tags. A total of 450,000 images are annotated with basic expressions by 12 labelers. Furthermore, this database contains continuous dimensional (valences and arousal) models for these images. However, each image was labeled by only one annotator due to time and budget constraints, and compound expressions are not included.</p><p>In contrast to these databases, RAF-DB simultaneously satisfies multiple requirements: sufficient data, various environments, group perception of facial expressions and data labels with minimal noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The framework for expression recognition</head><p>Automatic facial expression analysis procedures can generally be divided into three main components <ref type="bibr" target="#b18">[20]</ref>: face acquisition, facial feature extraction and facial expression classification.</p><p>In the face acquisition stage, an automatic face detector is used to locate faces in complex scenes. Feature points are then used to crop and align faces into a unified template by geometric transformations.</p><p>For facial feature extraction, previous methods can be categorized into two main groups: appearance-based methods and AU-based methods. Appearance-based methods <ref type="bibr" target="#b47">[49]</ref> use common handcrafted feature extraction methods, such as LBP <ref type="bibr" target="#b62">[64]</ref> and Haar <ref type="bibr" target="#b70">[72]</ref>. AU-based methods <ref type="bibr" target="#b66">[68]</ref> recognize expressions by detecting AUs. The most well-known AUs included in our study are the following: AU1-Inner Brow Raiser, AU2-Outer Brow Raiser, AU4-Brow Lowerer, AU5-Upper Lid Raiser, AU6-Cheek Raiser, AU7-Lid Tightener, AU9-Nose Wrinkler, AU10-Upper Lip Raiser, AU12-Lip Corner Puller, AU15-Lip Corner Depressor, AU17-Chin Raiser, AU20-Lip stretcher, AU23-Lip Tightener, AU24-Lip Pressor, AU 25-Lips part, AU26-Jaw Drop, AU 27-Mouth Stretch. Furthermore, mid-lever feature learning methods <ref type="bibr" target="#b6">[8]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b42">[44]</ref> based on manifold learning have been developed to enhance the discrimination ability of extracted low-level features.</p><p>Feature classification is performed in the final stage. The commonly used classification methods for emotion recognition include support vector machine (SVM), nearest neighbor (NN) based classifier, LDA, HMM, DBN and decision-level fusion on these classifiers <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b76">[78]</ref>. The extracted facial expression information is either classified as a particular facial action or a particular basic emotion <ref type="bibr" target="#b54">[56]</ref>. Most of the studies on automatic expression recognition focus on the latter; yet, the majority of the existing systems for emotional classification is based upon Ekman's cross-cultural theory of six basic emotions <ref type="bibr" target="#b15">[17]</ref>. Indeed, without making additional assumptions about how to determine what action units constitute an expression, there can be no exact definition for the expression category. The basic emotional expressions are therefore not universal enough to generalize expressions displayed on the human face <ref type="bibr" target="#b59">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep learning for expression recognition</head><p>Recently, deep learning algorithms have been applied to visual object recognition, face verification and detection, image classification and many other problems, and have achieved state-of-the-art results. So far, few deep neural networks have been used in facial expression recognition due to the lack of sufficient training samples. In ICML 2013 competition <ref type="bibr" target="#b20">[22]</ref>, the winner <ref type="bibr" target="#b65">[67]</ref> was based on Deep Convolutional Neural Network (DCNN) plus SVM. In EmotiW 2013 competition <ref type="bibr" target="#b9">[11]</ref>, the winner <ref type="bibr" target="#b30">[32]</ref> combined modality specific deep neural network models. In EmotiW 2015 <ref type="bibr" target="#b10">[12]</ref>, more competitors implemented deep learning methods: transfer learning was used to solve the problem of small database in <ref type="bibr" target="#b52">[54]</ref>, a hierarchical committee of multi-column DCNNs in <ref type="bibr" target="#b31">[33]</ref> gained the best result on SFEW 2.0 database, and LBP features combined with a DCNN structure were proposed in <ref type="bibr" target="#b36">[38]</ref>. In <ref type="bibr" target="#b39">[41]</ref>, AUaware Deep Networks (AUDN) was proposed to learn features with the interpretation of facial AUs. In <ref type="bibr" target="#b40">[42]</ref>, 3D Convolutional Neural Networks (3DCNN-DAP) with deformable action parts constraints were adopted to localize the action parts and encode them effectively. In DTAGN <ref type="bibr" target="#b28">[30]</ref>, two different models were combined to extract temporal appearance and geometry features simultaneously. In <ref type="bibr" target="#b50">[52]</ref>, a DCNN with inception layers was proposed to achieve comparable results. In <ref type="bibr" target="#b77">[79]</ref>, a DNNdriven feature learning method was proposed to address multiview facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REAL-WORLD EXPRESSION DATABASE: RAF-DB</head><p>A. Creating RAF-DB Data collection. At the very beginning, images' URLs collected from Flickr were fed into an automatic open source downloader to download images in batches. Considering that the results returned by Flickr's image search API were in well-structured XML format, from which the URLs can be easily parsed, we then used a set of keywords to pick out the images that were related to the six basic emotions plus the neutral emotion. At last, a total of 29,672 real-world facial images are presented in our database. Some of the emotionrelated keywords used are listed in Table <ref type="table" target="#tab_1">II</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows the pipeline of data collection. Database annotation. Annotating nearly 30,000 images is an extremely difficult and time-consuming task. Considering the compounded property of real-world expressions, multiple views of images' expression states should be collected from different labelers. Therefore, we employed 315 annotators (students and staff from universities) who were instructed with a one-hour tutorial of psychological knowledge on emotion for an online facial expression annotation assignment, during which they were asked to classify images into the most apparent one from seven classes. We developed a website to make it easy for our annotators to contribute, which shows each image with exclusive attribute options. Images were randomly and equally assigned to each labeler, ensuring that there was no direct correlation among the images labeled by one person. And each image was ensured to be labeled by about 40 independent labelers. After that, a multi-label annotation result was obtained for each image, i.e., a sevendimensional vector where each dimension indicates the votes for the relevant emotion. The UI of the database annotation application is shown in Figure <ref type="figure" target="#fig_2">3</ref>(a), and a example of a typical annotation result is shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>.  Metadata. The data are provided with precise locations and the size of the face region, as well as five manually located landmark points (the centers of two eyes, the tip of the nose and the two corners of the mouth) on the face. The 5 facial landmarks were accurately located by labelers who were asked to guess the locations of occluded landmarks. Specifically, the rough locations and points were first detected automatically using the Viola-Jones face detector <ref type="bibr" target="#b68">[70]</ref> and SDM <ref type="bibr" target="#b72">[74]</ref> based methods. Then, imprecise or missed detections and localizations were corrected by human labelers. Besides, an automatic landmark annotation mode without manual label is included: 37 landmarks were picked out from the annotation results provided by Face++ API <ref type="bibr" target="#b26">[28]</ref>. Figure <ref type="figure">4</ref>(a) shows sample faces with five precise landmarks and 37 landmarks. We also manually annotated the basic attributes (gender, age (5 ranges) and race) of all RAF faces. In summary, subjects in our database range in age from 0 to 70 years old. They are 52% female, 43% male, and 5% unclear. For racial distribution, there are 77% Caucasian, 8% African American, and 15% Asian. The pose of each image, including pitch, yaw and roll parameters, is computed from the manually labeled locations of the five facial landmarks. Figure <ref type="figure">4</ref>(b) shows the age (images with unclear gender are of infants.) and pose distributions in RAF-DB.</p><p>Reliability estimation. Due to the subjectivity and varied expertise of the labelers and the wide range of image difficulty, there was some disagreement among annotators. To get rid of noisy labels, motivated by <ref type="bibr" target="#b71">[73]</ref>, an Expectation Maximization (EM) framework was used to assess each labeler's reliability.</p><p>Let D = {(x j , y j , t 1 j , t 2 j , ..., t R j )} n j=1 denote a set of n labeled inputs, where y j is the gold standard label (hidden variable) for the jth sample x j , and t i j ∈ {1, 2, 3, 4, 5, 6, 7} is the corresponding label given by the ith annotator. The correct probability of t i j is formulated as a sigmoid function p(t i j = y j |α i , β j ) = (1 + exp(-α i β j )) -1 , where 1/β j is the difficulty of the jth image and α i is the reliability of the ith annotator.</p><p>Our goal is to optimize the log-likelihood of the given labels:</p><formula xml:id="formula_0">max β&gt;0 l(α, β) = j ln p(t|α, β) = j ln y p(t, y|α, β) = j ln y Q j (y) p(t, y|α, β) Q j (y) ≥ j y Q j (y) ln p(t, y|α, β) Q j (y) ,</formula><p>where Q j (y) is a certain distribution of hidden variable y,</p><formula xml:id="formula_1">Q j (y j ) = p(t j , y j |α, β) y p(t j , y j |α, β) = p(t j , y j |α, β) p(t j |α, β) = p(y j |t j , α, β).</formula><p>After revision, 285 annotators' labels have been remained and Cronbach's Alpha score of all labels is 0.966. Algorithm 1 summarizes the learning process of label reliability estimation.</p><p>In contrast to the Gaussian prior initialization in <ref type="bibr" target="#b71">[73]</ref>, we further introduced the prior knowledge of annotation for faster convergence. Subset Partitions. Let G j = {g 1 , g 2 , ..., g 7 } denote the 7-dimensional ground truth of the jth image, where g k = R i=1 α i 1 t i j =k (α i means the ith annotators' reliability. 1 A is an indicator function that evaluates to "1" if the Boolean expression A is true and "0" otherwise.), and label k ∈ {1, 2, 3, 4, 5, 6, 7} refers to surprise, fear, disgust, happiness,  Algorithm 1 Label reliability estimation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Training set D = {(xj, t 1 j , t We also optimize βj along with αi during M-step. However, the goal is to get each labeler's reliability, so we didn't include it in this step. For optimization, we take a derivative with respect to βj and αi respectively. Until convergence sadness, anger and neutral, respectively. We then divide RAF-DB into different subsets according to the 7-dimensional ground truth. For the Single-label Subset, we first calculate the mean distribution value g mean = 7 k=1 g k /7 for each image, then select label k w.r.t. g k &gt; g mean as the valid label. Images with a single valid label are classified into Single-label Subset.</p><p>For Two-tab Subset, the partition rule is similar. The only difference is that we removed images with neutral labels before the partition step. Figure <ref type="figure" target="#fig_4">5</ref> exhibits specific samples and the concrete proportion of 6-class basic emotions and 12-class compound emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CK+ and RAF Cross-Database Study</head><p>We then conducted a CK+ <ref type="bibr" target="#b44">[46]</ref> and RAF cross-database study to explore the specific differences between expressions of real-world affective faces and lab-controlled posed faces guided by psychologists. Here, "cross-database" means we use the images from one database for training and images from the other for testing. By this study, we aim to identify the real challenges of real-world affective face analysis. To eliminate the bias caused by different training sizes, the single-tab subset of RAF-DB has been sub-sampled to balance the size of these two databases.</p><p>To ensure the generalizability of the classifiers, we applied SVM for classification and implemented the HOG descriptor <ref type="bibr" target="#b7">[9]</ref> for representation. Specifically, facial images were first aligned by an affine transformation defined by the centers of the two eyes and the center of the two corners of the mouth and then normalized to the size of 100×100. Then, the HOG features were extracted for each aligned face image. Finally, SVM with radial basis function (RBF) kernel implemented by LibSVM <ref type="bibr" target="#b5">[7]</ref> was applied for classification. The parameters were optimized via grid search.</p><p>We then performed a cross-database experiment based on the six-class expression. Multiclass SVM (mSVM) and confusion matrix were used as the classification method and assessment criteria, respectively. Figure <ref type="figure">6</ref>  Analyzing the diagonals of these two matrixes, we can see that surprise, happiness and disgust are the emotions with the highest recognition rates in both cases. This result is in line with many single-database tests based on CK+, such as <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b56">[58]</ref> and <ref type="bibr" target="#b62">[64]</ref>. The average of the diagonals indicates that Matrix (a) was detected with 62% accuracy while Matrix (b) was detected with only 39% accuracy, which indicates that data collected from the real world are more varied and effective than lab-controlled data. This is particularly evident in the expression of sadness, happiness and surprise. Furthermore, anger and disgust are often confused with each other in both cases, which conforms to the survey in <ref type="bibr" target="#b3">[5]</ref>.</p><p>To explain the phenomena above, more detailed research must be conducted to identify the specific differences in each expression between these two databases. Therefore, a facial action coding system (FACS) analysis was employed on the experimental data from RAF-DB. FACS was first presented in <ref type="bibr" target="#b15">[17]</ref>, where the changes in facial behaviors were described by a set of action units (AUs). To ensure the reliability, two FACS coders were employed to label AUs for the 309 images randomly chosen from RAF-DB. During annotation, the magnified original color facial images were displayed on the screen. The inter-observer agreement quantified by coefficient kappa was 0.83, and the two coders discussed with each other to arbitrate the disagreements. We then quantitatively analyzed the AU presence for different emotions in CK+ and RAF. Some examples from CK+ and RAF are shown in Figure <ref type="figure">7</ref>. Additionally, the AU occurrence probabilities for each expression from the subset of RAF-DB are shown in Table <ref type="table" target="#tab_4">III</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP LOCALITY-PRESERVING FEATURE LEARNING</head><p>In addition to the difficulties such as variable lighting, poses and occlusions, real-world affective faces pose at least two challenges that require new algorithms to address. First, as indicated by our cross-database study, real-world expressions may associate with various AU combinations that require classification algorithms to model the multi-modality distribution of each emotion in the feature space. Second, as suggested by our crowdsourcing results, a large proportion of realworld affective faces express compound or even multiple emotions. Therefore, traditional hand-engineered representations that perform well on laboratory-controlled databases are not suitable for expression recognition tasks in the wild.</p><p>Recently, DCNN has been proved to outperform handcrafted features on large-scale visual recognition tasks. Conventional DCNN commonly uses the softmax loss layer to supervise the training process. By denoting the i-th input feature x i with the label y i , the softmax loss can be written as where f j denotes the j-th element (j = 1 . . . C, C is the number of classes) of the vector of the class scores f , and n is the number of training data. The softmax layer merely helps to keep the deeply learned features of different classes (expressions) separable. Unfortunately, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, facial expressions in the real world show significant intraclass differences in occlusion, illumination, resolution and head position. Moreover, individual differences can also lead to major differences in the same expression category, for example, laugh vs. smile.</p><formula xml:id="formula_2">L s = - 1 n n i=1 log e fy i j e fj ,<label>(1)</label></formula><p>To address these difficulties, we propose a novel DLP-CNN to address the ambiguity and multi-modality of real-world facial expressions. In DLP-CNN, we add a new supervised layer to the fundamental architecture shown in Table <ref type="table" target="#tab_5">IV</ref>, namely, locality preserving loss (LP loss), to improve the discrimination ability of the deep features. The basic idea is to preserve the locality of each sample x i and to make the local neighborhoods within each class as compact as possible. To formulate our goal:</p><formula xml:id="formula_3">min i,j S ij x i -x j 2 2 ,<label>(2)</label></formula><p>where matrix is a similarity matrix. The deep feature x ∈ R d denotes deep convolutional activation features (DeCaf) <ref type="bibr" target="#b11">[13]</ref> taken from the final hidden layer, i.e., just before the softmax layer that produce the class predictions. A possible way to define S is as follows.</p><formula xml:id="formula_4">S ij =    1, x j is among the k-nearest neighbors of x i or x i is among the k-nearest neighbors of x j 0, otherwise,<label>(3)</label></formula><p>where x i and x j belong to the same class of expression and k defines the size of the local neighborhood. This formulation effectively characterizes the intra-class local scatter. Note that x i should be updated as the iterative optimization of the CNN. To compute the summation of the pairwise distances, we need to consider the entire training set in each iteration, which is inefficient to implement. To address this difficulty, we do the approximation by searching the knearest neighbors for each sample x i , and the LP loss function of x i is defined as follows:</p><formula xml:id="formula_5">L lp = 1 2n n i=1 x i - 1 k x∈N k {xi} x 2 2 ,<label>(4)</label></formula><p>where N k {x i } denotes the ensemble of the k-nearest neighbors of sample x i with the same class.</p><p>The gradient of L lp with respect to x i is computed as:</p><formula xml:id="formula_6">∂L lp ∂x i = 1 n   x i - 1 k x∈N k {xi} x   .<label>(5)</label></formula><p>In this manner, we can perform the update based on minibatch. Note that the recently proposed center loss <ref type="bibr" target="#b69">[71]</ref> can be considered to be a special case of the LP loss if k = n c -1 (n c is the number of training samples in class c to which x i belong). While center loss simply pulls the samples to a single centroid, the proposed LP loss is more flexible, especially when the class conditional distribution is multi-modal.</p><p>We then adopt the joint supervision of softmax loss, which characterizes the global scatter, and the LP loss, which characterizes the local scatters within class, to train the CNNs for discriminative feature learning. The objective function is formulated as follows: L = L s + λL lp , where L s denotes the softmax loss and L lp denotes the LP loss. The hyperparameter λ is used to balance the two loss functions. Algorithm 2 summarizes the learning process in the DLP-CNN. Intuitively, the softmax loss forces the deep features of different classes to remain apart and the LP loss efficiently pulls the neighboring deep features of the same class together. With the joint supervision, both the inter-class feature differences and the intra-class feature correlations are enlarged. Hence, the discriminative power of the deeply learned features can be highly enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Optimization algorithm of DLP-CNN.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Training data {xi, yi} n i=1 , n is the size of the mini-batch Output:</p><p>Network layer parameters W Initialize: t ← 0 Network learning rate µ, hyperparameter λ, network layer parameters W , softmax loss parameters θ, neighboring nodes k. Repeat: </p><formula xml:id="formula_7">1: t ← t +</formula><formula xml:id="formula_8">W t+1 = W t -µ t ∂L t ∂W t = W t -µ t n i=1 ∂L t ∂x t i ∂x t i ∂W t</formula><p>Until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS ON BASELINE AND DLP-CNN</head><p>We first conducted baseline experiments on RAF-DB. Then, the proposed deep learning method, Deep Locality-preserving CNN, was employed to solve the difficulties of facial expression recognition in real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Experiment</head><p>In this section, to evaluate the tasks with real-world challenges, we performed two benchmark experiments on RAF-DB and presented affiliated baseline algorithms and performances. While our main purpose is to analyze the results of the aforementioned techniques on RAF-DB, we also conduct experiments on two small and popular datasets, CK+ and JAFFE <ref type="bibr" target="#b46">[48]</ref>.</p><p>For facial representations, we employ two types of information encoded in the feature space: shape and appearance. Experiments on human subjects demonstrate that shape representations play a role in the recognition of the emotion class from face images <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b60">[62]</ref>. Before computing our feature space, all images are aligned and downsized to 100*100 pixels within the given precise five landmarks.</p><p>The 37 fiducial points are used to determine the dimensions of our shape feature. More formally, given two fiducial points, z i and z j , where i = j, i and j = {1, ..., 37}, z i = i1 , z i2 ) T , z i1 and z i2 are the horizontal and vertical components of the fiducial point, respectively, and their relative positions are</p><formula xml:id="formula_9">d ijk = z ik -z jk , k = 1, 2.</formula><p>With these 37 facial landmarks, the feature vector f has 2 • (37 • 36)/2 = 1, 332 dimensions defining the shape of the face. Before passing the features into the classifier, we normalize the feature vector f to be f as follows: fi = 1 2</p><formula xml:id="formula_10">f i -µ i 2σ i + 1 ,<label>(6)</label></formula><p>where i = {1, ..., 1332}, and µ i and σ i are the mean and standard deviation of the ith feature across the training data, respectively. Then, we truncate the out-of-range elements to either 0 or 1. We also employ three widely used low-level appearance LBP <ref type="bibr" target="#b53">[55]</ref>, HOG <ref type="bibr" target="#b7">[9]</ref> and Gabor <ref type="bibr" target="#b38">[40]</ref> representations. For LBP, we select the 59-bin LBP u2 8,2 operator and divide the 100*100 pixel images into 100 regions with a 10*10 grid size, which was empirically found to achieve relatively good performance for expression classification. For HOG, we first divide the images into 10*10 pixel blocks of four 5*5 pixel cells with no overlapping. By setting 10 bins for each histogram, we obtain a 4,000-dimensional feature vector per aligned image. For the Gabor wavelet, we use a bank of 40 Gabor filters at five spatial scales and eight orientations. The downsampled image size is set to 10*10, yielding 4,000dimensional features.</p><p>For the classification task we use linear SVMs with a oneagainst-one strategy to decompose the multi-class classification problem into multiple binary-class classifications by voting. Given a training set {(x i , y i ), i = 1, ..., n}, where x i ∈ R d and y i ∈ {-1, +1}, any test sample x can be classified using: To objectively measure the performance for the followers entries, we split the dataset into a training set and a test set, where the training set is five times larger than the test set, and the expressions in both sets have a nearidentical distribution. Because real-world expressions have an imbalanced distribution, the accuracy metric, which is employed as the evaluation criterion in most datasets, is not used in RAF as it is especially sensitive to bias and is not effective for imbalanced data <ref type="bibr" target="#b19">[21]</ref>. Instead, we use the mean diagonal value of the confusion matrix as the metric. During the parameter optimization process, we also optimized the mean diagonal value of the confusion matrix rather than the accuracy directly provided by the SVMs.</p><formula xml:id="formula_11">min w 1 2 w 2 + C n i=1 max(1 -y i w T x i , 0). (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>Basic emotions. In this experiment, seven basic emotion classes were detected using all 15,339 images from the single-label subset. The best classification accuracy (output by SVM) was 66.82% for shape features, 72.71% for LBP, 74.35% for HOG, and 77.28% for Gabor. The accuracy results decreased to 50.52%, 55.98%, 58.45% and 65.12% when using the mean diagonal value of the confusion matrix as the metric. To assess the reliability of the basic emotion labels, we also assigned a uniform random label, which we call a naive emotion detector, to each sample. The best result for the naive classifier was 16.07% when using the Gabor feature, which is much lower than the former value.</p><p>For comparison, we employed the same methods on CK+ with 5-fold cross-validation and JAFFE with a leave-onesubject-out strategy. The results shown in Table V confirm that real-world expressions are more difficult to recognize and the current common methods that perform well on the existing databases cannot solve the expression recognition problem in challenging real-world conditions.</p><p>To evaluate the effectiveness of different classifiers, we have also trained LDA with nearest neighbor (NN) classification. We found that LDA+NN was inferior to mSVM when training on RAF, an extremely large database. Nevertheless, LDA+NN performed better when training on small-scale datasets (CK+ and JAFFE), even outperforming mSVM in some cases. The concrete results are given in Table <ref type="table" target="#tab_7">V</ref>.</p><p>Compound emotions. As suggested by our crowd-sourcing results, a large proportion of real-world affective faces express compound emotions. The performance evaluation on the single-emotional data set may not be sufficiently comprehensive for some real-world applications. Therefore, we conducted additional baseline experiments on compound emotions.</p><p>For compound emotion classification, we removed the fearfully disgusted emotion due to an insufficient number samples, leaving 11 classes of compound emotions, 3,954 in total. The best classification accuracy (output by SVM) was 45.96% for shape features, 45.51% for LBP, 51.89% for HOG, and 53.54% for Gabor. The accuracy decreased to 28.84%, 28.84%, 33.65% and 35.76% when using the mean diagonal value of the confusion matrix as the metric. Again, to demonstrate the reliability of the compound emotion labels, we computed the baseline for the naive emotion detector, which decreased to 5.79% when using the Gabor features.</p><p>As expected, the overall performance decreased substantially when more expressions were included in the classification. The significantly worse results compared to those of the basic emotion classification indicate that compound emotions are more difficult to detect and that new methods should be invented to solve this problem. In addition to the multi-modality, the lack of training samples for compound expressions from the real world is another major technical challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning Experiment</head><p>Nowadays, deep learning has been applied to large-scale visual recognition tasks and has performed exceedingly well with large amounts of training data. However, fully supervised deep models are easy to be overfitting on facial expression recognition tasks due to the insufficient training samples for model training. Therefore, most deep learning frameworks employed for facial expression recognition <ref type="bibr" target="#b52">[54]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b57">[59]</ref> are based on pre-trained models. These pre-trained models, such as the VGG network <ref type="bibr" target="#b64">[66]</ref> and AlexNet <ref type="bibr" target="#b33">[35]</ref>, were initially designed for face recognition, which are short of discrimination ability of expression characteristic. Therefore, in this paper, we directly trained our deep learning system on the sufficiently large self-collected RAF-DB from scratch without using other databases.</p><p>All our models were trained based on the open source deep learning framework, Caffe <ref type="bibr" target="#b27">[29]</ref>. The already aligned grayscale images were first normalized by dividing all the pixel values by 255. We then considered a network taking a fixed-size input (90*90) cropped from the images for data augmentation.</p><p>To compare different methods fairly, we adopted uniform training methods and used uniform fundamental network architectures. The learning rate was initially set to 0.01 and was decreased by a factor of 10 at 5k and 18k iterations, and we stopped training at 20k iterations. Moreover, we chose stochastic gradient descent (SGD) for optimization and used mini-batch with 64 samples. The momentum coefficient was set to 0.9.</p><p>The model was regularized using weight decay. We set the weight decay coefficient of the convolutional layer and first fully connected layer to 0.0005 and that of the second fully connected layer to 0.0025. MSRA <ref type="bibr" target="#b23">[25]</ref> was used to initialize the weight parameter of the convolutional layer and fully connected layer, while the bias parameter was set to 0 at the beginning of training. All our models were trained on an NVIDIA Tesla K40 GPU, and approximately 3 hours was required to train a model.</p><p>When conducting the experiments, we followed the same dataset partition standards, image processing methods and classification methods as those of the baseline system. Related research <ref type="bibr" target="#b11">[13]</ref> proved that well-trained deep convolutional network can work as a feature extraction tool with generalizability for the classification task. Following up this idea, we first trained each DCNN for the basic emotion recognition task (that is, we used the basic emotion training set as the training samples and the basic emotion test set as the validation samples) and then directly used the already trained DCNN models to extract deep features for both basic and compound expressions. The 2,000-dimensional deep features learned from the raw data were extracted from the penultimate fully connected layer of the DCNNs and were then classified by SVM.</p><p>To investigate the efficiency of different values of λ and k used in the DLP-CNN model, we conducted two experiments on the basic expression recognition task. The accuracies predicted directly by DLP-CNN for the basic expression recognition are shown in Figure <ref type="figure" target="#fig_7">8</ref>. In the first experiment (left), we fixed k to 20 and varied λ 0 to 0.1 to train different models. As the results show, the accuracies are sensitive to the choice of λ and λ = 0 is the case of using the softmax loss, which leads to relatively poor performance of the deeply learned features. In the second experiment (right), we fixed λ = 0.003 and varied k from 10 to 60 to train different models, and we achieved the best performance when was set to 20.</p><p>Concrete classification results of the basic and compound expression in RAF-DB when using DLP-CNN features are shown in Table <ref type="table" target="#tab_9">VII</ref> and Table <ref type="table" target="#tab_9">VIII</ref>. Figure <ref type="figure" target="#fig_9">9</ref>(b) shows the resulting 2-dimensional deep features learned from our DLP-CNN model, where we attach example face images with various intensity in different expression classes. Although the RAF images include various identities, poses, and lighting, the face images are mapped into a two-dimensional space with separable expression clusters and continuous change in expression intensity. This is because while trying to preserve the local structure of the deep features, DLP-CNN implicitly emphasizes the natural clusters in the data and preserves the smooth change within clusters. With its neighborhoodpreserving character, the deep features are able to capture the intrinsic expression manifold structure to a large extent.</p><p>From the results in Table <ref type="table" target="#tab_9">VI</ref>, we have the following observations. First, DCNNs, which achieve reasonable results for large-scale image recognition setting, such as the VGG network and AlexNet, are not efficient for facial expression recognition. Second, all the deep features outperform the unlearned features used in the baseline system by a significant margin, which indicates that the deep learning architecture is more robust and applicable for both basic and compound expression classification. Finally, our new LP loss model    achieved better performance than the based model and the center loss model. Note that the center loss, which efficiently converges unimodal class, can help to enhance the network performance when recognizing basic emotions, but it failed when applied to compound emotions. These results demonstrate the advantages of the LP loss for multi-modal facial expression recognition, including both basic and compound emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comprehensive Comparisons with Center Loss</head><p>As center loss is a special case of DLP-CNN, we further compared DLP-CNN with center loss in terms of time efficiency, convergence performance and parameter sensitivity. For the time efficiency comparison, we evaluated the running time speed using the same settings as those described in    loss with improved accuracy the convergence process. We also checked the sensitivity of parameter λ.  (a) CK+ Method Accuracy CSPL <ref type="bibr" target="#b79">[81]</ref> 88.89% FP+SAE <ref type="bibr" target="#b45">[47]</ref> 91.11% AUDN <ref type="bibr" target="#b39">[41]</ref> 92.05% AURF <ref type="bibr" target="#b39">[41]</ref> 92.22% 3DCNN-DAP <ref type="bibr" target="#b40">[42]</ref> 92.4% Inception <ref type="bibr" target="#b50">[52]</ref> 93.2% Dis-ExpLet <ref type="bibr" target="#b42">[44]</ref> 95.1% ESL <ref type="bibr" target="#b63">[65]</ref> 95.33% DLP-CNN (without fine-tuning) 95.78%</p><p>(b) SFEW 2.0 Method Accuracy DL-GPLVM <ref type="bibr" target="#b17">[19]</ref> 24.70% AUDN <ref type="bibr" target="#b39">[41]</ref> 26.14% STM-ExpLet <ref type="bibr" target="#b41">[43]</ref> 31.73% Inception <ref type="bibr" target="#b50">[52]</ref> 47.7% SFEW third <ref type="bibr" target="#b52">[54]</ref> 48.5% SFEW second <ref type="bibr" target="#b75">[77]</ref> 52.29% SFEW best <ref type="bibr" target="#b31">[33]</ref> 52.5% DLP-CNN (without fine-tuning)</p><p>51.05%</p><p>(c) MMI Method Accuracy 3DCNN-DAP <ref type="bibr" target="#b40">[42]</ref> 63.4% DTAGN <ref type="bibr" target="#b28">[30]</ref> 70.24% CSPL <ref type="bibr" target="#b79">[81]</ref> 73.53% AUDN <ref type="bibr" target="#b39">[41]</ref> 74.76% STM-ExpLet <ref type="bibr" target="#b41">[43]</ref> 75.12% F-Bases <ref type="bibr" target="#b61">[63]</ref> 75.12% Inception <ref type="bibr" target="#b50">[52]</ref> 77.6% Dis-ExpLet <ref type="bibr" target="#b42">[44]</ref> 77.6% DLP-CNN (without fine-tuning)</p><p>78.46%</p><p>extract fixed-length features of CK+, MMI and SFEW 2.0 without fine-tuning. Already aligned sample images from these three datasets are shown in Figure <ref type="figure" target="#fig_0">11</ref>. For the lab-controlled CK+ database, we selected the last frame of each sequence with the peak expression, 309 images in total. During the experiment, we followed the subject-independent experimental principle and performed fivefold cross-validation. For the lab-controlled MMI database, we selected the three peak frames in each sequence for prototypic expression recognition, 528 images in total. Similar to the settings in CK+, we followed the subject-independent experimental principle and performed fivefold cross-validation. For the real-world SFEW 2.0 database, we followed the rule in EmotiW 2015 <ref type="bibr" target="#b10">[12]</ref>. "S-FEW best <ref type="bibr" target="#b31">[33]</ref>", "SFEW second <ref type="bibr" target="#b75">[77]</ref>" and "SFEW third <ref type="bibr" target="#b52">[54]</ref>" indicate the best single model result of the winner, the runnerup and the second runner-up in EmotiW 2015, respectively. Note that in <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b52">[54]</ref>, <ref type="bibr" target="#b75">[77]</ref>, the authors all trained their models with additional data from SFEW.</p><p>From the comparison results in Table <ref type="table" target="#tab_13">IX</ref>, we can see that our network can also achieve comparable or even better performance than other state-of-the-art methods, not only for RAF, but also other databases. This indicates that our proposed network can be used as an efficient and effective feature extraction tool for facial expression databases, without a significant amount of time to execute in traditional DCNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>The main contribution of this paper is the presentation of a new real-world publicly available facial expression database with labeled data from the Internet, based on which we propose a novel optimized algorithm for crowdsourcing and a new locality-preserving loss layer for deep learning. The RAF-DB contains, 1) 29,672 real-world images labeled with different expressions, age range, gender and posture features, 2) a 7-dimensional expression distribution vector for each image, 3) two different subsets: single-label subset, including seven classes of basic emotions; two-tab subset, including twelve classes of compound emotions, 4) the locations of five manually labeled landmark points, and 5) baseline classifier outputs for basic emotions and compound emotions.</p><p>For the baseline results, the performances of the frequently used algorithms on RAF-DB, including both shape and appearance features and the SVM classifier, were compared with that of the laboratory-condition databases. The comparison suggests that these methods are unsuitable for expression detection in uncontrolled environments. To solve the problem of real-world expression detection, we tested various deep learning techniques. The proposed method, deep localitypreserving CNN (DLP-CNN), are able to learn more discriminative features for the expression recognition task and help to enhance the classification performance.</p><p>We hope that the release of this database will encourage more researches to study the effects of the real-world expression distribution or detection, and we believe that the database will be a useful benchmark resource for researchers to compare the validity of their facial expression analysis algorithms in challenging conditions. In the future, we will attempt to expand the quantity and diversity of our database, especially labels such as fear and disgust, which have relatively few images due to the imbalanced emotion distribution in realworld conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of aligned images under real-world conditions (RAF-DB in (a)) and laboratory environments (JAFFE, CK+ and Multi-PIE in (b)).Images provided in RAF-DB are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions (e.g., glasses, facial hair or special gestures that hide some of the feature points), post-processing operations (e.g., various filters and special effects), etc.</figDesc><graphic coords="2,59.36,56.07,113.39,150.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Overview of the construction and annotation of RAF-DB. Initially, images collected from Flickr were fed into an automatic downloader to download the images in batches. Then, a large number of real-world facial images were picked out using emotion-related keywords. To guarantee the reliability of the labeling results, we have invited sufficient well-trained labelers to independently annotate each image about 40 times.</figDesc><graphic coords="5,79.23,55.85,453.38,116.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Database annotations. (a): The web-based framework used for the annotation. (b): A Sadly Fearful sample from RAF-DB with its 7-dimensional expression distribution.</figDesc><graphic coords="5,315.45,235.59,55.80,82.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4. (a) Sample face with five accurate landmarks manually corrected by our experimenters and 37 landmarks automatically annotated using Face++ API; (b) Age and pose distributions of the images in RAF-DB.</figDesc><graphic coords="5,372.93,235.59,55.75,82.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of six-class basic emotions and twelve-class compound emotions from RAF-DB. The detailed data proportion and class distribution of RAF-DB are attached to each expression class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>shows the results of this experiment, where Matrix (a) refers to training on RAF-DB and testing on CK+ and Matrix (b) refers to training on CK+ and testing on RAF-DB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 6 .</head><label>76</label><figDesc>Fig. 7. Comparison of six basic emotions from CK+ and RAF. Facial expressions from top to bottom are Surprise, Happiness, Fear, Anger, Disgust and Sadness. It is evident that the expression AUs in RAF are more diverse than those in CK+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Basic expression recognition performance on RAF-DB for different values of λ (left) and k (right). (a) DLP-CNN models with λ and fixed k = 20. (b) DLP-CNN models with different k and fixed λ = 0.003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The distribution of deeply learned features in (a) DCNN without LP loss and (b) DLP-CNN.The locality-preserving loss layer helps the network to learn features with greater discrimination. Moreover, non-neutral expressions that have obvious intensity variations, such as happiness, sadness, fear, surprise and anger, change the intensity continuously and smoothly, from low to high, from center to periphery. Moreover, images with the disgust label, which is the most confused expression, are assembled in the middle. With the neighborhood-preserving character of DLP-CNN, the deep features are able to capture the intrinsic expression manifold structure to a large extent. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 Fig. 11 .</head><label>1011</label><figDesc>Fig. 11. Already aligned sample images in CK+, SFEW2.0 and MMI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF TEMPORAL FACIAL EXPRESSION DATABASES. VAR = VARIOUS, UNI = UNIFORM, P = POSED AND S = SPONTANEOUS.</figDesc><table><row><cell>Database</cell><cell>Images</cell><cell>Age range</cell><cell>Envir.</cell><cell>Occl.</cell><cell>Illum.</cell><cell>Elicit.</cell><cell>Expression distribution</cell><cell>Annotation methods</cell></row><row><cell>AR [50]</cell><cell>4,000</cell><cell>N/A</cell><cell>Lab</cell><cell>Y</cell><cell>var</cell><cell>P</cell><cell>Smile, anger, scream and neutral</cell><cell>Acted by subjects</cell></row><row><cell>JAFFE [48]</cell><cell>213</cell><cell>N/A</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P</cell><cell>6 basic expressions plus neutral</cell><cell>Semantic ratings over 60 subjects</cell></row><row><cell>MMI [57]</cell><cell>740</cell><cell>19-62</cell><cell>Lab</cell><cell>Y</cell><cell>uni</cell><cell>P</cell><cell>6 basic expressions plus neutral</cell><cell>FACS coded by two coders</cell></row><row><cell>BU-3D [76]</cell><cell>2,500</cell><cell>18-70</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P</cell><cell>6 basic expressions plus neutral and four levels of intensity</cell><cell>Expressions acted by subjects</cell></row><row><cell>BU-4D [75]</cell><cell>606 sequences</cell><cell>18-45</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P</cell><cell>6 basic expressions plus neutral</cell><cell>Expressions acted by subjects</cell></row><row><cell>Yale [3]</cell><cell>165</cell><cell>N/A</cell><cell>Lab</cell><cell>Y</cell><cell>var</cell><cell>P</cell><cell>Happy, sad, sleepy, surprised, wink and normal</cell><cell>Expressions acted by subjects</cell></row><row><cell>GEMEP [2]</cell><cell>7,000</cell><cell>Avg. 29</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P</cell><cell>18 expressions (including rare subtle expressions)</cell><cell>Expressions acted by actors</cell></row><row><cell>CK+ [46]</cell><cell>593 sequences</cell><cell>18-50</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P&amp;S</cell><cell>6 basic expressions plus contempt and neutral</cell><cell>FACS coded by two certified coders</cell></row><row><cell>Radbound [37]</cell><cell>8,040</cell><cell>N/A</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>P</cell><cell>6 basic expressions plus contempt and neutral</cell><cell>Percentage of agreement on emotion categorization</cell></row><row><cell>Multi-PIE [23]</cell><cell>755,370</cell><cell>Avg. 27.9</cell><cell>Lab</cell><cell>Y</cell><cell>var</cell><cell>P</cell><cell>Smile, surprised, squint, disgust, scream and neutral</cell><cell>Expressions acted by subjects</cell></row><row><cell>BP4D [80]</cell><cell>328 sequences</cell><cell>18-29</cell><cell>Lab</cell><cell>N</cell><cell>uni</cell><cell>S</cell><cell>6 basic emotions plus embarrassment and pain</cell><cell>Self-report and rating report</cell></row><row><cell>FER-2013 [22]</cell><cell>35,887</cell><cell>N/A</cell><cell>Web</cell><cell>Y</cell><cell>var</cell><cell>S</cell><cell>6 basic expressions plus neutral</cell><cell>Image search API</cell></row><row><cell>SFEW 2.0 [12]</cell><cell>1,635</cell><cell>1-70</cell><cell>Movie</cell><cell>Y</cell><cell>var</cell><cell>P &amp; S</cell><cell>6 basic expressions plus neutral</cell><cell>Two independent labelers per image</cell></row><row><cell>EmotioNet [4]</cell><cell>1,000,000</cell><cell>N/A</cell><cell>Web</cell><cell>Y</cell><cell>var</cell><cell>P &amp; S</cell><cell>23 basic expressions or compound expressions</cell><cell>10% annotated manually and 90% annotated automatically</cell></row><row><cell>AffectNet [53]</cell><cell>450,000 (labeled)</cell><cell>0-50+</cell><cell>Web</cell><cell>Y</cell><cell>var</cell><cell>P &amp; S</cell><cell>6 basic expressions plus neutral</cell><cell>One human annotator per image</cell></row><row><cell>RAF-DB</cell><cell>29,672</cell><cell>0-70+</cell><cell>Web</cell><cell>Y</cell><cell>var</cell><cell>P &amp; S</cell><cell>6 basic expressions plus neutral and 12 compound expressions</cell><cell>Distribution values from about 40 independent labelers per image</cell></row><row><cell cols="6">3. The number of labelers in these databases is too small,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">which reduces the reliability and validity of the emo-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tion labels. Additionally, emotion labels in most posed</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">expression databases have referred to what expressions</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">were requested rather than what was actually performed.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II KEYWORDS</head><label>II</label><figDesc>USED TO COLLECT THE IMAGES FOR RAF-DB</figDesc><table><row><cell>Joy</cell><cell>Sadness</cell><cell>Anger</cell><cell>Fear</cell><cell cols="2">Surprise Disgust Neutral</cell></row><row><cell>smile</cell><cell>sad</cell><cell>angry</cell><cell>scared</cell><cell cols="2">surprised disgust straight</cell></row><row><cell>laugh</cell><cell>annoyed</cell><cell cols="2">anger frightened</cell><cell>omg</cell><cell>disgusted portrait</cell></row><row><cell>giggle</cell><cell>cry</cell><cell>pissed-off</cell><cell>fear</cell><cell>shocked</cell><cell>potrait</cell></row><row><cell>big smile</cell><cell>crying</cell><cell>rage</cell><cell cols="2">horrified astonished</cell><cell></cell></row><row><cell></cell><cell>depressed</cell><cell></cell><cell>fearful</cell><cell>amazed</cell><cell></cell></row><row><cell></cell><cell>heartbroken</cell><cell></cell><cell>afraid</cell><cell>surprised</cell><cell></cell></row><row><cell></cell><cell>disappointed</cell><cell></cell><cell>terrified</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.73%) Surprised 1619 (10.55%) Fearful 355 (2.31%) Disgusted 877 (5.72%) Happy 5957 (38.84%) Sad 2460 (16.04%) Angry 867 (5.65%)</head><label></label><figDesc></figDesc><table><row><cell>Fearfully surprised 561 (14.15%)</cell><cell>Disgustedly surprised 184 (3</cell><cell>Happily Surprised 697 (17.58%)</cell><cell>Sadly Surprised 86 (2.17%)</cell><cell>Angrily surprised 176 (4.44%)</cell><cell>Sadly Fearful 129 (3.25%)</cell></row><row><cell>Fearfully disgusted 8 (0.20%)</cell><cell>Happily Disgusted 266 (6.71%)</cell><cell>Sadly disgusted 738 (18.61%)</cell><cell>Angrily disgusted 842 (21.24%)</cell><cell>Fearfully angry 150 (3.78%)</cell><cell>Sadly angry 164 (4.14%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The initial value of βj is image j's entropy. The higher the entropy, the more uncertain the image.</figDesc><table><row><cell>Repeat:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E-step:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Qj(yj) :=</cell><cell></cell><cell cols="2">p(yj|tj, αi, βj)</cell></row><row><cell></cell><cell></cell><cell>i</cell><cell></cell><cell></cell></row><row><cell>M-step:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>αi := arg max α i</cell><cell>j</cell><cell>y j</cell><cell>Qj(yj) ln</cell><cell>p(tj, yj|αi, βj) Qj(yj)</cell></row></table><note><p>2 j , ..., t R j )} n j=1 Output: Each annotator's reliability α * i Initialize: ∀j = 1, ..., n, initialize the true label yj using majority voting βj := -R i=1 p(t i j ) ln p(t i j ), αi := 1,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III AU</head><label>III</label><figDesc>OCCURRENCE PROBABILITIES FOR EACH EXPRESSION IN RAF-DB</figDesc><table><row><cell cols="10">(%) AU1 AU2 AU4 AU5 AU6 AU7 AU9 AU10 AU12 AU15 AU 17 AU20 AU25 AU 26 AU27</cell></row><row><cell>Sur</cell><cell>96</cell><cell>96</cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell>45  *</cell></row><row><cell>Fea</cell><cell>82</cell><cell>39</cell><cell>72</cell><cell>81</cell><cell>42</cell><cell></cell><cell>35  *</cell><cell>83</cell><cell>53  *</cell></row><row><cell>Dis</cell><cell></cell><cell></cell><cell>49</cell><cell></cell><cell></cell><cell>28  *  92  *</cell><cell>62</cell><cell>63  *</cell></row><row><cell>Hap</cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell>95</cell><cell></cell><cell>97</cell><cell>24</cell><cell>13</cell></row><row><cell cols="2">Sad 93</cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell>17  *</cell><cell>42</cell><cell>46  *</cell></row><row><cell>Ang</cell><cell></cell><cell></cell><cell cols="2">97 79  *</cell><cell>69</cell><cell>75</cell><cell>56</cell><cell cols="2">72  *  58  *</cell></row><row><cell cols="7">Missing data indicates the probability is less than 10%.</cell><cell></cell><cell></cell></row></table><note><p>An asterisk(*) indicates the AU's probability is quite different from CK+'s (at least 40% disparity).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV THE</head><label>IV</label><figDesc>CONFIGURATION PARAMETERS IN THE FUNDAMENTAL ARCHITECTURE (BASEDCNN).</figDesc><table><row><cell>Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell></row><row><cell>Type</cell><cell cols="15">Conv ReLu MPool Conv ReLu MPool Conv ReLu Conv ReLu MPool Conv ReLu Conv ReLu</cell><cell>FC</cell><cell cols="2">ReLu FC</cell></row><row><cell>Kernel</cell><cell>3</cell><cell>-</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>Output</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>96</cell><cell>-</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>-</cell><cell>256</cell><cell>-</cell><cell>256</cell><cell>-</cell><cell>2,000</cell><cell>-</cell><cell>7</cell></row><row><cell>Stride</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V BASIC</head><label>V</label><figDesc>EXPRESSION CLASS PERFORMANCE COMPARISON OF CK+, JAFFE AND RAF ALONG WITH COMPOUND EXPRESSION PERFORMANCE OF RAF BASED ON THE LBP, HOG AND GABOR DESCRIPTORS, AND SVM AND LDA+KNN CLASSIFICATION. THE METRIC IS THE MEAN DIAGONAL VALUE OF THE CONFUSION MATRIX.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>basic</cell><cell></cell><cell>compound</cell></row><row><cell></cell><cell></cell><cell cols="3">CK+ JAFFE RAF</cell><cell>RAF</cell></row><row><cell></cell><cell>shape</cell><cell>-</cell><cell>-</cell><cell>50.52</cell><cell>28.84</cell></row><row><cell>mSVM</cell><cell cols="4">LBP 88.92 78.81 55.98 HOG 90.50 84.76 58.45</cell><cell>28.84 33.65</cell></row><row><cell></cell><cell cols="4">Gabor 91.98 88.95 65.12</cell><cell>35.76</cell></row><row><cell></cell><cell>shape</cell><cell>-</cell><cell>-</cell><cell>42.87</cell><cell>20.44</cell></row><row><cell>LDA</cell><cell cols="4">LBP 85.84 77.74 50.97 HOG 91.77 80.12 51.36</cell><cell>22.89 24.01</cell></row><row><cell></cell><cell cols="4">Gabor 92.33 83.45 56.93</cell><cell>23.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2868382, IEEE Transactions on Image Processing</figDesc><table><row><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI EXPRESSION</head><label>VI</label><figDesc>RECOGNITION PERFORMANCE OF DIFFERENT DCNNS ON RAF. THE METRIC IS THE MEAN DIAGONAL VALUE OF THE CONFUSION MATRIX. basic compound Anger Disgust Fear Happiness Sadness Surprise Neutral Average † Accuracy Average † Accuracy</figDesc><table><row><cell></cell><cell>VGG [66]</cell><cell>68.52 27.50 35.13</cell><cell>85.32</cell><cell>64.85</cell><cell>66.32</cell><cell>59.88</cell><cell>58.22</cell><cell>70.53</cell><cell>31.63</cell><cell>49.62</cell></row><row><cell></cell><cell>AlexNet [35]</cell><cell>58.64 21.87 39.19</cell><cell>86.16</cell><cell>60.88</cell><cell>62.31</cell><cell>60.15</cell><cell>55.60</cell><cell>68.90</cell><cell>28.22</cell><cell>45.45</cell></row><row><cell>mSVM</cell><cell>baseDCNN</cell><cell>70.99 52.50 50.00</cell><cell>92.91</cell><cell>77.82</cell><cell>79.64</cell><cell>83.09</cell><cell>72.42</cell><cell>82.86</cell><cell>40.17</cell><cell>56.69</cell></row><row><cell></cell><cell cols="2">center loss [71] 68.52 53.13 54.05</cell><cell>93.08</cell><cell>78.45</cell><cell>79.63</cell><cell>83.24</cell><cell>72.87</cell><cell>83.68</cell><cell>39.97</cell><cell>55.81</cell></row><row><cell></cell><cell>DLP-CNN</cell><cell>71.60 52.15 62.16</cell><cell>92.83</cell><cell>80.13</cell><cell>81.16</cell><cell>80.29</cell><cell>74.20</cell><cell>84.13</cell><cell>44.55</cell><cell>57.95</cell></row><row><cell></cell><cell>VGG [66]</cell><cell>66.05 25.00 37.84</cell><cell>73.08</cell><cell>51.46</cell><cell>53.49</cell><cell>47.21</cell><cell>50.59</cell><cell>58.15</cell><cell>16.27</cell><cell>27.55</cell></row><row><cell></cell><cell>AlexNet [35]</cell><cell>43.83 27.50 37.84</cell><cell>75.78</cell><cell>39.33</cell><cell>61.70</cell><cell>48.53</cell><cell>47.79</cell><cell>57.43</cell><cell>15.56</cell><cell>26.41</cell></row><row><cell>LDA</cell><cell>baseDCNN</cell><cell>66.05 47.50 51.35</cell><cell>89.45</cell><cell>74.27</cell><cell>76.90</cell><cell>77.50</cell><cell>69.00</cell><cell>78.75</cell><cell>28.23</cell><cell>38.15</cell></row><row><cell></cell><cell cols="2">center loss [71] 64.81 49.38 54.05</cell><cell>92.41</cell><cell>74.90</cell><cell>76.29</cell><cell>77.21</cell><cell>69.86</cell><cell>78.91</cell><cell>27.33</cell><cell>37.46</cell></row><row><cell></cell><cell>DLP-CNN</cell><cell>77.51 55.41 52.50</cell><cell>90.21</cell><cell>73.64</cell><cell>74.07</cell><cell>73.53</cell><cell>70.98</cell><cell>79.95</cell><cell>32.29</cell><cell>42.93</cell></row><row><cell cols="3">† The mean diagonal value of the confusion matrix.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">The accuracy directly output by SVM.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF THE RESULTS OF DLP-CNN AND OTHER STATE-OF-THE-ART METHODS ON THE CK+, SFEW 2.0 AND MMI DATABASES. TO VALIDATE THE GENERALIZABILITY OF OUR MODEL, THE WELL-TRAINED DLP-CNN WAS EMPLOYED AS A FEATURE EXTRACTION TOOL WITHOUT FINE-TUNING.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.whdeng.cn/RAF/model1.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by the National Natural Science Foundation of China under Grant Nos. 61573068, 61471048, and 61375031, and Beijing Nova Program under Grant No. Z161100004916088. And we would like to acknowledge Dr. Wenjin Yan and Prof. Xiaotian Fu for employing certified FACS coders and consulting on AU annotation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Introducing the geneva multimodal emotion portrayal (gemep) corpus. Blueprint for affective computing: A sourcebook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="271" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR 2016)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR 2016)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Face expression recognition and analysis: the state of the art</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.6722</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">YouTube: Online video and participatory culture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manifold based analysis of facial expression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="605" to="614" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prodger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge 2013</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw 2015</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ramana Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1454" to="E1462" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social media update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lenhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pew Research Center</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universals and cultural differences in the judgments of facial expressions of emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Diacoyanni-Tarlatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lecompte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitcairn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Ricci-Bitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An experimental comparison of performance measures for classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Modroiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature learning for micro-expression recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotion recognition based on a novel triangular facial feature extraction method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Inc</surname></persName>
		</author>
		<title level="m">Face++ research toolkit. www.faceplusplus.com</title>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emonets: Multimodal deep learning approaches for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep convolutional neural networks for robust facial expression recognition</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facial expression recognition in image sequences using geometric deformation features and support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="187" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What is twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Presentation and validation of the radboud faces database</title>
		<author>
			<persName><forename type="first">O</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bijlstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wigboldus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Knippenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1377" to="1388" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition. Image processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning expressionlets via universal manifold model for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5920" to="5932" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Facial expression recognition via deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Computing (SMARTCOMP), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The japanese female facial expression (jaffe) database</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The ar face database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC Technical Report</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected in-the-wild</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">El</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: the state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000-12">Dec 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Facial animation parameters extraction and expression recognition using hidden markov models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pardàs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="675" to="688" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards facial expression recognition in the wild: A new database and deep recognition system</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Poell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Borra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twitter, youtube, and flickr as platforms of alternative journalism: The social media account of the 2010 toronto g20 protests</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="695" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Is there universal recognition of emotion from facial expressions? a review of the cross-cultural studies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Effective geometric features for human emotion recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Niese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elzobi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning bases of activity for facial expression recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robust representation and recognition of facial emotions using extreme sparse learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shojaeilangari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2140" to="2152" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001-02">Feb 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Flickr and public image-sharing: distant closeness and photo exhibition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Van House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;07 extended abstracts on Human factors in computing systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2717" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">511</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Haar features for facs au recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR 2006. 7th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>-F. Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A high-resolution 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference On</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR 2006. 7th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
	<note>Automatic face and gesture recognition</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A deep neural network-driven feature learning method for multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2528" to="2536" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Bp</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">he was a postgraduate exchange student in the School of Information Technologies, University of Sydney, Australia, under the support of the China Scholarship Council. He is currently a professor in School of Information and Telecommunications Engineering, BUPT. His research interests include statistical pattern recognition and computer vision</title>
		<imprint>
			<date type="published" when="2004">2016. 2004 and 2009. Oct. 2007 to Dec. 2008</date>
			<pubPlace>Beijing, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications (BUPT)</orgName>
		</respStmt>
	</monogr>
	<note>Weihong Deng received the B.E. degree in information engineering and the Ph.D. degree in signal and information processing from the Beijing University of Posts and Telecommunications (BUPT). with a particular emphasis in face recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
