<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Authoring Sensor-based Interactions by Demonstration with Direct Manipulation and Pattern Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Björn</forename><surname>Hartmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford University HCI Group Computer Science Dept</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Leith</forename><surname>Abdulla</surname></persName>
							<email>srk|leith.abdulla]@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford University HCI Group Computer Science Dept</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manas</forename><surname>Mittal</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">The MIT Media Laboratory</orgName>
								<address>
									<addrLine>20 Ames St</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford University HCI Group Computer Science Dept</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Authoring Sensor-based Interactions by Demonstration with Direct Manipulation and Pattern Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A093D32FDC8999605A579379AEB16A36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sensors, physical computing, design tools, PBD H.5.2. [Information Interfaces]: User Interfaces -input devices and strategies</term>
					<term>interaction styles</term>
					<term>prototyping</term>
					<term>user-centered design. D.2.2 [Software Engineering]: Design Tools and Techniques -User interfaces</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensors are becoming increasingly important in interaction design. Authoring a sensor-based interaction comprises three steps: choosing and connecting the appropriate hardware, creating application logic, and specifying the relationship between sensor values and application logic. Recent research has successfully addressed the first two issues. However, linking sensor input data to application logic remains an exercise in patience and trial-and-error testing for most designers. This paper introduces techniques for authoring sensor-based interactions by demonstration. A combination of direct manipulation and pattern recognition techniques enables designers to control how demonstrated examples are generalized to interaction rules. This approach emphasizes design exploration by enabling very rapid iterative demonstrate-edit-review cycles. This paper describes the manifestation of these techniques in a design tool, Exemplar, and presents evaluations through a first-use lab study and a theoretical analysis using the Cognitive Dimensions of Notation framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Sensing technologies are becoming pervasive, and sensor hardware is increasingly diverse and economical. Recent work in physical computing toolkits has lowered the threshold for connecting sensors and actuators to PCs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>; to design self-contained physical interfaces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>; and to prototype and evaluate the application logic of systems that make use of sensors and actuators <ref type="bibr" target="#b18">[19]</ref>. Accessing sensor data from software has come within reach of designers and end users.</p><p>However, our experience of first learning and later teaching physical interaction design, and our experience of deploying d.tools <ref type="bibr" target="#b18">[19]</ref> in the classroom, has shown that specifying the relationship between sensor input and application logic remains problematic for designers for three reasons. First, most current tools, such as Arduino <ref type="bibr" target="#b0">[1]</ref>, require using textual programming to author sensor-based behaviors. Representations are most effective when the constraints embedded in the problem are visually manifest in the representation <ref type="bibr" target="#b30">[31]</ref>. Thus, numbers alone are a poor choice for making sense of continuous signals as the relationship between performed action and reported values is not visually apparent. Second, existing visual tools (e.g., LabView <ref type="bibr" target="#b1">[2]</ref>) were created with the intent of helping engineers and scientists to perform signal analysis; as such, they do not directly support interaction design. This leaves users with a sizeable gulf of execution, the gap between their goals and the actions needed to attain those goals with the system <ref type="bibr" target="#b20">[21]</ref>. Third, the significant time and cognitive commitments implied by a lack of tools inhibit rapid iterative exploration. Creating interactive systems is not simply the activity of translating a pre-existing specification into code; there is significant value in the epistemic experience of exploring alternatives <ref type="bibr" target="#b21">[22]</ref>. One of the contributions of direct manipulation and WYSIWYG design tools for graphical interfaces is that they enable this "thinking through doing" -this paper's research aims to provide a similarly beneficial experience for sensor-based interaction. This paper contributes techniques for enabling a wider audience of designers and application programmers to turn raw sensor data into useful events for interaction design through programming by demonstration. It introduces a rapid prototyping tool, Exemplar (see Figure <ref type="figure" target="#fig_0">1</ref>), which embodies these ideas. The goal of Exemplar is to enable users to focus on design thinking (how the interaction should work) rather than algorithm tinkering (how the sensor signal processing works). Exemplar frames the design of sensor-based interactions as the activity of performing the actions that the sensor should recognize -we suggest this approach yields a considerably smaller gulf of execution than existing systems. With Exemplar, a designer first demonstrates a sensor-based interaction to the system (e.g., she shakes an accelerometer). The system graphically displays the resulting sensor signals. She then edits that visual representation by marking it up, and reviews the result by performing the action again. Through iteration based on real-time feedback, the designer can refine the recognized action and, when satisfied, use the sensing pattern in prototyping or programming applications. The primary contributions of this work are:</p><p>• A method of programming by demonstration for sensorbased interactions that emphasizes designer control of the generalization criteria for collected examples. • Integration of direct manipulation and pattern recognition through a common visual editing metaphor. • Support for rapid exploration of interaction techniques through the application of the design-test-analyze paradigm <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref> on a much shorter timescale as the core operation of a design tool. The rest of this paper is organized as follows: we first introduce programming by demonstration, and then describe relevant characteristics of sensors and sensor-based interactions to position our work. We provide an overview of the Exemplar research system and describe its interaction techniques and implementation. We then report the results of two evaluations we have employed to measure Exemplar's utility and usability, and discuss related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROGRAMMING BY DEMONSTRATION</head><p>Programming by demonstration (PBD) is the process of inferring general program logic from observation of examples of that logic. Because PBD builds program logic "under the hood" and does not require textual programming, it has been employed for end-user development <ref type="bibr" target="#b29">[30]</ref>. In many PBD systems, the examples or demonstrations are provided as mouse and keyboard actions in a direct manipulation interface (cf. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>). These systems often take the form of visual programming environments. PBD has been employed in educational software to introduce children to programming concepts; for the specification of macros; and in robotics to specify navigation <ref type="bibr" target="#b4">[5]</ref>, or grasp motions for assembly arms <ref type="bibr" target="#b28">[29]</ref>. Our research builds upon the idea of using actions performed in physical space as the example input.</p><p>The crucial step in the success or failure of PBD systems is the generalization from examples to rules that can be applied to new input <ref type="bibr" target="#b25">[26]</ref>. This inference step often leverages machine learning and pattern recognition techniques. But what if the system's generalization does not match the author's intent? Our research addresses the central importance of guided generalization by proposing a directmanipulation environment for editing the generalization rules applied to examples performed in physical space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENSOR-BASED INTERACTIONS</head><p>This section introduces an analysis of the space for sensorbased interactions from the designer's point of view. Prior work has successfully used design spaces as tools for thinking about task performance <ref type="bibr" target="#b11">[12]</ref> and communicative aspects [9] of sensing systems. Here we apply this approach to describe the interaction designer's experience of working with sensors and authoring for them. This design space foregrounds three central concerns: the nature of the input signals, the types of transformations applied to continuous input, and techniques for specifying the correspondence between continuous signals and discrete application events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary, Categorical, and Continuous Signals</head><p>As prior work points out <ref type="bibr" target="#b32">[33]</ref>, one principal distinction is whether sensing technologies report continuous or discrete data. Most technologies that directly sample physical phenomena (e.g., temperature, pressure, acceleration, magnetic field) output continuous data. For discrete sensors -because of different uses in interaction design -it is helpful to distinguish two sub-types: binary inputs such as buttons and switches are often used as general triggers; categorical (multi-valued) inputs such as RFID are principally used for identification. A similar division can be made for the outputs or actuators employed. Exemplar focuses on continuous input in single and multiple dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Working with Continuous Signals</head><p>Sensor input is nearly always transformed for use in an interactive application. Continuous transformation operations fall into three categories: conditioning, calibration, and mapping. Signal conditioning is about "tuning the dials" so the signal provides a good representation of the phenomenon of interest, thus maximizing the signal-tonoise ratio. Common steps in conditioning are de-noising a signal and adjusting its range through scaling and offsetting. Calibration relates input units to real-world units. In scientific applications, the exact value in real-world units of a measured phenomenon is of importance. However, for many sensor-based interfaces, the units of measurement are not of intrinsic value. Mapping refers to a transformation from one parameter range into another. Specifying how sensor values are mapped to application parameters is a creative process, one in which design intent is expressed. Exemplar offers support for both conditioning sensor signals and for mapping their values into binary, discrete, or   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Discrete Events</head><p>A tool to derive discrete actions from sensor input requires both a detection algorithm and appropriate interaction techniques for controlling that algorithm. The most computationally straightforward approach is thresholdingcomparing a single data point to fixed limits. However, without additional signal manipulations, e.g., smoothing and derivatives, thresholds are susceptible to noise and cannot characterize events that depend on change over time. Tasks such as gesture recognition require more complex pattern matching techniques. Exemplar offers both thresholding with filtering as well as pattern matching.</p><p>The user interface technique employed to control how the computation happens is equally important. Threshold limits can be effectively visualized and manipulated as horizontal lines overlaid on a signal graph. In our experience, the parameters of more complex algorithms are less well understood. Exemplar thus frames threshold manipulation as the principle technique for authoring discrete events. Exemplar contributes an interaction technique to cast parameterization of the pattern matching algorithm as a threshold operation on matching error. Through this technique, Exemplar creates a consistent user experience of authoring with both thresholding and pattern matching.</p><p>In the next section, we describe how the design concerns outlined here -support for continuous input, techniques for transforming that input, and techniques for discretizing input to application events -are manifest in Exemplar's UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DESIGNING WITH EXEMPLAR</head><p>Designers begin by connecting sensors to a compatible hardware interface, which in turn is connected to a PC running Exemplar (see Figure <ref type="figure" target="#fig_1">2</ref>). As sensors are connected, their data streams are shown inside Exemplar. The Exemplar UI is organized according to a horizontal data-flow metaphor: hardware sensor data arrives on the left-hand side of the screen, undergoes user-specified transformations in the middle, and arrives on the right-hand side as discrete or continuous events (see Figure <ref type="figure" target="#fig_2">3</ref>). The success of dataflow authoring languages such as Max/MSP attests to the accessibility of this paradigm to non-programmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peripheral Awareness</head><p>Live data from all connected sensors is shown in a small multiples configuration. Small multiples are side-by-side "graphical depictions of variable information that share context, but not content" <ref type="bibr" target="#b37">[38]</ref>. The small multiples configuration gives a one-glance overview of the current state of all sensors and enables visual comparison (see Figure <ref type="figure" target="#fig_1">2A</ref>). Whenever a signal is "interesting," its preview window briefly highlights in red to attract attention, then fades back to white. In the current implementation, this occurs when the derivative of a sensor signal exceeds a preset value. Together, small multiple visualization and highlighting afford peripheral awareness of sensor data and a visual means of associating sensors with their signals. This tight integration between physical state and software representation encourages discovery and narrows the gulf of evaluation, the difficulty of determining a system's state from its observable output <ref type="bibr" target="#b20">[21]</ref>. For example, to find out which output of a multi-axis accelerometer responds to a specific tilt action, a designer can connect all axes, tilt the accelerometer in the desired plane, and look for the highlighted thumbnail to identify the correct channel for her design.</p><p>Constant view of all signals is also helpful in identifying defective cables and connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drilling Down and Filtering</head><p>Designers can bring a sensor's data into focus in the large central canvas by selecting its preview thumbnail (see Figure <ref type="figure" target="#fig_1">2C</ref>). The thumbnails and the central canvas form an overview plus detail visualization <ref type="bibr" target="#b35">[36]</ref>. Designers can bring multiple sensor data streams into focus at once by controlclicking on thumbnails. Between the thumbnail view and the central canvas, Exemplar interposes a filter stack (see Figure <ref type="figure" target="#fig_1">2B</ref>). Filters transform sensor data interactively: the visualization always reflects the current set of filters and their parameter values. Exemplar maintains an independent filter stack for each input sensor. When multiple filters are active, they are applied in sequence from top to bottom, and these filters can be reordered.</p><p>Exemplar's filter stack library comprises four operations for conditioning and mapping -offset: adds a constant factor; y-axis scaling: multiplies the sensor value by a scalar value, including signal inversion; smoothing: convolves the signal with one-dimensional Gaussian kernel to suppress high frequency noise; and rate of change: takes the first derivative. These four operations were chosen as the most important for gross signal conditioning and mapping; a later section addresses filter set extensibility.</p><p>Interaction with the filtered signal in the central canvas is analogous to a waveform editor of audio recording software. By default, the canvas shows the latest available data streaming in, with the newest value on the right side. Designers can pause this streaming visualization, scroll through the data, and change how many samples are shown per screen. When fully zoomed out, all the data collected since the beginning of the session is shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Demonstration and Mark-up</head><p>To begin authoring, the designer performs the action she wants the system to recognize. As an example, to create an interface that activates a light upon firm pressure, the designer may connect a force sensitive resistor (FSR) and press on it with varying degrees of force. In Exemplar, she then marks the resulting signal curve with her mouse. The marked region is highlighted graphically and analyzed as a training example. The designer can manipulate this example region by moving it to a different location through mouse dragging, or by resizing the left and right boundaries. Multiple examples can be provided by adding more regions. Examples can be removed by right-clicking on a region.</p><p>In addition to post-demonstration markup, Exemplar also supports real-time annotation through a foot switch, chosen because it leaves the hands free for holding sensors. Using the switch, designers can mark regions and simultaneously work with sensors. Pressing the foot switch starts an example region. The region grows while the switch remains depressed, and concludes when the switch is released. While this technique requires some amount of hand-foot coordination, it enables truly interactive demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition and Generalization</head><p>Recognition works as follows: interactively, as new data arrives for a given sensor, Exemplar analyzes if the data matches the set of given examples. When the system finds a match with a portion of the input signal, that portion is highlighted in the central canvas in a fainter shade of the color than the one used to draw examples. This region grows for the duration of the match, terminating when the signal diverges from the examples.</p><p>Exemplar provides two types of matching calculationsthresholds and patterns -selectable as modes for each event (see Figure <ref type="figure" target="#fig_1">2D</ref>). With thresholding, the minimum and maximum values of the example regions are calculated. The calculation is applied to filtered signals, e.g., it is possible to look for maxima in the smoothed derivative of the input. Incoming data matches if its filtered value falls in between the extrema. Pattern matching compares incoming data against the entire example sequence and calculates a distance metric, the extent to which incoming data resembles the example.</p><p>Matching parameters can be graphically adjusted through direct manipulation. For threshold events, min and max values are shown as horizontal lines in the central canvas. These lines can be dragged with the mouse to change the threshold values (see Figure <ref type="figure" target="#fig_1">2G</ref>). When parameters are adjusted interactively, matched regions are automatically recalculated and redrawn. Thus, Exemplar always shows how things would have been classified. This affords rapid exploration of how changes affect the overall performance of the matching algorithm.</p><p>Sensor noise can lead to undesirable oscillation between matching and non-matching states. Exemplar provides three mechanisms for addressing this problem. First, a smoothing filter can be applied to the signal. Second, the designer can apply hysteresis, or double thresholding, where a boundary is represented by two values which must both be traversed for a state change. Dragging the hysteresis field of a graphical threshold (indicated by "H" in Figure <ref type="figure" target="#fig_1">2G</ref>) splits a threshold into two lines. Designers specify the difference between boundary values through the drag distance. Third, designers can drag a timeout bar from the right edge of the central canvas to indicate the minimum duration for a matching or non-matching state to be stable before an event is fired.</p><p>For pattern matching, Exemplar introduces a direct manipulation technique that offers a visual thresholding solution to the problem of parameterizing the matching algorithm. Exemplar overlays a graph that shows distance between the incoming data and the previously given example on the central canvas's vertical axis. The lower the distance, the better the match. Designers can then adjust a threshold line indicating the maximum distance for a positive match. An event is fired when the distance falls below the threshold line. With this technique, the designer's authoring experience is consistent whether applying thresholds or pattern matching: dragging horizontal threshold bars tunes the specificity of the matching criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Exemplar supports the transformation from sensor input into application events. Exemplar generates two kinds of output events: continuous data streams that correspond to filtered input signals, and discrete events that fire whenever a thresholding or pattern matching region is found. With these events in hand, the designer then needs to author some behavior, e.g., she needs to specify the application's response to the force sensor push. To integrate Exemplar with other tools, events and data streams can be converted into operating system input events such as key clicks or mouse movements. Injecting OS events affords rapid control over third party applications (cf. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>). However, injection is relatively brittle because it does not express association semantics (e.g., that the "P" key pauses playback in a video application). For tighter integration with application logic, Exemplar can also be linked to the d.tools authoring environment <ref type="bibr" target="#b18">[19]</ref> for rapid off-the-desktop prototypes. Exemplar events are then used to trigger transitions in d.tools' interaction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many Sensors, Many Events</head><p>Exemplar scales to more complex applications by providing mechanisms to author multiple events for a single sensor; to author individual events that depend on multiple sensors; and to run many independent events simultaneously.</p><p>To the right of the central canvas, Exemplar shows a list of event definitions for the currently active sensors (see Figure <ref type="figure" target="#fig_1">2E</ref>). Designers can add new events and remove unwanted ones in this view. Each event is given a unique color. A single event from this list is active for editing at a time to keep mouse actions in the central canvas unambiguous: regions drawn by the designer in the canvas always apply to that active event.</p><p>Exemplar enables combining sensor data in Boolean AND fashion (e.g., "scroll the map only if the accelerometer is tilted to the left and the center button is pressed"). When designers highlight multiple sensor thumbnails, their signals are shown stacked in the central canvas. Examples are now analyzed across all shown sensor signals and events are only generated when all involved sensors match their examples. Boolean OR between events is supported implicitly by creating multiple events. Together, AND/OR combinations enable flexibility in defining events. They reducebut do not replace -the need to author interaction logic separately.</p><p>The authored events for all sensors are always evaluatedand corresponding output is fired -regardless of which sensor is in focus in the central canvas. This allows designers to test multiple interactions simultaneously. To keep this additional state visible, a tree widget shows authored events for all sensors along with their example regions in the lower right corner of the UI (see Figure <ref type="figure" target="#fig_1">2F</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Demonstrate-Edit-Review</head><p>The demonstrate-edit-review cycle embodied in Exemplar is an application of the design-test-think paradigm for tools <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. This paradigm suggests that integrating support for evaluation and analysis into a design tool enables designers to more rapidly gain insight about their project. Exemplar is the first system to apply design-test-think to the domain of sensor data analysis. More importantly, through making demonstrate, edit, and review actions the fundamental authoring operations in the user interface, Exemplar radically shortens iteration times by an order of magnitude (from hours to minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARCHITECTURE AND IMPLEMENTATION</head><p>Exemplar was written using the Java 5.0 SDK as a plug-in for the Eclipse IDE. Integration with Eclipse offers two important benefits: the ability to combine Exemplar with the d.tools prototyping tool to add visual authoring of interaction logic; and extensibility for experts through an API that can be edited using Eclipse's Java tool chain. The graphical interface was implemented with the Eclipse Foundation's SWT toolkit <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal Input, Output and Display</head><p>Consistent with the d.tools architecture, our hardware communicates with Exemplar using OpenSoundControl (OSC). This enables Exemplar to connect to any sensor hardware that supports OSC. In addition to the d.tools I/O board <ref type="bibr" target="#b18">[19]</ref>, the Exemplar implementation also supports Wiring <ref type="bibr" target="#b7">[8]</ref> and Arduino <ref type="bibr" target="#b0">[1]</ref> boards with OSC firmware. OSC messages are also used to send events to other applications, e.g., d.tools, Max/MSP, or Flash (with the aid of a relay program). Translation of Exemplar events into system key presses and mouse movements or mouse clicks is realized through the Java Robots package.</p><p>Exemplar visualizes up to eight inputs. This number is not an architectural limit; it was chosen based on availability of analog-to-digital ports on common hardware interfaces. Sensors are sampled at 50 Hz with 10-bit resolution and screen graphics are updated at 15-20 Hz. These sampling and display rates have been sufficient for human motion sensing and interactive operation. However, we note that other forms of input, e.g., microphones, require higher sampling rates (8-40 kHz). Support for such devices is not yet included in the current library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Recognition</head><p>We implemented a Dynamic Time Warping (DTW) algorithm to match demonstrated complex patterns with incoming sensor data. DTW was first used as a spoken-word recognition algorithm <ref type="bibr" target="#b33">[34]</ref>, and has recently been used in HCI for gesture recognition from sensor data <ref type="bibr" target="#b27">[28]</ref>. DTW compares two time-series data sets and computes a metric of the distortion distance required to fit one to the other. It is this distance metric that we visualize and threshold against in pattern mode. DTW was chosen because, contrary to other machine learning techniques, only one training example is required. The DTW technique used in this work is sufficiently effective to enable the interaction techniques we have introduced. However, we point out that -like related work utilizing machine learning in UI tools <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> -we do not claim optimality of this algorithm in particular.</p><p>More broadly, this research -and that of related projectssuggests that significant user experience gains can be realized by integrating machine learning and pattern recognition with direct manipulation. From a developer's perspective, taking steps in this direction may be less daunting than it first appears. For example, Exemplar's DTW technique comprises only a small fraction of code size and development time. We have found that the primary challenge for HCI researchers is the design of appropriate interfaces for working with these techniques, so that users have sufficient control over their behavior without being overwhelmed by a large number of machine-centric "knobs." Extensibility While Exemplar's built-in filters are sufficient for many applications, developers also have the option of writing their own filters. Exemplar leverages Eclipse's autocompilation feature for real-time integration of usercontributed code. This architecture allows engineers on design teams to add to the set of available filters and for users to download filters off the web. Exemplar's filter architecture was inspired by audio processing architectures such as Steinberg's VST <ref type="bibr" target="#b3">[4]</ref>, which defines a mechanism for how plug-ins receive data from a host, process that stream, and send results back. VST has dramatically expanded the utility of audio-editing programs by enabling third parties to extend the audio-processing library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>Our evaluation employed a two-pronged approach. First, we applied the Cognitive Dimensions of Notation framework to Exemplar to evaluate its design tradeoffs as a visual authoring environment. Second, we conducted a first-use study in our lab to determine threshold and utility for novices, as well as to assess usability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cognitive Dimensions Usability Inspection</head><p>The Cognitive Dimension of Notation (CDN) framework <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> offers a high-level inspection method to evaluate the usability of information artifacts. In CDN, artifacts are analyzed as a combination of a notation they offer and an environment that allows certain manipulations of the notation. CDN is particularly suitable for analysis of visual programming languages. We conducted a CDN evaluation of Exemplar following Blackwell and Green's Cognitive Dimensions Questionnaire <ref type="bibr" target="#b10">[11]</ref>. This evaluation offers an analysis of Exemplar according to categories independently identified as relevant, and facilitates comparison with future research systems. We begin with an estimate of how time is spent within the authoring environment, and then proceed to evaluate the software against the framework's cognitive dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parts of the System</head><p>Exemplar's main notation is a visual representation of sensor data with user-generated mark-up. Lab use of Exemplar led us estimate that time is spend as follows: 30% Searching for information within the notation (browsing the signal, visually analyzing the signal) 10% Translating amounts of information into the system (demonstration) 20% Adding bits of information to an existing description (creating and editing mark up, filters) 10% Reorganizing and restructuring descriptions (changing analysis types, re-defining events) 30% Playing around with new ideas in notation without being sure what will result (exploration) This overview highlights the importance of search, and the function of Exemplar as an exploratory tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensions of the Main Notation</head><p>Given space constraints, we only present a subset of the 14 CDN dimensions that we deemed most relevant here.</p><p>Visibility and Juxtaposability (ability to view components easily): All current sensor inputs are always visible simultaneously as thumbnail views, enabling visual comparison of input data. Viewing multiple signals in close-up is also possible; however, since such a view is exclusively associated with "AND" events combining the shown signals, it is not possible to view independent events at the same time.</p><p>Viscosity (ease or difficulty of editing previous work): Event definitions and filter settings in Exemplar are straightforward to edit through direct manipulation. The hardest change to make is improving the pattern recognition if it does not work as expected. Thresholding matching error only allows users to adjust a post-match metric as the internals (the "how" of the algorithm) are hidden.</p><p>Diffuseness (succinctness of language): Exemplar's notation is brief, in that users only highlight relevant parts of a signal and define a small number of filter parameters through graphical interaction. However, the notation is not Turing-complete: the set of expressible statements is limited. The length of event descriptions is dependent on the Boolean complexity of the event expressed (how many ORs/ANDs of signal operations there are).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard Mental Operations:</head><p>The greatest mental effort is required to keep track of events that are defined and active, but not visible in the central canvas. To mitigate against this problem, we introduced the overview list of all defined interactions (see Figure <ref type="figure" target="#fig_1">2F</ref>) which minimizes cost to switch between event views. One important design goal was to make results of operations immediately visible.</p><p>Error-proneness (syntax provokes slips): One slip occurred repeatedly in our use of Exemplar: resizing example regions by dragging their boundaries. This was problematic because no visual feedback was given on what the valid screen area was to initiate resizing. Lack of feedback resulted in dupli- cate regions being drawn, with an accompanying undesired recalculation of thresholds or patterns. Improved input controls on regions can alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closeness of Mapping:</head><p>The sensor signals are the primitives that users operate on. This direct presentation of the signal facilitates consistency between the user's mental model and the system's internal representation.</p><p>Role-expressiveness (purpose of a component is readily inferred): Role-expressiveness problems often arise when compatibility with legacy systems is required. Since Exemplar was designed from scratch for the express purpose of viewing, manipulating and marking up signals, this it not a concern. While the result of applying operations is always visible, the implementation "meaning" of filters and pattern recognition is hidden.</p><p>Secondary Notations: Currently, Exemplar permits users to label events, but not filter settings or signal regions. As comments allow users to capture information outside the "syntax" of Exemplar, this is an area for future work.</p><p>Progressive Evaluation: Real-time visual feedback enables fluid evaluation of the state of an interaction design at any point. Furthermore, Exemplar sessions can be saved and retrieved for subsequent sessions through disk serialization.</p><p>In summary, Exemplar performs well with respect to visibility, closeness of mapping, and progressive evaluation. Many of the identified challenges stem from the difficulties of displaying multiple sensor visualizations simultaneously. These can be addressed through interface improvementsthey are not intrinsic shortcomings of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First-use Study</head><p>We conducted a controlled study of Exemplar in our laboratory to assess the ease of use and felicity of our tool for design prototyping. The study group comprised twelve participants. Ten were graduate students or alumni of our university; two were undergraduates. While all participants had some prior HCI design experience, they came from a variety of educational backgrounds: four from Computer Science/HCI, four from other Engineering fields, two from Education, and two from the Humanities. Participants' ages ranged from 19 to 31; five were male, seven female. Two of the twelve participants served as pilot testers. Eight participants had had some prior exposure to sensor programming, but none self-reported to be experts (see Figure <ref type="figure" target="#fig_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study Protocol</head><p>Participants were seated at a dual-screen workstation with an Exemplar hardware interface. The Exemplar software was shown on one screen; a help document on sensors was shown on the other. Participants were asked to author interactions that controlled graphics on a large projection dis-play (see Figure <ref type="figure" target="#fig_5">5</ref>). We chose this large display to encourage participants to think beyond the desktop in their designs. We chose graphical instead of physical output since our study focused on authoring responses to sensor input, not on actuation.</p><p>Individual study sessions lasted two hours. Sessions started with a demonstration of Exemplar. We also introduced the set of available sensors, which comprised buttons, switches, capacitive touch sensors, light sensors, infrared distance rangers, resistive position sensors, forcesensitive resistors (FSRs), load cells, bend sensors, 2D joysticks, and 3D accelerometers. Participants were given three design tasks. For each, we provided a mapping of triggers available in Exemplar to behaviors in the instructions (e.g., sending an event called "hello" activated the display of the hello graphic in the first task).</p><p>The first task was a "Hello World" application. Subjects were asked to display a hello graphic when a FSR was pressed (through thresholding) while independently showing a world graphic when a second FSR was pressed three times in a row (through pattern recognition).</p><p>The second task required participants to augment a bicycle helmet with automatic blinkers such that tilting the helmet left or right caused the associated blinkers to signal. Participants had to attach sensors to and test with a real helmet. Blinker output was simulated with graphics on a "mirror" on the projection display. This task was inspired by Selker et al.'s Smart Helmet <ref type="bibr" target="#b34">[35]</ref>.</p><p>The last task was an open-ended design exercise to author new motion-based controls for at least one of two computer games. The first was a spaceship navigation game in which the player has to keep a ship aloft, collect points and safely land using three discrete events to fire thrusters (up, left, and right). The second game was a shooting gallery with continuous X/Y position control used to aim, and a discrete trigger to shoot moving targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study Results</head><p>In our post-test survey, participants ranked Exemplar highly for decreasing the time required to build prototypes compared to their prior practice (mean = 4.8, median = 5 on a 5-point Likert scale, σ = 0.42); for facilitating rapid modification (mean = 4.7, median = 5, σ = 0.48); for enabling them to experiment more (mean = 4.7, median = 5, σ = 0.48); and for helping them understand user experience (mean = 4.3, median = 4; σ = 0.48). Responses were less conclusive on how use of Exemplar would affect the number of prototypes built, and whether it helped focus or distracted from design details (σ &gt; 1.0 in each case). Detailed results are presented in Figure <ref type="figure" target="#fig_7">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Successes</head><p>All participants successfully completed the two first two tasks and built at least one game controller. The game controller designs spanned a wide range of solutions (see Figure <ref type="figure" target="#fig_8">7</ref>). Once familiar with the basic authoring techniques, many participants spent the majority of their time sketching and brainstorming design solutions, and testing and refining their design. In aggregate, implementation composed less than a third of their design time. This rapid iteration enabled participants to explore up to four different control schemes for a game. We see this as a success of enabling epistemic activity: participants spent their time on design thinking rather than implementation tinkering.</p><p>Exemplar was used for exploration: given an unfamiliar sensor, participants were able to figure out how to employ it for their purposes. For example, real-time feedback enabled participants to find out which axes of a multi-axis accelerometer were pertinent for their design. Participants also tried multiple sensors for a given interaction idea to explore the fit between design intent and available technologies.</p><p>Interestingly, performance of beginners and experts under Exemplar was comparable in terms of task completion time and breadth of ideation. Two possible explanations for this situation are that either Exemplar was successful in lowering the threshold to entry for the types of scenarios tested, or that it encumbered experts from expressing their knowledge. The absence of complaints by experts in the post-test surveys provides some support for the first hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shortcomings discovered</head><p>Participants identified two key areas for improvement. One recurring theme in the feedback was the need for visualization of hidden state. At the time of the study, participants could only see events that corresponded to the sensor in focus. Although other events were still active, there was no comprehensive way of listing them. Also, the highlighted regions corresponding to training examples were hard to retrieve after time had passed because the example regions were pushed too far off-screen into the history. To address these difficulties, Exemplar now displays a full list of active events, along with the corresponding example regions. Selecting those regions jumps to the time of their definition in the central canvas.</p><p>Expert users expressed a need for finer control over hysteresis parameters for thresholding, and a visualization of time and value units on the axes of the signal display. In response, we added direct manipulation of hysteresis and timeout parameters to threshold events. The importance of displaying numerical data in the visualization to aid understanding of signal properties deserves further study. Participants also requested ways to provide negative examples, techniques for displaying multiple large sensor visualizations simultaneously, and finer control over the timing for pattern matching (in terms of both latency and duration). We leave these to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>The research embodied in Exemplar was directly motivated by our experience with d.tools <ref type="bibr" target="#b18">[19]</ref>, which   • Graphical feedback of matching criteria for sensor histories. • A raised ceiling by working with multiple sensors, multiple events, and extensible filters. • Evaluation through a lab study and CDN analysis. Exemplar also relates to three larger areas of work: research into programming by demonstration for ubiquitous computing, tools for musical controller design, and signal processing and analysis software. We discuss each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ubicomp PBD</head><p>The closest predecessor to Exemplar in approach and scope is a CAPella <ref type="bibr" target="#b13">[14]</ref>. This system focused on authoring binary context recognizers by demonstration (e.g., is there a meeting going on in the conference room?), through combining data streams from discrete sensors, a vision algorithm, and microphone input. Exemplar shares inspiration with a CAPella, but it offers important architectural contributions beyond this work. First, a CAPella was not a real-time interactive authoring tool: the authors of a CAPella reported the targeted iteration cycle to be on the order of days, not minutes as with Exemplar. Also, a CAPella did not provide strong support for continuous data. More importantly, a CAPella did not offer designers control over how the generalization step of the PBD algorithm was performed beyond marking regions. We believe that this limitation was partially responsible for the low recognition rates reported (between 50% and 78.6% for binary decisions).</p><p>We also drew inspiration for Exemplar from Fails and Olsen's interaction technique for end-user training of vision recognizers, Image Processing with Crayons <ref type="bibr" target="#b14">[15]</ref>. It enables users to draw on training images, selecting image areas (e.g., hands or note cards) that they would like the vision system to recognize. Crayons complements our work well, offering a compelling solution to learning from independent images, where as Exemplar introduces an interface for learning from time-series data.</p><p>Monet <ref type="bibr" target="#b24">[25]</ref> is a sketch-based prototyping system for animating hand-drawn widgets. Monet learns geometric transformations applied to widgets through mouse movement. Monet is a GUI-centric application of PBD that supports both continuous and discrete outputs. It uses a different mathematical approach (continuous function approximation using radial basis functions centered on screen pixels).</p><p>Papier-Mâché <ref type="bibr" target="#b22">[23]</ref>, while not a PBD system, also makes physical input processing more accessible. It supports how designers specify input of interest -its main insight was to create software event abstractions that were common across a set of input technologies. This enables designers to rapidly exchange input technologies. Papier-Mâché focused on identity-based input and discrete events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tools for musical controller design</head><p>Design tools for electronic musical instruments also address the issue of mapping continuous sensor input. There are two important differences between using sensors as inputs for user interfaces and using sensors as controls for real-time music synthesis: musicians are often concerned with continuous-to-continuous mappings, and the frequency of the output signal (the music) is much greater than that of the input signal (the performer's actions). Exemplar focuses on sparser output events that are frequently of discrete nature.</p><p>Steiner's [hid] toolkit <ref type="bibr" target="#b36">[37]</ref>, is a set of objects, called "externals," that support USB Human Interface Device input in Pd <ref type="bibr" target="#b31">[32]</ref>, the visual multimedia programming environment. The toolkit does not focus on the interaction design for exploring and specifying signal transformations. It uses Pd's existing object architecture and does not offer examplebased learning. MnM <ref type="bibr" target="#b9">[10]</ref>, also an extension for Pd and Max/MSP, focuses on advanced mathematical signal transformations. It aims to make complex operations accessible to those already familiar with sensor interaction design but does not integrate exploration, design, and test functions.</p><p>FlexiGesture is an electronic instrument that can learn gestures to trigger sample playback <ref type="bibr" target="#b27">[28]</ref>. It embodies programming by demonstration in a fixed form factor. Users can program which movements should trigger which samples by demonstration, but they cannot change the set of inputs. Exemplar generalizes FlexiGesture's approach into a design tool for variable of input and output configurations. We share the use of the DTW algorithm for pattern recognition with FlexiGesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrical Engineering Signal Analysis</head><p>Digital Signal Processing (DSP) software packages such as LabView <ref type="bibr" target="#b1">[2]</ref> are the state-of-the art engineering tools for working with sensors. LabView and Exemplar support different user populations, with different respective needs and expectations. LabView offers a sophisticated Turingcomplete visual programming language that allows professional users to create dataflow algorithms and signal visualizations. LabView's focus on measuring, analyzing, and processing signals supports professional engineers and trades off generality for a high threshold for use. In contrast, Exemplar integrates a subset of the DSP functions LabView offers into an integrated design environment that encourages exploration and rapid iteration by designers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>This paper introduced techniques for authoring sensorbased interactions through programming by demonstration, where the crucial generalization step is user-editable through direct manipulation of thresholding and pattern matching. Future work consists of investigating how interactive editing can be extended to other time-series signals and different kinds of matching strategies. Exemplar has been released as open source under the BSD license.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Iterative programming by demonstration for sensor-based interactions: A designer performs an action; annotates its recorded signal in Exemplar; tests the generated behavior, and exports it to a supported authoring tool.</figDesc><graphic coords="1,316.80,187.20,241.56,137.46" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The Exemplar authoring environment offers direct manipulation of live sensor data.</figDesc><graphic coords="3,54.18,54.18,502.74,181.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. In Exemplar, sensor data flows from left to right.</figDesc><graphic coords="3,316.86,594.30,248.34,109.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>continuous sets. When calibration is needed, experts can use Exemplar's extensible filter model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Prior experience of our study participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Physical setup of the first-use study.</figDesc><graphic coords="7,450.00,104.76,104.86,50.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>introduced a value entry by demonstration technique: in a visualization of a single sensor's signal, a user could copy the latest value into the threshold property of a state transition with a key press. Exemplar extends the d.tools work by introducing: • Direct manipulation techniques to control generalization from examples. • Pattern matching for complex signals with graphical editing of matching criteria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Post-experiment questionnaire results. Error bars indicate ½ standard deviation in each direction.</figDesc><graphic coords="8,54.00,525.54,244.80,159.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Interaction designs from the user study. A: turning on blinkers by detecting head tilt with bend sensors; B: accelerometer used as continuous 2D head mouse; C: aiming and shooting with accelerometer and bend sensor; D: navigation through full body movement; E: bi-pedal navigation through FSRs; F: navigation by hitting the walls of a booth (vibration sensed with accelerometers).</figDesc><graphic coords="8,323.34,53.94,228.12,261.90" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Wendy Ju for illustrating Figures <ref type="figure">5</ref> and<ref type="figure">7</ref>, Intel for donating PCs, and MediaX/DNP for funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Arduino Physical Computing Platform</orgName>
		</author>
		<ptr target="http://www.arduino.cc" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Labview</surname></persName>
		</author>
		<ptr target="http://www.ni.com/labview" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.eclipse.org/swt" />
		<title level="m">SWT: The Standard Widget Toolkit</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>The Eclipse Foundation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://steinbergcanada.com/technology/vst.htm" />
		<title level="m">Virtual Studio Technology (VST), 2006. Steinberg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Andreae</surname></persName>
		</author>
		<title level="m">Justified Generalization: Acquiring Procedures from Examples</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>MIT, Dept. of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Forming interactivity: A tool for rapid prototyping of physical interactive products</title>
		<author>
			<persName><forename type="first">D</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DIS: ACM Conference on Designing Interactive Systems</title>
		<meeting>DIS: ACM Conference on Designing Interactive Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">iStuff: A physical user interface toolkit for ubiquitous computing environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ballagas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ringel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<meeting>CHI: ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wiring: Prototyping Physical Interaction Design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barragan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Interaction Design Institute Ivrea</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making sense of sensing systems: five questions for designers and researchers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bellotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Grinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<meeting>CHI: ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MnM: a Max/MSP mapping toolbox</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIME: Conference on New Interfaces for Musical Expression</title>
		<meeting>NIME: Conference on New Interfaces for Musical Expression</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<ptr target="http://www.cl.cam.ac.uk/~afb21/CognitiveDimensions/CDquestionnaire.pdf" />
		<title level="m">A Cognitive Dimensions Questionnaire</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A morphological analysis of the design space of input devices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Roebertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="122" />
			<date type="published" when="1991">1991</date>
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Watch What I Do -Programming by Demonstration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cypher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page">652</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Programming by demonstration of context-aware applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><surname>Cappella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<meeting>CHI: ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A design tool for camera-based interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fails</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<meeting>CHI: ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cognitive dimensions of notations. People and Computers V</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R G</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="443" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Usability Analysis of Visual Programming Environments: A &apos;Cognitive Dimensions&apos; Framework</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R G</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="174" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phidgets: Easy development of physical interfaces through physical widgets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fitchett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST: ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST: ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reflective physical prototyping through integrated design, test, and analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Abdulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson-Mosher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST 2006: ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST 2006: ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rapid Construction of Functioning Physical Interfaces from Cardboard, Thumbtacks, Tin Foil and Masking Tape</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST: ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST: ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hollan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Direct Manipulation Interfaces. Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="338" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How Bodies Matter: Five Themes for Interaction Design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Takayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DIS: ACM Conference on Designing Interactive Systems</title>
		<meeting>DIS: ACM Conference on Designing Interactive Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Papier-Mâché: Toolkit Support for Tangible Input</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<meeting>CHI: ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUEDE: A Wizard of Oz Prototyping Tool for Speech User Interfaces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aboobaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST: ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST: ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Informal prototyping of continuous graphical interactions by demonstration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST: ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST: ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Your Wish is my Command</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page">416</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paternò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wulf</surname></persName>
		</author>
		<title level="m">End-User Development</title>
		<meeting><address><addrLine>Dordrecht; Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">492</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Personalization, Expressivity, and Learnability of an Implicit Mapping Strategy for Physical Interfaces</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Paradiso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of CHI: ACM Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robot programming by demonstration -using machine learning and user interaction methods for the development of easy and comfortable robot programming systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Munch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreuziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Industrial Robots</title>
		<meeting>International Symposium on Industrial Robots</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A Small Matter of Programming: Perspectives on End User Computing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Things that make us smart: Defending human attributes in the age of the machine</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Addison Wesley Publishing Company</publisher>
			<biblScope unit="page">290</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Puckette</surname></persName>
		</author>
		<author>
			<persName><surname>Pd</surname></persName>
		</author>
		<ptr target="http://crca.ucsd.edu/~msp/software.html" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A framework for designing sensorbased interactions to promote exploration and reflection in play</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978">1978</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A bike helmet built for road hazards</title>
		<author>
			<persName><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
		<ptr target="http://news.com.com/2300-1008_3-6111157-2.html" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overview + Detail</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Information Visualization</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">toolkit: a unified framework for instrument design</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIME: Conference on New Interfaces for Musical Expression</title>
		<meeting>NIME: Conference on New Interfaces for Musical Expression</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="140" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Envisioning Information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Graphics Press LLC</publisher>
			<biblScope unit="page">126</biblScope>
			<pubPlace>Cheshire, CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
