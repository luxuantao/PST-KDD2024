<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Interest Network with Dynamic Routing for Recommendation at Tmall</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mengmeng</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuchi</forename><surname>Xu</surname></persName>
							<email>yuchi.xyc@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
							<email>pipei.hpp@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Qiwei</forename><surname>Chen</surname></persName>
							<email>chenqiwei.cqw@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<email>guoliang.kang@student.uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lee</forename><forename type="middle">2019</forename><surname>Multi-Interest</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Guoliang Kang</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dik Lun Lee</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology Kowloon</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Network with Dynamic Routing for Recommendation at Tmall. In The 28th ACM International Conference on Information and Knowledge Management (CIKM &apos;19)</orgName>
								<address>
									<addrLine>November 3-7</addrLine>
									<postCode>2019</postCode>
									<settlement>Beijing, New York</settlement>
									<region>China. ACM, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Interest Network with Dynamic Routing for Recommendation at Tmall</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357814</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation system</term>
					<term>Deep learning</term>
					<term>User representation</term>
					<term>Dynamic Routing</term>
					<term>Capsule Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Industrial recommender systems have embraced deep learning algorithms for building intelligent systems to make accurate recommendations. At its core, deep learning offers powerful ability for learning representations from data, especially for user and item representations. Existing deep learning-based models usually represent a user by one representation vector, which is usually insufficient to capture diverse interests for large-scale users in practice. In this paper, we approach the learning of user representations from a different view, by representing a user with multiple representation vectors encoding the different aspects of the user's interests. To this end, we propose the Multi-Interest Network with Dynamic routing (MIND) for learning user representations in recommender systems. Specifically, we design a multi-interest extractor layer based on the recently proposed dynamic routing mechanism, which is applicable for modeling and extracting diverse interests from user's behaviors. Furthermore, a technique named label-aware attention is proposed to help the learning process of user representations. Through extensive experiments on several public benchmarks and one large-scale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods in terms of recommendation accuracy. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Figure <ref type="figure">1</ref>: Left: The areas highlighted with dashed rectangle are personalized for billion-scale users at Tmall; Right: User A interacts with products from several different categories, including clothes, sports and food, while user B interacts with products of books, toys and cellphones.</p><p>Tmall, the biggest Business-To-Customer (B2C) e-commerce platform in China, serves billion-scale users by providing billion-scale products online. On November 11-th of 2018, the well-known Tmall global shopping festival, the Gross Merchandise Volume (GMV) is around 213 billion yuan, achieving an increase rate of 26.9% compared with the same day of 2017. As the number of users and products is continuously growing, it becomes increasingly important to help each user find products that he/she might be interested in. In recent years, Tmall has spent huge efforts in developing personalized recommender systems (RS for short), which significantly contribute to the optimization of user experience and the increase of business value. For example, the homepage on Mobile Tmall App (as shown in Figure <ref type="figure">1</ref> (Left)), which accounts for about half of total traffic at Tmall, has deployed RS for displaying personalized products to meet customers' personalized need.</p><p>Due to the billion-scale users and items, the recommendation process designed for Tmall consists of two stages, the matching stage and the ranking stage. The matching stage is responsible for retrieving thousands of candidate items that are relevant to user interests, after which the ranking stage predicts precise probabilities of users interacting with these candidate items. For both of the two stages, it is vital to model user interests and find user representations capturing user interests, in order to support efficient retrieval of items that satisfy users' interests. However, it is non-trivial to model user interests at Tmall, due to the existence of diverse interests of users. On average, billion-scale users visit Tmall, each user interacts with hundreds of products every day. The interacted products tend to belong to different categories, indicating the diversity of user interests. For example, as shown in Figure <ref type="figure">1</ref> (Right), different users are distinct in terms of their interests and the same user may also be interested in various kinds of items. Therefore, the capability of capturing user's diverse interests becomes vital for RS at Tmall.</p><p>Existing recommendation algorithms model and represent user interests in different ways. Collaborative filtering-based methods represent user interests by historical interacted items <ref type="bibr" target="#b20">[21]</ref> or hidden factors <ref type="bibr" target="#b14">[15]</ref>, which suffer from sparsity problem or computationally demanding. Deep learning-based methods usually represent user interests with low-dimensional embedding vectors. For example, the deep neural network proposed for YouTube video recommendation (YouTube DNN) <ref type="bibr" target="#b4">[5]</ref> represents each user by one fixed-length vector transformed from the past behaviors of users, which can be a bottleneck for modeling diverse interests, as its dimension must be large in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type="bibr" target="#b29">[30]</ref> makes the user representation vary over different items with attention mechanisms to capture the diversity of user interests. Nevertheless, the adoption of attention mechanisms also makes it computationally prohibitive for large-scale applications with billion-scale items as it requires re-calculation of user representation for each item, making DIN only applicable for the ranking stage. Therefore, for the matching stage, one has to increase the dimension of both user and item representations in order to model diverse interests of billion-scale users if only one user representation vector is used. However, as the numbers of users and items are billion-scale, increasing dimension will induce huge costs for both computation and storage. Moreover, the increase of parameter size would make it difficult for model optimization. So, what we seek is a proper way to increase model capability without much additional cost.</p><p>Instead of using only one user representation vector, in this paper, we propose to use multiple representation vectors to model billion-scale users' diverse interests. Through this way, we have significantly increased model capability as well as recommendation accuracy, while no much additional cost is needed. Specifically, we propose the Multi-Interest Network with Dynamic routing (MIND) for learning multiple representation vectors for each user. Inspired by the dynamic routing algorithm <ref type="bibr" target="#b19">[20]</ref>, we design a novel layer called multi-interest extractor layer, and this layer can discover different aspects of interests and generate representations for diverse interests. Then, several feed-forward layers are stacked to transform these interest representation vectors into user representation vectors. These user representation vectors are computed only once and can be used in the matching stage for retrieving relevant items from billion-scale items. To summarize, the main contributions of this work are as follows:</p><p>• To capture diverse interests of users from user behaviors, we design the multi-interest extractor layer, which utilizes dynamic routing to adaptively aggregate user's historical behaviors into user representation vectors. The remainder of this paper is organized as follows: related works are reviewed in section 2; Section 3 elaborates the technical details of MIND; In section 4, we detail the experiments for comparing MIND with existing methods on several public benchmarks and online serving; Section 5 introduces the deployment of MIND in large-scale industrial application; The last section gives conclusion and future work of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep Learning for Recommendation. Inspired by the success of deep learning in computer vision and natural language processing <ref type="bibr" target="#b15">[16]</ref>, much effort has been put for developing deep learning-based recommendation algorithms <ref type="bibr" target="#b2">[3]</ref>. Besides the industrial applications proposed by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>, various types of deep models have gained significant attention. Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b8">[9]</ref>, DeepFM <ref type="bibr" target="#b6">[7]</ref> and Deep Matrix Factorization Models (DMF) <ref type="bibr" target="#b25">[26]</ref> construct a neural network composed of several MLPs to model the interaction between users and items. <ref type="bibr" target="#b21">[22]</ref> presents a novel solution to top-N sequential recommendation by providing an united and flexible network for capturing more features. User Representation. Representing users as vectors is commonly used in RS. Traditional methods assemble user preference as vectors composed of interested items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, keywords <ref type="bibr" target="#b5">[6]</ref> and topics <ref type="bibr" target="#b27">[28]</ref>. As the emergence of distributed representation learning, user embeddings obtained by neural networks are widely used. <ref type="bibr" target="#b3">[4]</ref> employs RNN-GRU to learn user embeddings from the temporal ordered review documents. <ref type="bibr" target="#b28">[29]</ref> learns user embedding vectors from word embedding vectors and applies them to recommending scholarly microblogs. <ref type="bibr" target="#b1">[2]</ref> proposes a novel convolutional neural network based model that explicitly learns and exploits user embeddings in conjunction with features derived from utterances.</p><p>Capsule Network. The concept of "Capsule", a small group of neurons assembled to output a whole vector, is firstly proposed by Hinton <ref type="bibr" target="#b10">[11]</ref> at 2011. Instead of backpropagation, dynamic routing [20] is used to learn the weights on the connections between capsules, which is improved by utilizing Expectation-Maximization algorithm <ref type="bibr" target="#b11">[12]</ref> to overcome several deficiencies and achieves better accuracy. These two main differences to conventional neural network make capsule networks capable of encoding the relationship between the part and the whole, which is adavanced not only in computer vision but also in natural language processing and knowledge graph. <ref type="bibr" target="#b26">[27]</ref> investigates the capsule networks for text classification and proposes three strategies to boost the performance. <ref type="bibr" target="#b18">[19]</ref> uses capsule network to model relationship triples for knowledge graph completion and search personalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Problem Formalization</head><p>The objective of the matching stage for industrial RS is to retrieve a subset of items from the billion-scale item pool I for each user u ∈ U such that the subset contains only thousands of items and each item is relevant to interests of the user. In order to achieve this objective, historical data generated by RS is collected for building a matching model. Specifically, each instance can be represented by a tuple (I u , P u , F i ), where I u denotes the set of items interacted by user u (also called user behavior), P u the basic profiles of user u (like user gender and age), F i the features of target item (such as item id, category id, brand id, seller id and title etc.).</p><p>The core task of MIND is to learn a function for mapping raw features into user representations, which can be formulated as</p><formula xml:id="formula_0">V u = f user (I u , P u ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">V u = − → v 1 u , ..., − → v K u ∈ R d×K denotes the representation</formula><p>vectors of user u, d the dimension, K the number of representation vectors. When K = 1, one representation vector is used, just like YouTube DNN. Besides, the representation vector of target item i is obtained by an embedding function as</p><formula xml:id="formula_2">− → e i = f it em (F i ) ,<label>(2)</label></formula><p>where − → e i ∈ R d×1 denotes the representation vector of item i, and the detail of f it em will be illustrated in the "Embedding &amp; Pooling Layer" section. When user representation vectors and item representation vectors are learned, top N candidate items are retrieved according to a scoring function. As a user may own several representation vectors, the item score is determined by the nearest representation vector so the scoring function is:</p><formula xml:id="formula_3">f scor e V u , − → e i = max 1≤k ≤K − → e T i − → v k u ,<label>(3)</label></formula><p>where N is the predefined number of items to be retrieved in the matching stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding &amp; Pooling Layer</head><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the input of MIND consists of three groups, user profile P u , user behavior I u , and label item F i . Each group contains several categorical id features, and these id features are of extremely high dimension. For instance, the number of item ids is about billions, thus we adopt the widely-used embedding technique to embed these id features into low-dimensional dense vectors (a.k.a embeddings), which significantly reduces the number of parameters and eases the learning process. For id features (gender, age, etc.) from P u , corresponding embeddings are concatenated to form the user profile embedding − → p u . For item ids along with other categorical ids (category id, brand id and seller id etc.) that have been proved to be useful for cold-start items <ref type="bibr" target="#b23">[24]</ref> from F i , corresponding embeddings are further passed through an average pooling layer to form the label item embedding − → e i . To enrich the feature space, other attributes of items such as color, texture and style, are easily to be embed and combined to the item embedding, which is expected to improve the expression ability of the embeddings. Lastly, for items from user behavior I u , corresponding item embeddings are collected to form the user behavior embedding E u = { − → e j , j ∈ I u }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Interest Extractor Layer</head><p>We argue that representing user interests by one representation vector can be a bottleneck for capturing diverse interests of users, because we have to compress all information related with diverse interests of users into one representation vector. Thus, all information about diverse interests of users is mixed together, causing inaccurate item retrieval for the matching stage. Instead, we adopt multiple representation vectors to express distinct interests of users separately. By this way, diverse interests of users are considered separately in the matching stage, enabling more accurate item retrieval for every aspect of interests.</p><p>To learn multiple representation vectors, we utilize clustering process to group user's historical behaviors into several clusters. Items from one cluster are expected to be closely related and collectively represent one particular aspect of user interests. Here, we design the multi-interest extractor layer for clustering historical behaviors and inferring representation vectors for each cluster. Since the design of multi-interest extractor layer is inspired by the recently proposed dynamic routing for representation learning in capsule network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, we firstly revisit essential basics in order to make this paper self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dynamic Routing Revisit.</head><p>We briefly introduce dynamic routing <ref type="bibr" target="#b19">[20]</ref> for representation learning of capsules, a new form of neural units represented by vectors. Suppose we have two layers of capsules, and we refer capsules from the first layer and the second layer as low-level capsules and high-level capsules respectively. The goal of dynamic routing is to compute the values of highlevel capsules given the values of low-level capsules in an iterative way. In each iteration, given low-level capsules i ∈ {1, ..., m} with corresponding vectors − → c l i ∈ R N l ×1 , i ∈ {1, ..., m} and high-level capsules j ∈ {1, ..., n} with corresponding vectors − → c h j ∈ R N h ×1 , j ∈ {1, ..., n}, the routing logit b i j between low-level capsule i and highlevel capsule j is computed by</p><formula xml:id="formula_4">b i j = ( − → c h j ) T S i j − → c l i ,<label>(4)</label></formula><p>where S i j ∈ R N h ×N l denotes the bilinear mapping matrix to be learned.</p><p>With routing logits calculated, the candidate vector for high-level capsule j is computed as weighted sum of all low-level capsules</p><formula xml:id="formula_5">− → z h j = m i=1 w i j S i j − → c l i ,<label>(5)</label></formula><p>where w i j denotes the weight for connecting low-level capsule i and high-level capsule j and is calculated by performing softmax on routing logits as</p><formula xml:id="formula_6">w i j = exp b i j m k=1 exp b ik .<label>(6)</label></formula><p>Finally, a non-linear "squash" function is applied to obtain the vectors of high-level capsules as</p><formula xml:id="formula_7">− → c h j = squash( − → z h j ) = − → z h j 2 1 + − → z h j 2 − → z h j − → z h j .<label>(7)</label></formula><p>The values of b i j are initialized to zeros, and the routing process is usually repeated three times to converge. When routing finished, high-level capsule's values − → c h j are fixed and can be used as inputs for next layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">B2I Dynamic</head><p>Routing. In a nutshell, capsule is a new kind of neuron represented by one vector instead of one scalar used in ordinary neural networks. The vector-based capsule is expected to be able to represent different properties of an entity, in which the orientation of a capsule represents one property and the length of the capsule is used to represent the probability that the property exists. Correspondingly, the objective of the multi-interest extractor layer is to learn representations for expressing properties of user interests as well as whether corresponding interests exist. The semantic connection between capsules and interest representations motivates us to regard the behavior/interest representations as behavior/interest capsules and employ dynamic routing to learn interest capsules from behavior capsules. Nevertheless, the original routing algorithm proposed for image data is not directly applicable for processing user behavior data. So, we propose Behavior-to-Interest (B2I) dynamic routing for adaptively aggregating user's behaviors into interest representation vectors, and it differs from original routing algorithm in three aspects.</p><p>Shared bilinear mapping matrix. We use fixed bilinear mapping matrix S instead of a separate bilinear mapping matrix for each pair of low-level capsules and high-level capsules in original dynamic routing due to two considerations. On the one hand, user behaviors are of variable-length, ranging from dozens to hundreds for Tmall users, thus the use of fixed bilinear mapping matrix is generalizable. On the other hand, we hope interest capsules lie in the same vector space, but different bilinear mapping matrice would map interest capsules into different vector spaces. Thus, the routing logit is calculated by</p><formula xml:id="formula_8">b i j = − → u T j S − → e i , i ∈ I u , j ∈ {1, ..., K },<label>(8)</label></formula><p>where − → e i ∈ R d denotes the embedding of behavior item i, − → u j ∈ R d the vector of interest capsule j. The bilinear mapping matrix S ∈ R d×d is shared across each pair of behavior capsules and interest capsules.</p><p>Randomly initialized routing logits. Owing to the use of shared bilinear mapping matrix S, initializing routing logits to zeros will lead to the same initial interest capsules. Then, the subsequent iterations will be trapped in a situation, where different interest capsules remain the same all the time. To mitigate this phenomenon, we sample a random matrix from gaussian distribution N (0, σ 2 ) for initial routing logits to make initial interest capsules differ from each other, similar to the well-established K-Means clustering algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic interest number.</head><p>As the number of interest capsules owned by different users may be different, we introduce a heuristic rule for adaptively adjusting the value of K for different users. Specifically, the value of K for user u is computed by</p><formula xml:id="formula_9">K ′ u = max(1, min(K, log 2 (|I u |))).<label>(9)</label></formula><p>This strategy for adjusting the number of interest capsules can save some computing resources for those users with fewer interests.</p><p>The whole dynamic routing procedure is listed in Algorithm 1.</p><p>Algorithm for all behavior capsule i: w i j ← so f tmax(b i j )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for all interest capsule j:</p><formula xml:id="formula_10">− → z j = i ∈I u w i j S − → e i 6:</formula><p>for all interest capsule j:</p><formula xml:id="formula_11">− → u j ← squash( − → z j ) 7:</formula><p>for all behavior capsule i and interest capsule j: b</p><formula xml:id="formula_12">i j ← − → u T j S − → e i 8: end for 9: return − → u j , j = 1, ..., K ′ u 3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Label-aware Attention Layer</head><p>Through multi-interest extractor layer, several interest capsules are generated from user's behavior embeddings. Different interest capsules represent different aspects of user interests, and the relevant interest capsule is used for evaluating user's preference on specific items. Therefore, during training, we design a label-aware attention layer based on scaled dot-product attention <ref type="bibr" target="#b22">[23]</ref> to make the target item choose which interest capsule is used. Specifically, for one target item, we calculate the compatibilities between each interest capsule and target item embedding, and compute a weighted sum of interest capsules as user representation vector for the target item, where the weight for one interest capsule is determined by corresponding compatibility. In label-aware attention, the label is the query and the interest capsules are both keys and values, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The output vector of user u with respect to item i is computed as</p><formula xml:id="formula_13">− → v u = Attention − → e i , V u , V u = V u softmax(pow(V T u − → e i , p)),</formula><p>where pow denotes element-wise exponentiation operator, p a tunable parameter for adjusting the attention distribution. When p is close to 0, each interest capsule attends to receive even attention. When p is bigger than 1, as p increases, the value has bigger dot-product will receive more and more weight. Consider the limit case, when p gets infinity, the attention mechanism becomes a kind of hard attention to pick the value who has the biggest attention and ignore others. In our experiments, we find out that using hard attention leads to faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training &amp; Serving</head><p>With the user vector − → v u and the label item embedding − → e i ready, we compute the probability of the user u interacting with the label item i as</p><formula xml:id="formula_14">Pr(i |u) = Pr − → e i | − → v u = exp − → v T u − → e i j ∈I exp − → v T u − → e j .<label>(10)</label></formula><p>Then, the overall objective function for training MIND is</p><formula xml:id="formula_15">L = (u,i)∈ D log Pr(i |u), (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where D is the collection of training data containing user-item interactions. Since the number of items scales to billions, the sum operation of the denominator ( <ref type="formula" target="#formula_14">10</ref>) is computationally prohibitive. Thus, we use the sampled softmax technique <ref type="bibr" target="#b17">[18]</ref> to make the objective function trackable and choose the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> for training MIND.</p><p>After training, the MIND network except for the label-aware attention layer can be used as user representation mapping function f user . At serving time, user's behavior sequence and user profile are fed into the f user function, producing multiple representation vectors for each user. Then, these representation vectors are used to retrieve top N items by an approximate nearest neighbor approach <ref type="bibr" target="#b12">[13]</ref>. These items with highest similarities with user's representation vectors are retrieved and constitute the final set of candidate items for the matching stage of RS. Please note that, when a user has new actions, it will alter his/her behavior sequence as well as the corresponding user representation vectors, thus MIND enables real-time personalization for the matching stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Connections with Existing Methods</head><p>Here, we make some remarks about the relations between MIND and two existing methods, illustrating their similarities as well as differences.</p><p>YouTube DNN. Both MIND and YouTube DNN utilize deep neural networks to model behavior data to generate user representations, which are used for large-scale item retrieval in the matching stage of industrial RS. However, YouTube DNN uses one vector to represent a user while MIND uses multiple vectors for that. When the value of K in Algorithm 1 equals to 1, MIND is similar to YouTube DNN, thus MIND can be viewed as generalization of YouTube DNN.</p><p>DIN. In terms of capturing diverse interests of users, MIND and DIN share the similar goal. However, the two methods differ in the way of achieving the goal as well as applicability. To deal with diverse interests, DIN applies an attention mechanism at the item level, while MIND employs dynamic routing to generate interest capsules and considers diversity at the interest level. Moreover, DIN focuses on the ranking stage as it handles thousands of items, however, MIND decouples the process of inferring user representations and measuring user-item compatibility, making it applicable to billion-scale items in the matching stage.</p><p>Session: Applied -Recommendation and Advertising CIKM '19, November 3-7, 2019, Beijing, China </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Offline Evaluation</head><p>In this section, we present the comparisons between MIND and existing methods in terms of recommendation accuracy on several datasets under offline settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Experimental</head><p>Setup. We choose two datasets for evaluating recommendation performance. One is Amazon Books 1 provided by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, representing one of the most widely-used public dataset for e-commerce recommendations. The other called Tmall-Data is held out from Mobile Tmall App, containing historical behaviors of randomly sampled two millions of Tmall users in 10 days. For Amazon Books, we only keep items which have been reviewed at least 10 times and users who have reviewed at least 10 items. For TmallData, we filter out items clicked by less than 600 unique users. The statistics of the two datasets are shown in Table <ref type="table" target="#tab_2">1</ref>.</p><p>We choose next item prediction problem, that is predicting a user's next interaction, to evaluate the methods' performance, because it is the core task in the matching stage of RS. After dividing the user-item interaction data of each dataset randomly into training set and test set by a ratio of 19:1, for each user, a randomly selected item interacted by the user is used as target item, while the items interacted before the target item are collected as the user behaviors. Hit rate is adopted as the main metric to measure the recommendation performance, define as:</p><formula xml:id="formula_17">HitRate@N = (u,i)∈ D t e s t I (target item occurs in top N ) |D t est | ,<label>(12)</label></formula><p>where D t est denotes the test set consisting of pairs of users and target items (u, i) and I denotes the indicator function.</p><p>Hyperparameter tuning for the dimension of embedding vectors d and the number of user interests K is conducted by experiments on a group of parameters predefined according to the scale and distribution of each dataset, and each method is tested with best hyperparameters for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparing Methods.</head><p>• WALS [1] WALS, short for Weighted Alternating Least Squares, is a classical matrix factorization algorithm for decomposing user-item interaction matrix into hidden factors of users and items. Recommendation is made based on compatibilities between hidden factors of users and target items. • YouTube DNN <ref type="bibr" target="#b4">[5]</ref> As mentioned above, YouTube DNN is one of the most successful deep learning method used for industrial recommendation systems.</p><p>• MaxMF <ref type="bibr" target="#b24">[25]</ref> The method introduces a highly scalable method for learning nonlinear latent factorization to model multiple user interests.</p><p>1 http://jmcauley.ucsd.edu/data/amazon/ 4.1.3 Experimental Results. Table <ref type="table">2</ref> summarizes the performance of MIND as well as baselines on two datasets in terms of HitRate@N (N = 10, 50, 100). Clearly, MIND accomplishes comparable performance to all of the baselines on both datasets. The matrix factorization approach, WALS, is beaten by other methods, revealing the power of deep learning for improving the matching stage of RS. However, equipped without deep learning, MaxMF performs much better than WALS, which can be explained by the fact that MaxMF generalizes standard MF to a nonlinear model and adopts multiple user representation vectors. It can be observed that methods employing multiple user representation vectors (MaxMF-K-interest, MIND-K-interest) performs generally better than other methods (WALS, YouTube DNN, MIND-1-interest). Therefore, using multiple user representation vectors is proved to be an effective way for modeling user's diverse interests as well as boosting recommendation accuracy. Moreover, we can observe that the improvement introduced by multiple user representation vectors is more significant for TmallData, as the users of Tmall tend to exhibit more diverse interests. This increasement of diversity can also be reflected by the best K for each dataset, where the best K for TmallData is larger than that for Amazon Books. The improvement of MIND-1-interest over YouTube DNN shows that dynamic routing serves as a better pooling strategy than average pooling. Considering the results of MaxMF and MIND-K-interest, it verifies that extracting multiple interests from user behaviors by dynamic routing outperforms the nonlinear modeling strategy used in MaxMF. This can be attributed to two points: 1) The multi-interest extractor layer utilizes a clustering procedure for generating interest representations, which achieves more precise representation of user. 2) Label-aware attention layer makes target item attend over multiple user representation vectors, enabling more accurate matching between user interests and target item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Hyperparameters</head><p>In this section, we conduct two experiments on Amazon Books to study the influence of the hyperparameters within multi-interest extractor layer and label-aware attention layer.</p><p>Initialization of routing logits. The random initialization for routing logits adopted in multi-interest extractor layer is similar to the initialization of K-means centroids, where the distributions of initial cluster centers have strong impact on the final clustering results. As the routing logits are initialized according to gaussian distribution N (0, σ 2 ), we concern about different values of σ may lead to different convergence which has effect on the performance. To study the impact of σ , we initialize the routing logits b i j with 3 different values of σ , 0.1, 1 and 5. The results are shown by the upper part of Figure <ref type="figure">3</ref>, where each curve of 3 values almost overlap. This observation reveals that MIND is robust to the values of σ , and it is rational to choose σ = 1 for our practical applications.</p><p>Power number in label-aware attention. As mentioned before, the power number p within label-aware attention controls the proportion of each interest to the combined label-aware interest representation. We compare the performance of MIND as p varies from 0 to ∞ and show the results by the lower part of Figure <ref type="figure">3</ref>. Clearly, the performance of p = 0 is much worse than the others. The reason is that, when taking p = 0 each interest has the same attention thus the combined interest representation equals the average of interests with no reference to the label. Taking p ⩾ 1, the attention scores are proportional to the similarities between interest representation vectors and target item embeddings, which makes the combined interest representation a weighted sum of interests. It also shows that performance gets better as p increases, since the representation vector of the interest with more similarity to the target item acquires larger attention, which evolves to a hard attention scheme as p = ∞. By this scheme, the interest representation nearest to the target item dominates the combined interest representation, enabling MIND converge faster and perform the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Online Experiments</head><p>We conduct online experiments by deploying MIND to handle real traffic at Tmall homepage for one week. To make comparisons fairly, all methods deployed in the matching stage are followed by the same ranking procedure. CTR, short for click-through-rate, a widely used industrial metric, is used to measure the performance of methods for serving online traffic. There are two baseline methods for online experiments. One is item-based CF, which is the base matching algorithm serving the majority of the online traffic. The other is YouTube DNN, which is the well-known deep learning-based matching model. We deploy all comparing methods in an A/B test framework, and one thousand of candidate items are retrieved by each method, which then fed to the ranking stage for final recommendation.</p><p>The experimental results are summarized in Figure <ref type="figure" target="#fig_2">4</ref>. It is clearly that MIND outperforms item-based CF and YouTube DNN, which indicates that MIND generates a better user representation. Besides, we make the following observations: 1) As is optimized by the long-term practice, item-based CF performs better than YouTube DNN which is also exceeded by MIND with single interest. 2) A very noticeable trend is that the performance of MIND gets better as the number of extracted interests increases from 1 to 5. 3) The performance of MIND peaks when the number of extracted interests reaches 5, after that the CTR remains constant and the improvement of 7 interests is ignorable. 4) MIND with dynamic interest number has the comparable performance with MIND with 7 interests. From the observations above, we make several conclusions. First, for Tmall, the optimal number of user interests is 5 ~7, which reveals the average diversity of user interests. Second, the dynamic interest number mechanism does not bring CTR gain, but during the experiments we recognize the scheme can decrease the cost of serving, which benefits large-scale service such as Tmall and is more adoptable in practice. In a word, the online experiments validate that MIND achieves an better solution to model users with diverse interests and can significantly advance the whole RS.  Figure <ref type="figure" target="#fig_3">5</ref> illustrates the coupling coefficients associated to two users randomly selected from Tmall daily active users, where each row corresponds to one interest capsule and each column corresponds to one behavior. It shows that user C (upper) has interacted with 4 classes of goods (headphones, snacks, handbags and clothes), each of which has the max coupling coefficients on one interest capsule and forms the corresponding interest. While user D (below) is interested only in clothes, thus the 3 interests with finer grain size (sweaters, overcoats and down jackets) are resolved from the behaviors. Regarding this result, we confirm that each class of user behaviors are clustered together and form the corresponding interest representation vector.   point is assembled by the items lying within the specific range, thus the size of each point represents the number of the items with the corresponding similarity. We also show some items selected randomly from all the candidates. As expected, the items recalled by MIND are strongly correlated with the corresponding interest, while that by YouTube DNN vary widely along the categories of items and have lower similarity to the user's behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM DEPLOYMENT</head><p>In this section, we describe the implementation and deployment of MIND at Tmall. A typical workflow composed of several basic platforms is shown as Figure <ref type="figure" target="#fig_8">7</ref> and detailed as below: As users launch Mobile Tmall APP, the requests for recommendation are sent to Tmall Personality Platform, the server cluster integrated with a bunch of plug-in modules and served as online recommender service of Tmall. Recent behaviors of users including real-time user feedback are retrieved by Tmall Personality Platform and sent to User Interest Extractor which is the main module implementing MIND for transforming user behaviors to multiple user interests. Subsequently, Recall Engine searches for the items with embedding vectors nearest to the user interests. Items triggered by different interests are merged together as candidate items and sorted by their similarity to the user interests. The whole procedure of selecting thousands of candidate items from the billion-scale item pool by User Interest Extractor and Recall Engine can be fulfilled in less than 15 milliseconds, which is an increase of only 10% compared to the original version with single interest, due to the parallelism of searching items for each interest generated by MIND. Taking a tradeoff between the scope of items and the response time of the system, top 1000 of these candidate items are scored by Ranking Service which predicts CTRs with a bunch of features. Finally, Tmall Personality Platform completes the item list as the recommendation results shown to users. Both User Interest Extractor and Ranking Service are trained on Model Training Platform using 100 GPUs, by which the training can be executed in 8 hours. Benefiting from the superior performance of Model Training Platform, the deep network served for prediction is updated every day, which guarantees the newly released products to be calculated and exposured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a new structure of neural network, namely Multi-Interest Network with Dynamic routing (MIND), to represent user's diverse interests for the matching stage in e-commerce recommendation, which involves billion scale users and items. Specifically, we design a multi-interest extractor layer with a variant dynamic routing to extract user's diverse interests which are then trained with a novel label-aware attention scheme. Offline experiments are conducted to demonstrate that MIND achieves superior performance on public benchmarks. Online CTRs are also reported to demonstrate the effectiveness and feasibility of MIND at Tmall's live production. For future work, we will pursue two directions. The first is to incorporate more information about user's behavior sequence, such as behavior time etc. The second is to optimize the initialization scheme of dynamic routing, referring to K-means++ initialization scheme, so as to achieve a better user representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of MIND. MIND takes user behaviors with user profile features as inputs, and outputs user representation vectors for item retrieval in the matching stage of recommendation. Id features from input layer are transformed into embeddings through the embedding layer, and embeddings of each item are further averaged by a pooling layer. User behavior embeddings are fed into the multi-interest extractor layer, which produces interest capsules. By concatenating interest capsules with user profile embedding and transforming the concatenated capsules by several ReLU layers, user representation vectors are obtained. An extra label-aware attention layer is introduced to guide the training process. At serving, the multiple user representation vectors are used to retrieve items through an approximate nearest neighbor lookup approach.</figDesc><graphic url="image-2.png" coords="3,67.82,83.69,476.35,217.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Table 2 :Figure 3 :</head><label>23</label><figDesc>Figure 3: Hyperparameters' impact. The upper part shows that MIND can achieve comparable results with different σ ; the lower part shows that MIND performs better with bigger p.</figDesc><graphic url="image-3.png" coords="7,55.92,217.24,236.00,224.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Online CTRs in a week. MIND with 5 ~7 interests performs best in all comparing methods. MIND significantly beats the two baseline methods, item-based CF and YouTube DNN.</figDesc><graphic url="image-4.png" coords="7,319.50,217.25,237.16,139.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Heatmap of coupling coefficients for two users. Each class of behaviors has the max coupling coefficients on the corresponding interest. User C (upper) and user D (below) have different granularity of interests.</figDesc><graphic url="image-5.png" coords="8,66.52,147.88,214.80,194.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 4 . 1</head><label>41</label><figDesc>Coupling Coefficients. The coupling coefficients between behavior capsules and interest capsules quantify the grade of membership of behaviors to interests. In this section, we visualize these coupling coefficients to show that the interest extraction process is interpretable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 4 . 2</head><label>42</label><figDesc>Item Distribution. At serving time, items similar to user interests are retrieved by nearest neighbor search. We visualize the distribution of these items recalled by each interest based on their similarity to the corresponding interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>shows the item distributions of the same user (user C) mentioned by Figure5(upper). The distributions are obtained by two methods respectively, where the upper 4 axes demonstrate the items recalled by 4 interests based on MIND while the lowest axis illustrates that based on YouTube DNN. The items are scattered at the axes according to their similarity with the interests, which has been scaled to 0 ~1 by min-max normalization and rounded to the nearest 0.5. One</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The distribution of items recalled by each interest corresponding to the user behaviors exampled on the left.Each interest is demonstrated by one axis, of which the coordinate is the similarity between items and interests. The size of the point is proportional to the number of the items with the specific similarity.</figDesc><graphic url="image-6.png" coords="8,319.78,83.69,236.60,181.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Architecture of the RS at Tmall.</figDesc><graphic url="image-7.png" coords="8,335.18,484.10,205.80,93.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• By using user representation vectors produced by the multiinterest extractor layer and a newly proposed label-aware attention layer, we build a deep neural network for personalized recommendation tasks. Compared with existing methods, MIND shows superior performance on several public datasets and one industrial dataset from Tmall.• To deploy MIND for serving billion-scale users at Tmall, we construct a system to implement the whole pipeline for data collecting, model training and online serving. The deployed system significantly improves the click-through rate (CTR) of the homepage on Mobile Tmall App.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 B2I Dynamic Routing.</figDesc><table><row><cell>Input: behavior embeddings − → e i , i ∈ I u , iteration times r ,</cell></row><row><cell>number of interest capsules K</cell></row><row><cell>Output: interest capsules − → u j , j = 1, ..., K ′ u</cell></row><row><cell>1: calculate adaptive number of interest capsules K ′ u by (9)</cell></row><row><cell>2: for all behavior capsule i and interest capsule j: initialize b i j ∼</cell></row><row><cell>N (0, σ 2 )</cell></row></table><note>3: for k ← 1, r do 4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the two datasets for offline evaluation.</figDesc><table><row><cell>Dataset</cell><cell>Users</cell><cell cols="2">Goods Categories</cell><cell>Samples</cell></row><row><cell cols="3">Amazon Books 351,356 393,801</cell><cell>1</cell><cell>6,271,511</cell></row><row><cell>TmallData</cell><cell cols="2">2,014,865 934,751</cell><cell>6,377</cell><cell>50,929,802</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank colleagues of our team -Jizhe Wang, Andreas Pfadler, Jiaming Xu, Wen Chen, Lifeng Wang, Xin Guo and Cheng Guo for useful discussions and supports on this work. We are grateful to our cooperative team -search engineering team. Dik Lun Lee is supported by the Research Grants Council HKSAR GRF (No. 16215019). We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recommender: An analysis of collaborative filtering techniques</title>
		<author>
			<persName><surname>Christopher R Aberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modelling Context with User Embeddings for Sarcasm Detection in Social Media</title>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="167" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review on deep learning for recommender systems: challenges and remedies</title>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Batmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Yurekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Bilge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihan</forename><surname>Kaleli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning user and product distributed representations using a sequence model for sentiment analysis</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-view deep learning approach for cross domain user modeling in recommendation systems</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mamdouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elkahky</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-machine Based Neural Network for CTR Prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI&apos;17)</title>
				<meeting>the 26th International Joint Conference on Artificial Intelligence (IJCAI&apos;17)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 25th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical analysis of design choices in neighborhood-based collaborative filtering algorithms</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="287" to="310" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04266</idno>
		<title level="m">A Capsule Network-based Embedding Model for Search Personalization</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
				<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear latent factorization by embedding multiple user interests</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
				<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Matrix Factorization Models for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hong-Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Investigating Capsule Networks with Dynamic Routing for Text Classification</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soufei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3110" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic user modeling in social media systems</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">User embedding for scholarly microblog recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
