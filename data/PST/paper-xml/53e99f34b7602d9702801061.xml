<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<settlement>Boca Raton</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEB168A4763AE6873323BB174FBABFF7</idno>
					<idno type="DOI">10.1109/TNN.2007.896861</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-This paper presents a novel background modeling and subtraction approach for video object segmentation. A neural network (NN) architecture is proposed to form an unsupervised Bayesian classifier for this application domain. The constructed classifier efficiently handles the segmentation in natural-scene sequences with complex background motion and changes in illumination. The weights of the proposed NN serve as a model of the background and are temporally updated to reflect the observed statistics of background. The segmentation performance of the proposed NN is qualitatively and quantitatively examined and compared to two extant probabilistic object segmentation algorithms, based on a previously published test pool containing diverse surveillance-related sequences. The proposed algorithm is parallelized on a subpixel level and designed to enable efficient hardware implementation.</p><p>Index Terms-Automated surveillance, background subtraction, neural networks (NNs), object segmentation, video processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE rapid increase in the amount of multimedia content produced by our society is a powerful driving force behind the significant scientific effort spent on developing automatic methods to infer meaning of this content. A vital part of this paper is directed towards the analysis of video sequences. Object segmentation represents a basic task in video processing and the foundation of scene understanding, various surveillance applications, as well as the emerging research into 2-D-to-pseudo-3-D video conversion. The task is complex and is exacerbated by the increasing resolution of video sequences, stemming from continuing advances in the video capture and transmission technology. As a result, research into more efficient algorithms for real-time object segmentation continues unabated.</p><p>In this paper, a common simplifying assumption that the video is grabbed from a stationary camera is made. The task is still difficult when the segmentation is to be done for natural scenes where the background contains shadows and moving objects, and undergoes illumination changes. In this context, the basic segmentation entities can be defined as follows:</p><p>• all objects that are present in the scene, during the whole sequence or longer than a predefined period of time, are considered background objects; • all other objects appearing in the scene are referred to as foreground. The goal of the video object segmentation is to separate pixels corresponding to foreground from those corresponding to background.</p><p>If the state of the background is known for every frame of the sequence and there are no changes in illumination, the segmentation can be accomplished by a simple comparison between the background image and a frame of the sequence. This, however, is unrealistic for almost all applications. In the absence of an exact model for the background, one has to be estimated based on the information in the sequence and some assumptions. The process of modeling the background and determining the foreground by comparison with the frames of the sequence is often referred to as background subtraction.</p><p>The following two broad classes of background-subtraction methods can be identified:</p><p>1) filter-based background subtraction; 2) probabilistic background subtraction.</p><p>Filter-based approaches were developed first and rely on some sort of low-pass filtering of the frames of the sequence to obtain a model of the background in the form of a background image. Their main weakness is the inherent assumption of the background changing more slowly than the foreground. High-frequency motion in the background such as that of moving branches or waves often leads to misclassification of these background objects. This makes filter-based approaches unsuitable for applications with complex and dynamic background changes <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. They are computationally inexpensive when compared to probabilistic methods, but are unable to achieve good segmentation results for many natural scenes.</p><p>Probabilistic methods are an effort to escape the limitations of the filter-based approaches by learning the statistics of the pixels corresponding to background and using them to distinguish between the background and the foreground. They are the preferred approach for segmentation of sequences with complex background. Their main shortcoming is that they are computationally complex and only able to achieve real-time processing of comparatively small video formats (e.g., 120 160 pixels) at reduced frame rates [e.g., 15 frames per second (fps)] <ref type="bibr" target="#b3">[4]</ref>.</p><p>The development of a parallelized probabilistic object-segmentation approach, which would allow for efficient hardware implementation and object detection in real time for high-complexity video sequences (in terms of the frame size as well as background changes), is the focus of this paper. In this respect, it is an extension of the previously published work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> pertinent to marine surveillance. Here, the proposed approach is described in more detail and examined in terms of applicability to a broader range of surveillance application. With this in mind, it is tested on a diverse set of surveillance-related sequences compiled by Li et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>The proposed solution employs a feedforward neural network (NN) to achieve background subtraction. To this end, a new NN structure is designed, representing a combination of probabilistic neural network (PNN) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and a winner-take-all (WTA) NN <ref type="bibr" target="#b8">[9]</ref>. In addition, rules for temporal adaptation of the weights of the network are set based on a Bayesian formulation of the segmentation problem. Such a network is able to serve both as an adaptive model of the background in a video sequence and a Bayesian classifier of pixels as background or foreground. NNs posses intrinsic parallelism which can be exploited in a suitable hardware implementation to achieve fast segmentation of foreground objects.</p><p>Section II provides insight into our motivation and a survey of related published work. Section III holds the discussion of the Bayesian inference framework used. Section IV describes the main aspects of the proposed approach. Section V is dedicated to the presentation and discussion of simulation results. Section VI contains the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND RELATED WORK</head><p>The segmentation approach described here is motivated by previous work in the domain of marine surveillance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. It is intended as a solution to the problem of segmentation in natural-scene, complex-background sequences. The ultimate goal of the project is to achieve real-time segmentation of high-resolution QuadHDTV images (frame size of 3840 2160 pixels). The segmentation module should eventually be implemented as a hardware component embedded in a QuadHDTV camera; hence, our interest in a hardware-friendly solution.</p><p>The discussion of published works in this section is localized to probabilistic background-subtraction approaches to foreground segmentation, with the exception of three papers concerning the application of NNs to computer vision problems <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. The motive for the former is the comparison of the proposed approach with other probabilistic methods used to address the same problem. The latter are included to point out the differences between the previous relevant applications of NNs and the approach proposed.</p><p>The Kalman filter approach to video object segmentation <ref type="bibr" target="#b12">[13]</ref> can be considered one of the first probabilistic approaches applied. It is an optimal solution to Bayesian estimation when the changes in the background are modeled by a dynamic linear process with normal distribution of errors in the measurement of the pixel values used to determine the state of the system. In the work referenced, the state of the system has been defined as the vector of the change in gray values of the pixel and its first derivative. The linearity assumption proved to be too restrictive to enable efficient segmentation in the case of complex background scenes with high-frequency changes. Nevertheless, extended Kalman filter remains the only probabilistic approach that considers the error in the measurements explicitly.</p><p>While the Kalman filter approach makes an assumption about the normal (Gaussian) distribution of the noise, other early probabilistic approaches assumed normal distribution of the values of a single pixel. Thus, they tried to approximate the probability density function (pdf) of these values by a single Gaussian, whose parameters are recursively updated in order to follow gradual background changes within the video sequence <ref type="bibr" target="#b13">[14]</ref>. These techniques achieve slightly better segmentation results than the Kalman filter.</p><p>A natural extension to the single Gaussian-based approaches is the mixture of Gaussians (MoG) models. These methods use multiple evolving Gaussian distributions as a model for the values of the background pixels <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>. They showed good foreground object segmentation results for many outdoor sequences. However, weaker results were reported <ref type="bibr" target="#b16">[17]</ref> for video sequences containing nonperiodical background changes. The improved performance of these methods can be attributed to the fact that they do not incorporate the assumption of the normal distribution of background pixel values. The shape of the pdfs they are trying to estimate can be any shape that can be approximated with a predetermined number of Gaussian curves. In fact, with an infinite number of Gaussian curves one can approximate any curve <ref type="bibr" target="#b17">[18]</ref>. In that case, the Gaussian curves represent what will be referred to as a kernel in the terminology of the approach proposed in the following. For reasons of computational complexity, the typical number of Gaussians used for modeling is 3-5 <ref type="bibr" target="#b2">[3]</ref>. This yields a rather inaccurate approximation of the PDFs and is the reason behind the poor performance in complex sequences.</p><p>Recently, Li et al. proposed a method for foreground object detection, which represents a combination of a filtering and a probabilistic approach <ref type="bibr" target="#b3">[4]</ref>. It can, therefore, be considered a hybrid in terms of the aforementioned classification. Initial segmentation is filter based and the model maintains a reference background image as a model of the background. The authors propose a novel approach to cope with the inability of the filter-based approaches to differentiate between the movements of the foreground objects and background objects in complex scenes. They model the pdfs of the pixel values detected by the initial segmentation. Thus, they are able to distinguish between the errors in the initial segmentation and the true foreground pixels. The PDFs are updated in time and a Bayes'-rule-based decision framework is formulated based on the assumption that the pixel values observed more often at a single pixel are more likely to be due to background object movement. The applicability of the stated assumption to the data that has been filtered through the initial segmentation is unclear. Nevertheless, the approach is able to achieve good segmentation results for sequences containing high-frequency changes in the pixels pertinent to background.</p><p>PNNs have not, to the best of our knowledge, been applied to achieve motion-based object segmentation. They have been used to enhance the performance of certain specific object-segmentation methods, as reported in <ref type="bibr" target="#b10">[11]</ref>. The PNN was used to enhance the segmentation results of a color-based classifier used to detect humans in specific scenes. The approach used a model foreground objects, rather than background. However, a supervising classifier was used to generate the training set for a PNN and to periodically retrain it, differing from the fully unsupervised approach proposed here.</p><p>An interesting application of PNNs in the domain of computer vision is reported in <ref type="bibr" target="#b9">[10]</ref>. The PNN was used for cloud classification and tracking in satellite imagery. The PNN was again a supervised classifier, and the approach did not incorporate background modeling.</p><p>Both applications of NNs to computer vision problems, discussed previously, are characterized by the use of certain problem specific classifiers to supervise the NN. In addition, the training of the network is not incremental and both approaches require the network to be retrained periodically.</p><p>More recently, Pajares <ref type="bibr" target="#b11">[12]</ref> proposed a Hopfield NN (HNN)based algorithm for change detection. As it is the case with the two preceding algorithms, this algorithm does not employ background modeling to achieve segmentation. The approach could potentially be used as a part of a filter-based approach instead of more traditional frame difference calculation methods. This would, however, severely increase the computational requirements, which represent a major advantage of filter-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBABILISTIC PIXEL CLASSIFICATION AND BACKGROUND MODEL</head><p>The goal of the probabilistic segmentation algorithm presented is to be able to classify the pixels in a frame of the sequence as foreground or background, based on statistics learned from the already observed frames of the video sequence. Different pixel features, such as intensity or red, green, or blue (RGB) color components, can be used as basis for segmentation. The value of these features changes with each new frame of the sequence. If the pixel feature used for segmentation is intensity, and the numerical intensity value of a pixel at frame of the sequence is for example 152, then the pixel feature value for that pixel at frame corresponds to its intensity value (152). In fact, the video sequence itself can be viewed as a set of pixel feature values varying over time. <ref type="bibr">Stauffer et al. [3]</ref> refer to these changing pixel feature values as "pixel processes." More formally, if is the location of a single pixel within the frames of the video sequence, then a pixel process of pixel is a set of all feature values of the pixel for all the frames in the sequence <ref type="bibr" target="#b0">(1)</ref> where is the pixel process at pixel, is the number of frames in the sequence, and is the feature value of the pixel at frame .</p><p>The observed features of pixels can be scalar in nature such as intensity or vectors (e.g., RGB values). Also, the features used for classification can be some higher level features extracted for location . All that is required is that in a certain frame the algorithm is able to decide whether the pixel at arbitrary location is pertinent to background or foreground, given the values and part of the process up to current frame . In the subsequent discussion, whatever the nature of the features actually used for classification, their value for a certain pixel will be simply referred to as pixel value.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows a plot of two sample pixel processes containing some 300 pixel values, corresponding to 10 s (300 frames) of a sample sequence. Plot 1(b) corresponds to a pixel of the water surface in the frame shown in 1(a), while 1(c) is a pixel pertinent to section of Fig. <ref type="figure" target="#fig_2">3</ref> on the far right of the frame.</p><p>In the proposed algorithm, the background model serves as the exclusive repository of the statistical information extracted form the observed parts of pixel processes. To classify pixels, a strategy that minimizes the expected risk of our decision is employed. Such strategies are known as Bayesian <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bayes Classification Strategy</head><p>The segmentation problem is formulated to enable the use of Bayes decision rule to achieve segmentation. For a certain frame , we are trying to estimate the dependent variable . The event of pixel at location being part of the foreground corresponds to , while when the pixel is pertinent to background.</p><p>is a function of the random variable taking values in the space of pixel feature values. Note that is itself a random variable. Using a Parzen estimator, we can construct the estimate of the pdf of . Although no direct information of the distribution of is available, suppose that one is able to infer some other knowledge about the conditional probability distribution of values of theta occurring when a certain value of has been observed. Then, a Bayesian decision rule that allows for the classification of pixels is formed as follows:</p><p>(2)</p><p>The costs are application-dependent and determined subjectively. In experiments presented in this paper, they are considered to be the same (i.e.,</p><p>). This means that the misclassification of a pixel is considered equally bad if it is labeled as foreground or as background. Thus, only knowledge of the pdf and the prior conditional probabilities of background and foreground occurring at pixel is needed to classify the pixel. However, the pdf and its shape is unknown, as is the case with the prior probabilities and as well. To classify the pixels, they have to be estimated. This must be done efficiently if one hopes to achieve real-time performance and segmentation of large frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Background Model</head><p>At each given frame of the sequence, the background model stores the values of estimated probabilities [</p><p>, and ] for each pixel of the frame. The prior probabilities and , for a specific value , are scalar values and can be stored efficiently. The values of the pdf should, in general, be known for any pixel value . This makes storing of the estimated pdf a significant problem.</p><p>If a certain shape is assumed for the pdf, it can be efficiently represented by the parameters of the proposed distribution <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b2">[3]</ref>. When this is not the case, the naive approach is to store the complete histogram of the pdf. Assuming that the feature used for classification is the RGB value of the pixels, coded as 24 b, this would result in a structure containing 256 ( 16.8 million) entries per pixel. However, as Li et al. <ref type="bibr" target="#b3">[4]</ref> show, it may not be necessary to store the whole histogram. They make a case for the assumption that the features that are part of the background tend to be located in the small subset of the his-togram, since the processes occurring in the background tend to be repetitive. Plots in Fig. <ref type="figure" target="#fig_0">1</ref> illustrate this effect, since the RGB values concentrate in small parts of the entire space of feature values. They were able to achieve adequate segmentation results for complex-background sequences by storing the statistics for 80 values (they covered of the histogram) <ref type="bibr" target="#b16">[17]</ref>. For other features that spanned an even larger space, Li et al. performed binning of the features, considering all values to be the same if they differed by less than 3 in each dimension.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, color-based segmentation was employed. RGB values were used as features for pixel classification. Each channel was coded with 8 b. Here, we turn to intensity of the pixel as a lowlevel feature, to perform baseline evaluation of the approach. The intensity is calculated as an average of the RGB values and coded with 8 b. Instead of representing the pdf in the form of histogram and applying the binning procedure, the pdf is estimated using Parzen estimators <ref type="bibr" target="#b17">[18]</ref>. A Parzen estimator of a pdf based on a set of measurements has the following analytical form: <ref type="bibr" target="#b2">(3)</ref> where is the dimension of the feature vector, is the number of patterns used to estimate the pdf (observed pixel values), are the pixel values observed up to the frame , is a smoothing parameter.</p><p>The Parzen estimator defined by ( <ref type="formula">3</ref>) is a sum of multivariate Gaussian distributions centered at the observed pixel values. As the number of observed values approaches infinity, the Parzen estimator converges to its underlying parent density, provided that it is smooth and continuous. The smoothing parameter controls the width of the Gaussians and its impact on the representation is treated in more detail in the following. The scaling factor preceding the sum in (3) has no impact when Bayes decision rule ( <ref type="formula">2</ref>) is used, and can be discarded. Using a Parzen estimator-based approach it would be enough to store all the values of a certain pixel observed in the known part of the sequence. This would still be inefficient, especially since the values of the background pixels concentrate in a small part of the value space and are, therefore, similar. A representation based on a relatively small number of Gaussians can be achieved if each Gaussian is used to represent a portion of observed patterns which are similar according to the predefined threshold <ref type="bibr" target="#b7">[8]</ref>. The procedure is similar to the binning used by Li et al., but the resultant pdf representation retains the notion of how close a value that is assigned to a particular Gaussian is to its center. This is not the case with the binning, since all the values within a bin are assigned the same value of pdf. Fig. <ref type="figure" target="#fig_1">2</ref> shows the plot of a Parzen estimator for three stored points with values in 2-D plane (e.g., if only and values for a pixel are considered). The horizontal planes in Fig. <ref type="figure" target="#fig_1">2</ref> represent the threshold values used to decide which feature values are covered by a single Gaussian. All features within the circle defined by the cross section of the Parzen estimate and the threshold plane are deemed close enough to the center of the peak to be within the cluster pertinent to the Gaussian. The selection of smoothing parameter value and the threshold controls the size of the circle and the cluster. Larger values of lead to less pronounced peaks in the estimation, i.e., make the estimation "smoother." For a fixed smoothing parameter value, lower values for the threshold will lead to larger coverage of the space of feature values by the estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BACKGROUND MODELING NEURAL NETWORK</head><p>In 1990, Specht <ref type="bibr" target="#b6">[7]</ref> introduced an NN architecture to be used as a Bayesian classifier, based on the Parzen estimation of the pdfs involved and a Bayes decision rule given by (2). He dubbed these networks PNNs. This architecture is a natural way to implement the classifier described in Section III. The background segmentation approach proposed here relies on an adapted PNN component to both classify the pixels and to store the model of the background within its weights. To achieve the functionality needed by a probabilistic video object segmentation algorithm, the adapted PNN component has been extended and combined with a WTA NN. This resulted in a fairly complex solution with some unique properties. Namely, the proposed solution is a truly unsupervised classifier, requiring no training set and it is capable of online learning. To the best of our knowledge, this is the first PNN-based framework to achieve these properties, despite the use of PNN classifiers in myriad application domains <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. The proposed neural network is referred to as background modeling neural network (BNN) since it is suitable to serve both as a statistical model of the background at each pixel position in the video sequences and highly parallelized background subtraction algorithm, as single BNN is used to model the pixel process and classify the pixel at a single pixel location .</p><p>The basic idea that forms the basis of all probabilistic background modeling and video object segmentation approaches discussed in Section II and the one presented here is a direct consequence of the definition of the background stated in the introduction: Feature values corresponding to background objects will occur more often than those pertinent to the foreground. In addition to this assumption, these methods share a set of common tasks that need to be performed to learn, update, and store the background model that enables efficient segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. These tasks, which have been used as guidelines in the design of BNN, are as follows:</p><p>1) storing the values of the pixel features and learning the probability with which each value corresponds to background/foreground; 2) determining the state in which new feature values should be introduced into the model (i.e., when the statistics already learned are insufficient to make a decision); 3) determining which stored feature value should be replaced with the new value. The two latter requirements are consequences of the fact that real systems are limited in terms of the number of feature values that can be stored to achieve efficient performance. In terms of the NN implementation proposed here, this translates into the number of patterns stored, i.e., the number of neurons used per pixel.</p><p>The structure of BNN, shown in Fig. <ref type="figure" target="#fig_2">3</ref>  input neuron is connected to all pattern neurons. The output of the pattern neurons is a nonlinear function of Euclidean distance between the input of the network and the stored pattern for that specific neuron. The only parameter of this subnet is the smoothing parameter of the Parzen estimator, discussed previously. The output of a single pattern neuron corresponds to the value of a single Gaussian of the pdf estimation for the observed pixel value. Fig. <ref type="figure" target="#fig_4">4</ref> shows the structure of a pattern neuron of classification subnet. The output of the summation units of the classification subnet is the sum of their inputs. The subnet has two summation neurons, each of them connected to all pattern neurons. The output values of the summation neurons correspond to initial Parzen estimates of joint probabilities and for the pixel value observed . These estimates are input to the last (output) layer, containing a single neuron. The final output of the network is a binary value indicating whether the pixel corresponds to foreground (output high) or background (output low), i.e., the result of the comparison in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classification Subnet 1) Topology and Learning:</head><p>The topology of the classification subnet is that of a PNN, as is the way in which the learned patterns are stored in the network. We discuss these briefly and refer the reader to <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref> for a more in-depth discussion. In a PNN, the patterns are stored in the weights connecting the input neurons and the pattern neurons. Originally, each pattern neuron corresponded to a single training pattern, and the weights of the connections between the input neurons and the pattern neuron were set to the values of the elements of the feature vector. This is inefficient and various clustering methods have been employed to reduce the number of patterns needed. The clustering method used in BNN is as proposed by Specht in <ref type="bibr" target="#b7">[8]</ref>. It requires no external procedure to determine whether a training pattern corresponds to a certain cluster. The pattern is simply fed to the network and if the output value of a pattern neuron exceeds a predefined threshold, the pattern is within the cluster covered by the pattern neuron. If no neuron achieves significant activation, the network is regarded as inactive and a pattern neuron is assigned to the pattern. It becomes a new cluster center. This procedure allows the network to adapt to the time-varying environment. In BNN, all the values in the pixel processes are considered training patterns and the network undergoes permanent adaptation, i.e., the training of the network is done online.</p><p>In the classification subnet of BNN, the weights between the pattern and summation neurons are used to store the prior probabilities inferred for the pattern neuron value <ref type="bibr">[ and ]</ref>. Since these values are unknown, rules were formed which allow the BNN to estimate them based on the observed parts of a pixel process and the frequency of specific feature values observed. The weights of these connections are updated with each new value of a pixel at a certain position received (i.e., with each frame), according to the following recursive equations:</p><p>(4) <ref type="bibr" target="#b4">(5)</ref> where is the value of the weight between the th pattern neuron and the background summation neuron at time , is the value of the weight between the th pattern neuron and the foreground summation neuron at time is the learning rate, is the number of the pattern neurons of BNN, is a clipping function defined by ( <ref type="formula">6</ref>), and indicates the neuron with the maximum response (activation potential) at frame , according to <ref type="bibr" target="#b5">(6)</ref> for neuron with maximum response otherwise <ref type="bibr" target="#b6">(7)</ref> Equations ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) express the notion that whenever an instance pertinent to a pattern neuron is encountered, the probability that the pattern neuron is activated by a feature value belonging to the background is increased. Naturally, if that is the case, the probability that the pattern neuron is excited by a pattern belonging to foreground is decreased. Conversely, the more seldom a feature value corresponding to a pattern neuron is encountered, the more likely it is that the patterns represented by it belong to foreground objects. Since the pdf value of a single pattern is increased, while all the others are decreased, the decay rate is set to a value smaller than the increase rate by a factor equal to the number of stored patterns (pattern neurons). By adjusting the learning rate , it is possible to control the speed of the learning process.</p><p>2) Convergence of the Learning Process: Let and denote the number of frames in the sequence in which certain feature value of a pixel is observed and the number of frames in which it is not observed, respectively. Naturally, the overall number of frames in the sequence is <ref type="bibr" target="#b7">(8)</ref> If a simplifying assumption is made, that the feature value is not observed for the first frames of the sequence and then observed for frames, the weights for the pattern neuron corresponding to the feature value are determined by ( <ref type="formula">4</ref>)-( <ref type="formula">7</ref>) (9) <ref type="bibr" target="#b9">(10)</ref> where corresponds to the initial weight set when the pattern is first observed. The initial weight has low value</p><p>, to indicate that it is not likely that the value observed for the first time corresponds to a background pixel. In addition, the values of the learning parameter are between 0 and 1. Observe that <ref type="bibr" target="#b10">(11)</ref> Therefore <ref type="bibr" target="#b11">(12)</ref> Thus, if a pixel value is encountered in consecutive frames, it will be classified as background with maximum support. On the other hand, the confidence that a feature value belongs to the background will decay from the maximum to if it is not encountered for frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Activation and Replacement Subnets</head><p>The adaptation of the classification subnet requires the BNN to be able to detect the state of low activation of all the neurons in the net. This indicates that the feature value fed to the network is not within the clusters stored and that the feature value should be stored in the network weights as a new cluster center. The activation part of BNN is concerned with the detection of this state. In addition, when the new value is to be stored, the network must be able to decide which pattern neuron's weights are to be replaced with the new ones. This is the function of the replacement subnet.</p><p>The activation and replacement subnets are WTA NNs. A WTA network is a parallel and fast way to determine minimum or the maximum of a set of values. In particular, these subnets are extensions of one-layer feedforward MAXNET (1LF-MAXNET) proposed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>To detect the state of low activation in BNN, the activation subnet determines which of the neurons of the network has maximum activation (output) and whether that value exceeds a threshold provided as a parameter to the algorithm. If it does not, the BNN is considered inactive and new cluster center learning process is initiated. If the network is inactive, the pixel is considered to belong to a foreground object, since this is a value that has not been present in the background model.</p><p>The first layer of the activation network has the structure of a 1LF-MAXNET network and a single neuron is used to indicate whether the network is active. The output of the neurons of the first layer of the network can be expressed in the following form (see Fig. <ref type="figure" target="#fig_5">5</ref>): <ref type="bibr" target="#b12">(13)</ref> where if if <ref type="bibr" target="#b13">(14)</ref> As ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>) indicate, the output of the first layer of the activation subnet will differ from 0 only for the neurons with maximum activation and will be equal to the maximum activation. In Fig. <ref type="figure" target="#fig_2">3</ref>, these outputs are indicated with and . The structure of a processing neuron of 1LF-MAXNET is shown in Fig. <ref type="figure" target="#fig_5">5</ref>. A single neuron in the second layer of the activation subnet is concerned with detecting whether the BNN is active or not and its function can be expressed in the following form: <ref type="bibr" target="#b14">(15)</ref> where is given by ( <ref type="formula">14</ref>) and is the activation threshold, which is provided to the network as a parameter. Finally, the replacement subnet in Fig. <ref type="figure" target="#fig_2">3</ref> can be viewed as a separate neural network with the unit input. However, it is inextricably related to the classification subnet since each of the replacement subnet first-layer neurons is connected with the input via synapses that have the same weight as the two output synapses between the pattern and summation neurons of the classification subnet. Each pattern neuron has a corresponding neuron in the replacement net.</p><p>The function of the replacement net is to determine the pattern neuron that minimizes the criterion for cluster center replacement, expressed by the following: replacement criterion ( <ref type="formula">16</ref>)</p><p>The criterion is a mathematical expression of the idea that those patterns that are least likely to belong to the background and those that provide least confidence to make the decision should be the first to be eliminated from the model. The neurons of the first layer calculate the negated value of the replacement criterion for the pattern neuron they correspond to. This inversion of the sign is done to enable the use of 1LF-MAXNET component to detect the lowest value of the criterion. The second layer of the network is a 1LF-MAXNET that yields nonzero output corresponding to the pattern neuron to be replaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware Implementation Considerations</head><p>Suitability of the proposed solution for hardware implementation is a primary concern in our research.</p><p>Since each neuron within a layer of the BNN is able of performing its function in parallel, the proposed approach is parallelized on a subpixel level. More precisely, it is parallel on the level of a single pattern stored for a pixel, since the pattern neurons of the classification subnet are able to perform their calculations in parallel.</p><p>The presence of the WTA networks eliminates the need for sorting operations employed both in the hybrid approach of Li et al. and MoG. In addition, there is no need to perform any operations on the whole frame, such as histogram extraction for adaptive thresholding performed in the approach of Li et al. <ref type="bibr" target="#b23">[24]</ref>, which limit the extent to which the approach can be parallelized and make the speed of segmentation dependent on the size of the frame. No publications have been identified dealing with parallel or hardware implementations of MoG and the approach of Li et al.. However, Li et al. report <ref type="bibr" target="#b3">[4]</ref> that their approach can achieve processing speed of 15 fps for 160 120-pixel-large frames and 3 fps for 320 240-pixel-large frames when run on a 1.7-GHz Pentium CPU. For MoG containing five Gaussians per pixel, processing rate of 11-13 fps (frame size 160 120 pixels), on an SGI O2 with an R10000 processor, has been reported <ref type="bibr" target="#b2">[3]</ref>.</p><p>The speed of the segmentation of the proposed approach, in a parallel hardware implementation, does not depend on the size of the frame. The delay of the network (segmentation time) corresponds to the time needed by the signal to propagate through the network and time required to update it. In a typical field-programmable gate array (FPGA) implementation this can be done in less than 20 clock cycles, which corresponds to a 2-ms delay through the network, for an FPGA core running at 10-MHz clock rate. Thus, the networks themselves are capable of achieving a throughput of some 500 fps, which is more than sufficient for real-time segmentation of video sequences.</p><p>BNN is clearly suitable to serve as basis for efficient hardware implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND RESULTS</head><p>To evaluate the performance of the NN, a sequential PC-based implementation has been developed. Previously, experiments have been conducted on a set of sequences pertinent to marine surveillance <ref type="bibr" target="#b5">[6]</ref>. Here, a set of diverse sequences containing complex background conditions<ref type="foot" target="#foot_0">1</ref> was used. The results of the segmentation were evaluated both qualitatively and quantitatively, using a set of ground-truth frames provided by the same authors for the different sequences. To evaluate the performance of the approach, a well-known probabilistic modeling approach MoG <ref type="bibr" target="#b2">[3]</ref> and the approach of Li et al. <ref type="bibr" target="#b3">[4]</ref>, have been implemented and the segmentation results of different algorithms compared. The same pixel feature, namely its intensity value, was used for all approaches. Since all three are general in terms of features used, the intent is to compare baseline modeling and classification ability rather than explore the problem of selecting the best features in order to achieve best segmentation results. Note, however, that use of different features can affect the segmentation quality and that employing different features should be considered as a possibility both in applications and as a venue to further explore the quality of the proposed method.</p><p>The NNs used in the experiments are fairly simple. The simulation application implements BNNs containing 30 pattern neurons in their classification subnets. With the additional two summation and one output neuron required in the classification subnet (see Fig. <ref type="figure" target="#fig_2">3</ref>), the total number of neurons in this part of BNN is 33. The input neurons of the classification shown in Fig. <ref type="figure" target="#fig_2">3</ref> just map the input to the output and need not be implemented as such. The number of neurons required in the activation and replacement subnets is determined by the number of pattern neurons of the classification subnet. These two subnets attribute for additional 31 neurons in the activation subnet and 60 processing units in the replacement subnet. Thus, the total number of neurons in a single BNN used is 124. A single BNN is used to model the background at a single pixel.</p><p>The learning rate varied from sequence to sequence between three different settings (0.05, 0.01, and 0.003), as shown in Table <ref type="table" target="#tab_1">I</ref>. The learning rates have been set based on the observed speed of motion of the objects in the foreground and rate of changes in the background. Larger learning rates enable the network to learn the changes corresponding to background faster but lead to faster absorption of stationary foreground objects by the background model. Lower learning rates make the network slower to adapt to sudden changes in the background (e.g., due to switching of lights) but will make the model less prone to errors due to absorption of stationary foreground objects.</p><p>The smoothing parameter for the classification subnet used was set to 7 (a value approximately twice the size standard deviation of the intensity values for a single pixel). The activation threshold of the activation subnet was set to 0.5, meaning that the values further than form the pattern neuron weights were deemed outside the scope of the cluster covered by that particular neuron. The results for MoG were obtained for a mixture containing 30 Gaussians, to ensure a fair comparison. While the authors suggest the use of 3-5 Gaussians in the mixture to achieve realtime performance <ref type="bibr" target="#b2">[3]</ref>, they speculated that a larger number of Gaussians would lead to the better segmentation results. The initial value of the deviation for MoG was set to the value of the smoothing parameter of BNN <ref type="bibr" target="#b6">(7)</ref> while the threshold selecting the number cases to be covered by Gaussians used in the decision process was set to 0.5. A Gaussian covered the values within 2.5 standard deviations of the mean, as suggested in <ref type="bibr" target="#b2">[3]</ref>. The learning rates for the MoG were set to the same values as for the BNN.</p><p>In the experiments with the approach of Li et al., the number of pixel intensity values and pixel intensity cooccurrence values was adopted from <ref type="bibr" target="#b16">[17]</ref>. No binning of the intensity values was performed, while bins of width 4 were used for cooccurrence features. In addition, to ensure a fair comparison, the probabilistic background model has been updated based on the initial segmentation results, before the application of morphological processing, as it was done for the other two approaches. The learning rates used for the experiments were those suggested in <ref type="bibr" target="#b3">[4]</ref>. While Li et al. suggest that the same learning rates should be used for their approach and MoG <ref type="bibr" target="#b3">[4]</ref>, they do not consider the interplay of two different learning rates used on the two levels of their algorithm. Thus, this suggestion was not followed in the experiments performed here.</p><p>Morphological operations such as morphological closing and opening as well as connected components algorithm and the elimination of small objects have been used to enhance the segmentation results, as it was done in <ref type="bibr" target="#b3">[4]</ref>. <ref type="foot" target="#foot_1">2</ref>A possible alternative to morphological processing could be the use of a still image segmentation algorithm, such as that proposed by Blekas et al. <ref type="bibr" target="#b24">[25]</ref> to enhance the segmentation results. The resulting algorithm would have the benefit of exploiting spatial information. To improve the performance, still image segmentation could be localized to regions already detected as foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Results</head><p>Qualitative evaluation of the segmentation results is performed by visual inspection. In this section, a number of representative frames from the test sequences is presented. We discuss the nature and the causes of complexity of background changes. The discussion is limited and the reader is referred to <ref type="bibr" target="#b3">[4]</ref> for a more in-depth treatment.</p><p>Ten testing sequences were obtained in several different environments. Rather than following the classification originally   used in <ref type="bibr" target="#b3">[4]</ref>, they are grouped here based on the sources of complexity in background variation, pertinent to each environment. The following three groups of sequences (environments) are identified:</p><p>1) outdoor environments; 2) small indoor environments; 3) large (public) indoor environments. The sources of complexity in the sequences obtained in outdoor environments are usually due to objects moved by wind (e.g., Fig. <ref type="figure" target="#fig_2">3</ref> or waves) and illumination changes due to changes in cloud cover. For small indoor environments, such as offices, the source of complexity relates mostly to objects such as curtains or fans moving in the background or screens flickering. The illumination changes are mostly due to switching lights on and off. Large public indoor environments (e.g., subway stations, airport halls, shopping centers, etc.) are characterized by lighting distributed from the ceiling and presence of secular surfaces, inducing complex shadow and glare effects. In addition, these spaces can contain large moving objects such as escalators and elevators.</p><p>Initial results presented in <ref type="bibr" target="#b5">[6]</ref> are concerned with outdoor sequences with background containing water surfaces as well as objects undergoing motion due to wind. To evaluate the segmentation results for the outdoor environments further, four sequences were used. The first sequence is of a campus environment (CAM), showing vehicles and pedestrians moving along a road in front of a thicket of Fig. <ref type="figure" target="#fig_2">3</ref> moving rapidly in the wind. Second is that of a buffet sidewalk sequence (SW) with pedes-trians moving along. The complexity of the background in the third sequence (FT) is due to a water fountain. The fourth (WS) is a sequence of a person at walking at a waterfront, and the complexity is due to the water surface in the background. The proposed algorithm was able to cope with complex background variation in all these sequences. Representative frames and corresponding segmentation results are given in Figs. <ref type="figure" target="#fig_6">6</ref><ref type="figure" target="#fig_7">7</ref><ref type="figure" target="#fig_8">8</ref><ref type="figure" target="#fig_9">9</ref>, for the sequences CAM, SW, FT, and WS, respectively. The figures show the original frame, the segmentation result obtained using BNN, ground-truth frame, segmentation result of MoG, and segmentation result for the model of the background proposed by Li et al., from left to right. The ground truths are manually segmented frames. All the images referred to in this section have the same format.</p><p>Two sequences are used to test the performance for small indoor environments. The first was captured in a meeting room (MR) with the curtain moving in the background. The second (LB) was taken in the lobby of the office building, with the lights switching on and off. Representative frames for the two sequences are shown in Figs. <ref type="figure" target="#fig_10">10</ref> and<ref type="figure" target="#fig_11">11</ref>.</p><p>Four sequences pertinent to large indoor environments were used. They were taken in a shopping center (SC), an airport (AP), a buffet restaurant (BR) <ref type="bibr" target="#b1">[2]</ref>, and a subway station (SS). They illustrate the capability of the approach to cope with shadow effects. The SS sequence contains moving escalators in the background. Segmentation results for these sequences are illustrated in Figs. 12-15. The algorithm was able to learn the behavior of the escalators in the SS sequence and absorb     them into the background. The algorithm does not incorporate shadow cancellation and has in some cases segmented the shadows as foreground objects, as shown in 14(b). A possible solution to this problem lies in the use of features able to cope with these effects, as are features in gradient domain proposed in <ref type="bibr" target="#b25">[26]</ref>. In addition, a more sophisticated approach, such as that proposed by Gu et al. <ref type="bibr" target="#b26">[27]</ref>, could be applied to the segmented regions, to remove shadows at the higher levels of processing.</p><p>An additional weakness of the proposed algorithm is the tendency to incorporate foreground objects that stop moving for extended periods of time, into the background. This is a weakness of all background modeling segmentation approaches, stemming from the basic definitions of background   and foreground stated previously. A possible solution to this problem is the use of top-down information from higher level object tracking and recognition algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Evaluation</head><p>For each of the ten test sequences, we calculate a measure of the segmentation accuracy following the methodology used in <ref type="bibr" target="#b16">[17]</ref>. If is a detected (segmented) region and the corresponding ground truth, then the similarity measure between these two regions is defined as <ref type="bibr" target="#b16">(17)</ref> The similarity of the regions reaches the maximum value of 1 if they are the same. Otherwise, it varies between 1 and 0 according to how similar the regions are. is the measure of the overall misclassification. The values of obtained for test sequences along with the values provided by Li et al. for the MoG and their own approach are given in Table <ref type="table" target="#tab_2">II</ref>. The average values obtained for the three approaches are 0.566, 0.494, and 0.184 for BNN, MoG, and the approach of Li et al., respectively.</p><p>As Table <ref type="table" target="#tab_2">II</ref> indicates, proposed approach achieved better segmentation results than the MoG containing the same number of Gaussians as there are pattern neurons in the BNN. The approach proposed by Li et al. performed poorly when only intensity values and their cooccurrences are used for segmentation. This is quite different than the result reported in <ref type="bibr" target="#b3">[4]</ref>. The discrepancy is probably due to the fact that the authors used higher level features and first-order moments in addition to intensity values to classify pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>A novel background modeling and subtraction approach for video object segmentation in complex sequences has been proposed. The proposed method is probabilistic and relies on an NN to achieve estimation of required pdfs and segmentation. New BNN architecture has been proposed and rules for adaptation of its weights have been formulated. The network is a truly unsupervised classifier, differing from previously published approaches. The algorithm is parallel on a subpixel level. Of the published segmentation approaches, it is most suitable for an efficient hardware implementation.</p><p>The approach was evaluated on a set of diverse sequences, pertinent to the automatic surveillance application domain. Good segmentation results have been obtained for these complex sequences. The proposed approach represents an improvement in segmentation ability when compared to a well-known pure probabilistic approach MoG. For several sequences, MoG featuring 30 Gaussians achieved better results (FT, WS, MR, and LB). This result indicates that the proposed approach could benefit from introduction of adaptive kernel width and center. Both MoG and the proposed approach performed significantly better than the hybrid model-based approach of Li et al., when the pixel intensity values were used as the basis for segmentation.</p><p>The approach is independent of the features used to achieve segmentation and use of features other than intensity values should be explored to enhance the segmentation results, especially in terms of shadow suppression. The approach would also benefit from the introduction of mechanisms that would allow it to exploit spatial information, typically used in still-image segmentation. Currently, the extension of the approach to use the feedback from higher processing modules of object tracking to enhance the segmentation is being examined. Such top-down control could be used to cope with the problem of foreground objects being absorbed by the background.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample pixel process plots. (a) Sequence frame. (b) Water-surface pixel. (c) Tree pixel.</figDesc><graphic coords="4,126.42,66.18,337.00,624.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Plots of Parzen estimators for different values of "smoothing parameter." (a) = 12. (b) = 24.</figDesc><graphic coords="5,44.64,66.22,504.00,215.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Structure of BNN.</figDesc><graphic coords="6,146.22,66.34,297.00,152.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, has three distinct subnets, corresponding to each of the tasks enumerated previously: classification, activation, and replacement. The classification subnet is the adapted PNN discussed previously. It is a central part of BNN shown in Fig. 3. The classification subnet contains four layers of neurons annotated at the far right of Fig. 3. Input neurons of this network simply map the inputs of the network, which are the values of the features for a specific pixel. Each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pattern neuron of PNN.</figDesc><graphic coords="6,334.62,259.78,184.00,176.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Processing neuron of a 1LF-MAXNET structure.</figDesc><graphic coords="8,71.04,69.28,185.00,188.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Segmentation of the CAM. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (d) Li et al. result.</figDesc><graphic coords="10,43.14,67.10,504.00,85.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Segmentation of the SW. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="10,43.14,185.44,504.00,83.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Segmentation of the FT. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="10,43.14,301.08,504.00,87.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Segmentation of the WS. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="11,44.64,66.82,504.00,86.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Segmentation of the MR. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="11,44.64,186.78,504.00,87.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Segmentation of the LB. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="11,44.64,307.80,504.00,87.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Segmentation of the SC. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="11,44.64,429.04,504.00,86.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Segmentation of the AP. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="11,44.64,549.44,504.00,88.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Segmentation of the BR. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="12,43.14,66.26,504.00,82.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Segmentation of the SS. (a) Original frame. (b) BNN segmentation result. (c) Ground truth. (d) MoG result. (e) Li et al. result.</figDesc><graphic coords="12,43.14,180.98,504.00,88.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Neural Network Approach to Background Modeling for Video Object Segmentation Dubravko Culibrk, Oge Marques, Member, IEEE, Daniel Socek, Member, IEEE, Hari Kalva, and Borko Furht</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I LEARNING</head><label>I</label><figDesc>RATE USED IN EXPERIMENTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SIMILARITY</head><label>II</label><figDesc>MEASURE VALUES FOR EACH TEST SEQUENCE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Provided by Li et al.<ref type="bibr" target="#b3">[4]</ref> and publicly available at http://perception.i2r.a-star. edu.sg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>All the segmentation results, along with the binaries used for segmentation and MATLAB scripts to perform the morphological processing and extract the statistics used for quantitative comparison can be found at http://mlab.fau.edu.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank S. Magliveras for his support and valuable suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Center for Coastline Security and Center for Cryptography and Information Security.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">W4 real-time surveillance of people and their activities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="809" to="830" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wallflower: Principles and practice of background maintenance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using realtime tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical modeling of complex backgrounds for foreground object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1459" to="1472" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid color-based foreground object detection method for automated marine surveillance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Socek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Concepts Intell. Vis. Syst. Conf. (ACIVS)</title>
		<meeting>Adv. Concepts Intell. Vis. Syst. Conf. (ACIVS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural network approach to Bayesian background modeling for video object segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Theory Appl. (VISAPP)</title>
		<meeting>Int. Conf. Comput. Vis. Theory Appl. (VISAPP)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Specht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general regression neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Specht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="1991">NOv. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-layer feedforward neural network fast maximum/ minimum determination</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="page" from="1583" to="1585" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal updating scheme for probabilistic neural network with application to satellite cloud classification-further results</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Azimi-Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H V</forename><surname>Haar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient fully unsupervised video object segmentation scheme using an adaptive neural-network classifier architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ntalianis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="616" to="630" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Hopfield neural network for image change detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pajares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1250" to="1264" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moving object recognition using an adaptive background memory</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Karmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time-varying Image Processing and Moving Object Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Frame-rate omnidirectional surveillance and tracking of camouflaged and occluded targets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Micheals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Visual Surveillance</title>
		<meeting>IEEE Workshop Visual Surveillance</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection and tracking in an open and dynamic world</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Performance Eval. Tracking Surveillance (PETS)</title>
		<meeting>2nd IEEE Int. Workshop Performance Eval. Tracking Surveillance (PETS)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moving object detection and tracking based on background subtraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guangyou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Object Detection Classif</title>
		<meeting>SPIE Object Detection Classif</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Foreground object detection from videos containing complex background</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM Int. Conf. Multimedia</title>
		<meeting>11th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Graybill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<publisher>Macmillan</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to the Theory of Statistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic neural network array architecture for ecg classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Belina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Int. Conf</title>
		<meeting>Annu. Int. Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="807" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the generalization ability of neural-network classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Musavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hummels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalantri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="659" to="663" />
			<date type="published" when="1994-06">Jun. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervised texture classification using a probabilistic neural network and constraint satisfaction model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="516" to="522" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optical Chinese character recognition using probabilistic neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Thibadeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1279" to="1292" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Thresholding for change detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>6th Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A spatially constrained mixture model for image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Blekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Lagaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="498" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast video segmentation algorithm with shadow cancellation, global motion compensation, and adaptive threshold techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="732" to="748" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dubravko Culibrk received the B.Eng. degree in automation and system control and the M.Sc. degree in computer engineering from the University of Novi Sad</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 and 2005, respectively, and the Ph.D. degree in computer engineering from Florida Atlantic University</title>
		<meeting><address><addrLine>Novi Sad, Serbia; Boca Raton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005. 2006</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="692" to="698" />
		</imprint>
	</monogr>
	<note>His research interests include video and image processing. computer vision, NNs and their applications, cryptography, hardware design, and evolutionary computing</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">He is affiliated with the Center for Coastline Security Technology and the Center for Cryptology and Information Security at FAU. He worked and served as a consultant for various IT companies such as IBM, Panasonic, Matsushita Electric Industrial, and Avaton/ Crypton. His current research interests include multimedia security, biometrics-based authentication, digital image and video processing, and applications of NNs</title>
		<author>
			<persName><forename type="first">Cefet-Pr)</forename><surname>Curitiba</surname></persName>
		</author>
		<author>
			<persName><surname>Brazil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000, and the M.Sc. degree in mathematics and the Ph.D. degree in computer science from Florida Atlantic University</title>
		<meeting><address><addrLine>Eindhoven; Boca Raton; Boca Raton; Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986">1986. 1989 and 2000. 2002 and 2006</date>
		</imprint>
		<respStmt>
			<orgName>Centro Federal de Educacão Tecnológica do Paraná ; Philips International Institute of Technological Studies ; Department of Computer Science and Engineering, Florida Atlantic University ; Department of Computer Science and Engineering, Florida Atlantic University (FAU</orgName>
		</respStmt>
	</monogr>
	<note>IEEE Computer Society, and the honor societies of Phi Kappa Phi and Upsilon Pi Epsilon. Daniel Socek (S&apos;05-M&apos;06) received the B.Sc. degree in computer science from the University of Nebraska-Lincoln. Dr. Socek was awarded the Graduate Fellowship for Academic Excellence and the Dr. Daniel B. and Aural B. Newell Doctoral Fellowship in 2003 and 2004, respectively, for his outstanding academic and research performance</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
