<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SYNCHRONOUS TRANSFORMERS FOR END-TO-END SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengkun</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SYNCHRONOUS TRANSFORMERS FOR END-TO-END SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Asynchronous Problem</term>
					<term>Online Speech Recognition</term>
					<term>Synchronous Transformer</term>
					<term>Chunk by Chunk</term>
					<term>Forward-Backward Algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For most of the attention-based sequence-to-sequence models, the decoder predicts the output sequence conditioned on the entire input sequence processed by the encoder. The asynchronous problem between the encoding and decoding makes these models difficult to be applied for online speech recognition. In this paper, we propose a model named synchronous transformer to address this problem, which can predict the output sequence chunk by chunk. Once a fixed-length chunk of the input sequence is processed by the encoder, the decoder begins to predict symbols immediately. During training, a forward-backward algorithm is introduced to optimize all the possible alignment paths. Our model is evaluated on a Mandarin dataset AISHELL-1. The experiments show that the synchronous transformer is able to perform encoding and decoding synchronously, and achieves a character error rate of 8.91% on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Attention-based sequence-to-sequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer model <ref type="bibr" target="#b1">[2]</ref>, have shown great success in various tasks, e.g. neural machine translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image captioning <ref type="bibr" target="#b2">[3]</ref> and speech recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>For conventional attention-based sequence-to-sequence models, the inference process can be divided into two stages. The encoder first processes an entire input sequence into a high-level state sequence. After that, the decoder predicts the output sequence conditioned on the previous predicted symbol and context vector extracted from the entire encoded state sequence. This makes the models encode and decode sequences asynchronously, and prevents it from being applied for online speech recognition. There are some works trying to solve this problem. Tjandra et al. <ref type="bibr" target="#b6">[7]</ref> propose a local monotonic attention mechanism that forces the model to predict a central position at every decoding step and calculate soft attention weights only around the central position.</p><p>However, it's difficult to accurately predict the next central position just based on limited information. Monotonic chunkwise attention <ref type="bibr" target="#b7">[8]</ref> is proposed to adaptively split the encoded state sequence into small chunks based on the predicted selection probabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type="bibr" target="#b8">[9]</ref> utilizes the spikes produced by connectionist temporal classification (CTC) model to split the sequence into many state chunks, and then the decoder predicts the output sequence in a chunkwise way. However, triggered attention requires forced alignment to assist model training. Most of the proposed models introduce additional components and have very tricky training methods.</p><p>In this paper, we propose a synchronous transformer model (Sync-Transformer), which can perform encoding and decoding at the same time. The Sync-Transformer combines the transformer <ref type="bibr" target="#b5">[6]</ref> and self-attention transducer (SA-T) <ref type="bibr" target="#b9">[10]</ref> in great depth. Similar to the original transformer, the Sync-Transformer has an encoder and a decoder. In order to eliminate the dependencies of self-attention mechanism on the future information, we first force every node in the encoder to only focus on its left contexts and ignore its right contexts completely. Once a fixed-length chunk of state sequence is produced by the encoder, the decoder begins to predict symbols immediately. Similar to the Neural Transducer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, the decoder generates the output sequence chunk by chunk. However, restricted by the time-dependent property of RNNs, the Neural Transducer model only optimizes the approximate best alignment path corresponding to the chunk sequence. By contrast, we adopt a forwardbackward algorithm to optimize all possible alignment paths and calculate the negative log loss function as the same as the RNN-Transducer <ref type="bibr" target="#b13">[14]</ref> and SA-T <ref type="bibr" target="#b9">[10]</ref>. We evaluate our Sync-Transformer on a Mandarin dataset AISHELL-1. The experiments show that the Sync-Transformer is able to encode and decode sequences synchronously and achieves a comparable performance with the transformer.</p><p>The remainder of this paper is organized as follows. Section 2 describes our proposed Sync-Transformer. Section 3 presents our experimental setup and results. The conclusions and future work will be given in Section 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYNCHRONOUS TRANSFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model</head><p>Similar to the transformer <ref type="bibr" target="#b1">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as depicted in Fig. <ref type="figure" target="#fig_1">1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a), we put a 2D convolution front end at the bottom of the encoder to process the input speech feature sequences simply, including dimension transformation(transform feature dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type="bibr" target="#b1">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequence can be expressed as s 1:L , where T and L are the lengths of these two sequences respectively.</p><p>In order to get rid of the dependencies on the entire input state sequences, we make the following modifications on the original self-attention encoder, as depicted in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. On the one hand, we force every node in the encoder to focus on its left context and ignore its right contexts completely during calculating self-attention weights. Although each intermediate node can only model local dependencies information, the top node of the encoder can still model long-term dependencies. On the other hand, similar to transformer-xl <ref type="bibr" target="#b14">[15]</ref>, the encoder of Sync-Transformer processes the input sequence chunk by chunk. There is an overlap between two adjacent chunks to maintain a smooth transition of information between chunks. For the processed input sequence s 1:L , the encoder can split it into M encoded state chunks C 1:M . This means that the calculation of attention weights just depend on a W -length chunk rather than the entire input sequence. Let B be the overlapping length of two adjacent chunks. The relationship between M and L can be expressed as</p><formula xml:id="formula_0">M = L−W W −B + 1 . C 1:M = Encoder(s 1:L )<label>(1)</label></formula><p>At every decoding step, the decoder predicts a symbol conditioned on the previous predicted symbols y 0:u−1 (0 ≤ u ≤ U + 1) and one chunk. Once a blk symbol is predicted, the decoder will switch to the next chunk and continue decoding. This process can be expressed by the following formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(y u |y</head><formula xml:id="formula_1">0:u−1 , C m ) = Decoder(y 0:u−1 , C m )<label>(2)</label></formula><p>where C m (1 ≤ m ≤ M ) represents the m-th chunk produced by the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training</head><p>The training process is divided into two steps. In order to accelerate the convergence, we first use a trained transformer model to initialize the parameters of Sync-Transformer. Then apply the following forward-backward algorithm to train a Sync-Transformer. The encoder splits the input sequence into M chunks. The decoding process in every chunk ends with blk . It is difficult to figure out which chunk each target symbol should belong to. Therefore, we construct an output probability lattice graph as shown in Fig. <ref type="figure" target="#fig_1">1(c</ref>) by calculating the probabilities of all target symbols in each chunk. Given the input sequence x 1:T , the probability of the output y 1:U is calculated by summing over the probabilities of all possible alignment paths.</p><formula xml:id="formula_2">p(y 1:U |x 1,T ) = y∈Y p(y 1 , y 2 , ..., y (U +M ) |x 1:T )<label>(3)</label></formula><p>Where Y represents the set of all possible alignment paths. It's intractable and inefficient to calculate the probability p(y 1:U |x 1,T ) by enumerating all possible alignment paths. Therefore, like transducer-based models in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, we introduce a forward-backward algorithm to calculate the probabilities efficiently.</p><p>The forward variable α(m, u) means the sum of the probability of all the possible paths, which begin with the start symbol y 0 ( = blk ) and end with y u in m-th chunk. Given m-th chunk and the predicted symbol sequence y 0:u−1 , the probabilities of predicting blk and y u are represented as φ(m, u − 1) and y u (m, u − 1) respectively. For all 1 ≤ m ≤ M and 1 ≤ u ≤ U + 1, the forward variables can be calculated recursively using</p><formula xml:id="formula_3">α(m, u) = α(m − 1, u)φ(m, u) + α(m, u − 1)y u (m, u − 1)<label>(4)</label></formula><p>And all paths begin with y 0 ( = blk ), it means α(1, 1) = 1.</p><p>The probability p(y 1:U |x 1,T ) can be expressed by the forward variable at the terminal node.</p><formula xml:id="formula_4">p(y 1:U |x 1,T ) = α(M, U + 1)φ(M, U + 1)<label>(5)</label></formula><p>Similarly, the backward variable β(m, u) means the sum of the probabilities of all possible paths, which begin with y u in m-th chunk and end with y U +1 (= blk ) in the last chunk. The backward variables can be expressed as</p><formula xml:id="formula_5">β(m, u) = β(m + 1, u)φ(m, u) + β(m, u + 1)y u+1 (m, u)<label>(6)</label></formula><p>where the initial condition β(M, U + 1) = φ(M, U + 1).</p><p>Given an input feature sequence x 1,T and a target sequence y 1:U , the probability p(y 1:U |x 1,T ) is equal to the sum of α(m, u)β(m, u) over any top-left to bottom-right diagonal through the nodes. That is, ∀n : 2</p><formula xml:id="formula_6">≤ n ≤ U + M + 1 p(y 1:U |x 1,T ) = (m,u):m+u=n α(m, u)β(m, u)<label>(7)</label></formula><p>We train the model to minimize the negative log-loss function L = −lnp(y 1:U |x 1,T ). The calculation of gradients is exactly the same as RNN-T <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference</head><p>The inference process is displayed in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. During inference, the decoder will predict the output symbols conditioned on a fixed-length chunk of encoded state sequences and all the previous predicted non-blank symbols. it might predict one or more symbols in a chunk. Once a blk is predicted, It will switch to the next chunk and continue decoding. The decoder will repeat the above steps till all the chunks are processed. To simplify the inference process, we don't try to merge some alignment paths with the same prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>In this work, all experiments are conducted on a public Mandarin speech corpus AISHELL-1<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b16">[17]</ref>. The training set contains about 150 hours of speech (120,098 utterances) recorded by 340 speakers. The development set contains about 20 hours (14,326 utterances) recorded by 40 speakers. And about 10 hours (7,176 utterances) of speech is used to be test set. The speakers of different sets are not overlapped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiment Setup</head><p>For all experiments, we use 40-dimensional Mel-filter bank coefficients (Fbank) features computed on a 25ms window with a 10ms shift. Each feature is re-scaled to have zero mean and unit variance for each speaker. We chose 4232 characters (including a blank symbol ' blk ' and a unknown symbol ' unk ' ) as model units.</p><p>We utilize Kaldi<ref type="foot" target="#foot_1">2</ref> for data preparation. And our Sync-Transformer is built on ESPNet <ref type="bibr" target="#b17">[18]</ref> and warp-rnnt <ref type="foot" target="#foot_2">3</ref> . It consists of 6 encoder blocks and 6 decoder blocks. There are 8 heads in multi-head attention. The 2D convolution front end utilizes two-layer time-axis CNN with ReLU activation, stride size 2, channels 256 and kernel size 3. The output size of the multi-head attention and the feed-forward layers are 256. In order to accelerate the convergence, we replace the ReLU activation function in the feed-forward network with gated linear units <ref type="bibr" target="#b18">[19]</ref>. We empirically set the left context of every node in the encoder to 20 and the right context to 0. More context parameter settings will be explored in the future. What's more, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type="bibr" target="#b1">[2]</ref>.</p><p>During decoding, we use a beam search with a width of 5 for all the experiments. And set the maximum length of symbols generated in a chunk is 10. We use character error rate (CER) to evaluate the performance of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Comparison of different window lengths</head><p>We first explore how chunks with different lengths W affect the performance of Sync-Transformer. For every experiment, the overlapping range of adjacent chunks is set to 20% of the chunk length. As shown in Table <ref type="table" target="#tab_0">1</ref>, the Sync-Transformer with chunk length 10 can achieve a CER of 9.06% on the test set. When the length is reduced to 5, Sync-Transformer still performs well. However, if the length is greater than 20, it leads to severe performance degradation. We suppose that there may be more than one character in a long chunk, which might make the model difficult to predict the output sequence accurately. The length of speech segment represented by a fixedlength is W × 4 × 10ms, where 4 means that 2D convolution front end can reduce the speech length by 4 times and 10ms represents the frame shift. The Sync-Transformer can achieve a competitive performance depending on a latency of 0.4s. If the overlap is taken into account, it is actually 0.32s.</p><p>When the chunk length is 1, Sync-Transformer is similar to a transducer model, which decodes a sequence frame by frame. In turn, when the length is large enough, there is one chunk for any utterances. In this case, it is equivalent to a transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Comparison of different overlap lengths</head><p>Next, we try to figure out the effects of overlap between chunks on the performance. Based on previous experiments, we set the length of the chunks to 10 for all experiments in this section. From Table <ref type="table" target="#tab_1">2</ref>, we find that the overlapping between the chunks plays an important role. The Sync-Transformer with overlap length 3 can achieve a CER of 8.91% on the test set. When the overlap is set to 1 or 0, too little overlap between two adjacent chunks may be the main cause of the degradation of performance. Therefore, we suppose that a decent overlap can maintain smooth transition of information flow between the chunks. The performance of the model also decreases when the overlap is set to 4. We guess that the large overlap will cause the information contained in the two adjacent chunks to be very similar, which will further degrade the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Comparison with other end-to-end models</head><p>We also compare the Sync-Transformer with other end-toend models. The transformer model is trained according to the recipe in ESPnet <ref type="bibr" target="#b17">[18]</ref>, which has the same settings as our Sync-Transformer. The second column indicates whether the model can decode in a streaming way. And the third column indicates the number of steps required to decode a U -length sentence. And M is the number of chunks and T is the number of speech frames.</p><p>The experiments show that the Sync-Transformer can achieve a comparable result with the best transformer, which is better than LAS <ref type="bibr" target="#b19">[20]</ref>, RNN-T and our previous (Chunk-Flow) SA-T <ref type="bibr" target="#b9">[10]</ref>. By contrast, the Sync-Transformer can achieve online decoding with only a little degradation of the performance. The relationship between the decoding steps of different models is U ≤ U + M ≤ T . Most of the attention-based models, like LAS and transformer, require the least steps during inference. However, restricted by the dependencies of attention on the entire input sequence, they cannot be directly applied to online speech recognition tasks. Chunk-Flow SA-T, an RNN-free transducer model, decodes a sequence frame by frame. And it also consumes much more memory during training. However, Sync-Transformer requires less decoding steps compared with SA-T and RNN-T, which means fewer costs in memory and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND DISCUSSION</head><p>In this paper, we propose a Sync-Transformer, which combines the advantages of transformer and transducers model. In order to get rid of the dependence on the entire input state sequences, we force every node in the encoder to focus on its left context and ignore its right contexts completely during calculating self-attention. Once a fixed-length chunk of state sequence is produced by the encoder, the decoder begins to predict symbols immediately. During training, we introduce a forward-backward algorithm to sum over the probabilities of all possible alignment paths and apply a negative logloss function to optimize Sync-Transformer. The experiments show that the Sync-Transformer can encode and decode synchronously. What's more, it outperforms our previous selfattention transducer and achieves a comparable result with the advanced transformer model. The experimental results reveal that Sync-Transformer is a very promising model for online speech recognition. However, there are some aspects to be improved in the future. For example, Sync-Transformer might raise more delete errors during inference, and recognize the wrong words with similar pronunciation. All of these prevent Sync-Transformer from being applied immediately. And it will be our next research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) illustrates the whole structure of Sync-Transformer and the inference process. The Sync-Transformer consists of an encoder and a decoder. Every node in the encoder only pays attention to its left contexts. The decoder generates symbols chunk by chunk. Once a fixed-length chunk of sequence is processed by the encoder, the decoder begins to predict the output symbols immediately. (b) illustrates the details of the decoder. (c) illustrates a output probability lattice utilized to sum over the probabilities of all possible alignment paths by forward-backward algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different window lengths (CER %).</figDesc><table><row><cell>W</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell cols="6">Dev 8.64 7.99 8.57 8.68 11.04</cell></row><row><cell cols="6">Test 9.73 9.06 9.51 9.76 11.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different overlap lengths (CER %).</figDesc><table><row><cell>B</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell cols="5">Dev 8.60 7.91 7.99 9.53</cell><cell>9.61</cell></row><row><cell cols="6">Test 9.56 8.91 9.06 10.39 10.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with other models (CER %).</figDesc><table><row><cell>Model</cell><cell cols="3">Online Steps Dev</cell><cell>Test</cell></row><row><cell>LAS [20]</cell><cell>No</cell><cell>U</cell><cell>-</cell><cell>10.56</cell></row><row><cell>Transformer</cell><cell>No</cell><cell>U</cell><cell>7.80</cell><cell>8.64</cell></row><row><cell>RNN-T [10]</cell><cell>No</cell><cell>T</cell><cell cols="2">10.13 11.82</cell></row><row><cell>SA-T [10]</cell><cell>No</cell><cell>T</cell><cell>8.30</cell><cell>9.30</cell></row><row><cell>Chunk-Flow SA-T [10]</cell><cell>Yes</cell><cell>T</cell><cell>8.58</cell><cell>9.80</cell></row><row><cell>Sync-Transformer</cell><cell>Yes</cell><cell>U+M</cell><cell>7.91</cell><cell>8.91</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.openslr.org/13/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/kaldi-asr/kaldi</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/1ytic/warp-rnnt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGE</head><p>This work is supported by the National Key Research &amp; Development Plan of China (No.2018YFB1005003), the National Natural Science Foundation of China (NSFC) (No.61831022, No.61771472, No.61773379, No.61901473) and Inria-CAS Joint Research Project (No.173211KYSB2017 0061 and No.173211KYSB20190049).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Local monotonic attention mechanism for endto-end speech and language processing</title>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08091</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Monotonic chunkwise attention</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05382</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Triggered attention for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Niko</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-attention transducers for end-toend speech recognition</title>
		<author>
			<persName><forename type="first">Zhengkun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="4395" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A neural transducer</title>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An online sequence-to-sequence model using partial conditioning</title>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5067" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving the performance of online neural transducer models</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5864" to="5868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>20th Conference of the Oriental Chapter</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Component fusion: Learning replaceable language model component for end-to-end speech recognition system</title>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Guangsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
