<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-07">7 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
							<email>anwar.i.walid@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
							<email>guojian@idea.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
							<email>jh.zheng@siat.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
							<email>zhaoranwang@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University Jiahao Zheng</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Inst. of Advanced Tech. Zhaoran Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Amazon &amp; Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>Conference&apos;17, July 2017</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-07">7 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.05188v1[q-fin.CP]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Machine learning</term>
					<term>Neural networks</term>
					<term>Markov decision processes</term>
					<term>Reinforcement learning RLOps in finance, deep reinforcement learning, stock trend prediction, scalability, GPU cloud</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning techniques are playing more and more important roles in finance market investment. However, finance quantitative modeling with conventional supervised learning approaches has a number of limitations, including the difficulty in defining appropriate labels, lack of consistency in modeling and trading execution, and lack of modeling the dynamic nature of the finance market. The development of deep reinforcement learning techniques is partially addressing these issues. Unfortunately, the steep learning curve and the difficulty in quick modeling and agile development are impeding finance researchers from using deep reinforcement learning in quantitative trading. In this paper, we propose an RLOps in finance paradigm and present a FinRL-Podracer framework to accelerate the development pipeline of deep reinforcement learning (DRL)-driven trading strategy and to improve both trading performance and training efficiency. FinRL-Podracer is a cloud solution that features high performance and high scalability and promises continuous training, continuous integration, and continuous delivery of DRL-driven trading strategies, facilitating a rapid transformation from algorithmic innovations into a profitable trading strategy. First, we propose a generational evolution mechanism with an ensemble strategy to improve the trading performance of a DRL agent, and schedule the training of a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out the training of DRL components with high-performance optimizations on GPUs. Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% ∼ 35% improvements in annual return, 0.1 ∼ 0.6 improvements in Sharpe ratio and 3× ∼ 7× speed-up in training time. We show the high scalability by training a trading agent in 10 minutes with 80 A100 GPUs, on NASDAQ-100 constituent stocks with minute-level data over 10 years.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Algorithmic trading is increasingly deployed in the financial investment process. A conventional supervised learning pipeline consists of five stages <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b40">40]</ref>, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (left), namely data pre-process, modeling and trading signal generation, portfolio optimization, trade execution, and post-trade analysis. Recently, deep reinforcement learning (DRL) <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref> has been recognized as a promising alternative for quantitative finance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, since it has the potential to overcome some important limitations of supervised learning, such as the difficulty in label specification and the gap between modeling, positioning and order execution. We advocate extending the principle of MLOps <ref type="bibr" target="#b0">[1]</ref> <ref type="foot" target="#foot_0">1</ref> to the RLOps in finance paradigm that implements and automates the continuous training (CT), continuous integration (CI), and continuous delivery (CD) for trading strategies. We argue that such a paradigm has vast profits potential from a broadened horizon and fast speed, which is critical for wider DRL adoption in real-world financial tasks.</p><p>The RLOps in finance paradigm, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (right), integrates middle stages (i.e., modeling and trading signal generation, portfolio optimization, and trade execution) into a DRL agent. Such a paradigm aims to help quantitative traders develop an end-to-end trading strategy with a high degree of automation, which removes the latency between stages and results in a compact software stack. The major benefit is that it can explore the vast potential profits behind the large-scale financial data, exceeding the capacity of human traders; thus, the trading horizon is lifted into a potentially new dimension. Also, it allows traders to continuously update trading strategies, which equips traders with an edge in a highly volatile market. However, the large-scale financial data and fast iteration of trading strategies bring imperative challenges in terms of computing power.</p><p>Existing works are not satisfactory with respect to the usage of large-scale financial data and the efficiency of agent training. For DRL strategy design, existing works studied algorithmic trading on a daily time-frame <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref> or hourly time-frame <ref type="bibr" target="#b13">[14]</ref>, which is hard to fully capture the dynamics of a highly volatile market. For DRL library/package development, existing works may not be able to meet the intensive computing requirement of relatively high frequency trading tasks, large-scale financial data processing and tick-level trade execution. We evaluate the training time of three popular DRL libraries, FinRL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, RLlib <ref type="bibr" target="#b18">[19]</ref> and Stable Baseline3 <ref type="bibr" target="#b8">[9]</ref>, on NASDAQ-100 constituent stocks with minutelevel data (described in Section 5.2) in Table <ref type="table">1</ref>, which shows that it is difficult for them to effectively train a profitable trading agent in a short cycle time.</p><p>In recent years, distributed DRL frameworks and massively parallel simulations have been recognized as the critical software development for the RLOps paradigm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. It is promising to utilize extensive computing resources, e.g., a GPU cloud, to accelerate the development of trading strategies, including both financial simulation and model training. Therefore, we investigate a candidate solution on a GPU cloud, an NVIDIA DGX SuperPOD cloud <ref type="bibr" target="#b38">[38]</ref> that is the most powerful AI infrastructure for enterprise deployments.</p><p>In this paper, we propose a FinRL-Podracer framework as a highperformance and scalable solution for RLOps in finance. At a high level, FinRL-Podracer schedules the training process through a multi-level mapping and employs a generational evolution mechanism with an ensemble strategy. Such a design guarantees scalability on a cloud platform. At a low level, FinRL-Podracer realizes hardware-oriented optimizations, including parallelism encapsulation, GPU acceleration, and storage optimization, thus achieving high performance. As a result, FinRL-Podracer can effectively exploit the supercomputing resources of a GPU cloud, which provides an opportunity to automatically design DRL trading strategies with fast and flexible development, deployment and production.</p><p>Our contributions can be summarized as follows • We propose a FinRL-Podracer framework built on two previous projects, FinRL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and ElegantRL <ref type="bibr" target="#b22">[23]</ref> 2 , to initiate a paradigm shift from conventional supervised learning approaches to RLOps in finance. • FinRL-Podracer employs a generational evolution mechanism during the training of DRL agents and provides highperformance optimizations for financial tasks. • We show the training efficiency by obtaining a trading agent in 10 minutes on an NVIDIA DGX SuperPOD cloud <ref type="bibr" target="#b38">[38]</ref> with 80 A100 GPUs, for a stock trend prediction task on NASDAQ-100 constituent stocks with minute-level data over 10 years. We evaluate the trained agent for one year and show its trading performance outperforms three DRL libraries, FinRL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, Ray RLlib <ref type="bibr" target="#b18">[19]</ref> and Stable Baseline3 <ref type="bibr" target="#b12">[13]</ref>, i.e., 12% ∼ 35% improvements in annual return, 0.1 ∼ 0.6 improvements in Sharpe ratio and 3× ∼ 7× speed-up in training time.</p><p>The remainder of this paper is organized as follows. Section 2 describes related works. Section 3 models a typical stock trend prediction task as a Markov Decision Process. In Section 4, we present the FinRL-Podracer framework and describe its evolution and training layers, respectively. In Section 5, we describe the experimental setup for the trading task and present experimental results. We conclude this paper and discuss future directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>This section summarizes related works from two aspects: DRL applications in quantitative finance and the MLOps development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DRL in Finance</head><p>With the successes of DRL in game playing, e.g., Atari games <ref type="bibr" target="#b30">[30]</ref> and GO games <ref type="bibr" target="#b35">[35]</ref>, more and more finance researchers show their interests in this area, and they have done some early attempts to applying DRL in quantitative finance investment. In this paper, we take the stock trend prediction (STP) task as an example to introduce existing works and show great potentials of DRL in finance area.</p><p>Stock trend prediction task is often considered a challenging application of machine learning in finance due to its noisy and volatile nature. Traditionally, the STP task is formulated as a supervised learning problem, where features of stocks are extracted from a past time window of technical indices, fundamental data and alternative data <ref type="bibr" target="#b3">[4]</ref>, and labels are usually extracted from a future time window of concerned criteria such as rise/fall, returns, excess returns or Sharpe ratios. Recently, deep reinforcement learning has been applied to solving STP tasks. Zhang et al. <ref type="bibr" target="#b45">[45]</ref> constructed a trading agent using three DRL algorithms, DQN, PG, and A2C, for both discrete and continuous action spaces. Yang et al. <ref type="bibr" target="#b44">[44]</ref> used an ensemble strategy to integrate different DRL algorithms, A2C, DDPG, and PPO based on the Sharpe ratio. They applied the idea of a rolling window, where the best algorithm is picked to trade in the following period. Recently, many researchers provide more DRL solutions for STP tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. However, most existing works are based on several assumptions, which limits the practicality. For example, the backtesting has no impacts on the market; there are almost no other complex actions besides buying, holding, and selling; only one stock type is supported for each agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Principle of MLOps</head><p>Recently, Google trends put Machine Learning Operations (MLOps) as one of the most promisingly increasing trends <ref type="bibr" target="#b41">[41]</ref>. MLOps is a practice in developing and operating large-scale machine learning systems, which facilitates the transformation of machine learning models from development to production <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. In essence, MLOps entails cloud computing power to integrate and automate a standard machine learning pipeline: 1) data pre-processing; 2) feature engineering; 3) continuous ML model training; 4) continuous ML model deployment; 5) output production, thus building applications that enable developers with limited machine-learning expertise to train high-quality models specific to their domain or data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>However, the DRL is quite different from conventional machine learning approaches. For example, training data of DRL is not prepared in advance compared with conventional supervised learning but collected through an agent-environment interaction inside the training process. Such a significant difference requires a reintegration of the automated pipeline and a re-scheduling of the cloud computing resources with respect to the conventional MLOps principle. Therefore, we advocate extending the principle of MLOps to the RLOps in finance paradigm to seek an opportunity for the wider DRL adoption in production-level financial services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STOCK TREND PREDICTION TASK</head><p>We describe the problem formulation of a typical financial task, stock trend prediction, which locates at the task layer of Fig. <ref type="figure" target="#fig_1">2</ref>. Our setup follows a similar setting in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>A stock trend prediction task is modeled as a Markov Decision Process (MDP): given state 𝑠 𝑡 ∈ S at time 𝑡, an agent takes an action 𝑎 𝑡 ∈ A according to policy 𝜋 𝜃 (𝑠 𝑡 ), transitions to the next state 𝑠 𝑡 +1 and receives an immediate reward 𝑟 (𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 +1 ) ∈ R. The policy 𝜋 𝜃 (𝑠) with parameter 𝜃 is a function that maps a state to an action vector over 𝑛 stocks. The objective is to find an optimal policy 𝜋 * 𝜃 (a policy network parameterized by 𝜃 ) that maximizes the expected return (the fitness score used in Fig. <ref type="figure" target="#fig_1">2</ref>) over 𝑇 times slots</p><formula xml:id="formula_0">𝜋 * 𝜃 = argmax 𝜃 𝐽 (𝜋 𝜃 ), where 𝐽 (𝜋 𝜃 ) = E 𝑇 ∑︁ 𝑡 =0 𝛾 𝑡 𝑟 (𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 +1 ) ,<label>(1)</label></formula><p>where 𝛾 ∈ (0, 1] is a discount factor. Then, for the stock trend predictiont task with 𝑛 stocks, we specify the state space S, action space A, reward function 𝑟 (𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 +1 ), and the state transition, as in <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b44">[44]</ref>.</p><p>State space S describes an agent's perception of a market environment. We summarize various features that are used by human trader and use them to construct the state space:</p><p>• Balance 𝑏 𝑡 ∈ R + : the account balance at time 𝑡. </p><formula xml:id="formula_1">M 𝑡 ∈ R 𝑛 , Relative Strength Index (RSI) R 𝑡 ∈ R 𝑛 + , Commodity Channel Index (CCI) C 𝑡 ∈ R 𝑛</formula><p>+ , etc. Action space A describes the allowed actions an agent can take at states 𝑠 𝑡 , 𝑡 = 1, ...,𝑇 . For one stock, action is 𝑎 ∈ {−𝑘, ..., −1, 0, 1, ..., 𝑘 }, where 𝑘 ∈ Z or −𝑘 ∈ Z denotes the number of shares to buy or sell, respectively, and 𝑎 = 0 means to hold. Users can set a maximum number of shares ℎ max for a transaction, i.e., 𝑘 ≤ ℎ max , or set a maximum ratio of capital to allocate on each stock.</p><p>Reward 𝑟 𝑡 for taking action 𝑎 𝑡 at state 𝑠 𝑡 and arriving at state 𝑠 𝑡 +1 . Reward is the incentive for an agent to improve its policy for the sake of getting higher rewards. A relatively simple reward can be defined as the change of the account value, i.e.,</p><formula xml:id="formula_2">𝑟 𝑡 = (𝑏 𝑡 +1 + p 𝑇 𝑡 +1 h 𝑡 +1 ) − (𝑏 𝑡 + p 𝑇 𝑡 h 𝑡 ) − 𝑐 𝑡 ,<label>(2)</label></formula><p>where the first and second terms are the account values at 𝑠 𝑡 +1 and 𝑠 𝑡 , and 𝑐 𝑡 denotes the transaction cost (market friction).</p><p>Transition (𝑠 𝑡 , 𝑎 𝑡 , 𝑟 𝑡 , 𝑠 𝑡 +1 ). Taking action 𝑎 𝑡 at state 𝑠 𝑡 , the environment steps forward and arrives at state 𝑠 𝑡 +1 . A transition involves the change of balance, number of shares, and the stock prices due to the market changes. We split the stocks into three sets: selling set 𝑆, buying set 𝐵 and holding set 𝐻 , respectively. The new balance is</p><formula xml:id="formula_3">𝑏 𝑡 +1 = 𝑏 𝑡 + (p 𝑆 𝑡 ) 𝑇 k 𝑆 𝑡 − (p 𝐵 𝑡 ) 𝑇 k 𝐵 𝑡 ,<label>(3)</label></formula><p>where p 𝑆 ∈ R 𝑛 and k 𝑆 ∈ R 𝑛 are the vectors of prices and number of selling shares for the selling stocks, and p 𝐵 ∈ R 𝑛 and k 𝐵 ∈ R 𝑛 for the buying stocks. The number of shares becomes</p><formula xml:id="formula_4">h 𝑡 +1 = h 𝑡 − k 𝑆 𝑡 + k 𝐵 𝑡 ≥ 0.<label>(4)</label></formula><p>The supercomputing power is necessary to achieve the massively parallel simulations for an STP task. During the training, a DRL agent keeps observing and trading on the historical market data to sample trajectories (one trajectory is a series of transitions). However, the historical data has to be significantly large in order to provide a broadened horizon. For example, the historical data could scale up in two dimensions: the data volume and data type <ref type="bibr" target="#b15">[16]</ref>:</p><p>• The data volume varies with respect to:</p><p>-the length of data period: from several months up to more than ten years. -the time granularity: from daily-level to minute-level, secondlevel or microsecond-level. -the number of stocks: from thirty (Dow 30) to hundreds (NASDAQ 100 or S&amp;P 500), or even covers the whole market. • The data type varies with respect to:</p><p>-the raw market data includes data points of open-high-lowclose-volume (OHLCV) for each stock, which provides a direct understanding of a stock's market performance. -the alternative data usually refers to the large-scale collection of both structured and unstructured data, e.g., market news, academic graph data <ref type="bibr" target="#b3">[4]</ref>, credit card transactions and GPS traffic. The agent could employ different encoders to analyze the insights of investment techniques provided by the alternative data. -the indexes and labels could be directly given as a kind of powerful technical indicator, which helps the agent make decisions.</p><p>In practice, the market simulation, alternative data processing and index analyzing are computationally expensive, therefore, a cloud-level solution is critical to a fast iteration of a trading strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FINRL-PODRACER FRAMEWORK</head><p>We propose a FinRL-Podracer framework to utilize the supercomputing power of a GPU cloud for training DRL-driven trading strategies. We first present an overview of FinRL-Podracer and then describe its layered architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Based on the experiments in Table <ref type="table">1</ref>, we found that existing DRL libraries/packages <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> have three major issues that restrict the trading performance and training efficiency:</p><p>• There is no criteria to determine overfitting or underfitting of models (trading strategies) during the training process. It is critical to overcome underfitting by utilizing more computing power and avoid overfitting that wastes computing power, while both cases would lead to suboptimal models. • The training process of a trading strategy is sensitive to hyperparameters, which may result in unstable trading performance in backtesting and trading. However, it is tedious for human traders to search for a good combination of hyper-parameters, and thus an automatic hyper-parameter search is favored. • Computing power is critical to effectively explore and exploit large-scale financial data. Sufficient exploration guarantees a good trading performance, and then smart exploitation results in good training efficiency. A strong computing power helps achieve a balance between exploration and exploitation.</p><p>Therefore, we provide a high performance and scalable solution on a GPU cloud, FinRL-Podracer, to develop a profitable DRL-driven trading strategy within a small time window. To fully utilize a GPU cloud, say an NVIDIA DGX SuperPod cloud <ref type="bibr" target="#b38">[38]</ref>, we organize FinRL-Podracer into a three-layer architecture in Fig. <ref type="figure" target="#fig_1">2</ref>, a trading task layer on the top, an evolution layer in the middle and a training layer at the bottom.</p><p>In the evolution layer, we employ a generational evolution mechanism with the ensemble strategy and address the issues of overfitting and hyper-parameter sensitivity through the synergy of an evaluator and a selector. The evaluator computes the fitness scores 𝐽 (𝜋 𝜃 ) in (1) of a population of 𝑁 agents and mitigates the performance collapse caused by overfitting. The hyper-parameter search is automatically performed via agent evolution, where the selector uses the fitness scores to guide the search direction. An effective cloud-level evolution requires a high-quality and scalable scheduling, therefore we schedule a population of parallel agents through a multi-level mapping.</p><p>In the training layer, we realize high-performance GPU-oriented optimizations of a decomposable DRL training pipeline. We locally optimize each component (a container within a pod), namely explorer, replay buffer, and learner, through parallelism encapsulation, GPU acceleration, efficient parameter fusion, and storage optimization. Thus, we maximize the hardware usage and minimize the communication overhead, which allows each component to be efficiently executed on a GPU cloud.</p><p>Such an evolution-and-training workflow pipelines the development of a trading strategy on a GPU cloud. It enjoys great performance and scalability, which promotes fast and flexible development, deployment and production of profitable DRL trading strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalable Evolution Layer</head><p>FinRL-Podracer exploits a generational evolution mechanism with an ensemble strategy to coordinate the parallel agents and to automatically search the best hyper-parameters. For each generation, it is composed of model ensemble and population ranking, as shown in the middle layer of Fig. <ref type="figure" target="#fig_1">2</ref>. At present, we utilize an evaluator and a selector to schedule the agent evolution, where more modules can be incorporated, e.g., a monitor, an allocator, etc.</p><p>The evaluator evaluates agents and provides their fitness scores as the metric for the future ranking, as shown in the ranking stage of Fig. <ref type="figure" target="#fig_1">2</ref>. From our observations, it is difficult for users to use existing libraries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> to train a profitable trading strategy because the overfitting agent may be treated as the best agent as the training process moves forward. When the dataset scales up, we need to increase the training time/steps to fully explore the large-scale data, making it harder to set appropriate stop criteria, and the resulting agent may hardly be the best one. The evaluator effectively mitigates the performance collapse brought by overfitting: in the course of the training, it evaluates the agent at each iteration, outputs a fitness score, and keeps track of the best agent so far; when the fitness score in (1) drops, the evaluator would stop the training process using the early stopping mechanism and output the best agent as the final agent.</p><p>The selector acts as a central controller to perform the selection strategy as in a genetic algorithm (GA) <ref type="bibr" target="#b29">[29]</ref>. GA is an optimization algorithm inspired by natural evolution: at every generation, a population of N agents is trained, and the evaluator calculates their fitness scores in (1) based on an objective function; then the selector redistributes the agents with the highest scores to form a new population for the next generation. Since the agents are parallel and replicable, the concept of natural selection scales up well on a GPU cloud. As shown in the evolution layer of Fig. <ref type="figure" target="#fig_1">2</ref>, there are 𝑁 agents with different hyper-parameters in a population. The synergy of the evaluator and selector enables FinRL-Podracer to naturally select the best agent for the future generation and eliminates the potential negative impact from poorly evolved agents, which effectively improves the stability and efficiency of the training.</p><p>FinRL-Podracer achieves the ensemble training of an agent by concurrently running K pods (training processes) in parallel and fusing the trained models from K pods at each epoch. All parallel pods for each agent are initialized with same hyper-parameters but different random seeds. Such a design, as shown in the ensemble stage of the evolution layer in Fig. <ref type="figure" target="#fig_1">2</ref>, guarantees randomness and stabilizes the learning process of the agent. The experiment results in Section 5 will perform an ablation study of the improvement brought by the generational evolution mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Packaging Worker-Learner into Pod</head><p>FinRL-Podracer achieves effective and efficient allocation of cloud resources through a multi-level mapping, which follows the principle of decomposition-and-encapsulation. We employ a workerlearner decomposition <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> that splits a DRL training process (pod) into three components (containers):</p><p>• Worker (exploration): samples transitions through the actor-environment interactions. • Replay buffer: stores transitions from a worker and feeds a batch of transitions to a learner. • Learner (exploitation): consumes transitions and trains the neural networks. Each training process of an agent consists of the three types of components, which are packaged into a suite that is mapped into a GPU pod. In addition, We run those components separately where each component is mapped to a GPU container. Such a two-level mapping is natural since a GPU pod consists of multiple containers, while correspondingly a training process of an agent consists of different components.</p><p>The above pod-container structure enables scalable allocation of GPU computing resources. We take advantage of a GPU cloud software stack and use the Kubernetes (K8S) software to scalably coordinate pods among severs. Consider a cloud with 10 servers (i.e., 80 A100 GPUs), we encapsulate a package of components into a pod, replicate it 80 times, and send them to a K8S server. Then, K8S distributes these 80 pod replicas to computing nodes that carry out the training process. The pod replication reflects strong parallelism, and it is highly scalable since a GPU cloud can support a large number of pods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">High Performance Training Layer</head><p>The optimization of each component is critical to the overall performance. We describe the hardware-oriented optimizations of components, including parallelism encapsulation, GPU acceleration, efficient parameter fusion and storage optimization.</p><p>Market simulation with GPU-acceleration. The market simulation is both computing-and communication-intensive. We propose a batch mode to perform massively parallel simulations, which maximizes the hardware utilization (either CPUs or GPUs). We instantiate multiple independent sub-environments in a batched environment, and a batched environment is exposed to a rollout worker that takes a batch of actions and returns a batch of transitions.</p><p>Fig. <ref type="figure" target="#fig_2">3 a</ref>) illustrates a GPU-accelerated environment. Environments of financial tasks are highly suitable to GPUs because financial simulations involve "simple" arithmetics, where a GPU with thousands of cores has the natural advantages of matrix computations and parallelism. Then, financial environments written in CUDA can speed up the simulation. The GPU-accelerated environment also effectively reduces the communication overhead by bypassing CPUs, as supported by a GPU cloud <ref type="bibr" target="#b38">[38]</ref>. The output transitions are stored as a tensor in GPU memory, which can be directly fetched by learners, avoiding the data transfer between CPU and GPU back and forth. Fig. <ref type="figure" target="#fig_2">3 b</ref>) presents an environment on CPUs. There are some financial simulations with frequent CPU usage (addressing trading constraints), making it inefficient to compute on GPUs. In our experiments, some environments run much slower on GPUs than CPUs. Thus, we simulate those environments on CPUs.</p><p>Replay buffer on GPU. We allocate the replay buffer on the contiguous memory of GPUs, which increases the addressing speed and bypasses CPUs for faster data transfer. As the worker and learner are co-located on GPUs, we store all transitions as tensors on the contiguous memory of GPUs. Since the collected transitions are packed together, the addressing speed increases dramatically well when a learner randomly samples a batch of transitions to update network parameters.</p><p>Learner with optimizations. To better support the ensemble training in the evolution layer, we propose a novel and effective way for learners of each pod to communicate, i.e., sending the network parameters rather than the gradients. Most existing libraries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> send the gradients of learners by following a traditional synchronization approach on supervised learning. Such an approach is inefficient for DRL algorithms since the learner will update the neural network hundreds of times within each training epoch, namely it needs to send gradients hundreds of times. By taking advantage of the soft update <ref type="bibr" target="#b20">[21]</ref>, we send the model parameters rather than the gradients. The parameter of the models is amenable to communication because model size in DRL is not comparable to that in other deep learning fields. Here, communication happens once at the end of each epoch, which is a significantly lower frequency of communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE EVALUATION</head><p>We describe the GPU cloud platform, the performance metrics and compared methods, and then evaluate the performance of FinRL-Podracer for a stock trend prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GPU Cloud Platform</head><p>All experiments were executed on NVIDIA DGX-2 servers <ref type="bibr" target="#b4">[5]</ref> in an NVIDIA DGX SuperPOD platform <ref type="bibr" target="#b38">[38]</ref>, a cloud-native supercomputer. We use 256 CPU cores of Dual AMD Rome 7742 running at 2.25GHz for each experiment. An NVIDIA DGX-2 server has 8 A100 GPUs and 320 GB GPU memory <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Metrics</head><p>We evaluate the trading performance and training efficiency of the FinRL-Podracer for a stock trend prediction task.</p><p>Data pre-processing. We select the NASDAQ-100 constituent stocks as our stock pool, accessed at 05/13/2019 (the starting time of our testing period), and use the datasets with two time granularities: minute-level and daily. The daily dataset is directly downloaded from Yahoo!Finance, while the minute-level dataset is first downloaded as raw data from the Compustat database through the Wharton Research Data Services (WRDS) <ref type="bibr" target="#b33">[33]</ref> and then pre-processed to an open-high-low-close-volume (OHLCV) format. We split the datasets into training period and backtesting period: the daily data from 01/01/2009 to 05/12/2019 for training; the minute-level data from 01/01/2016 to 05/12/2019 for training; For both datasets, we backtest on the same period from 05/13/2019 to 05/26/2021. Evaluation metrics. Six common metrics are used to evaluate the experimental results:</p><p>• Cumulative return: subtracting the initial value from the final portfolio value, then dividing by the initial value. • Annual return and volatility: geometric average return in a yearly sense, and the corresponding deviation. • Sharpe ratio: the average return earned in excess of the risk-free rate per unit of volatility. • Max drawdown: the maximum observed loss from a historical peak to a trough of a portfolio, before a new peak is achieved. Maximum drawdown is an indicator of downside risk over a time period. • Cumlative return vs. training time: the cumulative return during the testing period, achieved by an agent trained within a certain amount of time.</p><p>Compared methods. For trading performance evaluation, we compare FinRL-Podracer and vanilla FinRL-Podracer (without agent evolution) with FinRL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, RLlib <ref type="bibr" target="#b18">[19]</ref>, Stable Baseline3 <ref type="bibr" target="#b8">[9]</ref>, and NASDAQ Composite/Invesco QQQ ETF. We use Proximal Policy Optimization (PPO) <ref type="bibr" target="#b32">[32]</ref> as the DRL algorithm in the reported results and fine-tune each library to maximize its performance. Each library is allowed to use up to 80 GPUs.</p><p>For training efficiency evaluation, the experiments are conducted on multiple GPUs. We compare with RLlib <ref type="bibr" target="#b18">[19]</ref> since it has high performance on distributed infrastructure. However, both FinRL <ref type="bibr" target="#b24">[25]</ref> and Stable Baseline 3 <ref type="bibr" target="#b8">[9]</ref> do not support the training on multiple GPUs, thus we do not compare with them. We keep hyperparameters and computing resources the same to guarantee fair comparisons, and a general hyper-parameter setting is given in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Trading Performance</head><p>We backtest the trading performance from 05/13/2019 to 05/26/2021 on both daily and minute-level datasets. From Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref>, all DRL agents are able to achieve a better or equal performance than the market in cumulative return, which demonstrates the profit potentials of DRL-driven trading strategies. Comparing Fig. <ref type="figure">4</ref> with Fig. <ref type="figure">5</ref>, we observe that all methods have a much better performance on the minute-level dataset than that on the daily dataset. The trading performance of most agents is almost the same as that of the market on daily dataset, however, all agents significantly outperform the market if they have a larger dataset to explore. With a higher granularity data, the Sharpe ratios are also lifted up to a new level. From Table <ref type="table" target="#tab_3">3</ref>, agents achieve Sharpe ratios of 2.42, 2.05, 2.33, 1.82, 1.35 on the minute-level dataset, which are 0.3, 0.78, 1.42, 0.7, and 0.41 higher than those on the daily dataset. Therefore, we conclude that the capability to process large-scale financial data is critical for the development of a profitable DRL-driven trading strategy since the agent can better capture the volatility and dynamics of the market.</p><p>From Table <ref type="table" target="#tab_3">3</ref>, Fig. <ref type="figure">4</ref>, and Fig. <ref type="figure">5</ref>, we also observe that our FinRL-Podracer outperforms other baselines on both datasets, in terms of expected return, stability, and Sharpe ratio. As can be seen from Table <ref type="table" target="#tab_3">3</ref>, our FinRL-Podracer achieves the highest cumulative returns of 149.533% and 362.408%, annual returns of 56.431% and 111.549%, and Sharpe ratios of 2.12 and 2.42, which are much higher than the others. Furthermore, FinRL-Podracer also shows an outstanding stability during the backtesting: it achieves Max drawdown -13.834% and -15.874%, which is much lower than other methods. Consider the vanilla FinRL-Podracer as a direct comparison, we find that vanilla FinRL-Podracer has a similar trading performance with other baseline frameworks, which is in consistent with our expectation since the settings are the same. Such a performance improvement of FinRL-Podracer over vanilla FinRL-Podracer demonstrates the effectiveness of the generational evolution mechanism, as further verified by Fig. <ref type="figure">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Efficiency</head><p>We compare the training efficiency of FinRL-Podracer with RLlib <ref type="bibr" target="#b18">[19]</ref> on a varying number of A100 GPUs, i.e., 8, 16, 32, and 80. We store the model snapshots at different training time, say every 100 seconds, then later we use each snapshot model to perform inference on the backtesting dataset and obtain the generalization performance, namely, the cumulative return.</p><p>In Fig. <ref type="figure">6</ref>, as the number of GPUs increases, both FinRL-Podracer and RLlib achieve a higher cumulative return with the same training time (wall-clock time). FinRL-Podracer with 80 GPUs has a much steeper generalization curve than others, e.g., it can achieve a cumulative return of 4.0 at 800s, which means it learns in a much faster speed.  2, 200s and 3, 200s to achieve the same cumulative return, respectively. The generalization curves of RLlib with different numbers of GPUs are relatively similar, and we do not observe much speed-up. For example, FinRL-Podracer needs approximately 300s to achieve a cumulative return of 3.5, however, RLlib needs 2, 200s to achieve the same cumulative return. FinRL-Podracer is 3× ∼ 7× faster than RLlib.</p><p>It is counter-intuitive that the increase of GPU resources not only makes FinRL-Podracer have a fast training, but also improves the trading performance over RLlib <ref type="bibr" target="#b18">[19]</ref>. We know from Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref> that the generational evolution mechanism promotes the trading performance of FinRL-Podracer, therefore, we empirically investigate the agent evolution process. Fig. <ref type="figure">7</ref> explicitly demonstrates an evolution of 𝑁 = 10 agents, where the selector chooses the best model to train in the next generation every 800s. The inner figure of Fig. <ref type="figure">7</ref> depicts the generalization curves of the ten agents in the first generation (without using the agent evolution mechanism). The curve with the evolution mechanism (the thick green curve) is substantially higher than the other ten curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>In this paper, we have proposed a high-performance and scalable deep reinforcement learning framework, FinRL-Podracer, to initiate a paradigm shift from conventional supervised learning approaches to RLOps in finance. FinRL-Podracer provides a highly automated development pipeline of DRL-driven trading strategies on a GPU cloud, which aims to help finance researchers and quantitative traders overcome the steep learning curve and take advantage of supercomputing power from the cloud platforms.</p><p>FinRL-Podracer achieved promising performance on a cloud platform, mainly by following the two principles, the virtues of nested hierarchies and getting smart from dumb things <ref type="bibr" target="#b14">[15]</ref>. For low-level training, FinRL-Podracer realizes nested hierarchies by empolying hardware-oriented optimizations, including parallelism encapsulation, GPU acceleration, and storage optimizations. As a high level scheduling, FinRL-Podracer obtains a smart agent from hundreds of weak agents, which is the essence of ensemble methods, by employing a generational evolution mechanism. We further investigate the evolution and training layers in a followup work <ref type="bibr" target="#b23">[24]</ref> for a cloud-native solution. We believe that ensemble multiple weak agents is preferable to aiming to train one strong agent. Thus we propose a new orchestration mechanism, a tournament-based ensemble training method <ref type="bibr" target="#b23">[24]</ref> with asynchronous parallelism, which involves relatively low communication overhead. Also, we observe the great potential of massively parallel simulation, which lifts the exploration capability up into a potentially new dimension.</p><p>FinRL-Podracer is our first step from building a standard DRL pipeline of financial tasks to using DRL agents to understand the dynamics of the markets. We believe that FinRL-Podracer is critical for the ecosystem of the FinRL community <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> because it offers opportunities for many future directions. First, FinRL-Podracer provides a way to take advantage of large-scale financial data. It is possible to allow DRL agents to work in second or microsecond level and cover all stocks in the market, which is meaningful for the exploration and understanding of the dynamics of the market. Moreover, training on the cloud makes DRL agents adapt to much more complex financial simulations and neural networks, thus achieving wider DRL applications to various financial tasks, e.g., portfolio allocation, fraud detection, DRL-driven insights for yield improvement and optimization. Furthermore, the low-level optimizations in FinRL-Podracer could be also useful for the future development of financial simulators, such as using GPU-accelerated techniques to reduce latency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Software stack for an algorithmic trading process: conventional approach vs. RLOps in finance.</figDesc><graphic url="image-1.png" coords="2,55.12,83.69,237.60,114.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of FinRL-Podracer that has three layers: trading task layer, evolution layer and training layer.</figDesc><graphic url="image-2.png" coords="4,54.00,83.69,504.01,242.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two implementations of a training process: a) the environment simulation (green), action inference (yellow), and model update (red) are all located on GPUs. b) the environment simulation is executed on CPUs, and action inference and model update are on GPUs. An NVIDIA DGX-A100 server [38][5] contains 8 A100 GPUs.</figDesc><graphic url="image-3.png" coords="5,319.28,83.69,237.60,159.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Generalization performance on backtesting dataset, using the model snapshots of FinRL-Podracer and RLlib [19] at different training time (wall clock time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• Shares h 𝑡 ∈ Z 𝑛 + : the number of shares for 𝑛 stocks at 𝑡. • Closing price p 𝑡 ∈ R 𝑛 + : the closing prices of 𝑛 stocks at 𝑡. • Technical indicators help the agent make decisions. Users can select existing indicators or add new indicators. E.g., Moving Average Convergence Divergence (MACD)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameter settings in our experiments.</figDesc><table><row><cell>Hyper-parameters</cell><cell>Value</cell></row><row><cell>Total #GPUs</cell><cell>80</cell></row><row><cell>#Agent (𝑁 )</cell><cell>10</cell></row><row><cell>#Pods per agent (𝐾)</cell><cell>8</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Learning rate</cell><cell>2 −14</cell></row><row><cell>Discount factor</cell><cell>𝛾 = 0.99</cell></row><row><cell>Total steps</cell><cell>2 20</cell></row><row><cell>Batch size</cell><cell>2 10</cell></row><row><cell>Repeat times</cell><cell>2 3</cell></row><row><cell>Replay buffer Size</cell><cell>2 12</cell></row><row><cell>Ratio clip (PPO)</cell><cell>0.25</cell></row><row><cell>Lambda entropy (PPO)</cell><cell>0.02</cell></row><row><cell cols="2">Evaluation interval (second) 64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of stock trading on NASDAQ-100 constituent stocks with daily (red) and minute-level (blue) data.</figDesc><table><row><cell>Cumulative Return</cell><cell>1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6</cell><cell cols="2">FinRL-Podracer FinRL-Podracer (vanilla) RLlib Stable Baseline 3 FinRL QQQ</cell><cell></cell><cell></cell><cell></cell><cell>Cumulative Return</cell><cell>2.0 3.0 4.0 5.0</cell><cell cols="2">FinRL-Podracer FinRL-Podracer (vanilla) RLlib Stable Baseline 3 FinRL QQQ</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>May 13 2019</cell><cell>Oct 03</cell><cell>Feb 27 2020</cell><cell>Jul 21</cell><cell>Dec 10</cell><cell>May 6 2021</cell><cell></cell><cell>May 13 2019</cell><cell>Oct 03</cell><cell>Feb 27 2020</cell><cell>Jul 21</cell><cell>Dec 10 May 26 2021</cell></row><row><cell cols="7">Figure 4: Cumulative returns on daily dataset during</cell><cell cols="5">Figure 5: Cumulative returns on minute dataset during</cell></row><row><cell cols="7">05/13/2019 to 05/26/2021. Initial capital $1, 000, 000, transac-</cell><cell cols="5">05/13/2019 to 05/26/2021. Initial capital $1, 000, 000, transac-</cell></row><row><cell cols="7">tion cost percentage 0.2%, and Invesco QQQ ETF is a market</cell><cell cols="5">tion cost percentage 0.2%, Invesco QQQ ETF is a market</cell></row><row><cell cols="2">benchmark.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">benchmark.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cumul. return</cell><cell cols="2">Annual return</cell><cell cols="2">Annual volatility</cell><cell cols="2">Max drawdown</cell><cell>Sharpe ratio</cell></row><row><cell></cell><cell cols="3">FinRL-Podracer (Ours)</cell><cell cols="8">149.553%/362.408% 56.431%/111.549% 22.331%/33.427% -13.834%/-15.874%</cell><cell>2.12/2.42</cell></row><row><cell></cell><cell cols="3">FinRL-Podracer (vanilla)</cell><cell cols="2">73.546%/231.747%</cell><cell cols="2">30.964%/79.821%</cell><cell cols="2">23.561%/31.024%</cell><cell cols="2">-18.428%/-21.002%</cell><cell>1.27/2.05</cell></row><row><cell></cell><cell cols="2">RLlib [19]</cell><cell></cell><cell cols="2">58.926%/309.54%</cell><cell cols="2">25.444%/99.347%</cell><cell cols="2">30.009%/31.893%</cell><cell cols="2">-23.248%/-22.292%</cell><cell>0.91/2.33</cell></row><row><cell></cell><cell cols="3">Stable Baseline3 [13]</cell><cell cols="2">85.539%/218.531%</cell><cell cols="2">35.316%/76.28%</cell><cell cols="2">31.592%/34.595%</cell><cell cols="2">-24.056%/-23.75%</cell><cell>1.12/1.82</cell></row><row><cell></cell><cell cols="2">FinRL [25]</cell><cell></cell><cell cols="2">78.255%/169.975%</cell><cell cols="2">32.691%/62.576%</cell><cell cols="2">37.641%/42.908%</cell><cell cols="2">-26.774%/-27.267%</cell><cell>0.94/1.35</cell></row><row><cell></cell><cell cols="2">Invesco QQQ ETF</cell><cell></cell><cell cols="2">89.614%</cell><cell>36.763%</cell><cell></cell><cell cols="2">28.256%</cell><cell cols="2">-28.559%</cell><cell>1.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>However, FinRL-Podracer with 32 GPUs and 16 GPUs need</figDesc><table><row><cell>Cumulative Return</cell><cell>1.0 2.0 3.0 4.0</cell><cell>0</cell><cell>Training Time (Seconds) 500 1000 1500 2000 2500 3000 3500 FinRL-Podracer 8 GPUs FinRL-Podracer 16 GPUs FinRL-Podracer 32 GPUs FinRL-Podracer 80 GPUs RLlib 8 GPUs RLlib 16 GPUs RLlib 32 GPUs RLlib 80 GPUs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">MLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev) and ML system operation (Ops).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research used computational resources of the GPU cloud platform <ref type="bibr" target="#b38">[38]</ref> provided by the IDEA Research institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What Is MLOps?</title>
		<author>
			<persName><forename type="first">Sridhar</forename><surname>Alla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adari</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Beginning MLOps with MLFlow</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gonon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<title level="m">Deep hedging. Quantitative Finance</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1271" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Application of deep reinforcement learning on automated stock trading</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quantifying ESG alpha using scholar big data: an automated machine learning approach</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACM International Conference on AI in Finance</title>
				<meeting>the First ACM International Conference on AI in Finance</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NVIDIA A100 tensor core GPU: Performance and innovation</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wishwesh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Krashinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">delivery-and-automation-pipelines-in-machine-learning#mlops_level_0_ manual_process</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/architecture/mlops-continuous-" />
		<imprint>
			<date type="published" when="2020-01-07">2020. Jan. 07, 2020</date>
			<publisher>Google Cloud</publisher>
		</imprint>
	</monogr>
	<note>MLOps: Continuous delivery and automation pipelines in machine learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">our-insights/its-time-for-businesses-to-chart-acourse-for-reinforcement-learning</title>
		<author>
			<persName><forename type="first">Jacomo</forename><surname>Corbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Flemin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hohn</surname></persName>
		</author>
		<ptr target="https://www.mckinsey.com/business-functions/mckinsey-analytics/" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>McKinsey Analytics</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>It&apos;s time for businesses to chart a course for reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning in stock trading</title>
		<author>
			<persName><forename type="first">Quang-Vinh</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCSAMA</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Dlr-Rm</forename></persName>
		</author>
		<ptr target="https://github.com/DLR-RM/stable-baselines3" />
		<title level="m">Stable-baseline 3</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SEED RL: scalable and efficient deep-RL with accelerated central inference</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">IMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymir</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Podracer architectures for scalable reinforcement learning</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iurii</forename><surname>Kemaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<idno>ArXiv abs/2104.06272</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonin</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anssi</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rene</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/hill-a/stable-baselines" />
		<title level="m">Stable baselines</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A deep reinforcement learning framework for the financial portfolio management problem</title>
		<author>
			<persName><forename type="first">Zhengyao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Liang</surname></persName>
		</author>
		<idno>ArXiv abs/1706.10059</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Out of control: The rise of neo-biological civilization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Big data and AI strategies: machine learning and alternative data approach to investing</title>
		<author>
			<persName><forename type="first">Marko</forename><surname>Kolanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><forename type="middle">T</forename><surname>Krishnamachari</surname></persName>
		</author>
		<ptr target="https://www.cognitivefinance.ai/" />
	</analytic>
	<monogr>
		<title level="j">J.P. Morgan Securities LLC</title>
		<imprint>
			<date type="published" when="2017-05-18">2017. May. 18, 2017</date>
		</imprint>
	</monogr>
	<note>single-post/big-data-and-ai-strategies</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modern perspectives on reinforcement learning in finance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Petter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kolm</surname></persName>
		</author>
		<author>
			<persName><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrics: Mathematical Methods &amp; Programming eJournal</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimistic bull or pessimistic bear: Adaptive deep reinforcement learning for stock portfolio allocation</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuancheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Applications and Infrastructure for Multi-Agent Learning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ray RLLib: a composable and scalable reinforcement learning library</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno>ArXiv abs/1712.09381</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GPU-accelerated robotic simulation for distributed reinforcement learning</title>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Viktor Makoviychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chentanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning (CoRL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
		<idno>CoRR abs/1509.02971</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><forename type="middle">Andrew</forename><surname>Tamburri</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCC.2018.111121612</idno>
		<ptr target="https://doi.org/10.1109/MCC.2018.111121612" />
		<title level="m">TOSCA solves big problems in the cloud and beyond! IEEE Cloud Computing</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ElegantRL: A Scalable and Elastic Deep Reinforcement Learning Library</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://github.com/AI4Finance-Foundation/ElegantRL" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ElegantRL-Podracer: Scalable and elastic library for cloud-native deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Reinforcement Learning Workshop at NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FinRL: a deep reinforcement learning library for automated stock trading in quantitative finance</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Reinforcement Learning Workshop at NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FinRL: Deep reinforcement learning framework to automate trading in quantitative finance</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiechao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on AI in Finance (ICAIF)</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Makoviychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wawrzyniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kier</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hoeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Allshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10470</idno>
		<title level="m">Isaac Gym: High performance GPU-based physics simulation for robot learning</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">What Is MLOps?</title>
		<author>
			<persName><forename type="first">Rick</forename><surname>Merritt</surname></persName>
		</author>
		<ptr target="https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">NVIDIA</title>
		<imprint>
			<date type="published" when="2020-09-03">Sep. 03, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An introduction to genetic algorithms</title>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><surname>Legg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithmic trading</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahnoosh</forename><surname>Mirghaemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Treleaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaiyakorn</forename><surname>Yingsaeree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>John Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Klimov</surname></persName>
		</author>
		<idno>ArXiv abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">Standard &amp; poor&apos;s compustat. Data retrieved from Wharton Research Data Service</title>
				<imprint>
			<publisher>Wharton Research Data Service</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Thore Graepel, and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">NVIDIA DGX SuperPOD: Scalable infrastructure for AI leadership</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NVIDIA Corporation</publisher>
		</imprint>
	</monogr>
	<note>NVIDIA DGX A100 system reference architecture</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sustainable MLOps: Trends and challenges</title>
		<author>
			<persName><forename type="first">Damian</forename><forename type="middle">A</forename><surname>Tamburri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Algorithmic trading review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Treleaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lalchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="76" to="85" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="https://www.google.com/trends" />
		<title level="m">Google Trends. 2021. What Is MLOps</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantitative trading on stock market based on deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Practical deep reinforcement learning approach for stock trading</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for automated stock trading: An ensemble strategy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on AI in Finance (ICAIF)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for trading</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Zohren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Financial Data Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="40" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
