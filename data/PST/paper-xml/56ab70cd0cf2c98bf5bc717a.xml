<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Silver</surname></persName>
							<email>davidsilver@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Nham</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>1600 Amphitheatre Parkway, Mountain View</addrLine>
									<postCode>94043</postCode>
									<settlement>Google</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>1600 Amphitheatre Parkway, Mountain View</addrLine>
									<postCode>94043</postCode>
									<settlement>Google</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
							<email>demishassabis@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1038/nature16961</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>A.G.</term>
					<term>L.S.</term>
					<term>A.H.</term>
					<term>I.A.</term>
					<term>V.P.</term>
					<term>S.D.</term>
					<term>D.G.</term>
					<term>N.K.</term>
					<term>I.S.</term>
					<term>K.K. and D.S. designed and trained the neural networks in AlphaGo. J.S.</term>
					<term>J.N.</term>
					<term>A.H. and D.S. designed and implemented the evaluation framework for AlphaGo. D.S.</term>
					<term>M.Le.</term>
					<term>T.L.</term>
					<term>T.G.</term>
					<term>K.K. and D.H. managed and advised on the project. D.S.</term>
					<term>T.G.</term>
					<term>A.G. and D.H. wrote the paper</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All games of perfect information have an optimal value function, v * (s), which determines the outcome of the game, from every board position or state s, under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately b d possible sequences of moves, where b is the game's breadth (number of legal moves per position) and d is its depth (game length). In large games, such as chess (b ≈ 35, d ≈ 80) <ref type="bibr" target="#b1">1</ref> and especially Go (b ≈ 250, d ≈ 150) <ref type="bibr" target="#b1">1</ref> , exhaustive search is infeasible <ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3</ref> , but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state s and replacing the subtree below s by an approximate value function v(s) ≈ v * (s) that predicts the outcome from state s. This approach has led to superhuman performance in chess <ref type="bibr" target="#b4">4</ref> , checkers <ref type="bibr" target="#b5">5</ref> and othello <ref type="bibr" target="#b6">6</ref> , but it was believed to be intractable in Go due to the complexity of the game <ref type="bibr" target="#b7">7</ref> . Second, the breadth of the search may be reduced by sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in position s. For example, Monte Carlo rollouts <ref type="bibr" target="#b8">8</ref> search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy p. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon <ref type="bibr" target="#b8">8</ref> and Scrabble <ref type="bibr" target="#b9">9</ref> , and weak amateur level play in Go <ref type="bibr" target="#b10">10</ref> .</p><p>Monte Carlo tree search (MCTS) <ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12</ref> uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function <ref type="bibr" target="#b12">12</ref> . The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves <ref type="bibr" target="#b13">13</ref> . These policies are used to narrow the search to a beam of high-probability actions, and to sample actions during rollouts. This approach has achieved strong amateur play <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref> . However, prior work has been limited to shallow policies <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref> or value functions <ref type="bibr" target="#b16">16</ref> based on a linear combination of input features.</p><p>Recently, deep convolutional neural networks have achieved unprecedented performance in visual domains: for example, image classification <ref type="bibr" target="#b17">17</ref> , face recognition <ref type="bibr" target="#b18">18</ref> , and playing Atari games <ref type="bibr" target="#b19">19</ref> . They use many layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, localized representations of an image <ref type="bibr" target="#b20">20</ref> . We employ a similar architecture for the game of Go. We pass in the board position as a 19 × 19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.</p><p>We train the neural networks using a pipeline consisting of several stages of machine learning (Fig. <ref type="figure" target="#fig_8">1</ref>). We begin by training a supervised learning (SL) policy network p σ directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. Similar to prior work <ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15</ref> , we also train a fast policy p π that can rapidly sample actions during rollouts. Next, we train a reinforcement learning (RL) policy network p ρ that improves the SL policy network by optimizing the final outcome of games of selfplay. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, we train a value network v θ that predicts the winner of games played by the RL policy network against itself. Our program AlphaGo efficiently combines the policy and value networks with MCTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised learning of policy networks</head><p>For the first stage of the training pipeline, we build on prior work on predicting expert moves in the game of Go using supervised learning <ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref> . The SL policy network p σ (a | s) alternates between convolutional layers with weights σ, and rectifier nonlinearities. A final softmax layer outputs a probability distribution over all legal moves a. The input s to the policy network is a simple representation of the board state (see Extended Data Table <ref type="table" target="#tab_5">2</ref>). The policy network is trained on randomly</p><p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE RESEARCH</head><p>sampled state-action pairs (s, a), using stochastic gradient ascent to maximize the likelihood of the human move a selected in state s</p><formula xml:id="formula_0">∆σ σ ∝ ∂ ( | ) ∂ σ p a s log</formula><p>We trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. The network predicted expert moves on a held out test set with an accuracy of 57.0% using all input features, and 55.7% using only raw board position and move history as inputs, compared to the state-of-the-art from other research groups of 44.4% at date of submission <ref type="bibr" target="#b24">24</ref> (full results in Extended Data Table <ref type="table" target="#tab_6">3</ref>). Small improvements in accuracy led to large improvements in playing strength (Fig. <ref type="figure">2a</ref>); larger networks achieve better accuracy but are slower to evaluate during search. We also trained a faster but less accurate rollout policy p π (a|s), using a linear softmax of small pattern features (see Extended Data Table <ref type="table">4</ref>) with weights π; this achieved an accuracy of 24.2%, using just 2 μs to select an action, rather than 3 ms for the policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement learning of policy networks</head><p>The second stage of the training pipeline aims at improving the policy network by policy gradient reinforcement learning (RL) <ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26</ref> . The RL policy network p ρ is identical in structure to the SL policy network, and its weights ρ are initialized to the same values, ρ = σ. We play games between the current policy network p ρ and a randomly selected previous iteration of the policy network. Randomizing from a pool of opponents in this way stabilizes training by preventing overfitting to the current policy. We use a reward function r(s) that is zero for all non-terminal time steps t &lt; T. The outcome z t = ± r(s T ) is the terminal reward at the end of the game from the perspective of the current player at time step t: +1 for winning and −1 for losing. Weights are then updated at each time step t by stochastic gradient ascent in the direction that maximizes expected outcome 25</p><formula xml:id="formula_1">∆ρ ρ ∝ ∂ ( | ) ∂ ρ p a s z log t t t</formula><p>We evaluated the performance of the RL policy network in game play, sampling each move ~(⋅| ) ρ a p s t t from its output probability distribution over actions. When played head-to-head, the RL policy network won more than 80% of games against the SL policy network. We also tested against the strongest open-source Go program, Pachi <ref type="bibr" target="#b14">14</ref> , a sophisticated Monte Carlo search program, ranked at 2 amateur dan on KGS, that executes 100,000 simulations per move. Using no search at all, the RL policy network won 85% of games against Pachi. In comparison, the previous state-of-the-art, based only on supervised Positions and outcomes were sampled from human expert games. Each position was evaluated by a single forward pass of the value network v θ , or by the mean outcome of 100 rollouts, played out using either uniform random rollouts, the fast rollout policy p π , the SL policy network p σ or the RL policy network p ρ . The mean squared error between the predicted value and the actual game outcome is plotted against the stage of the game (how many moves had been played in the given position). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement learning of value networks</head><p>The final stage of the training pipeline focuses on position evaluation, estimating a value function v p (s) that predicts the outcome from position s of games played by using policy p for both players <ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref> </p><formula xml:id="formula_2">E ( ) = | = . v s z s s a p [ , ] p t t t T</formula><p>Ideally, we would like to know the optimal value function under perfect play v * (s); in practice, we instead estimate the value function</p><p>The naive approach of predicting game outcomes from data consisting of complete games leads to overfitting. The problem is that successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the KGS data set in this way, the value network memorized the game outcomes rather than generalizing to new positions, achieving a minimum MSE of 0.37 on the test set, compared to 0.19 on the training set. To mitigate this problem, we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game. Each game was played between the RL policy network and itself until the game terminated. Training on this data set led to MSEs of 0.226 and 0.234 on the training and test set respectively, indicating minimal overfitting. Figure <ref type="figure">2b</ref> shows the position evaluation accuracy of the value network, compared to Monte Carlo rollouts using the fast rollout policy p π ; the value function was consistently more accurate. A single evaluation of v θ (s) also approached the accuracy of Monte Carlo rollouts using the RL policy network p ρ , but using 15,000 times less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searching with policy and value networks</head><p>AlphaGo combines the policy and value networks in an MCTS algorithm (Fig. <ref type="figure" target="#fig_1">3</ref>) that selects actions by lookahead search. Each edge (s, a) of the search tree stores an action value Q(s, a), visit count N(s, a), and prior probability P(s, a). The tree is traversed by simulation (that is, descending the tree in complete games without backup), starting from the root state. . The leaf node is evaluated in two very different ways: first, by the value network v θ (s L ); and second, by the outcome z L of a random rollout played out until terminal step T using the fast rollout policy p π ; these evaluations are combined, using a mixing parameter λ, into a leaf evaluation V(s L )</p><formula xml:id="formula_3">λ λ ( )= ( − ) ( )+ θ V s v s z 1 L L L</formula><p>At the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge</p><formula xml:id="formula_4">∑ ∑ ( )= ( ) ( )= ( ) ( ) ( ) = = N s a sa i Q s a N s a s a i V s , 1 , ,<label>, 1 , 1 ,</label></formula><p>,</p><formula xml:id="formula_5">i n i n L i 1 1</formula><p>where s L i is the leaf node from the ith simulation, and 1(s, a, i) indicates whether an edge (s, a) was traversed during the ith simulation. Once the search is complete, the algorithm chooses the most visited move from the root position.</p><p>It is worth noting that the SL policy network p σ performed better in AlphaGo than the stronger RL policy network p ρ , presumably because humans select a diverse beam of promising moves, whereas RL optimizes for the single best move. However, the value function</p><formula xml:id="formula_6">( ) ≈ ( ) θ ρ v s v s p</formula><p>derived from the stronger RL policy network performed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating the playing strength of AlphaGo</head><p>To evaluate AlphaGo, we ran an internal tournament among variants of AlphaGo and several other Go programs, including the strongest commercial programs Crazy Stone <ref type="bibr" target="#b13">13</ref> and Zen, and the strongest open source programs Pachi 14 and Fuego <ref type="bibr" target="#b15">15</ref> . All of these programs are based  <ref type="table" target="#tab_8">6-11</ref>). Each program used approximately 5 s computation time per move. To provide a greater challenge to AlphaGo, some programs (pale upper bars) were given four handicap stones (that is, free moves at the start of every game) against all opponents. Programs were evaluated on an Elo scale 37 : a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS <ref type="bibr" target="#b38">38</ref> ; an approximate correspondence to human ranks is also shown, horizontal lines show KGS ranks achieved online by that program. Games against the human European champion Fan Hui were also included; these games used longer time controls. 95% confidence intervals are shown. b, Performance of AlphaGo, on a single machine, for different combinations of components. The version solely using the policy network does not perform any search. c, Scalability study of MCTS in AlphaGo with search threads and GPUs, using asynchronous search (light blue) or distributed search (dark blue), for 2 s per move.  The results of the tournament (see Fig. <ref type="figure" target="#fig_2">4a</ref>) suggest that singlemachine AlphaGo is many dan ranks stronger than any previous Go program, winning 494 out of 495 games (99.8%) against other Go programs. To provide a greater challenge to AlphaGo, we also played games with four handicap stones (that is, free moves for the opponent); AlphaGo won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi, respectively. The distributed version of AlphaGo was significantly stronger, winning 77% of games against single-machine AlphaGo and 100% of its games against other programs.</p><p>We also assessed variants of AlphaGo that evaluated positions using just the value network (λ = 0) or just rollouts (λ = 1) (see Fig. <ref type="figure" target="#fig_2">4b</ref>). Even without rollouts AlphaGo exceeded the performance of all other Go programs, demonstrating that value networks provide a viable alternative to Monte Carlo evaluation in Go. However, the mixed evaluation (λ = 0.5) performed best, winning ≥95% of games against other variants. This suggests that the two position-evaluation mechanisms are complementary: the value network approximates the outcome of games played by the strong but impractically slow p ρ , while the rollouts can precisely score and evaluate the outcome of games played by the weaker but faster rollout policy p π . Figure <ref type="figure" target="#fig_3">5</ref> visualizes the evaluation of a real game position by AlphaGo.</p><p>Finally, we evaluated the distributed version of AlphaGo against Fan Hui, a professional 2 dan, and the winner of the 2013, 2014 and 2015 European Go championships. Over 5-9 October 2015 AlphaGo and Fan Hui competed in a formal five-game match. AlphaGo won the match 5 games to 0 (Fig. <ref type="figure" target="#fig_5">6</ref> and Extended Data Table <ref type="table" target="#tab_4">1</ref>). This is the first time that a computer Go program has defeated a human professional player, without handicap, in the full game of Go-a feat that was previously believed to be at least a decade away <ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b31">31</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this work we have developed a Go program, based on a combination of deep neural networks and tree search, that plays at the level of the strongest human players, thereby achieving one of artificial intelligence's "grand challenges" <ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref> . We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination   During the match against Fan Hui, AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov <ref type="bibr" target="#b4">4</ref> ; compensating by selecting those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network-an approach that is perhaps closer to how humans play. Furthermore, while Deep Blue relied on a handcrafted evaluation function, the neural networks of AlphaGo are trained directly from gameplay purely through general-purpose supervised and reinforcement learning methods.</p><p>Go is exemplary in many ways of the difficulties faced by artificial intelligence <ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34</ref> : a challenging decision-making task, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function. The previous major breakthrough in computer Go, the introduction of MCTS, led to corresponding advances in many other domains; for example, general game-playing, classical planning, partially observed planning, scheduling, and constraint satisfaction <ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36</ref> . By combining tree search with policy and value networks, AlphaGo has finally reached a professional level in Go, providing hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE RESEARCH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHODS</head><p>Problem setting. Many games of perfect information, such as chess, checkers, othello, backgammon and Go, may be defined as alternating Markov games <ref type="bibr" target="#b39">39</ref> . In these games, there is a state space S (where state includes an indication of the current player to play); an action space ( ) A s defining the legal actions in any given state s ∈ S; a state transition function f(s, a, ξ) defining the successor state after selecting action a in state s and random input ξ (for example, dice); and finally a reward function r i (s) describing the reward received by player i in state s. We restrict our attention to two-player zero-sum games, r 1 (s) = −r 2 (s) = r(s), with deterministic state transitions, f(s, a, ξ) = f(s, a), and zero rewards except at a terminal time step T. The outcome of the game z t = ±r(s T ) is the terminal reward at the end of the game from the perspective of the current player at time step t. A policy p(a|s) is a probability distribution over legal actions a ∈ ( ) A s . A value function is the expected outcome if all actions for both players are selected according to policy p, that is,</p><formula xml:id="formula_7">E ( ) = | = ... v s z s s a p [ , ] p t t t T</formula><p>. Zero-sum games have a unique optimal value function v*(s) that determines the outcome from state s following perfect play by both players,</p><formula xml:id="formula_8">⁎ ⁎ ( ) =        = − ( ( )) v s z ss v f s a if , max</formula><p>, otherwise</p><formula xml:id="formula_9">T T a</formula><p>Prior work. The optimal value function can be computed recursively by minimax (or equivalently negamax) search <ref type="bibr" target="#b40">40</ref> . Most games are too large for exhaustive minimax tree search; instead, the game is truncated by using an approximate value function v(s) ≈ v * (s) in place of terminal rewards. Depth-first minimax search with alpha-beta pruning <ref type="bibr" target="#b40">40</ref> has achieved superhuman performance in chess <ref type="bibr" target="#b4">4</ref> , checkers <ref type="bibr" target="#b5">5</ref> and othello <ref type="bibr" target="#b6">6</ref> , but it has not been effective in Go <ref type="bibr" target="#b7">7</ref> .</p><p>Reinforcement learning can learn to approximate the optimal value function directly from games of self-play <ref type="bibr" target="#b39">39</ref> . The majority of prior work has focused on a linear combination v θ (s) = ϕ(s) • θ of features ϕ(s) with weights θ. Weights were trained using temporal-difference learning <ref type="bibr" target="#b41">41</ref> in chess <ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43</ref> , checkers <ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45</ref> and Go <ref type="bibr" target="#b30">30</ref> ; or using linear regression in othello <ref type="bibr" target="#b6">6</ref> and Scrabble <ref type="bibr" target="#b9">9</ref> . Temporal-difference learning has also been used to train a neural network to approximate the optimal value function, achieving superhuman performance in backgammon <ref type="bibr" target="#b46">46</ref> ; and achieving weak kyu-level performance in small-board Go 28,29,47 using convolutional networks.</p><p>An alternative approach to minimax search is Monte Carlo tree search (MCTS) <ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12</ref> , which estimates the optimal value of interior nodes by a double approximation,</p><formula xml:id="formula_10">⁎ ( ) ≈ ( ) ≈ ( ) V s v s v s n P n</formula><p>. The first approximation, ( ) ≈ ( ) V s v s n P n , uses n Monte Carlo simulations to estimate the value function of a simulation policy P n . The second approximation,</p><formula xml:id="formula_11">⁎ ( ) ≈ ( ) v s v s P n</formula><p>, uses a simulation policy P n in place of minimax optimal actions. The simulation policy selects actions according to a search control function</p><formula xml:id="formula_12">( ( ) + ( )) Q s a u s a argmax , ,<label>a n</label></formula><p>, such as UCT <ref type="bibr" target="#b12">12</ref> , that selects children with higher action values, Q n (s, a) = −V n (f(s, a)), plus a bonus u(s, a) that encourages exploration; or in the absence of a search tree at state s, it samples actions from a fast rollout policy ( | ) π p a s . As more simulations are executed and the search tree grows deeper, the simulation policy becomes informed by increasingly accurate statistics. In the limit, both approximations become exact and MCTS (for example, with UCT) converges <ref type="bibr" target="#b12">12</ref> to the optimal value function</p><formula xml:id="formula_13">⁎ ( ) = () = ( ) →∞ →∞ V s v s v s lim lim n n n P n</formula><p>. The strongest current Go programs are based on MCTS <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b36">36</ref> .</p><p>MCTS has previously been combined with a policy that is used to narrow the beam of the search tree to high-probability moves <ref type="bibr" target="#b13">13</ref> ; or to bias the bonus term towards high-probability moves <ref type="bibr" target="#b48">48</ref> . MCTS has also been combined with a value function that is used to initialize action values in newly expanded nodes <ref type="bibr" target="#b16">16</ref> , or to mix Monte Carlo evaluation with minimax evaluation <ref type="bibr" target="#b49">49</ref> . By contrast, AlphaGo's use of value functions is based on truncated Monte Carlo search algorithms <ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9</ref> , which terminate rollouts before the end of the game and use a value function in place of the terminal reward. AlphaGo's position evaluation mixes full rollouts with truncated rollouts, resembling in some respects the well-known temporal-difference learning algorithm TD(λ). AlphaGo also differs from prior work by using slower but more powerful representations of the policy and value function; evaluating deep neural networks is several orders of magnitude slower than linear representations and must therefore occur asynchronously.</p><p>The performance of MCTS is to a large degree determined by the quality of the rollout policy. Prior work has focused on handcrafted patterns <ref type="bibr" target="#b50">50</ref> or learning rollout policies by supervised learning <ref type="bibr" target="#b13">13</ref> , reinforcement learning <ref type="bibr" target="#b16">16</ref> , simulation balancing <ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52</ref> or online adaptation <ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b53">53</ref> ; however, it is known that rollout-based position evaluation is frequently inaccurate <ref type="bibr" target="#b54">54</ref> . AlphaGo uses relatively simple rollouts, and instead addresses the challenging problem of position evaluation more directly using value networks.</p><p>Search algorithm. To efficiently integrate large neural networks into AlphaGo, we implemented an asynchronous policy and value MCTS algorithm (APV-MCTS). Each node s in the search tree contains edges (s, a) for all legal actions a ∈ ( ) A s . Each edge stores a set of statistics, The APV-MCTS algorithm proceeds in the four stages outlined in Fig. <ref type="figure" target="#fig_1">3</ref>. Selection (Fig. <ref type="figure" target="#fig_1">3a</ref>). The first in-tree phase of each simulation begins at the root of the search tree and finishes when the simulation reaches a leaf node at time step L. At each of these time steps, t &lt; L, an action is selected according to the statistics in the search tree, = ( (  <ref type="figure" target="#fig_1">3d</ref>). At each in-tree step t ≤ L of the simulation, the rollout statistics are updated as if it has lost n vl games, N r (s t , a t ) ← N r (s t , a t ) + n vl ; W r (s t , a t ) ← W r (s t , a t ) −n vl ; this virtual loss <ref type="bibr" target="#b55">55</ref> discourages other threads from simultaneously exploring the identical variation. At the end of the simulation, t he rollout statistics are updated in a backward pass through each step t ≤ L, replacing the virtual losses by the outcome, N r (s t , a t ) ← N r (s t , a t ) −n vl + 1; W r (s t , a t ) ← W r (s t , a t ) + n vl + z t . Asynchronously, a separate backward pass is initiated when the evaluation of the leaf position s L completes. The output of the value network v θ (s L ) is used to update value statistics in a second backward pass through each step t ≤ L, N v (s t , a t ) ← N v (s t , a t ) + 1, W v (s t , a t ) ← W v (s t , a t ) + v θ (s L ). The overall evaluation of each state action is a weighted average of the Monte Carlo estimates,</p><formula xml:id="formula_14">( ) ( ) ( ) ( ) ( ) ( ) P s a N s a N s a W s a W s a Q s a { , , ,<label>, , , , , , , , } v</label></formula><formula xml:id="formula_15">)+ ( )) a Q s a u s</formula><formula xml:id="formula_16">λ λ ( )= ( − ) + ( ) ( ) ( ) ( ) Q s a , 1 W s a N s a W s a N s a , , , , v v r r</formula><p>, that mixes together the value network and rollout evaluations with weighting parameter λ. All updates are performed lock-free <ref type="bibr" target="#b56">56</ref> . Expansion (Fig. <ref type="figure" target="#fig_1">3b</ref>). When the visit count exceeds a threshold, N r (s, a) &gt; n thr , the successor state s′ = f(s, a) is added to the search tree. The new node is initialized to {N(s′, a) = N r (s′, a) = 0, W(s′, a) = W r (s′, a) = 0, P(s′,a) = p σ (a|s′)}, using a tree policy p τ (a|s′) (similar to the rollout policy but with more features, see Extended Data Table <ref type="table">4</ref>) to provide placeholder prior probabilities for action selection. The position s′ is also inserted into a queue for asynchronous GPU evaluation by the policy network. Prior probabilities are computed by the SL policy network (⋅| ′) σ β p s with a softmax temperature set to β; these replace the placeholder prior probabilities, ( ′ ) ← ( | ′) σ β P s a p a s , , using an atomic update. The threshold n thr is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate at which the GPUs evaluate the policy network. Positions are evaluated by both the policy network and the value network using a mini-batch size of 1 to minimize end-to-end evaluation time.</p><p>We also implemented a distributed APV-MCTS algorithm. This architecture consists of a single master machine that executes the main search, many remote worker CPUs that execute asynchronous rollouts, and many remote worker GPUs that execute asynchronous policy and value network evaluations. The entire search tree is stored on the master, which only executes the in-tree phase of each simulation. The leaf positions are communicated to the worker CPUs, which execute the rollout phase of simulation, and to the worker GPUs, which compute network features and evaluate the policy and value networks. The prior probabilities of the policy network are returned to the master, where they replace placeholder prior probabilities at the newly expanded node. The rewards from rollouts and the value network outputs are each returned to the master, and backed up the originating search path.</p><p>At the end of search AlphaGo selects the action with maximum visit count; this is less sensitive to outliers than maximizing action value <ref type="bibr" target="#b15">15</ref> . The search tree is reused at subsequent time steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. The match version of AlphaGo continues searching during the opponent's move. It extends the search if the action maximizing visit count and the action maximizing action value disagree. Time controls were otherwise shaped to use most time in the middle-game <ref type="bibr" target="#b57">57</ref> . AlphaGo resigns when its overall evaluation drops below an estimated 10% probability of winning the game, that is, ( )&lt; − . Q s a max , 08 a . AlphaGo does not employ the all-moves-as-first <ref type="bibr" target="#b10">10</ref> or rapid action value estimation <ref type="bibr" target="#b58">58</ref> heuristics used in the majority of Monte Carlo Go programs; when using policy networks as prior knowledge, these biased heuristics do not appear to give any additional benefit. In addition AlphaGo does not use progressive widening <ref type="bibr" target="#b13">13</ref> , dynamic komi <ref type="bibr" target="#b59">59</ref> or an opening book <ref type="bibr" target="#b60">60</ref> . The parameters used by AlphaGo in the Fan Hui match are listed in Extended Data Table <ref type="table" target="#tab_7">5</ref>. Rollout policy. The rollout policy ( | ) π p a s is a linear softmax policy based on fast, incrementally computed, local pattern-based features consisting of both 'response' patterns around the previous move that led to state s, and 'non-response' patterns around the candidate move a in state s. Each non-response pattern is a binary feature matching a specific 3 × 3 pattern centred on a, defined by the colour (black, white, empty) and liberty count (1, 2, ≥3) for each adjacent intersection. Each response pattern is a binary feature matching the colour and liberty count in a 12-point diamond-shaped pattern <ref type="bibr" target="#b21">21</ref> centred around the previous move. Additionally, a small number of handcrafted local features encode common-sense Go rules (see Extended Data Table <ref type="table">4</ref>). Similar to the policy network, the weights π of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board.</p><p>Our rollout policy p π (a|s) contains less handcrafted knowledge than stateof-the-art Go programs <ref type="bibr" target="#b13">13</ref> . Instead, we exploit the higher-quality action selection within MCTS, which is informed both by the search tree and the policy network. We introduce a new technique that caches all moves from the search tree and then plays similar moves during rollouts; a generalization of the 'last good reply' heuristic <ref type="bibr" target="#b53">53</ref> . At every step of the tree traversal, the most probable action is inserted into a hash table, along with the 3 × 3 pattern context (colour, liberty and stone counts) around both the previous move and the current move. At each step of the rollout, the pattern context is matched against the hash table; if a match is found then the stored move is played with high probability. Symmetries. In previous work, the symmetries of Go have been exploited by using rotationally and reflectionally invariant filters in the convolutional layers <ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29</ref> . Although this may be effective in small neural networks, it actually hurts performance in larger networks, as it prevents the intermediate filters from identifying specific asymmetric patterns <ref type="bibr" target="#b23">23</ref> . Instead, we exploit symmetries at run-time by dynamically transforming each position s using the dihedral group of eight reflections and rotations, d 1 (s), …, d 8 (s). In an explicit symmetry ensemble, a mini-batch of all 8 positions is passed into the policy network or value network and computed in parallel. For the value network, the output values are simply averaged, ( ) = ∑ ( ( ))</p><formula xml:id="formula_17">θ θ = v s v d s j j 1 8 1 8</formula><p>. For the policy network, the planes of output probabilities are rotated/reflected back into the original orientation, and averaged together to provide an ensemble prediction, (⋅| ) = ∑ ( (⋅| ( )))</p><formula xml:id="formula_18">σ σ = − p s d p d s j j j 1 8 1<label>8 1</label></formula><p>; this approach was used in our raw network evaluation (see Extended Data Table <ref type="table" target="#tab_6">3</ref>). Instead, APV-MCTS makes use of an implicit symmetry ensemble that randomly selects a single rotation/reflection j ∈ [1, 8] for each evaluation. We compute exactly one evaluation for that orientation only; in each simulation we compute the value of leaf node s L by v θ (d j (s L )), and allow the search procedure to average over these evaluations. Similarly, we compute the policy network for a single, randomly selected rotation/reflection, ( (⋅| ( ))) Policy network: reinforcement learning. We further trained the policy network by policy gradient reinforcement learning <ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26</ref> . Each iteration consisted of a minibatch of n games played in parallel, between the current policy network p ρ that is being trained, and an opponent ρ − p that uses parameters ρ − from a previous iter- ation, randomly sampled from a pool of opponents, so as to increase the stability of training. Weights were initialized to ρ = ρ − = σ. Every 500 iterations, we added the current parameters ρ to the opponent pool. Each game i in the mini-batch was played out until termination at step T i , and then scored to determine the outcome = ± ( ) z rs t i T i from each player's perspective. The games were then replayed to determine the policy gradient update, ∆ρ = ∑ ∑ ( − ( ))</p><formula xml:id="formula_19">α ρ = = ∂ ( | ) ∂ ρ z v s n i n t T p a s t i t i 1 1 log i t i t i</formula><p>, using the REINFORCE algorithm <ref type="bibr" target="#b25">25</ref> with baseline ( ) v s t i for variance reduction. On the first pass through the training pipeline, the baseline was set to zero; on the second pass we used the value network v θ (s) as a baseline; this provided a small performance boost. The policy network was trained in this way for 10,000 minibatches of 128 games, using 50 GPUs, for one day. Value network: regression. We trained a value network ( ) ≈ ( )</p><formula xml:id="formula_20">θ ρ v s v s p</formula><p>to approximate the value function of the RL policy network p ρ . To avoid overfitting to the strongly correlated positions within games, we constructed a new data set of uncorrelated self-play positions. This data set consisted of over 30 million positions, each drawn from a unique game of self-play. Each game was generated in three phases by randomly sampling a time step U ~ unif{1, 450}, and sampling the first t = 1,… U − 1 moves from the SL policy network, a t ~ p σ (•|s t ); then sampling one move uniformly at random from available moves, a U ~ unif{1, 361} (repeatedly until a U is legal); then sampling the remaining sequence of moves until the game terminates, t = U + 1, … T, from the RL policy network, a t ~ p ρ (•|s t ). Finally, the game is scored to determine the outcome z t = ±r(s T ). Only a single training example (s U+1 , z U+1 ) is added to the data set from each game. This data provides unbiased samples of the value function</p><formula xml:id="formula_21">E ( )= | ~ρ + + + + ... ρ v s z s a p [ , ] p U U U U T 1 1 1 1 ,</formula><p>. During the first two phases of generation we sample from noisier distributions so as to increase the diversity of the data set. The training method was identical to SL policy network training, except that the parameter update was based on mean squared error between the predicted values and the observed rewards,</p><formula xml:id="formula_22">∆θ = ∑ ( − ( )) α θ θ = ∂ ( ) ∂ θ z v s m k m k k v s 1 k</formula><p>. The value network was trained for 50 million mini-batches of 32 positions, using 50 GPUs, for one week. Features for policy/value network. Each position s was pre-processed into a set of 19 × 19 feature planes. The features that we use come directly from the raw representation of the game rules, indicating the status of each intersection of the Go board: stone colour, liberties (adjacent empty points of stone's chain), captures, legality, turns since stone was played, and (for the value network only) the current colour to play. In addition, we use one simple tactical feature that computes the outcome of a ladder search <ref type="bibr" target="#b7">7</ref> . All features were computed relative to the current colour to play; for example, the stone colour at each intersection was represented as either player or opponent rather than black or white. Each integer feature value is split into multiple 19 × 19 planes of binary values (one-hot encoding). For example, separate binary feature planes are used to represent whether an intersection has 1 liberty, 2 liberties,…, ≥8 liberties. The full set of feature planes are listed in Extended Data Table <ref type="table" target="#tab_5">2</ref>. Neural network architecture. The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. <ref type="figure">2b</ref> and Extended Data Table <ref type="table" target="#tab_6">3</ref> additionally show the results of training with k = 128, 256 and 384 filters.</p><p>The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit. Evaluation. We evaluated the relative strength of computer Go programs by running an internal tournament and measuring the Elo rating of each program. We estimate the probability that program a will beat program b by a logistic function Go player Fan Hui (2,908 at date of submission) <ref type="bibr" target="#b62">62</ref> . All programs received a maximum of 5 s computation time per move; games were scored using Chinese rules with a komi of 7.5 points (extra points to compensate white for playing second). We also played handicap games where AlphaGo played white against existing Go programs; for these games we used a non-standard handicap system in which komi was retained but black was given additional stones on the usual handicap points. Using these rules, a handicap of K stones is equivalent to giving K − 1 free moves to black, rather than K − 1/2 free moves using standard no-komi handicap rules. We used these handicap rules because AlphaGo's value network was trained specifically to use a komi of 7.5.</p><p>With the exception of distributed AlphaGo, each computer Go program was executed on its own single machine, with identical specifications, using the latest available version and the best hardware configuration supported by that program (see Extended Data Table <ref type="table" target="#tab_8">6</ref>). In Fig. <ref type="figure" target="#fig_2">4</ref>, approximate ranks of computer programs are based on the highest KGS rank achieved by that program; however, the KGS version may differ from the publicly available version.</p><p>The match against Fan Hui was arbitrated by an impartial referee. Five formal games and five informal games were played with 7.5 komi, no handicap, and Chinese rules. AlphaGo won these games 5-0 and 3-2 respectively (Fig. <ref type="figure" target="#fig_5">6</ref> and Extended Data Table <ref type="table" target="#tab_4">1</ref>). Time controls for formal games were 1 h main time plus three periods of 30 s byoyomi. Time controls for informal games were three periods of 30 s byoyomi. Time controls and playing conditions were chosen by Fan Hui in advance of the match; it was also agreed that the overall match outcome would be determined solely by the formal games. To approximately assess the relative rating of Fan Hui to computer Go programs, we appended the results of all ten games to our internal tournament results, ignoring differences in time controls. The policy network architecture consists of 128, or 256 filters in convolutional layers; an explicit symmetry ensemble over 2, 4 or 8 symmetries; using only the first 4, 12 or 20 input feature planes listed in Extended Data Table <ref type="table" target="#tab_4">1</ref>. The results consist of the test and train accuracy on the KGS data set; and the percentage of games won by given policy network against AlphaGo's policy network (highlighted row 2): using the policy networks to select moves directly (raw wins); or using AlphaGo's search to select moves (AlphaGo wins); and finally the computation time for a single evaluation of the policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extended Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extended Data Table 4 | Input features for rollout and tree policy</head><p>Features used by the rollout policy (first set) and tree policy (first and second set). Patterns are based on stone colour (black/white/empty) and liberties (1, 2, ≥3) at each intersection of the pattern. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE RESEARCH</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |Figure 2 |</head><label>12</label><figDesc>Figure 1 | Neural network training pipeline and architecture. a, A fast rollout policy p π and supervised learning (SL) policy network p σ are trained to predict human expert moves in a data set of positions.A reinforcement learning (RL) policy network p ρ is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network. A new data set is generated by playing games of self-play with the RL policy network. Finally, a value network v θ is trained by regression to predict the expected outcome (that is, whether</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 |</head><label>3</label><figDesc>Figure3| Monte Carlo tree search in AlphaGo. a, Each simulation traverses the tree by selecting the edge with maximum action value Q, plus a bonus u(P) that depends on a stored prior probability P for that edge. b, The leaf node may be expanded; the new node is processed once by the policy network p σ and the output probabilities are stored as prior probabilities P for each action. c, At the end of a simulation, the leaf node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 |</head><label>4</label><figDesc>Figure 4 | Tournament evaluation of AlphaGo. a, Results of a tournament between different Go programs (see Extended Data Tables6-11). Each program used approximately 5 s computation time per move. To provide a greater challenge to AlphaGo, some programs (pale upper bars) were given four handicap stones (that is, free moves at the start of every game) against all opponents. Programs were evaluated on an Elo scale 37 : a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS<ref type="bibr" target="#b38">38</ref> ; an approximate correspondence to human ranks is also shown,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | How AlphaGo (black, to play) selected its move in an informal game against Fan Hui. For each of the following statistics, the location of the maximum value is indicated by an orange circle. a, Evaluation of all successors s′ of the root position s, using the value network v θ (s′); estimated winning percentages are shown for the top evaluations. b, Action values Q(s, a) for each edge (s, a) in the tree from root position s; averaged over value network evaluations only (λ = 0). c, Action values Q(s, a), averaged over rollout evaluations only (λ = 1).</figDesc><graphic url="image-323.png" coords="4,248.88,561.24,105.62,105.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Publishers Limited. All rights reserved ARTICLE RESEARCH on high-performance MCTS algorithms. In addition, we included the open source program GnuGo, a Go program using state-of-the-art search methods that preceded MCTS. All programs were allowed 5 s of computation time per move.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 |</head><label>6</label><figDesc>Figure 6 | Games from the match between AlphaGo and the European champion, Fan Hui. Moves are shown in a numbered sequence corresponding to the order in which they were played. Repeated moves on the same intersection are shown in pairs below the board. The first</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>r v r</head><label></label><figDesc>where P(s, a) is the prior probability, W v (s, a) and W r (s, a) are Monte Carlo estimates of total action value, accumulated over N v (s, a) and N r (s, a) leaf evaluations and rollout rewards, respectively, and Q(s, a) is the combined mean action value for that edge. Multiple simulations are executed in parallel on separate search threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 ..</head><label>1</label><figDesc>Policy network: classification. We trained the policy network p σ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games. The data set was split into a test set (the first million positions) and a training set (the remaining 28.4 million positions). Pass moves were excluded from the data set. Each position consisted of a raw board description s and the move a selected by the human. We augmented the data set to include all eight reflections and rotations of each position. Symmetry augmentation and input features were pre-computed for each position. For each training step, we sampled a randomly selected mini-batch of m samples from the augmented KGS data set, an asynchronous stochastic gradient descent update to maximize the log likelihood of the action, The step size α was initialized to 0.003 and was halved every 80 million training steps, without momentum terms, and a mini-batch size of m = 16. Updates were applied asynchronously on 50 GPUs using DistBelief<ref type="bibr" target="#b61">61</ref> ; gradients older than 100 steps were discarded. Training took around 3 weeks for 340 million training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, and estimate the ratings e(•) by Bayesian logistic regression, computed by the BayesElo program 37 using the standard constant c elo = 1/400. The scale was anchored to the BayesElo rating of professional</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>At each time step t of each simulation, an action a t is selected from state s t</figDesc><table><row><cell>a</cell><cell>t</cell><cell>=</cell><cell cols="3">( ( Qs a argmax , t</cell><cell>)+ ( u s a , t</cell><cell>))</cell></row><row><cell></cell><cell></cell><cell></cell><cell>a</cell><cell></cell><cell></cell></row><row><cell cols="7">so as to maximize action value plus a bonus</cell></row><row><cell></cell><cell></cell><cell></cell><cell>( )∝ u s a ,</cell><cell>1</cell><cell cols="2">( ) + ( ) P s a N s a , ,</cell></row><row><cell cols="7">that is proportional to the prior probability but decays with</cell></row><row><cell cols="7">repeated visits to encourage exploration. When the traversal reaches a</cell></row><row><cell cols="7">leaf node s L at step L, the leaf node may be expanded. The leaf position</cell></row><row><cell cols="7">s L is processed just once by the SL policy network p σ . The output prob-</cell></row><row><cell cols="7">abilities are stored as prior probabilities P for each legal action a,</cell></row><row><cell>( )= ( | ) σ P s a p a s ,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.</figDesc><table><row><cell>Game 1</cell><cell>Game 2</cell><cell>Game 3</cell></row><row><cell>Fan Hui (Black), AlphaGo (White)</cell><cell>AlphaGo (Black), Fan Hui (White)</cell><cell>Fan Hui (Black), AlphaGo (White)</cell></row><row><cell>AlphaGo wins by 2.5 points</cell><cell>AlphaGo wins by resignation</cell><cell>AlphaGo wins by resignation</cell></row><row><cell>Game 4</cell><cell>Game 5</cell><cell></cell></row><row><cell>AlphaGo (Black), Fan Hui (White)</cell><cell>Fan Hui (Black), AlphaGo (White)</cell><cell></cell></row><row><cell>AlphaGo wins by resignation</cell><cell>AlphaGo wins by resignation</cell><cell></cell></row></table><note>© 2016 Macmillan Publishers Limited. All rights reserved of supervised and reinforcement learning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The leaf position s L is added to a queue for evaluation v θ (s L ) by the value network, unless it has previously been evaluated. The second rollout phase of each simulation begins at leaf node s L and continues until the end of the game. At each of these time-steps, t ≥ L, actions are selected by both players according to the rollout policy, ~(⋅| )</figDesc><table><row><cell>t</cell><cell>argmax a</cell><cell>t</cell><cell>,</cell><cell>t</cell><cell>,</cell><cell>a</cell><cell>using a variant of the PUCT</cell></row><row><cell>algorithm 48 , ( )= u s a ,</cell><cell>( ) c P s a , puct</cell><cell cols="2">∑ + ( ) ( ) N s b N s a , 1 , b r r</cell><cell cols="4">, where c puct is a constant determining</cell></row><row><cell cols="8">the level of exploration; this search control strategy initially prefers actions with</cell></row><row><cell cols="8">high prior probability and low visit count, but asymptotically prefers actions with</cell></row><row><cell>high action value.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">a p s t Evaluation (Fig. 3c). π outcome = ± ( ) z rs t T is computed from the final score. t . When the game reaches a terminal state, the</cell></row><row><cell>Backup (Fig.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 | Details of match between AlphaGo and Fan Hui</head><label>1</label><figDesc>The match consisted of five formal games with longer time controls, and five informal games with shorter time controls. Time controls and playing conditions were chosen by Fan Hui in advance of the match.</figDesc><table><row><cell>Date</cell><cell>Black</cell><cell>White</cell><cell cols="2">Category Result</cell></row><row><cell cols="2">5/10/15 Fan Hui</cell><cell cols="2">AlphaGo Formal</cell><cell>AlphaGo wins by 2.5 points</cell></row><row><cell cols="2">5/10/15 Fan Hui</cell><cell cols="3">AlphaGo Informal Fan Hui wins by resignation</cell></row><row><cell cols="3">6/10/15 AlphaGo Fan Hui</cell><cell>Formal</cell><cell>AlphaGo wins by resignation</cell></row><row><cell cols="3">6/10/15 AlphaGo Fan Hui</cell><cell cols="2">Informal AlphaGo wins by resignation</cell></row><row><cell cols="2">7/10/15 Fan Hui</cell><cell cols="2">AlphaGo Formal</cell><cell>AlphaGo wins by resignation</cell></row><row><cell cols="2">7/10/15 Fan Hui</cell><cell cols="3">AlphaGo Informal AlphaGo wins by resignation</cell></row><row><cell cols="3">8/10/15 AlphaGo Fan Hui</cell><cell>Formal</cell><cell>AlphaGo wins by resignation</cell></row><row><cell cols="3">8/10/15 AlphaGo Fan Hui</cell><cell cols="2">Informal AlphaGo wins by resignation</cell></row><row><cell cols="2">9/10/15 Fan Hui</cell><cell cols="2">AlphaGo Formal</cell><cell>AlphaGo wins by resignation</cell></row><row><cell cols="3">9/10/15 AlphaGo Fan Hui</cell><cell cols="2">Informal Fan Hui wins by resignation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 | Input features for neural networks</head><label>2</label><figDesc></figDesc><table><row><cell>Feature</cell><cell># of planes Description</cell></row><row><cell>Stone colour</cell><cell>3 Player stone / opponent stone / empty</cell></row><row><cell>Ones</cell><cell>1 A constant plane filled with 1</cell></row><row><cell>Turns since</cell><cell>8 How many turns since a move was played</cell></row><row><cell>Liberties</cell><cell>8 Number of liberties (empty adjacent points)</cell></row><row><cell>Capture size</cell><cell>8 How many opponent stones would be captured</cell></row><row><cell>Self-atari size</cell><cell>8 How many of own stones would be captured</cell></row><row><cell>Liberties after move</cell><cell>8 Number of liberties after this move is played</cell></row><row><cell>Ladder capture</cell><cell>1 Whether a move at this point is a successful ladder capture</cell></row><row><cell>Ladder escape</cell><cell>1 Whether a move at this point is a successful ladder escape</cell></row><row><cell>Sensibleness</cell><cell>1 Whether a move is legal and does not fill its own eyes</cell></row><row><cell>Zeros</cell><cell>1 A constant plane filled with 0</cell></row><row><cell>Player color</cell><cell>1 Whether current player is black</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 | Supervised learning results for the policy network</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Architecture</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Evaluation</cell><cell></cell></row><row><cell>Filters</cell><cell cols="2">Symmetries Features</cell><cell>Test accu-</cell><cell>Train accu-</cell><cell>Raw</cell><cell>net</cell><cell>AlphaGo</cell><cell>Forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell>racy %</cell><cell>racy %</cell><cell>wins %</cell><cell></cell><cell>wins %</cell><cell>time (ms)</cell></row><row><cell>128</cell><cell>1</cell><cell></cell><cell>54.6</cell><cell>57.0</cell><cell>36</cell><cell></cell><cell>53</cell><cell>2.8</cell></row><row><cell>192</cell><cell>1</cell><cell>48</cell><cell>55.4</cell><cell>58.0</cell><cell>50</cell><cell></cell><cell>50</cell><cell>4.8</cell></row><row><cell>256</cell><cell>1</cell><cell></cell><cell>55.9</cell><cell>59.1</cell><cell>67</cell><cell></cell><cell>55</cell><cell>7.1</cell></row><row><cell>256</cell><cell>2</cell><cell></cell><cell>56.5</cell><cell>59.8</cell><cell>67</cell><cell></cell><cell>38</cell><cell>13.9</cell></row><row><cell>256</cell><cell>4</cell><cell></cell><cell>56.9</cell><cell>60.2</cell><cell>69</cell><cell></cell><cell>14</cell><cell>27.6</cell></row><row><cell>256</cell><cell>8</cell><cell></cell><cell>57.0</cell><cell>60.4</cell><cell>69</cell><cell></cell><cell>5</cell><cell>55.3</cell></row><row><cell>192</cell><cell>1</cell><cell></cell><cell>47.6</cell><cell>51.4</cell><cell>25</cell><cell></cell><cell>15</cell><cell>4.8</cell></row><row><cell>192</cell><cell>1</cell><cell></cell><cell>54.7</cell><cell>57.1</cell><cell>30</cell><cell></cell><cell>34</cell><cell>4.8</cell></row><row><cell>192</cell><cell>1</cell><cell></cell><cell>54.7</cell><cell>57.2</cell><cell>38</cell><cell></cell><cell>40</cell><cell>4.8</cell></row><row><cell>192</cell><cell>8</cell><cell></cell><cell>49.2</cell><cell>53.2</cell><cell>24</cell><cell></cell><cell>2</cell><cell>36.8</cell></row><row><cell>192</cell><cell>8</cell><cell></cell><cell>55.7</cell><cell>58.3</cell><cell>32</cell><cell></cell><cell>3</cell><cell>36.8</cell></row><row><cell>192</cell><cell>8</cell><cell></cell><cell>55.8</cell><cell>58.4</cell><cell>42</cell><cell></cell><cell>3</cell><cell>36.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 |</head><label>5</label><figDesc>Parameters used by AlphaGo</figDesc><table><row><cell cols="2">Extended Data Symbol Parameter</cell><cell>Value</cell></row><row><cell>β</cell><cell>Softmax temperature</cell><cell>0.67</cell></row><row><cell>λ</cell><cell>Mixing parameter</cell><cell>0.5</cell></row><row><cell>n vl</cell><cell>Virtual loss</cell><cell>3</cell></row><row><cell>n thr</cell><cell>Expansion threshold</cell><cell>40</cell></row><row><cell>c puct</cell><cell>Exploration constant</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 | Results of a tournament between different Go programs</head><label>6</label><figDesc>Each program played with a maximum of 5 s thinking time per move; the games against Fan Hui were conducted using longer time controls, as described in Methods. CN4, ZN4 and PC4 were given 4 handicap stones; komi was 7.5 in all games. Elo ratings were computed by BayesElo.</figDesc><table><row><cell cols="2">Short name Computer Player</cell><cell>Version</cell><cell cols="4">Time settings CPUs GPUs KGS Rank</cell><cell>Elo</cell></row><row><cell>α d rvp</cell><cell cols="2">Distributed AlphaGo See Methods</cell><cell>5 seconds</cell><cell>1202</cell><cell>176</cell><cell>-</cell><cell></cell></row><row><cell>α rvp</cell><cell>AlphaGo</cell><cell>See Methods</cell><cell>5 seconds</cell><cell>48</cell><cell>8</cell><cell>-</cell><cell></cell></row><row><cell>CS</cell><cell>CrazyStone</cell><cell>2015</cell><cell>5 seconds</cell><cell>32</cell><cell>-</cell><cell>6 d</cell><cell></cell></row><row><cell>ZN</cell><cell>Zen</cell><cell>5</cell><cell>5 seconds</cell><cell>8</cell><cell>-</cell><cell>6d</cell><cell></cell></row><row><cell>P C</cell><cell>Pachi</cell><cell>10.99</cell><cell>400,000 sims</cell><cell>16</cell><cell>-</cell><cell>2 d</cell><cell></cell></row><row><cell>F G</cell><cell>Fuego</cell><cell>svn1989</cell><cell>100,000 sims</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>GG</cell><cell>GnuGo</cell><cell>3.8</cell><cell>level 10</cell><cell>1</cell><cell>-</cell><cell>5 k</cell><cell>431</cell></row><row><cell>CS 4 ZN 4 P C 4</cell><cell>CrazyStone Zen Pachi</cell><cell cols="2">4 handicap stones 5 seconds 4 handicap stones 5 seconds 4 handicap stones 400,000 sims</cell><cell>32 8 16</cell><cell>---</cell><cell>---</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 | Results of a tournament between AlphaGo and distributed AlphaGo, testing scalability with hardware</head><label>8</label><figDesc>Each program played with a maximum of 2 s thinking time per move. Elo ratings were computed by BayesElo.</figDesc><table><row><cell>AlphaGo</cell><cell>Search threads</cell><cell>CPUs</cell><cell>GPUs</cell><cell>Elo</cell></row><row><cell>Asynchronous</cell><cell>1</cell><cell>4 8</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>2</cell><cell>4 8</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>4</cell><cell>4 8</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>8</cell><cell>4 8</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>16</cell><cell>48</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>32</cell><cell>48</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>40</cell><cell>48</cell><cell>8</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>40</cell><cell>48</cell><cell>1</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>40</cell><cell>48</cell><cell>2</cell><cell></cell></row><row><cell>Asynchronous</cell><cell>40</cell><cell>48</cell><cell>4</cell><cell></cell></row><row><cell>Distributed</cell><cell>12</cell><cell>428</cell><cell>64</cell><cell></cell></row><row><cell>Distributed</cell><cell>24</cell><cell>764</cell><cell>112</cell><cell></cell></row><row><cell>Distributed</cell><cell>40</cell><cell>1202</cell><cell>176</cell><cell></cell></row><row><cell>Distributed</cell><cell>64</cell><cell>1920</cell><cell>280</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 | Cross-table of win rates in per cent between programs in the single-machine scalability study</head><label>10</label><figDesc>;17] 19[12;29] 38[30;47] -6 1[51;71] 65[51;76] 73[62;82] 74[59;85] 64[55;73] 12 [3;34] 16 8 14 [6;28] 14 [7;24] 29 [20;39] 39 [29;49] -5 2 [41;63] 61 [50;71] 52 [41;64] 41 [32;51] 5 [1;25] ;14] 16 [10;26] 27 [18;38] 39 [29;50] 48 [37;58] -4 3 [30;56] 41 [26;58] 4 [1;18] 40 4 0 [0;24] 17 [9;31] 19 [11;31] 26 [15;41] 48 [36;59] 56 [43;68] 57 [44;70] -2 9 [18;41] 2 [0;11] 40 2 4 [2;9] 16 [10;25] 22 [12;37] 36 [27;45] 59 [49;68] 74 [64;83] 59 [42;74] 71 [59;82] -5[1;17] 40 1 62 [48;75] 74 [62;83] 82 [72;90] 88 [66;97] 95 [75;99] 100 [70;100] 96 [82;99] 98 [89;100] 95 [83;99] -95% Agresti-Coull confidence intervals in grey. Each program played with 2 s per move; komi was 7.5 in all games.</figDesc><table><row><cell>Threads</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1 6</cell><cell>3 2</cell><cell>4 0</cell><cell>4 0</cell><cell>4 0</cell><cell>4 0</cell></row><row><cell></cell><cell>GPU</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell>1</cell><cell>8</cell><cell>-</cell><cell cols="9">7 0 [61;78] 90 [84;94] 94 [83;98] 86 [72;94] 98 [91;100] 98 [92;99] 100 [76;100] 96 [91;98] 38 [25;52]</cell></row><row><cell>2</cell><cell cols="3">8 30 [22;39] -</cell><cell cols="8">7 2 [61;81] 81 [71;88] 86 [76;93] 92 [83;97] 93 [86;96] 83 [69;91] 84 [75;90] 26 [17;38]</cell></row><row><cell>4</cell><cell cols="4">8 10 [6;16] 28 [19;39] -</cell><cell cols="7">6 2 [53;70] 71 [61;80] 82 [71;89] 84 [74;90] 81 [69;89] 78 [63;88] 18 [10;28]</cell></row><row><cell cols="3">8 6 [232 8 8 2 [0;9]</cell><cell cols="4">8 [3;17] 18 [11;29] 35 [24;49] 48 [37;59]</cell><cell>-</cell><cell cols="4">5 2 [42;63] 44 [32;57] 26 [17;36] 0 [0;30]</cell></row><row><cell>40</cell><cell>8</cell><cell>2 [1;8]</cell><cell>8 [4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">© 2016 Macmillan Publishers Limited. All rights reserved</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Feature planes used by the policy network (all but last feature) and value network (all features).© 2016 Macmillan Publishers Limited. All rights reserved</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Evaluating positions using rollouts only (αrp, αr), value nets only (αvp, αv), or mixing both (αrvp, αrv); either using the policy network pσ(αrvp, αvp, αrp), or no policy network (αrvp, αvp, αrp), that is, instead using the placeholder probabilities from the tree policy pτ throughout. Each program used 5 s per move on a single machine with 48 CPUs and 8 GPUs. Elo ratings were computed by BayesElo.© 2016 Macmillan Publishers Limited. All rights reserved</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="95" xml:id="foot_3">95% Agresti-Coull confidence intervals in grey. Each program played with 2 s per move; komi was 7.5 in all games.© 2016 Macmillan Publishers Limited. All rights reserved</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Fan Hui for agreeing to play against AlphaGo; T. Manning for refereeing the match; R. Munos and T. Schaul for helpful discussions and advice; A. Cain and M. Cant for work on the visuals; P. Dayan, G. Wayne, D. Kumaran, D. Purves, H. van Hasselt, A. Barreto and G. Ostrovski for reviewing the paper; and the rest of the DeepMind team for their support, ideas and encouragement.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supplementary Information is available in the online version of the paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE RESEARCH</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Online Content Methods, along with any additional Extended Data display items and Source Data</title>
		<idno type="DOI">10.1038/nature16961</idno>
		<imprint>
			<date type="published" when="2015-11-11">11 November 2015. January 2016</date>
		</imprint>
	</monogr>
	<note>are available in the online version of the paper; references unique to these sections appear only in the online paper</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Searching for Solutions in Games and Artificial Intelligence</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Allis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Maastricht, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Limburg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Games solved: now and in the future</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Den Herik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Uiterwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rijswijck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="277" to="311" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The games computers (and people) play</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computers</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="189" to="266" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Deep</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><surname>Blue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A world championship caliber checkers program</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="273" to="289" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From simple features to sophisticated evaluation functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="126" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Computer</surname></persName>
		</author>
		<author>
			<persName><surname>Go</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="145" to="179" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On-line policy improvement using Monte-Carlo search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galperin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1068" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">World-championship-caliber Scrabble</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="241" to="275" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monte-Carlo Go developments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Helmstetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Advances in Computer Games</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in Monte-Carlo tree search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing Elo ratings of move patterns in the game of Go</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICGA J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">State of the art open source Go program</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudiš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gailly</surname></persName>
		</author>
		<author>
			<persName><surname>Pachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Games</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="24" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fuego -an open-source framework for board games and Go engine based on Monte-Carlo tree search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Arneson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Intell. AI in Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="259" to="270" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining online and offline learning in UCT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural-network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian pattern ranking for move prediction in the game of Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mimicking Go experts with convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Move evaluation in Go using deep convolutional neural networks. 3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training deep convolutional neural networks to play go</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1766" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: an Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal difference learning of position evaluation in the game of Go</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="817" to="824" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluation in Go by a neural network using soft segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Advances in Computer Games Conference</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">267</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal-difference search in computer Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="183" to="219" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The mystery of Go, the ancient game that computers still can&apos;t win</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinovitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wired Magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">All Systems Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Sciences</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Computational intelligence in mind games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mandziuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Challenges for Computational Intelligence</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="407" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A chronology of computer chess and its literature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="201" to="214" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of Monte-Carlo tree search methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Intell. AI in Games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The grand challenge of computer Go: Monte Carlo tree search and extensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="106" to="113" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Whole-history rating: A Bayesian rating system for players of time-varying strength</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="http://www.gokgs.com/help/rmath.html" />
		<title level="m">KGS. Rating system math</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An analysis of alpha-beta pruning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="293" to="326" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to predict by the method of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to play chess using temporal differences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tridgell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="243" to="263" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bootstrapping from game tree search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Uther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Some studies in machine learning using the game of checkers II -recent progress</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="601" to="617" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal difference learning applied to a high-performance game-playing program</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hlynka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jussila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TD-gammon, a self-teaching backgammon program, achieves master-level play</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="215" to="219" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Go-playing program using neural nets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Honte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machines that learn to play games</title>
				<meeting><address><addrLine>Nova Science</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="205" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-armed bandits with episode context</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monte Carlo tree search with heuristic evaluations using implicit minimax backups</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pepels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Sturtevant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Intelligence and Games</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modification of UCT with patterns in Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<biblScope unit="volume">6062</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monte-Carlo simulation balancing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monte-Carlo simulation balancing in practice</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Computers and Games</title>
				<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The power of forgetting: improving the last-good-reply policy in Monte Carlo Go</title>
		<author>
			<persName><forename type="first">H</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Drake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Intell. AI in Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="309" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Investigating the limits of Monte-Carlo tree search methods in computer Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the scalability of parallel UCT</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Games</title>
		<imprint>
			<biblScope unit="volume">6515</biblScope>
			<biblScope unit="page" from="36" to="47" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A lock-free multithreaded Monte-Carlo tree search algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Advances in Computer Games Conference</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="14" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Time management for Monte-Carlo tree search applied to the game of Go</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Technologies and Applications of Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="462" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Monte-Carlo tree search and rapid action value estimation in computer Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="1856" to="1875" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Balancing MCTS by dynamically adjusting the komi value</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICGA J</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Active opening book application for Monte-Carlo tree search in 19×19 Go</title>
		<author>
			<persName><forename type="first">H</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Winands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Benelux Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<idno>81; 100] 100 [78; 100] 100 [78; 100] 71 [36; 92] 22 [6; 55] - 3 0 [16; 48] α p [81; 100] 99 [94; 100] 96 [79; 99] 52 [35; 67] 22 [16; 29] 70 [52; 84] - CS [97; 100] 74 [66; 81] 98 [94; 99] 80 [70; 87] 5 [3; 7] 36 [16; 61] 8 [5; 14] ZN [93; 100] 84 [67; 93] 98 [93; 99] 92</idno>
		<ptr target="http://www.goratings.org.αrv" />
		<title level="m">Go ratings</title>
				<imprint>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>45; 94] 78 [71; 84. 67; 99] 6 [2; 19] 40 [12; 77] 100 [65; 100</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P C</forename></persName>
		</author>
		<idno>98; 100] 99 [95; 100] 100 [98; 100] 98 [89; 100] 78 [73; 81] 87 [68; 95] 55 [47; 62]</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F G</forename></persName>
		</author>
		<idno>97; 100] 99 [93; 100] 100 [96; 100] 100 [91; 100] 78 [73; 83] 100 [65; 100] 65 [55; 73] GG [44; 100] 100 [34; 100] 100 [68; 100] 100 [57; 100] 99 [97; 100] 67 [21; 94] 99 [95; 100] CS 4 [69; 84] 12 [8; 18] 53 [44; 61] 15 [8; 24] 0 [0; 3] 0 [0; 30] 0 [0; 8] ZN 4 [77; 92] 25 [16; 38] 67 [56; 76] 14</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
	<note>7; 27] 0 [0; 12] 0 [0;</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><surname>P C 4</surname></persName>
		</author>
		<idno>97; 100] 82 [75; 88] 98 [95; 99] 89 [79; 95] 32 [26; 39] 13 [3; 36] 35 [25; 46]</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Each program played with a maximum of 5 s thinking time per move</title>
	</analytic>
	<monogr>
		<title level="m">CN4, ZN4 and PC4 were given 4 handicap stones</title>
				<imprint/>
	</monogr>
	<note>Agresti-Coull confidence intervals in grey</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">5 in all games. Distributed AlphaGo scored 77% [70; 82] against αrvp and 100% against all other programs (no handicap games were played)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
