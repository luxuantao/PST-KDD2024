<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Sentiment Prediction based on Automatic Discovery of Affective Regions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
							<email>yangjufeng@nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Dongyu</forename><surname>She</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
							<email>msunming@foxmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
							<email>paul.rosin@cs.cf.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Computer Science and Control Engineering</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<settlement>Wales</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Sentiment Prediction based on Automatic Discovery of Affective Regions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E1DF1253A2C86AB446D81FDA43C7157A</idno>
					<idno type="DOI">10.1109/TMM.2018.2803520</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2803520, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2803520, IEEE Transactions on Multimedia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affective region</term>
					<term>convolutional neural networks</term>
					<term>sentiment classification</term>
					<term>visual sentiment analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions via images and videos on-line. This paper investigates the problem of visual sentiment analysis, which involves a high-level abstraction in the recognition process. While most of the current methods focus on improving holistic representations, we aim to utilize the local information, which is inspired by the observation that both the whole image and local regions convey significant sentiment information. We propose a framework to leverage affective regions, where we first use an off-the-shelf objectness tool to generate the candidates, and employ a candidate selection method to remove redundant and noisy proposals. Then a convolutional neural network (CNN) is connected with each candidate to compute the sentiment scores, and the affective regions are automatically discovered, taking the objectness score as well as the sentiment score into consideration. Finally, the CNN outputs from local regions are aggregated with the whole images to produce the final predictions. Our framework only requires image-level labels, thereby significantly reducing the annotation burden otherwise required for training. This is especially important for sentiment analysis as sentiment can be abstract, and labeling affective regions is too subjective and labor-consuming. Extensive experiments show that the proposed algorithm outperforms the state-of-the-art approaches on eight popular benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the increasing popularity of social networks, more and more Internet users tend to express their opinions with different media types <ref type="bibr" target="#b0">[1]</ref>. Algorithms to identify sentiment can be helpful to understand such user behaviors <ref type="bibr" target="#b1">[2]</ref>. In particular, understanding the sentiment in visual media content (i.e., images, videos) has attracted increasing research attention. Potential use of approaches developed for visual sentiment analysis is broad, including affective image retrieval <ref type="bibr" target="#b2">[3]</ref>, aesthetic quality categorization <ref type="bibr" target="#b3">[4]</ref>, opinion mining <ref type="bibr" target="#b4">[5]</ref>, comment assistant <ref type="bibr" target="#b5">[6]</ref>, etc.</p><p>Inspired by psychology and the principles of art, researchers have investigated different groups of hand-crafted features (e.g., color <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, texture <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, shape <ref type="bibr" target="#b10">[11]</ref>) from image level, with the goal of endowing computers with the capability Twitter II <ref type="bibr" target="#b16">[17]</ref>. The bounding boxes indicate the local Affective Regions labeled by users. As can be seen, sentiments are evoked by the affective regions as well as the whole image appearance. of perceiving sentiment in the same manner as humans. Instead of designing visual features manually, Convolutional Neural Network (CNN) can automatically learn deep representations of images <ref type="bibr" target="#b11">[12]</ref>. Several researchers have also applied CNN to image sentiment classification <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> and demonstrated the superior performance of the deep features against hand-tuned features for sentiment classification.</p><p>Visual sentiment analysis is inherently more challenging than traditional recognition tasks, since it involves a much higher level of abstraction and subjectivity in the human recognition process <ref type="bibr" target="#b17">[18]</ref>. Recognizing sentiments evoked by images from social media is more difficult than many other visual recognition tasks, e.g., object classification <ref type="bibr" target="#b18">[19]</ref>, scene recognition <ref type="bibr" target="#b19">[20]</ref>, etc. It is necessary to take a rich set of cues into consideration for visual sentiment prediction. Most existing methods employing CNNs try to learn sentiment representations from the global perspective of whole images, whereas the visual sentiment can also be evoked from the local regions within images <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Different from detecting concrete visual objects <ref type="bibr" target="#b23">[24]</ref>, there are difficulties modeling the sentiment due to the "affective gap" between the low-level visual features and high-level sentiment <ref type="bibr" target="#b9">[10]</ref>.</p><p>Little work has paid close attention to the use of local information for sentiment analysis. Li et al. <ref type="bibr" target="#b22">[23]</ref> propose a context-aware classification model based on a bilayer sparse representation that simultaneously takes the local and global context into account. However, this approach is limited by its heavy dependence on the initial segmentation results to model appearances of different objects. In addition, they suppose that all regions have the same weights for sentiment prediction, which may go against the human attention theories that the human vision system selectively processes parts of an image in detail <ref type="bibr" target="#b24">[25]</ref>. You et al. <ref type="bibr" target="#b21">[22]</ref> try to match local image regions with the descriptive visual attributes, aiming to discover the specific-attribute regions, but lack generalization ability for sentiment analysis.</p><p>To address these problems, we propose to leverage local details as well as the global information for visual sentiment analysis. We introduce a new notion named Affective Regions (ARs), which contains two distinguishing characteristics:</p><p>1) an AR is a salient region and probably contains one or more objects, which can attract people's attention, and 2) an AR conveys significant sentiments. Fig. <ref type="figure" target="#fig_0">1</ref> shows some ARs in popular datasets <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. As can be seen, the visual sentiment can be induced from the ARs within images. For example, in the fourth image of (a), the sentiment is mostly evoked from the region of the bleeding hand, while in the second image of (b), the beautiful leaf rather than the gray stone conveys the positive sentiment. However, manually labeling the ARs of images for training the detector is too subjective and labor-consuming. This paper proposes a framework that only requires the image-level label to discover AR automatically, thereby significantly reducing the annotation burden.</p><p>In detail, we first use an off-the-shelf tool to generate bounding box candidates along with their objectness score for the input image, which is inspired by the strong cooccurrence relationships between objects and sentiment <ref type="bibr" target="#b25">[26]</ref>. Then a candidate selection method is employed to remove the redundant proposals while preserving several valuable ones. The deep CNN is connected with each candidate and used to compute sentiment score. The objectness score and sentiment score are combined to calculate the AR score, based on which the top-K ARs are discovered by re-ranking the candidate regions considering both the objectness score as well as the sentiment score. Finally, the CNN outputs from the global and local views are aggregated through alternative fusion operations (i.e., max pooling, sum pooling and concatenation) to produce the final predictions.</p><p>Our contributions are summarized as follows:</p><p>• We propose a deep framework for automatically discovering the affective regions of images which are likely to evoke significant sentiment information. Our framework is independent of object categories and requires no bounding box annotation, which is more general than the existing methods. • We build a visual sentiment prediction model using a deep CNN, which utilizes the holistic and local information from both the global image and the local regions. The final representation is effective for visual sentiment classification, and outperforms the state-of-the-art approaches on the affective datasets. • Experimental results show that our proposed framework can be generalized to the small-scale benchmarks with the help of transfer learning. This journal paper extends our earlier work <ref type="bibr" target="#b26">[27]</ref> in four aspects. <ref type="bibr" target="#b0">(1)</ref> The framework is improved by adding the candi-date selection module to suppress the possibly noisy proposals and reduce computational load. (2) Three alternative fusion operations are employed to combine the holistic representation with the affective regions, which aim to capture the local information in different ways. (3) More implementation details are provided and extensive experimental results on both largescale and small-scale datasets are presented, where the hyperparameters are determined systematically. (4) The consistency of the discovered affective regions and the ground truth is evaluated on the sentiment benchmark, showing that our proposed method can automatically find high-quality ARs without human annotations.</p><p>The rest of this paper is organized as follows. Sec. II summarizes the related work on visual sentiment analysis and deep learning. Sec. III introduces the proposed method of discovering affective regions and our deep framework for sentiment prediction. In Sec. IV and V, we present and visualize the experimental results on the popular benchmark datasets. And finally, Sec. VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Numerous methods for visual sentiment analysis have been developed based on still images <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and videos <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In this section, we review the methods for affective image prediction and region-based CNNs that are closely related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Affective Image Prediction</head><p>Previous methods on affective image prediction can be roughly divided into dimensional approaches and categorical ones. The dimensional approaches represent sentiment in the two dimensional (2-D) valence-arousal coordinate space <ref type="bibr" target="#b29">[30]</ref> or a three dimensional space <ref type="bibr" target="#b30">[31]</ref>. Hanjalic <ref type="bibr" target="#b31">[32]</ref> represents human affective response using three basic dimensions, i.e., valence, arousal and control (dominance), where there is a corresponding value for every affective state. Zhao et al. <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> propose to predict the personalized emotion perceptions of images in the valence-arousal space using shared sparse regression as a learning model. Meanwhile, the categorical approaches map sentiment into one of the representative categories. There is also some work predicting the discrete probability of different sentiment categories <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b38">[39]</ref>. Since categorical approaches make it easier for a human to understand, we target categorical sentiment prediction in this work.</p><p>1) Shallow modeling methods: Most previous methods on affective image prediction employ traditional low-level features. Machajdik et al. <ref type="bibr" target="#b9">[10]</ref> define a combination of rich hand-crafted features based on art and psychology theory, including composition, color variance and image texture, etc. Lu et al. <ref type="bibr" target="#b10">[11]</ref> investigate how shape features in natural images influence sentiments aroused in human beings, and provide evidence for the significance of roundness-angularity and simplicity-complexity for predicting sentiment content. Zhao et al. <ref type="bibr" target="#b7">[8]</ref> introduce more robust and invariant visual features designed according to art principles. These handcrafted visual features are proven to be effective on several small datasets, whose images are selected from a few specific domains, e.g., abstract paintings and art photos <ref type="bibr" target="#b9">[10]</ref>.</p><p>To bridge the "affective gap" between low-level features and high-level sentiment, Borth et al. <ref type="bibr" target="#b16">[17]</ref> model a midlevel concept, i.e., Adjective Noun Pairs (ANPs), which are used to detect image concepts instead of expressing sentiments directly. Li et al. <ref type="bibr" target="#b39">[40]</ref> further compute the weighted sum of the textual sentiment values of ANPs describing the image and take the textual sentiment into account. Yuan et al. <ref type="bibr" target="#b40">[41]</ref> propose the Sentribute, an image-sentiment analysis algorithm based on 102 mid-level attributes, which are easier to interpret and ready to use for high-level understanding. Furthermore, Zhao et al. <ref type="bibr" target="#b41">[42]</ref> combine features of different levels including low-level features from elements-of-art, midlevel features from principles-of-art and high-level features from a semantic concepts detector in a multi-graph learning framework. Chen et al. <ref type="bibr" target="#b25">[26]</ref> build object detection models to recognize six frequent objects including car, dog, dress, face, flower and food, and propose a new classification model to handle attributive and proportional similarity between visual sentiment concepts. In contrast, our algorithm concentrates on whether a selected region contains objects or not, which is independent to object categories and more robust for real applications.</p><p>2) Deep modeling methods: In recent years, CNNs have been incorporated into a number of visual recognition systems in a wide variety of domains <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The strength of these models lies in their ability to learn discriminative features from raw data inputs using the back propagation algorithm <ref type="bibr" target="#b46">[47]</ref>, in contrast to more traditional recognition pipelines which compute hand-engineered features on images as an initial preprocessing step <ref type="bibr" target="#b47">[48]</ref>.</p><p>Several recent methods exploit deep CNNs for image sentiment prediction. Based on their previous work <ref type="bibr" target="#b16">[17]</ref>, Chen et al. <ref type="bibr" target="#b48">[49]</ref> adapt deep networks for constructing DeepSentiBank, a classification model for visual sentiment concepts, which shows significant improvements in both annotation accuracy and retrieval performance. Also, some methods incorporate the model weights learned from a large-scale general dataset <ref type="bibr" target="#b49">[50]</ref>, and further fine-tune the CNNs for the task of visual sentiment prediction <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, two types of activations from CNNs are used as image-level features for classification, namely the 4096-dimensional output from fc7 and the 1000-dimensional output from fc8. You et al. <ref type="bibr" target="#b13">[14]</ref> employ a progressive strategy to train a CNN making use of half a million images that are labeled with the website meta data, and further perform benchmarking analysis on the Flickr and Instagram (FI) dataset. In <ref type="bibr" target="#b21">[22]</ref>, a method based on the attention model is developed in which local visual regions induced by sentiment related visual attributes are considered.</p><p>Due to the expensive manual annotation of sentiment labels, the existing affective datasets, including IAPSa <ref type="bibr" target="#b42">[43]</ref>, ArtPhoto <ref type="bibr" target="#b9">[10]</ref>, Abstract Paintings <ref type="bibr" target="#b9">[10]</ref>, Twitter I <ref type="bibr" target="#b13">[14]</ref>, Twitter II <ref type="bibr" target="#b16">[17]</ref> and EmotionROI <ref type="bibr" target="#b43">[44]</ref> typically contain less than two thousand images (see also Fig. <ref type="figure">2</ref>). This is far from the required scale for training robust deep models. The Flickr dataset <ref type="bibr" target="#b16">[17]</ref> is weakly-labeled with 2 categories using the meta-data provided by the up-loaders. Moreover, only the EmotionROI dataset has provided ground truth affective regions. Note that in this paper we focus on the binary sentiment (i.e., positive and negative) prediction problem, for which a variety of benchmark datasets with reliable ground-truth can be employed to validate the effectiveness of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Region-based CNNs</head><p>We trace the roots of our approach to region-based CNN (R-CNN) <ref type="bibr" target="#b45">[46]</ref>, an algorithm applying deep CNN to bottomup generate region proposals in order to localize and segment objects. It has been proved that when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly <ref type="bibr" target="#b50">[51]</ref>. Girshick <ref type="bibr" target="#b51">[52]</ref> shows that it is possible to further reduce training and testing time, while improving detection accuracy and simplifying the training process, using an approach called Fast R-CNN. Fast R-CNN reduces detection time excluding region proposal computation to 50-300ms per image, depending on network architecture. Ren et al. <ref type="bibr" target="#b23">[24]</ref> introduce a fully-convolutional network version that simultaneously predicts object bounds and objectness scores at each position. Meanwhile, R-CNN has been applied to various tasks, e.g., pedestrian detection <ref type="bibr" target="#b52">[53]</ref>, action detection <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> and semantic segmentation <ref type="bibr" target="#b55">[56]</ref>.</p><p>Different from the traditional methods on region based CNNs for finding salient objects in an image, our work aims to automatically identify the ARs that evoke sentiment and use the local information as the supplementary sentiment representation. This requires us to analyze not only the regions containing objects but also the surrounding background <ref type="bibr" target="#b20">[21]</ref>, which may have affective influence on the selected regions. Moreover, R-CNN based methods require ground truth bounding box annotations for training, but it is time-and labor-consuming to label affective regions manually. In this paper, we employ an off-the-shelf tool to generate object proposals as candidate affective regions and propose to select the AR considering the low-level as well as the affectivelevel content. Compared with the methods requiring accurate segmentation <ref type="bibr" target="#b22">[23]</ref> or concrete category information <ref type="bibr" target="#b25">[26]</ref>, it is much easier to acquire object proposals in the preprocessing stage, and will better generalize to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discovering Affective Regions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Convolutional Neural Networks for Affective Image Classification Fusion Operation</head><p>Output Sentiment Prediction Fig. <ref type="figure">3</ref>. Pipeline of the proposed approach. Given the input image, thousands of candidates along with the objectness scores are generated, and the candidate selection method is applied to remove the candidates which are overlapped and less important. The sentiment score of each proposal is roughly computed through CNN, which is then combined with the objectness score to discover affective regions. Finally, the sentiment label is predicted by fusing the local information with the holistic representation using several alternative operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we aim to develop an algorithm to automatically discover ARs carrying significant sentiments and combine the standard holistic representation with a local representation for image sentiment analysis. Fig. <ref type="figure">3</ref> shows the pipeline of our proposed framework. We use an object detection technique, i.e., Edgeboxes <ref type="bibr" target="#b56">[57]</ref>, to produce the candidate windows guiding the search for ARs, and then apply the candidate selection method to reduce redundant and noisy proposals. Thus, the sentiment content of each proposal is estimated at both the low-level and affective-level for the ARs detection. Finally, the deep representation of the detected ARs is combined with the holistic representation through three alternative fusion strategies, i.e., max pooling, sum pooling and concatenation, to generate the final predictions.</p><p>A. Producing Candidate Proposals 1) Generating: Detecting concrete visual objects like dogs and cars has been researched extensively in computer vision <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b57">[58]</ref>. However, modeling abstract emotional concepts like amusement and excitement is very challenging. The difficulty comes from the "affective gap" that lies between low-level visual features and high-level sentiment. Previous methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref> have proved that associating adjectives with concrete objects can make the combined visual concepts more detectable and tractable for visual sentiment analysis. Inspired by the strong co-occurrence relationships between objects and sentiment, we suggest that object proposals can be used as the potential sentiment regions.</p><p>Since our framework takes the object proposals as inputs and obtains the final prediction by fusing the prediction of each affective region with the holistic representation, the performance of the proposed framework largely depends on the quality of the candidate regions. However, an effective candidate extraction approach is challenging since the affective region detection needs to capture not only objects but also regions of the background that may evoke sentiment. There are two criteria that should be satisfied. First, the proposed framework is based on the assumption that the candidate proposals can cover the objects in the affective images as well as parts of the background, which requires a high detection recall rate. Second, since the selected affective region proposals are then fed into the CNN, only a limited number of candidates should be produced so as to allow for efficiency whilst maintaining accuracy.</p><p>During the past decades, many object proposal methods have been proposed to tackle the object detection problem. According to <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, EdgeBoxes <ref type="bibr" target="#b56">[57]</ref> and BING <ref type="bibr" target="#b60">[61]</ref> are faster than methods such as Selective Search <ref type="bibr" target="#b61">[62]</ref> and Objectness <ref type="bibr" target="#b62">[63]</ref>, while EdgeBoxes achieves better quality of proposals compared to BING. Considering the balance between the speed and quality, this paper uses EdgeBoxes to generate a set of candidate windows as it provides the best trade-off. Such an off-the-shelf tool can generate thousands of candidate boxes in a fraction of a second, from which a subsequent refinement step based on object boundary estimates is applied to improve localization. For a given image I, a set of candidate bounding boxes with objectness score B = {b i ; Obj score I i } n i=1 is produced by EdgeBoxes. 2) Selecting and filtering: To achieve high recall for object detection, Zitnick et al. <ref type="bibr" target="#b56">[57]</ref> employ a bottom-up strategy, generating thousands of proposals in each image. However, most of the candidate proposals are heavily overlapped and redundant for predicting sentiment. It is necessary to filter out the noisy region proposals carrying little sentiment, and removing noisy proposals at the initial stage of the algorithm can greatly reduce the computation time of the subsequent steps. To address this problem, we introduce the candidate selection module to select proposals from the affective region candidates inspired by <ref type="bibr" target="#b63">[64]</ref>.</p><p>EdgeBoxes generates thousands of proposals in each image that can achieve high recall for object detection. However, since it still generates a large number of original object windows for the CNN to process, following <ref type="bibr" target="#b64">[65]</ref>, we first check the same geometric characteristics (i.e., the area and height/width ratio) of candidate bounding boxes. We empirically filter out the regions with small areas (&lt; 800 pixels) or with high height/width (or width/height) aspect ratios above a threshold (&gt; 6), since objects which are either too small or too long are unlikely to attract people's attention. Thus, a much smaller number of proposals can be fed into the candidate selection method. Following previous algorithms <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, we build the affinity matrix W ∈ R n×n for each image, in which each element denotes the intersection-over-union (IoU) scores between any pair of the bounding boxes and n denotes the number of candidates:</p><formula xml:id="formula_0">W ij = |b i ∩ b j | |b i ∪ b j | ,<label>(1)</label></formula><p>where | • | is used to measure the numbers of pixels. We then apply the normalized cut algorithm <ref type="bibr" target="#b66">[67]</ref> to group the candidate bounding boxes into m clusters. In detail, the normalized graph Laplacian matrix</p><formula xml:id="formula_1">L = D -1 2 (D -W )D -1 2 is computed where D ∈ R n×n is a diagonal matrix with D ii = n j=1 W ij . Then the eigenvector matrix V = [v 1 , • • • , v m ] ∈ R n×m is constructed where {v 1 , • • • , v m }</formula><p>are the m smallest eigenvectors of L. Finally the k-means clustering algorithm is used to obtain m cluster labels, where each row of V is the feature of the corresponding sample <ref type="bibr" target="#b66">[67]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">4</ref>, the bounding boxes are first filtered out to reduce the computational load. Then with the m clusters' bounding boxes, we pick the proposal with the highest objectness score in each cluster and generate m candidate regions H = {h i } m i=1 for each image. Compared to the greedy non-maximum suppression (NMS) method that is widely used for filtering <ref type="bibr" target="#b45">[46]</ref>, our candidate selection method can generate a specific number of proposals while removing the redundant and noisy bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discovering Affective Regions</head><p>1) Initializing the framework: CNNs achieve the stateof-the-art performance in the related computer vision tasks, e.g., aesthetic quality rating <ref type="bibr" target="#b3">[4]</ref> and image style recognition <ref type="bibr" target="#b67">[68]</ref> by fine-tuning the pre-trained ImageNet model. In this work, the CNN is based on the deep model VGGNet <ref type="bibr" target="#b68">[69]</ref> with 16 layers. In order to adapt the pre-trained model on ImageNet for sentiment analysis, the CNN is first fine-tuned on the target affective dataset (e.g., Flickr and Instagram) utilizing the original images (without any bounding boxes) to adjust the parameters of the deep model. As a supervised learning approach, the fine-tuned CNN is applied to learn a function f : I → L, from a collection of affective training examples </p><formula xml:id="formula_2">{(I i , l i )} N i=1</formula><p>, where N is the size of the training set, I i is the input image, and l i is the associated sentiment label. In the standard training process, the traditional classification loss is optimized to maximize the probability of the correct class <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Let d i be the output from the penultimate layer, then the fine-tuning of the last layer is done by minimizing the softmax loss function as follows:</p><formula xml:id="formula_3">l(W) = N i=1 j∈l 1(l i = j) log p(l i = j|d i , w j ),<label>(2)</label></formula><p>where W = {w j } j l is the set of model parameters, and the indicator function</p><formula xml:id="formula_4">1(s) = 1 if s is true, otherwise 1(s) = 0.</formula><p>The probability of each sentiment label p(l i = j|d i , w j ) can be defined by the softmax function:</p><formula xml:id="formula_5">p(l i = j|d i , w j ) = exp(w T j d i ) j l exp(w T j d i )<label>(3)</label></formula><p>Since the number of categories in the affective dataset is not equal to that of ImageNet, the fc8 classification layer is changed to 2-way required by the sentiment dataset, which can produce a probability prediction over the sentiment classes.</p><p>2) Estimating sentiment score: For the affective-level quality of the candidate proposals, we compute the sentiment scores by feeding the proposal to the CNN. For the generated affective candidates H = {h i } m i=1 of the input image I, let {y ij } c j=1 be the output vector of the last layer indicating the probability of the i-th proposal carrying the j-th class sentiment, and c is set to 2 as the number of sentiment classes. If the prediction values for each sentiment are similar then this usually indicates that it is difficult to distinguish the sentiments evoked by the proposal. Therefore, we aim to keep only those proposals which contain a dominant sentiment. We define a 0 1</p><p>Senti_score Obj_score Fig. <ref type="figure">5</ref>. Visualization of the objectness score and sentiment score in an image. For example, the region with a high objectness score indicates that the corresponding bounding box is extremely likely to be an object.</p><p>probabilistic sampling function to evaluate the sentiment score of the i-th region in an affective-level perspective as follows:</p><formula xml:id="formula_6">Senti score I i = c j=1 y ij * log y ij + 1,<label>(4)</label></formula><p>where the score ranges between 0 and 1 for binary classification. The information entropy defined in Eqn. ( <ref type="formula" target="#formula_6">4</ref>) represents the degree of uncertainty when predicting sentiment, which is also consistent with the affective-level estimation of the proposal. The Senti score I i can be high-level providing a more semantic measurement compared to the traditional methods.</p><p>3) Selecting affective regions: We choose ARs according to two aspects: i) how likely the region contains an object, which is represented as Obj score I i , and ii) how much the region carries sentiment at the affective-level, referred to as Senti score I i . Fig. <ref type="figure">5</ref> demonstrates that an affective region should have both high Obj score I i and Senti score I i . The reason is that the Obj score I i only measures the probability of regions containing an object and is based on the texture appearance, which lacks the guidance of semantic information. The Senti score I i reflects the sentiment of images at the affective level, which enables lots of noisy regions to be removed with little impact on the sentiment analysis. Such a score allows certain flexibility for the object regions, which may occur in the background as well. Considering the characteristics of each score, we introduce the AR score to evaluate the sentiment quality of each region with the following definition:</p><formula xml:id="formula_7">AR score I i = (1 -α) * Obj score I i 2 + α * Senti score I i 2 ,<label>(5)</label></formula><p>where α controls the trade-off between low-level and affectivelevel perspectives. In this paper, we select α by the cross validation of the large-scale affective dataset. The proposals with high AR score are considered be an AR and used for sentiment prediction, while the proposals with low AR score are removed from the candidate set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sentiment Classification</head><p>Based on the initialized framework, the sentiment classification of a given image can be summarized as follows. Given a test image, we first generate the affective candidates based on EdgeBoxes. In order to reduce redundancy, we apply the candidate selection method based on their IoU scores and keep just the best candidates. Both objectness score and sentiment score are considered for selecting affective regions that are likely to attract people's attention and include emotional content. Then, for each proposal as well as the holistic image, a c-dimensional predictive result is obtained by the CNN, which is then fused into a final prediction. In particular, we consider three strategies, namely max pooling, sum pooling and concatenation. We utilize the cross-candidates pooling operation to fuse the outputs from the CNN into an integrated prediction. With max pooling, the high prediction scores from those candidates containing sentiment are preserved and the noisy ones are ignored. The sentiment probability Y of a given image can be defined as follows:</p><formula xml:id="formula_8">Y = max Y Global , { Y ARj } K j=1 ,<label>(6)</label></formula><p>where Y Global represents the prediction of the whole image and Y ARj represents the prediction of the j-th affective region, and we select the top K affective regions based on Eqn. <ref type="bibr" target="#b4">(5)</ref>. Y , Y Global and Y ARj share the same vector structure of (y pos , y neg ), where y pos and y neg indicate the predicted probability of positive and negative sentiments, respectively. The sum pooling fuses the prediction probability of all the proposals, where the weights of consistent proposals can be emphasized.</p><formula xml:id="formula_9">Y = (1 -β) * Y Global + β * 1 K * K j=1 Y ARj ,<label>(7)</label></formula><p>where β is the trade-off between global and local prediction. The β is also estimated by cross-validation of the large-scale affective dataset. Both max pooling and sum pooling can generate the sentiment probability as the final prediction. Concatenation is a simple but effective way by combing the features for a comprehensive representation:</p><formula xml:id="formula_10">Y = Y Global , { Y ARj } K j=1 .<label>(8)</label></formula><p>The final feature is generated by concatenating all the prediction results, and the dimension of Y is (K + 1) × c. In our experiments, we set the number of affective regions in all samples to be the same, making it feasible to classify the concatenated feature vector using an SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we present our experiments and evaluate our method against the state-of-the-art deep methods to validate the effectiveness of our framework for sentiment classification and sentiment detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We evaluate our proposed method on eight widely-used datasets, including IAPSa <ref type="bibr" target="#b42">[43]</ref>, ArtPhoto <ref type="bibr" target="#b9">[10]</ref>, Abstract Paintings <ref type="bibr" target="#b9">[10]</ref>, Twitter I <ref type="bibr" target="#b13">[14]</ref>, Twitter II <ref type="bibr" target="#b16">[17]</ref>, EmotionROI <ref type="bibr" target="#b20">[21]</ref>, Flickr <ref type="bibr" target="#b16">[17]</ref> and Flickr and Instagram (FI) <ref type="bibr" target="#b15">[16]</ref>. We divide the datasets into small-scale and large-scale datasets with respect to the number of images, as shown in Fig. <ref type="figure" target="#fig_3">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Small-scale datasets:</head><p>The International Affective Picture System (IAPS) <ref type="bibr" target="#b69">[70]</ref> is a common stimulus dataset which is widely used in visual sentiment analysis research <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b70">[71]</ref>. IAPSa selects 395 pictures from IAPS and is labeled with Mikel's eight sentiment categories. ArtPhoto contains 806 artistic photographs from a photo sharing site and the ground truth labeling is provided by the owner of each image. Abstract Paintings contains 228 peer rated abstract paintings consisting of color and texture. Twitter I is collected from social websites and labeled with two categories (i.e., positive, negative) by Amazon Mechanical Turk (AMT) workers, and contains 1,269 images in total. We test our method on all of the three subsets of Twitter I, including "Five agree", "At least four agree" and "At least three agree", in a similar fashion to <ref type="bibr" target="#b13">[14]</ref>. "Five agree" indicates that all the five AMT workers give the same sentiment label for a given image. Twitter II contains 603 images from the Twitter website, and the ground truths are obtained by AMT annotation too, resulting in 470 positive and 133 negative labels. EmotionROI is created as a sentiment prediction benchmark, and is collected from Flickr resulting in 1,980 images with six sentiment categories. They use AMT to collect 15 responses to the regions that evoke sentiment and represent the ground truth by assuming the influence of each pixel on evoked sentiments is proportional to the number of drawn rectangles covering that pixel.</p><p>2) Large-scale datasets: FI is currently the largest welllabeled dataset, which is collected by querying with eight sentiment categories as keywords from social websites. 225 AMT workers were employed to label the images which resulted in 23,308 images receiving at least three agreements. We divide FI into binary datasets the same as the IAPSa. Flickr contains 484,258 images in total, where each image was automatically labeled using the corresponding ANP.</p><p>Since we focus on the binary sentiment prediction, we  convert the multi-sentiment labels into positive and negative ones according to their valance for datasets except for Twitter I, Twitter II and Flickr, which were originally labeled with binary sentiment. Specifically, for IAPSa, ArtPhoto, Abstract Paintings, and FI, we divide Mikel's eight sentiment categories into binary labels according to <ref type="bibr" target="#b42">[43]</ref>, which suggests that amusement, awe, contentment and excitement are positive sentiments and anger, disgust, fear and sadness are negative sentiments. EmotionROI is labeled with seven sentiments (i.e., anger, disgust, fear, joy, sadness, surprise, neutral) along with Valance-Arousal scores, where anger, disgust, fear, sadness can similarly be considered as the negative sentiments.</p><p>Since the mean valance of the set of joy and surprise images is higher than the mean valance of the set of negative images, we treat them as positive sentiment. Note that we do not include images with neutral sentiment in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>CNNs have the capability to incorporate model weights learned from a more general dataset, which is a convenient property for tasks lacking sufficient training data. We employ the VGGNet with 16 layers <ref type="bibr" target="#b68">[69]</ref> as our basic architecture. Following previous works <ref type="bibr" target="#b12">[13]</ref>, we initialize our model with the weights trained from ImageNet. Then the pre-trained network is fine-tuned on the large-scale datasets with the 1000way fc8 classification layer replaced by the 2-way layer, and the data are split randomly into 80% training, 5% validation and 15% testing sets. The learning rates of the convolutional layers and the last fully-connected layer are initialized as 0.001 and 0.01 respectively. We fine-tune all layers by stochastic gradient descent through the whole net using a batch size of 64. A total of 100,000 iterations is run to update the parameters extract more precise sentiment-related information. All our experiments are carried out on two NVIDIA GTX 1080 GPUs with 32 GB of CPU memory. For the candidate selection method, we set m = 50 for each image as the experimental trade-off between the performance and computational time, which provides the initial candidate proposals for discovering the affective regions.</p><p>With the help of transfer learning, we also employ our framework on small-scale datasets with limited training examples. In detail, we use the parameters of the CNN trained on FI on other datasets and fine-tune the model on the training set of other datasets. The small datasets are randomly split into 80% training and 20% testing sets except those with a specified training/testing split <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b43">[44]</ref> and we conduct the experiments using 5-fold cross validation and average the accuracies as the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline</head><p>In the following subsections, we evaluate the proposed method against the state-of-the-art algorithms for image sentiment prediction, including those based on hand-crafted features and deep methods. In addition, we also show the results with different configurations of the proposed method on the validation set, especially with different components and fusion strategies.</p><p>1) Hand-crafted features: We extract several low-level features from the small-scale datasets, including local descriptors like SIFT, HOG, GIST, etc. The global color histograms (GCH) features consists of 64-bin RGB histogram, while the local color histogram features (LCH) first divide the image into 16 blocks and use a 64-bin RGB histogram for each block <ref type="bibr" target="#b71">[72]</ref>. We use the ColorName to count the pixels of each of the 11 basic colors presented on the image using the algorithm in <ref type="bibr" target="#b9">[10]</ref>. We also use SentiBank <ref type="bibr" target="#b16">[17]</ref>, a concept detector library based on the constructed ontology, to exploits the 1,200 dimensional features as mid-level representation. Zhao et al. <ref type="bibr" target="#b7">[8]</ref> propose the principle of art features (PAEF) for sentiment analysis. We use a simplified version provided by the author to extract 27 dimension features.</p><p>2) Deep methods: PCNN proposed by You et al. <ref type="bibr" target="#b13">[14]</ref> is a novel progressive CNN architecture. They suggest that leveraging larger amounts of weakly supervised data can improve the generalizability of the model. We fine-tune the PCNN with the noisy Flickr dataset based on VGGNet and extract the deep visual features. DeepSentiBank <ref type="bibr" target="#b48">[49]</ref> is a visual sentiment concept classification based on CNNs for discovering ANPs. We apply the pre-trained DeepSentiBank to extract 2,089 ANPs as mid-level representations for sentiment. We also show the performance of deep visual features of CNN models on ImageNet and fine-tuned on the affective datasets, including different architectures, i.e., AlexNet and VGGNet. To compare with the ImageNet CNN, we show the results of using LIBSVM <ref type="bibr" target="#b72">[73]</ref> trained on features extracted from the second to the last layer of the model and reduce the dimensionality employing PCA. In practice, we find that different cost values (parameter C in LIBSVM) produce similar accuracy, so we just use the default value and use the one v.s. all strategy following the same evaluation routine described in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Large-Scale Datasets</head><p>We first fine-tune the CNN on the large scale datasets (i.e., FI and Flickr), and compare the performance of our framework with the deep methods. Fig. <ref type="figure" target="#fig_4">7</ref> reports the performance of the baselines on the test set of the FI and Flickr datasets. As can be seen, the pre-trained model on the ImageNet is inferior to the fine-tuned model due to the differences between the distributions in the ImageNet and sentiment datasets, while VGGNet with a deeper architecture performs better than AlexNet. The fine-tuned VGG achieves 83.05% on the FI dataset, which outperforms DeepSentiBank (61.54%) and PCNN (75.34%). Compared to the weakly-labeled Flickr, the fine-tuned CNN on FI shows greater improvement in performance due to the reliable annotation.</p><p>When selecting and combining affective regions in the deep model, we have several choices: we can use the objectness score or sentiment score only, or use the AR score proposed in this work. We roughly consider the objectness score as a low-level cue and sentiment score as a high-level cue. The experimental results show that the sentiment score is more effective than the objectness score, which is mainly because Fig. <ref type="figure" target="#fig_0">10</ref>. Precision-recall curve for discovering affective regions. Our method is more consistent with human annotation than objectness (i.e., using the proposals with the highest objectness scores generated by EdgeBoxes).</p><p>the objectness score just indicates how likely a region contains an object. When both scores are combined into a deep model, our method using the most confident affective regions achieves 84.83%, which performs favorably against the state-of-the-art methods as well as combing the proposals selected by only one score, demonstrating the benefit of using local details for classification. Analyzing the objectness and sentiment score of different regions, we observe that the sentiment score often gives different values even when the area of overlap of two different proposals is more than half. For two different regions proposals both containing an affective region, the sentiment scores are usually similar, and thus it only needs to evaluate whether the proposal contains an affective region and ignore the area of proposal.</p><p>1) The effect of the hyper-parameters: We report the classification performance of using "AR + sum pooling" methods on the validation set of the FI dataset, and different α and β are employed for comparison. As shown in Fig. <ref type="figure" target="#fig_5">8</ref>, setting α = 0.6 achieves the best overall accuracy for discovering affective regions in the validation set. Using only the objectness score (α = 0) gives limited performance, which indicates it is necessary to use the sentiment score for selecting affective regions. On the other hand, combining local regions can boost the classification performance compared with using a single global representation. Setting β = 0.3 achieves a balance in most cases. Therefore, we use α = 0.6, β = 0.3 in the remaining experiments.</p><p>2) The effect of the fusion operations: When fusing the outputs of the affective region and the entire image, we consider three fusion operations for combing the most confident affective regions. Fig. <ref type="figure" target="#fig_4">7</ref> (bottom) shows that all three combinations are useful for capturing information in the holistic and regional view, while concatenation is the most effective way since it retains all the information.</p><p>3) The effect of the hyper-parameter K: Given an input image, we not only predict the sentiment of the whole image but also find the affective regions. Although the dataset does not provide annotations of affective regions, the number of affective regions is usually small. Here we show an experiment to determine how many affective regions should be evoked in our proposed framework. It is hard to evaluate the quality of the discovered affective regions directly due to lack of annotations. Therefore, our aim is to discover how many affective regions can boost sentiment prediction accuracy. We show the classification performance when combing different numbers of affective regions for sentiment analysis. As shown Fig. <ref type="figure" target="#fig_6">9</ref>, as the number of affective regions is increased, the accuracy increases as more information becomes available. However, a further increase in the number of regions leads to a slight decrease in performance due to the introduction of noisy regions. Therefore, as a good balance, we choose to combine 8 affective regions for sentiment analysis in the remaining experiments, which outperforms the fine-tuned VGGNet by 3.3% on FI (86.35%) and 1% on Flickr (71.13%). We also report the true positive rate of different sentiments on the large-scale datasets. In detail, the positive and negative sentiments achieve 92.10% and 72.65% on FI, respectively; and on Flickr achieve 73.56% and 47.92%, respectively. For both datasets, the positive class receives a higher accuracy than the negative class, which is consistent with the number of training images. More training images can lead to a higher probability that the corresponding sentiment receives a higher true positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on Small-Scale Datasets</head><p>We transfer the parameters learned on the FI dataset to small-scale datasets, then show our experimental results in Fig. <ref type="figure" target="#fig_0">11</ref> and provide comparisons to several state-of-the-art works. Note that our method is based on the fine-tuned VGGNet. "obj" means that we only regard the proposals with relatively high objectness score as affective regions and "senti" refers to the proposals having high sentiment score. Our "AR" method selects affective regions, where both objectness score and sentiment score are considered.</p><p>For the color features, ColorName is usually not enough to describe the distribution of image color compared to GCH and LCH except for the Abstract dataset. For the texture features, the HOG descriptor is able to achieve the best prediction accuracy in most datasets compared with other texture representations like SIFT, Gist, LBP and Gabor. Texture has better discriminative power than color on these small datasets. The reason is that sentiments are usually conveyed through complicated texture regions, e.g., faces, dogs, buildings etc. In addition, we also compare the different encoding algorithms in Fig. <ref type="figure" target="#fig_0">11</ref>. As can be seen, it achieves better performance while using the Fisher Vector to encode these descriptors on most datasets.</p><p>Compared with the traditional representations based mainly on color and texture information, the deep methods achieve better results, as expected. Our proposed method employs affective regions and outperforms both hand-crafted featuresbased methods and deep approaches, and achieves the best accuracy in all the small datasets. In detail, compared to the SentiBank and DeepSentiBank which do not use affective regions and represent images at the mid-level, our method outperforms them by a large margin. Furthermore, our method also shows an advantage over PCNN on all the affective datasets, and the three fusion operations are all useful with concatenation being the most effective method. According to our experimental findings on the large-scale datasets, when we increase the number of affective regions many regions have little impact on image sentiment and can even decrease the prediction accuracy. Therefore, we combine the same number of regions for the final sentiment prediction on the small-scale datasets and achieve the best performance. This shows another advantage of our method, which is that we do not need many local regions to be included in the deep model, ensuring an acceptable increase in computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Affective Regions Evaluation</head><p>We evaluate the affective regions detected by our framework on the EmotionROI dataset, taking the same training/testing split as the previous works <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b73">[74]</ref>. Since the dataset only provides as ground truth the normalized Emotion Stimuli Map, which is based on 15 bounding boxes, we first binarize the Emotion Stimuli Map with threshold values γ ∈ [0..255]/255, and compare the ground truth region with the most confident discovered affective regions. Precision and recall are employed, which represents the percentages of detected emotionally involved pixels out of all the pixels identified in the predicted region or the ground truth. Following <ref type="bibr" target="#b43">[44]</ref>, all the predicted affective regions and ground truth are normalized to 0 to 1 for evaluation. Fig. <ref type="figure" target="#fig_0">10</ref> shows the precision-recall curve of the objectness score and our proposed AR score. The average precision and recall of our method are 0.69 and 0.59, while the objectness measure achieves 0.63 and 0.53, indicating that the selected affective regions are more consistent with the human annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VISUALIZATION OF AFFECTIVE REGIONS</head><p>For the image classification approaches, a natural question is whether the proposed model can identify the target part in the image. In this section, we attempt to answer this question by visualizing the crucial location for classifying sentiment. Following the previous works <ref type="bibr" target="#b74">[75]</ref>, we use sliding windows to occlude different portions of the input image with a gray square, and then generate the heat-map by plotting the estimated probability of the ground truth class at that location. Compared to other visualization methods, e.g., embedding the features with t-SNE or visualizing the filters of the network, this method tends to directly show the regions that the CNN focuses on. As shown in Fig. <ref type="figure" target="#fig_7">12</ref>, the first column is the input image and the second column is the prediction probability of the correct class using the fine-tuned VGGNet when occluding the corresponding portions of the image. If the occluded portion is essential for the sentiment prediction, the corresponding probability in the heat-map will obviously decrease (blue pixels). As can be seen in the three examples, the fine-tuned deep model has the ability to discover the parts in the images that can evoke the sentiment. For example, occluding the salient objects (e.g., people, fire) that can evoke the sentiment leads to decreasing prediction probabilities. However, due to the affective gap, the CNN is not discriminative enough to capture the most significant sentiment information in the images.</p><p>We also visualize the top-1 regions by re-ranking the candidate proposals according to different scores (i.e., Obj score, Senti score, AR) in Fig. <ref type="figure" target="#fig_7">12</ref> (c) (d) (e), respectively. Column (c) and (d) refer to the regions that are selected using only objectness or sentiment scores. The objectness score selects the regions which contain rich information at the low level, while the sentiment score usually evaluates the regions' sentiment at the affective level. Considering information from both of these two aspects, our proposed method is able to discover more accurate affective regions, see column (e). The detected affective regions can be not only complementary for the salient objects in the image (first example), but also extend the regions of interest to the additional contextual background (last two examples). Thus, combing the global and local information can be discriminative for the visual sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we address the problem of automatically recognizing sentiments in images. Inspired by the observation that both global appearance and local regions produce significant sentiment responses, we propose a framework to discover affective regions and combine both information using CNN. We estimate the level of sentiment content in a region considering the objectness score and sentiment score. The objectness score usually finds regions containing rich texture information while the sentiment score evaluates the regions' sentiment at the affective level. We also consider three alternative fusion operations and implement the proposed model on VGGNet. The experimental results show that our method outperforms the state-of-the-art methods on the popular affective datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images from popular affective datasets: (a) Twitter I [14] and (b) Twitter II [17]. The bounding boxes indicate the local Affective Regions labeled by users. As can be seen, sentiments are evoked by the affective regions as well as the whole image appearance.</figDesc><graphic coords="1,381.18,248.22,57.99,52.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Given the input image (a), the candidate windows are generated by EdgeBoxes and small or high aspect ratio boxes are filtered out. As shown in (b), the proposals with blue bounding boxes are dropped in this step. Different colors in (c) indicate the different clusters produced by normalized cut, from which the representative proposals are selected.</figDesc><graphic coords="5,331.40,170.74,91.93,91.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 2 : 3 :</head><label>123</label><figDesc>Visual Sentiment Analysis using Affective Regions Input: Input Image: I The number of desired affective regions: K Output: Predicted sentiment label : Y 1: Generate n bounding boxes with their objectness scores B = {b i ; Obj score I i } n i=1 . Apply candidate selection method to generate m candidate regions H = {h i } m i=1 . Initialize the framework with pre-trained CNN. 4: Let Y Global be the predictions of the whole image. 5: Pass H through the CNN model from the second layer to the last layer. 6: Let y ∈ R m×c be the sentiment probability of m proposal using the CNN model, compute the sentiment score in Eqn. (4) 7: Compute the AR score for the each region in Eqn. (5). 8: Rank proposals with AR scores and select top K as affective regions. 9: Predict the label Y using the cross-candidates pooling operation. 10: return Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example images from (a) small-scale and (b) large-scale affective datasets. The images come from a variety of domains including art, real life, abstract and so on, in which the sentiment distributions are different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Classification accuracy (%) on the test set of the large scale dataset, i.e., FI and Flickr. We compare our proposed method with different deep methods including ImageNet models (row 1-2), fine-tuned models (row 3-4), and state-of-the-art algorithms (row 5-6). Our proposed method with different configurations are also given, i.e., combining with the top-1 region (row 7-11), and leveraging more Affective Regions(row 12). Note that obj/senti indicate that only objectness score/sentiment score is used, whereas our "AR" method selects Affective Regions considering both objectness score and sentiment score are considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Impact of different α and β on the validation sets of the FI dataset. We choose α = 0.6, β = 0.3 in the remaining experiments.</figDesc><graphic coords="8,311.97,53.14,251.06,180.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9.Impact of different K on the validation set of the FI dataset. We set K = 8 in the remaining experiments.</figDesc><graphic coords="9,48.96,53.14,251.05,173.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visualization of images from the FI dataset. Given the input image (a), we systematically cover up different portions of the image with a gray square and see how the classifier output (b) changes. Column (b) denotes a map of the probabilities estimated by the CNN for the ground-truth class, indicating the relative importance of locations in the affective image for the CNN. We also show the top-1 regions ranked by different scores (i.e., Obj score, Senti score, AR) as object region (c), sentiment region (d), and affective region (e).</figDesc><graphic coords="11,56.75,59.07,498.51,309.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2803520, IEEE Transactions on Multimedia</figDesc><table><row><cell>Dataset</cell><cell cols="3">GT-Box Positive Negative</cell><cell>Sum</cell></row><row><cell>IAPSa [43]</cell><cell>N</cell><cell>209</cell><cell>186</cell><cell>395</cell></row><row><cell>Abstract [10]</cell><cell>N</cell><cell>139</cell><cell>89</cell><cell>228</cell></row><row><cell>ArtPhoto [10]</cell><cell>N</cell><cell>378</cell><cell>428</cell><cell>806</cell></row><row><cell>Twitter I [14]</cell><cell>N</cell><cell>769</cell><cell>500</cell><cell>1,269</cell></row><row><cell>Twitter II [17]</cell><cell>N</cell><cell>463</cell><cell>133</cell><cell>596</cell></row><row><cell>EmotionROI [44]</cell><cell>Y</cell><cell>660</cell><cell>1,320</cell><cell>1,980</cell></row><row><cell>Flickr&amp;Instagram [16]</cell><cell>N</cell><cell>16,430</cell><cell>6,878</cell><cell>23,308</cell></row><row><cell>Flickr [17]</cell><cell>N</cell><cell>435,798</cell><cell cols="2">48,424 484,222</cell></row><row><cell cols="5">Fig. 2. Statistics of the available affective datasets. Most datasets developed</cell></row><row><cell cols="5">in this field contain no more than two thousand samples, mainly due to the</cell></row><row><cell cols="5">subjective and labor intensive labeling process. Note that the Flickr dataset is</cell></row><row><cell cols="5">weakly-labeled and none of these datasets except EmotionROI provide ground</cell></row><row><cell cols="4">truth bounding box (GT-Box) corresponds to affective regions.</cell><cell></cell></row></table><note><p>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2803520, IEEE Transactions on Multimedia</figDesc><table><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IEEE TRANSACTIONS ON MULTIMEDIA</cell></row><row><cell>Algorithm</cell><cell cols="3">IAPS-Subset Abstract ArtPhoto</cell><cell></cell><cell>Twitter I</cell><cell></cell><cell cols="2">Twitter II EmotionROI</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Twitter I 5 Twitter I 4 Twitter I 3</cell><cell></cell><cell></cell></row><row><cell>GCH</cell><cell>71.76</cell><cell>71.50</cell><cell>67.00</cell><cell>67.91</cell><cell>67.20</cell><cell>65.41</cell><cell>77.68</cell><cell>66.53</cell></row><row><cell>LCH</cell><cell>52.91</cell><cell>73.26</cell><cell>64.01</cell><cell>70.18</cell><cell>68.54</cell><cell>65.93</cell><cell>75.98</cell><cell>64.29</cell></row><row><cell>ColorName + BoW</cell><cell>57.72</cell><cell>73.28</cell><cell>66.26</cell><cell>64.51</cell><cell>64.79</cell><cell>60.83</cell><cell>70.10</cell><cell>60.13</cell></row><row><cell>Gist</cell><cell>65.05</cell><cell>60.97</cell><cell>63.40</cell><cell>65.87</cell><cell>61.47</cell><cell>60.68</cell><cell>77.68</cell><cell>60.38</cell></row><row><cell>LBP</cell><cell>56.73</cell><cell>59.85</cell><cell>55.06</cell><cell>55.78</cell><cell>53.94</cell><cell>57.29</cell><cell>65.15</cell><cell>55.26</cell></row><row><cell>Gabor</cell><cell>79.21</cell><cell>50.43</cell><cell>58.43</cell><cell>55.37</cell><cell>54.03</cell><cell>53.90</cell><cell>63.72</cell><cell>58.73</cell></row><row><cell>SIFT + BoW</cell><cell>86.06</cell><cell>53.54</cell><cell>59.05</cell><cell>63.15</cell><cell>63.71</cell><cell>60.36</cell><cell>70.32</cell><cell>65.30</cell></row><row><cell>SIFT + VLAD</cell><cell>83.02</cell><cell>60.53</cell><cell>64.75</cell><cell>70.29</cell><cell>68.91</cell><cell>67.14</cell><cell>77.34</cell><cell>72.15</cell></row><row><cell>SIFT + FisherVector</cell><cell>83.28</cell><cell>60.10</cell><cell>62.40</cell><cell>71.09</cell><cell>67.29</cell><cell>65.56</cell><cell>76.34</cell><cell>70.92</cell></row><row><cell>DenseSIFT + BoW</cell><cell>56.22</cell><cell>54.38</cell><cell>56.58</cell><cell>64.29</cell><cell>59.94</cell><cell>58.94</cell><cell>60.07</cell><cell>59.85</cell></row><row><cell>DenseSIFT + VLAD</cell><cell>58.25</cell><cell>55.74</cell><cell>64.38</cell><cell>67.12</cell><cell>66.49</cell><cell>65.01</cell><cell>77.17</cell><cell>62.13</cell></row><row><cell>DenseSIFT + FisherVector</cell><cell>62.55</cell><cell>59.21</cell><cell>64.01</cell><cell>71.76</cell><cell>68.01</cell><cell>65.96</cell><cell>78.01</cell><cell>62.97</cell></row><row><cell>HOG + BoW</cell><cell>79.99</cell><cell>60.95</cell><cell>62.40</cell><cell>68.48</cell><cell>61.92</cell><cell>60.99</cell><cell>61.23</cell><cell>61.05</cell></row><row><cell>HOG + VLAD</cell><cell>82.52</cell><cell>57.49</cell><cell>68.97</cell><cell>71.99</cell><cell>67.74</cell><cell>66.43</cell><cell>61.92</cell><cell>63.38</cell></row><row><cell>HOG + FisherVector</cell><cell>83.76</cell><cell>61.41</cell><cell>68.11</cell><cell>76.07</cell><cell>70.34</cell><cell>68.32</cell><cell>68.12</cell><cell>65.33</cell></row><row><cell>PAEF [8]</cell><cell>62.81</cell><cell>70.05</cell><cell>67.85</cell><cell>72.90</cell><cell>69.61</cell><cell>67.92</cell><cell>77.51</cell><cell>75.24</cell></row><row><cell>SentiBank [17]</cell><cell>81.79</cell><cell>64.95</cell><cell>67.74</cell><cell>71.32</cell><cell>68.28</cell><cell>66.63</cell><cell>65.93</cell><cell>66.18</cell></row><row><cell>DeepSentiBank [49]</cell><cell>85.63</cell><cell>71.19</cell><cell>68.73</cell><cell>76.35</cell><cell>70.15</cell><cell>71.25</cell><cell>70.23</cell><cell>70.11</cell></row><row><cell>PCNN (VGGNet) [14]</cell><cell>88.84</cell><cell>70.84</cell><cell>70.96</cell><cell>82.54</cell><cell>76.52</cell><cell>76.36</cell><cell>77.68</cell><cell>73.58</cell></row><row><cell>VGGNet</cell><cell>88.51</cell><cell>68.86</cell><cell>67.61</cell><cell>83.44</cell><cell>78.67</cell><cell>75.49</cell><cell>71.79</cell><cell>72.25</cell></row><row><cell>Fine-tuned VGGNet</cell><cell>89.37</cell><cell>72.48</cell><cell>70.09</cell><cell>84.35</cell><cell>82.26</cell><cell>76.75</cell><cell>76.99</cell><cell>77.02</cell></row><row><cell>obj + concatenation</cell><cell>88.47</cell><cell>73.38</cell><cell>71.34</cell><cell>84.24</cell><cell>81.81</cell><cell>76.68</cell><cell>75.97</cell><cell>77.83</cell></row><row><cell>senti + concatenation</cell><cell>88.74</cell><cell>74.23</cell><cell>72.86</cell><cell>84.35</cell><cell>82.44</cell><cell>76.57</cell><cell>78.18</cell><cell>77.95</cell></row><row><cell>AR + concatenation</cell><cell>89.39</cell><cell>74.71</cell><cell>73.76</cell><cell>86.10</cell><cell>83.25</cell><cell>77.97</cell><cell>78.89</cell><cell>78.52</cell></row><row><cell>AR + sum-pooling</cell><cell>90.32</cell><cell>73.72</cell><cell>73.63</cell><cell>86.39</cell><cell>83.41</cell><cell>77.57</cell><cell>78.32</cell><cell>78.43</cell></row><row><cell>AR + max-pooling</cell><cell>89.04</cell><cell>73.92</cell><cell>73.32</cell><cell>86.19</cell><cell>83.11</cell><cell>77.67</cell><cell>78.52</cell><cell>78.32</cell></row><row><cell>AR + concatenation (K = 8)</cell><cell>92.39</cell><cell>76.03</cell><cell>74.80</cell><cell>88.65</cell><cell>85.10</cell><cell>81.06</cell><cell>80.48</cell><cell>81.26</cell></row></table><note><p><p>Fig. 11</p>. Classification results of different methods on the small-scale datasets. GCH represents the features of global color histogram and LCH corresponds to local color histogram. Note that "obj" means that we only regard the proposals with high objectness score as affective regions, "senti" refers to the proposals having high sentiment score are used. Note that our method is based on the fine-tuned VGGNet.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by NSFC (NO. 61620106008, 61572264, 61633021, 61525306, 61301238, 61201424), the Open Project Program of the National Laboratory of Pattern Recognition (NLPR), Huawei Innovation Research Program, CAST YESS Program, and IBM Global SUR award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time multimedia social event detection in microblog</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cyber</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Ret</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for affective analysis and retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2008" to="2020" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RAPID: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assistive image comment robota novel mid-level concept-based representation</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="311" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Who&apos;s afraid of Itten: Using the art theory of color combination to analyze emotions in abstract paintings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SIFT meets CNN: a decade survey of instance retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diving deep into sentiment: Understanding fine-tuned CNNs for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giró I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Affect &amp; Sentiment in Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From pixels to sentiment: Finetuning CNNs for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gir-I-Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large scale dataset for image emotion recognition: The fine print and the benchmark</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adv. Neural Inform. Process. Syst.</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Where do emotions come from? predicting the emotion stimuli map</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware affective images classification based on bilayer sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural mechanisms of selective visual attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Objectbased visual sentiment concept analysis and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discovering affective regions in deep convolutional neural networks for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A connotative space for supporting movie affective recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1356" to="1370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting viewer perceived emotions in animated GIFs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-layer hybrid framework for dimensional emotion classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Color based bags-of-emotions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Solli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Anal. Images Patterns</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: Towards truly personalized tv</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting personalized emotion perceptions of social images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting personalized image emotion perceptions in social networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting continuous probability distribution of image emotions in valence-arousal space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint image emotion classification and distribution learning via deep convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning visual sentiment distributions via augmented conditional probability neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distribution of image emotions by multi-modal features fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Continuous probability distribution prediction of image emotions via multitask shared sparse regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="632" to="645" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image sentiment prediction based on textual descriptions with adjective noun pairs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Affective image retrieval via multi-graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Emotional category data on images from the international affective picture system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Where do emotions come from? predicting the emotion stimuli map</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Is faster R-CNN doing well for pedestrian detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contextual action recognition with R*CNN</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Mach. Vis. Conf</publisher>
		</imprint>
	</monogr>
	<note>in Brit</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="814" to="830" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">HCP: A flexible CNN framework for multi-label image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Typical target detection in satellite images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">&quot; in Int. Conf. Syst. Man. Cy</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Recognizing image style</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Mach. Vis. Conf</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Cuthbert</surname></persName>
		</author>
		<title level="m">International affective picture system (IAPS): Affective ratings of pictures and instruction manual</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image retrieval by emotional semantics: A study of emotional space and feature extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weining</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yinglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shengming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in Int. Conf. Syst. Man. Cy.</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intel. Syst. Tec</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A mixed bag of emotions: Model, predict, and transfer emotion distributions</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Jufeng Yang is an associate professor in the College of Computer and Control Engineering, Nankai University. He received the PhD degree from Nankai University in 2009</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From 2015 to 2016, he was working at the Vision and Learning Lab</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Merced ; Computer and Control Engineering, Nankai University</orgName>
		</respStmt>
	</monogr>
	<note>His research falls in the field of computer vision, machine learning and multimedia. Dongyu She is currently a Master student with the College of. Her current research interests include computer vision, pattern recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
