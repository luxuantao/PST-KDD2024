<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Deep Neural Network -Hidden Markov Model (DNN-HMM) Based Speech Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongmei</forename><surname>Jiang</surname></persName>
							<email>jiangdm@nwpu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fengna</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isabel</forename><surname>Gonzalez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Enescu</forename><surname>Valentin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hichem</forename><surname>Sahli</surname></persName>
							<email>hsahli@vub.ac.be</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">VUB-NPU Joint AVSP Research Lab Northwestern</orgName>
								<orgName type="institution">Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shaanxi Provincial Key Lab on Speech and Image Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">VUB-NPU Joint AVSP Research Lab Vrije</orgName>
								<orgName type="institution">Universiteit Brussel</orgName>
								<address>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Electronics &amp; Informatics Department</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Deep Neural Network -Hidden Markov Model (DNN-HMM) Based Speech Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9494B801ABBBD190BBFC14C732249934</idno>
					<idno type="DOI">10.1109/ACII.2013.58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Neural Network Hidden Markov Models, or DNN-HMMs, are recently very promising acoustic models achieving good speech recognition results over Gaussian mixture model based HMMs (GMM-HMMs). In this paper, for emotion recognition from speech, we investigate DNN-HMMs with restricted Boltzmann Machine (RBM) based unsupervised pre-training, and DNN-HMMs with discriminative pre-training.</p><p>Emotion recognition experiments are carried out on these two models on the eNTERFACE'05 database and Berlin database, respectively, and results are compared with those from the GMM-HMMs, the shallow-NN-HMMs with two layers, as well as the Multi-layer Perceptrons HMMs (MLP-HMMs). Experimental results show that when the numbers of the hidden layers as well hidden units are properly set, the DNN could extend the labeling ability of GMM-HMM. Among all the models, the DNN-HMMs with discriminative pre-training obtain the best results. For example, for the eNTERFACE'05 database, the recognition accuracy improves 12.22% from the DNN-HMMs with unsupervised pre-training, 11.67% from the GMM-HMMs, 10.56% from the MLP-HMMs, and even 17.22% from the shallow-NN-HMMs, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Speech emotion recognition aims at recognizing the underlying emotional state of the speaker from his or her speech signal. This is mainly motivated by intelligent Human -Machine Interaction required for different kinds of applications. In the field of speech emotion recognition, a number of classification approaches have already been explored. According to the used acoustic emotional features, the recognition models can be classified into two types: 1) for suprasegmental prosodic features, such as the mean, median, standard deviation, range, or percentile of short time pitch (energy), estimated over the whole utterance, global models such as Gaussian mixture model(GMM), support vector machine(SVM), artificial neutral networks (ANN) and k-NN have been adopted. 2) for frame based dynamic spectral features like Mel Filterbank Cepstrum Coefficient (MFCC), dynamical models such as Hidden Markov Model (HMM) are considered <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Compared to the global models, dynamic modeling approaches provide a better consideration of the temporal dynamics of emotions. But at the same time, the limited representational capacity of HMMs prevents them from modeling streams of interacting knowledge sources in the speech signal which may require deeper architectures with multiple layers of representations <ref type="bibr" target="#b2">[3]</ref>. Moreover, Gaussian mixture models(GMMs) in HMM have a serious shortcoming that they are statistically inefficient for modeling data that lie on or near a nonlinear manifold in the data space <ref type="bibr" target="#b3">[4]</ref>.</p><p>In recent years, a novel hybrid model architecture, Deep Neural Network -Hidden Markov Model (DNN-HMM), has been proposed and widely used in speech recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. A deep neural network (DNN), which is able to capture the underlying nonlinear relationship among data, is the conventional multi-layer perceptrons with many layers, where training is typically initialized by a pre-training algorithm <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. For example, <ref type="bibr" target="#b4">[5]</ref> proposed an unsupervised pre-training method to train a deep belief network, which has the strong ability of feature learning and provides a better recognition result. In <ref type="bibr" target="#b5">[6]</ref>, the authors further replaced the unsupervised pre-training method of <ref type="bibr" target="#b4">[5]</ref> by a supervised pre-training method and claimed improved recognition results.</p><p>In the literature, several works have been dedicated to DNN-HMMs based large vocabulary continuous speech recognition. However, to our knowledge only few works on the application of DNN-HMMs in emotion recognition, have been reported. In <ref type="bibr" target="#b7">[8]</ref>, a Generalized Discriminant Analysis (GerDA) based on DNNs, is proposed to learn the discriminative features for classifying high or low of arousal and positive or negative valence.</p><p>In this work, we go beyond feature selection and follow the idea of <ref type="bibr" target="#b4">[5]</ref> for continuous speech recognition, we propose exploring the application of DNN-HMM framework in speech Architecture of Restricted Boltzmann Machine, the connection between the units is symmetrical. emotion recognition. In this approach the DNN can be viewed as a complex discriminative feature extractor extracting environment and speaker-independent representations optimized to predict the emotion class. Furthermore, we demonstrate that these models show improvement in emotion classification performance over baselines that do not employ deep learning. Moreover, we compare the DNN-HMMs with unsupervised and supervised pre-training methods, and illustrate our approach with experimental results on the eNTERFACE'05 <ref type="bibr" target="#b8">[9]</ref> and Berlin <ref type="bibr" target="#b9">[10]</ref> speech emotion databases.</p><p>The remainder of this paper is organized as follows. In section II we briefly introduce the deep neural network, and outline the general pre-training strategies we used in this work. In section III, we describe the basic ideas of the proposed DNN-HMMs for speech emotion recognition along with the training and decoding strategies. In section IV experimental results using the eNTERFACE'05 and the Berlin databases are discussed. Finally, Section V draws conclusions and outlines the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP NEURAL NETWORK</head><p>DNN is a feed-forward artificial neural network that has more than one hidden layers. Each hidden unit uses a nonlinear function to map the feature input from the layer below to the current unit. In our work, we use the traditional logistic function as the mapping function.</p><formula xml:id="formula_0">y = 1 1 + e -(b+xw) (1)</formula><p>where x denotes the input feature, w denotes the weights between connections, b denotes the bias and y denotes the output unit. DNN is capable of modeling very complex and highly nonlinear relationships between inputs and outputs, due to its flexible structure with multiple hidden layers and multiple hidden units. DNN can be discriminatively trained by back-propagating (BP) derivatives of a cost function that measures the discrepancy between the target outputs and the actual outputs produced for each training case <ref type="bibr" target="#b5">[6]</ref>. There are two methods to pre-train a Deep Neural Network, the unsupervised pretraining method <ref type="bibr" target="#b6">[7]</ref>, and the so called discriminative pretraining method, being a supervised pre-training approach <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised pre-training</head><p>Unsupervised pre-training method uses stacked Restricted Boltzmann Machine (RBM) to initialize the Deep Neural Network. RBM is a type of undirected graphical model constructed from a layer of binary stochastic hidden units and a layer of stochastic visible units, which will either be Bernoulli or Gaussian distributed conditional on the hidden or visible units. Fig. <ref type="figure" target="#fig_0">1</ref> depicts the structure of a RBM. Since in our work, the input of the network are real values, we use a RBM in which the hidden units are Bernoulli distributed, and the visible units are linear real-valued variables with Gaussian noise <ref type="bibr" target="#b10">[11]</ref>.</p><p>A RBM assigns an energy to every configuration of visible and hidden state vectors, denoted by v and h respectively, according to <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_1">E(v, h) = i (v i -a i ) 2 2σ 2 i - j b j h j - i,j w ij v i σ i h j , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where w = {w ij } is the matrix of visible/hidden connection weights, b j is the bias of the hidden unit h j , a i and σ i are the mean and variance of the input variable v i . When the visible unit is Gaussian distributed and the hidden unit is Bernoulli distributed, the conditional distribution (with σ i = 1) are:</p><formula xml:id="formula_3">p(h j = 1|v) = 1 1 + exp(-b j -i v i w ij )<label>(3)</label></formula><formula xml:id="formula_4">p(v i |h) = N (c i + j h j w ij , 1),<label>(4)</label></formula><p>where c i is the bias of the visible unit a i , N is the probability density function of normal distribution. Before writing an expression for the log probability assigned by a RBM to some visible vector v, it is convenient to define a quantity known as the free energy <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_5">F (v) = -log( h e -E(v,h) )<label>(5)</label></formula><p>Using F (v), we can write the per-training-case log likelihood as</p><formula xml:id="formula_6">ξ(θ) = -F (v) -log( v e -F (v) )<label>(6)</label></formula><p>with θ denoting the model parameters. From the iteration t to t + 1, the typical model parameter w ij is updated as</p><formula xml:id="formula_7">Δw ij (t + 1) = mΔw ij (t) - ∂ξ(θ) ∂w ij (7)</formula><p>where is the learning rate of RBM, m is the momentum factor used to smooth out the weight updates, and</p><formula xml:id="formula_8">- ∂ξ(θ) ∂w ij =&lt; v i h j &gt; data -&lt; v i h j &gt; recon (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where &lt; • &gt; data is the expectation of the training data, and &lt; • &gt; recon is the expectation w.r.t the distribution of the Fig. <ref type="figure">2</ref>. Supervised pre-training method -using a network with less hidden layers to initialize a deeper network reconstructed data. The learning rule of the bias of the hidden unit is:</p><formula xml:id="formula_10">Δb j = (&lt; h j &gt; data -&lt; h j &gt; recon )<label>(9)</label></formula><p>Once the unsupervised pre-training is done, the obtained parameters are used as the initial values of the DNN, and BP is adopted to fine-tune the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discriminative pre-training</head><p>To remedy the modeling inaccuracies in unsupervised pretraining, we follow the alternative proposed by <ref type="bibr" target="#b4">[5]</ref> referred to as discriminative pre-training (DPT). The general architecture is shown in Fig. <ref type="figure">2</ref>. It works as follows, in a first step, a layerwise Back Propagation (BP) is used to train a one-hiddenlayer DNN to full convergence using every frame's state label, then the softmax layer <ref type="bibr" target="#b4">[5]</ref> is replaced by another randomly initialized hidden layer and a new random softmax layer on top, and the network is discriminatively trained again to full convergence. The process is repeated until the desired number of hidden layers is reached. As stated in <ref type="bibr" target="#b4">[5]</ref>, this is similar to a greedy layer-wise training <ref type="bibr" target="#b2">[3]</ref>, but differs in that of <ref type="bibr" target="#b2">[3]</ref> only by the updates of newly added hidden layers. <ref type="bibr" target="#b4">[5]</ref> showed that DPT outperforms DNNs with unsupervised pre-training and DNNs without pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DNN-HMM BASED EMOTION RECOGNITION</head><p>In conventional GMM-HMMs based emotion recognition, the observation probabilities are modeled using GMMs under the maximum likelihood criterion. The potential of such model is restricted since GMMs are statistically inefficient for modeling data that lie on or near a nonlinear manifold in the data space <ref type="bibr" target="#b4">[5]</ref>. To overcome this restriction, we propose a hybrid Deep Neural Network -Hidden Markov Model (DNN-HMM) for speech emotion recognition, where the output of the DNN are fed to the HMM as replacement of the GMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hidden Markov Models</head><p>A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. A HMM, represented as λ = (A, B, π), consists of the following elements:</p><p>1) The number of states in the model denoted as Q, the set of states denoted as S = {s 1 , s 2 , . . . , s Q }, and q t the state at time t. 2) A = {a ij }, the state transition probability distribution, with</p><formula xml:id="formula_11">a ij = P (q t+1 = s j |q t = s i ), 1 ≤ i, j, ≤ Q (10)</formula><p>3) B = {b i (o t )}, the observation probabilities, where b i (o t ) represents the probability of observing o t at state s i . B is represented by a finite mixture:</p><formula xml:id="formula_12">b i (o t ) = M m=1 c im ℵ(o t , μ im , U im ), 1 ≤ i ≤ Q (11)</formula><p>where c im is the mixture coefficient for the mth mixture in state s i , and ℵ any log-concave or elliptically symmetric density, with mean vector μ im and covariance matrix U im for the mth mixture component in state i. When ℵ is a Gaussian function, Equation 11 is a GMM, and the corresponding HMM is called a GMM-HMM. 4) π = {π i }, the initial state distribution, where π i = P (q 1 = s i ), 1 ≤ i ≤ Q To be able to use the HMM, there are two problems that one should solve <ref type="bibr" target="#b12">[13]</ref>:</p><p>• Learning problem: Given a set of ground truth X (referred to as training set in the following), the learning procedure is to find the set of model parameters λ * = {A * , B * , π * } such that λ * = argmax λ P (X|λ), i.e., find the model parameters that better fit the training set. The forwardbackward algorithm is used to calculate P (X|λ), while the Baum-Welch algorithm is employed to solve the learning problem <ref type="bibr" target="#b12">[13]</ref>.</p><p>• Decoding problem: Given a model λ and a sequence of new observations O = (o 1 , o 2 , . . . , o T ) (referred to as testing set in the following), the decoding procedure is defined as the problem of finding the hidden state sequence (q 1 , ..., q T ) that have most likely produced that observation. The solution of this problem is given by the Viterbi algorithm <ref type="bibr" target="#b12">[13]</ref> as</p><formula xml:id="formula_13">P (O|λ) = max q1,...,qT π q1 T t=2 p(q t |q t-1 )b qt (o t )<label>(12)</label></formula><p>In the case of speech emotion recognition, we train C HMMs {λ c , (c = 1, . . . , C)} for C discrete emotion classes. For a new speech input O, it is assigned to the emotional class</p><formula xml:id="formula_14">c * = argmax 1≤c≤C P (O|λ c )<label>(13)</label></formula><p>with P (O|λ c ) calculated from the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN-HMM</head><p>The key difference between DNN-HMM and GMM-HMM is the using of DNN (instead of GMM) to estimate the observation probabilities. We actually use the DNN to model p(q t |o t ), the posterior probability of the state given the observation vector o t , which is possible since p(q t ) is easy to estimate from an initial state-level alignment of the training set.  <ref type="formula" target="#formula_13">12</ref>, is performed on λ c to obtain an optimal state sequence (q c 1 , ..., q c T ), and each state q c t is assigned a label L i (i ∈ (1, . . . , C × Q)) according to a statelabel mapping table. c) All the training sentences, together with their labeled state sequences are used as inputs to train a DNN, whose outputs are the posterior probabilities of the C × Q output units. The training of the DNN is performed using BP algorithm with (i) the unsupervised pre-training, or (ii) the discriminative pretraining described in Section II.</p><p>2) DNN-HMM Recognition Procedure: In the emotion recognition process, for an input speech sentence O = (o 1 , o 2 , . . . , o T ), one should estimate the probability p(O|λ c ) for each emotion class c, and get the final recognition result according to Equation <ref type="formula" target="#formula_14">13</ref>. In GMM-HMM, this probability is obtained via the Viterbi algorithm with Equation <ref type="formula" target="#formula_13">12</ref>.</p><p>In DNN-HMM, we adopt the following procedure to calculate the probability p(O|λ c ).</p><p>a) The input feature sequence O is firstly input into the DNN, obtaining the posterior probabilities {p(L i |o t )} i=1,...,C×Q as outputs. Then the posterior probability p(q t = S c k |o t ) can be obtained from p(L i |o t ), by mapping the label L i to the state k of the model c, using a state-label mapping table.</p><p>b) According to the Bayesian principle, we calculate the likelihood probability p(o t |q t ) as</p><formula xml:id="formula_15">p(o t |q t ) = p(q t |o t )p(o t ) p(q t )<label>(14)</label></formula><p>In our implementation, the prior probability of each state, p(q t ), is calculated from (occurrences of) the training set, and p(o t ) can be assigned a constant since the observation feature vectors are regarded as independent of each other. c) For each emotion model λ c , the Viterbi algorithm is performed to calculate the likelihood probability p(O|λ c ) according to Equation 12. However, here the probability b qt (o t ) is replaced by p(o t |q t ) calculated using Equation <ref type="formula" target="#formula_15">14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS A. Emotional Speech Databases and Emotion Features</head><p>In this study we perform emotion recognition experiments on the eNTERFACE'05 and Berlin emotion databases. eN-TERFACE'05 contains 495 utterances in 6 emotions (anger, happiness, sadness, fear, surprise and disgust). Berlin database contains 493 utterances selected from the utterances of 10 professional actors (five males and five females) each speaking 10 sentences in 7 emotions (anger, boredom, disgust, fear, happiness, sadness and neutral). In each database, we randomly select 50% of the speech sentences as the training set, 10% as the developing set to find the optimal parameters of DNN such as the learning rate and momentum, and the rest 40% as the testing set. Both the training and testing partitions are speaker independent in the two databases.</p><p>For the speech emotion recognition, MFCC feature vectors of dimension 42 are extracted, including 14 MFCC coefficients and their first as well second order derivatives, with a Hamming window of 25ms and window shift of 10ms. In DNN based speech recognition, concatenated features of several adjacent frames are often adopted to improve the recognition performance <ref type="bibr" target="#b13">[14]</ref>. Therefore, in our DNN-HMM modeling, we adopt a sliding hamming window of m frames (5 for the eNTERFACE'05 database, and 3 for the Berlin database) to weight the MFCC features, then concatenate the weighted MFCC features into a higher dimension feature vector. The sliding step of the Hamming window is 1 frame.</p><p>For each of the emotions in the database, the HTK toolkit <ref type="bibr" target="#b14">[15]</ref> has been used to train a baseline left-right GMM-HMM with 5 states and 17 Gaussian mixtures. In the training process of DNN, we adopt a mini-batch approach on the training data. Each 100 concatenated MFCC feature vectors, together with their labeled states, are input as a batch to train the DNN, and the DNN parameters are updated by Equations 7 and 9 using the following batch of 100 concatenated MFCC feature vectors. After all the training data has been used to update the parameters, the connection weights of the trained DNN are compared to the weights of the last iteration. If the difference between the weights of successive iterations is lower than a given threshold, then the training process stops, otherwise the training process will continue until 30 iterations.</p><p>Once the training process finishes, following the step a) of Section III-B2, the concatenated MFCC feature vectors of the developing set are input into the DNN to get the state labels of each frame. These states are then compared to the "ground-truth" states obtained by the Viterbi algorithm based on the GMM-HMM, and an error rate is calculated (details can be found in section IV-D). The DNN parameters, i.e. the learning rate, momentum and number of hidden layers, which get the low error rates on the developing set, are chosen as the candidate DNN parameters for emotion recognition experiments on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Emotion Recognition on the eNTERFACE'05 Database</head><p>We perform speech emotion recognition on the DNN-HMMs with RBM based unsupervised pre-training and discriminative pre-training, respectively, and compare the results with those from the shallow-NN-HMM with discriminative pre-training in which the DNN has only two layers, and those from the Multi-Layer Perceptrons HMM (MLP-HMM) in which BP is used directly to train the DNN without pretraining <ref type="bibr" target="#b5">[6]</ref>. Emotion recognition accuracies are listed in Table I. One can notice that the DNN-HMM with discriminative pre-training obtains the best result, which is 12.22% higher than that of the DNN-HMM with unsupervised pre-training, 11.67% higher than that of the GMM-HMM, 10.56% higher  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.</figDesc><graphic coords="2,70.30,72.96,215.60,97.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>DNN-HMM Training Procedure: The detailed training process for emotion recognition is as follows: a) For each emotion class c(c = 1, . . . , C), a left to right GMM-HMM λ c with Q states is trained using the training speech sentences of class c. b) For each speech sentence O = (o 1 , o 2 , . . . , o T ) in the training set c, the Viterbi algorithm of the GMM-HMM according to Equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II ENTERFACE</head><label>II</label><figDesc>'05 -DNN-HMM V.S. (GMM-HMM) CONFUSION MATRIX (%)</figDesc><table><row><cell></cell><cell>anger</cell><cell>sadness</cell><cell>happiness</cell><cell>disgust</cell><cell>fear</cell><cell>surprise</cell></row><row><cell>angry</cell><cell>55.00 (20.00)</cell><cell>6.67 (13.33)</cell><cell>5.00 (20.00)</cell><cell>11.67 ( 0.00)</cell><cell>11.67 (36.67)</cell><cell>10.00 (10.00)</cell></row><row><cell>sad</cell><cell>0.00 ( 0.00)</cell><cell>85.00 (83.33)</cell><cell>0.00 ( 0.00)</cell><cell>5.00 ( 6.67)</cell><cell>6.67 (10.00)</cell><cell>3.33 ( 0.00)</cell></row><row><cell>happy</cell><cell>8.33 ( 0.00)</cell><cell>6.67 (13.33)</cell><cell>46.67 ( 40.00)</cell><cell>16.67 ( 6.67)</cell><cell>11.67 (30.00)</cell><cell>10.00 ( 10.00)</cell></row><row><cell>disgust</cell><cell>10.00 (13.33)</cell><cell>10.00 (13.33)</cell><cell>3.33 ( 6.67)</cell><cell>43.33 (20.00)</cell><cell>20.00 (23.33)</cell><cell>13.33 (23.33)</cell></row><row><cell>fear</cell><cell>11.67 ( 0.00)</cell><cell>20.00 (23.33)</cell><cell>6.67 ( 0.00)</cell><cell>6.67 ( 6.67)</cell><cell>41.67 (60.00)</cell><cell>13.33 (10.00)</cell></row><row><cell>surprise</cell><cell>18.33 ( 6.67)</cell><cell>11.67 (23.33)</cell><cell>1.67 ( 3.33)</cell><cell>6.67 ( 3.33)</cell><cell>10.00 (33.33)</cell><cell>51.67 (30.00)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-5048-0/13 $26.00 © 2013 IEEE DOI 10.1109/ACII.2013.58</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this work, we propose Deep Neural Network Hidden Markov Models(DNN-HMMs) based emotion recognition from speech. Emotion recognition experiments are carried out on the eNTERFACE'05 database and Berlin database, on the DNN-HMMs with RBM based unsupervised pre-training, or with discriminative pre-training, respectively. Results are compared with those from the GMM-HMMs, the shallow-NN-HMMs, and the Multi-layer Perceptrons HMMs (MLP-HMMs). Experimental results show that when the numbers of the hidden layers as well hidden units are properly set, the DNN-HMM could extend the labeling ability of GMM-HMM. Therefore among all the models, the DNN-HMMs with discriminative pre-training obtain the best results. In our future work, we will consider evaluating this approach for Facial Action Unit recognition, as well audio-visual emotion recognition, to improve the emotion recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENT</head><p>This work is supported within the framework of the national natural science foundation project of China (grant 61273265), the Shaanxi provincial key international cooperation project (2011KW-04), the LIAMA-CAVSA project, the EU FP7 project ALIZ-E (grant 248116), and the VUB IRP5 project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>than that of MLP-HMM, and even 17.22% higher than that of the shallow-NN-HMM with 2 hidden layers. The confusion matrix of the emotion recognition using DNN-HMM with discriminative pre-training, which gets the best recognition result, is shown in Table <ref type="table">II</ref>. One can notice that sadness obtains 85% correction rate. Nevertheless, the correction rates of the other 5 emotions are relatively low, with the values ranging from 41.67% to 55%. However, the correction rates from DNN-HMMs are more coherent than those from GMM-HMMs as shown between brackets in Table <ref type="table">II</ref>, where the correct rates range from 20% to 60% except for 83.33% for sadness. Moreover, for most of the emotions having low correct rates in GMM-HMM, the recognition performance can be efficiently improved by DNN-HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Emotion Recognition on the Berlin Database</head><p>The emotion recognition results on the Berlin database are summarized in Table <ref type="table">III</ref>. One can notice that the DNN-HMM of 5 hidden layers with discriminative pre-training obtains the highest accuracy up to 77.92%, which is 3.64% higher than that of DNN-HMM with unsupervised pre-training, 7.98% higher than that of DNN-HMM without pre-training, and 1.74% higher than that of the traditional GMM-HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis of the Numbers of Hidden Layers and Hidden Units</head><p>In order to further explore the influence of the numbers of hidden layers as well as hidden units, we have carried out DNN-HMM based emotion recognition experiments using DNNs with different numbers of hidden layers.</p><p>Fig. <ref type="figure">3</ref> shows the curve of the recognition accuracies versus the number of hidden layers in DNN-HMMs for the eNTER-FACE'05 database. One can notice that the recognition accuracies increase with more hidden layers. When there are 6 hidden layers, the DNN-HMMs get the best performance. However, when the number of hidden layers is further increased to 7, the emotion recognition accuracy decreases.</p><p>Further, we evaluated the states which are output by the DNN-HMM, as well as checked the fitting extent of the DNN-HMM with respect to the GMM-HMM. For this analysis, after the DNN has been trained with discriminative pre-training by the training sentences of the database, the speech sentences from the developing set, along with their state labels obtained by the Viterbi algorithm based on GMM-HMM, were fed to the trained DNN to obtain the output states, which were then compared to the labeled states of corresponding frames. An error rate, defined as the number of states that are not correctly output by the DNN, divided by the number of states in all testing sentences, has been used as measure of robustness. To ensure correctness of this measure, we repeated the analysis 3 times by selecting different developing sets. The final error rate is calculated as the average of the 3 error rates.</p><p>The red curve in Fig. <ref type="figure">4</ref> shows the error rates versus the number of hidden layers on the eNTERFACE'05 database. One can notice that with increasing number of hidden layers, the error rates decreases rapidly. In other words, the more hidden layers, the more closely DNN-HMM fits the GMM-HMM. However, this does not mean that the DNN-HMM with more hidden layers would provides better emotion recognition performance. From Fig. <ref type="figure">3</ref>, one can notice that the DNN-HMM with 6 hidden layers obtains 53.89% accuracy, while the one with 7 hidden layers obtains only 40.83%. This confirms that the DNN-HMM could indeed extend the ability of GMM-HMM. While the states are labeled from GMM-HMM, they are obtained via the Viterbi algorithm based on GMM-HMM which is trained using the Maximum Likelihood Estimation (MLE) method, some states could be unavoidably falsely labeled. However, DNN-HMM with proper number of hidden layers could improve the state labeling and hence obtains better emotion recognition results. Similar conclusion could be drawn for the Berlin database from Fig. <ref type="figure">3</ref> and Fig. <ref type="figure">4</ref>.</p><p>In order to evaluate the number of hidden units in DNN, we list in Table IV the recognition accuracy versus number of hidden units, for the eNTERFACE'05. One can notice that the DNNs with 512 hidden units obtain the best results. When the hidden units are further increased to 1k, the recognition accuracy decreases, which means that proper number of hidden units should also be properly set to get the best result. The same results can be obtained for the Berlin database.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using hidden markov models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Nwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C De</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep belief networks for phone recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>A.-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning for Speech Recognition and Related Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine, IEEE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-dependent pretrained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature engineering in contextdependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic emotion recognition: raising the benchmarks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlsatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zieike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="5688" to="5691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The enterface05 audio-visual emotion database</title>
		<author>
			<persName><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 22nd International Conference on</title>
		<meeting>22nd International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="8" to="8" />
		</imprint>
	</monogr>
	<note>Data Engineering Workshops</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A database of german emotional speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sendlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 2007</date>
			<publisher>MIT</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1345</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotion recognition in speech using neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="290" to="296" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical structures of neural networks for phoneme recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The htk book</title>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
