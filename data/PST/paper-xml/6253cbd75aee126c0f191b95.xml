<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning inverse folding from millions of predicted structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chloe</forename><surname>Hsu</surname></persName>
							<email>&lt;chloehsu@berkeley.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>Work</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Hie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
							<email>adamlerer&lt;alerer@fb.com&gt;</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
							<email>&lt;arives@fb.com&gt;.</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Alexander Rives</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning inverse folding from millions of predicted structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2022.04.10.487779</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing novel amino acid sequences that encode proteins with desired properties, known as de novo protein design, is a central challenge in bioengineering <ref type="bibr" target="#b23">(Huang et al., 2016)</ref>. The most well-established approaches to this problem use an energy function which directly models the physical basis of a protein's folded state <ref type="bibr" target="#b0">(Alford et al., 2017)</ref>.</p><p>Recently a new class of deep learning based approaches has been proposed, using generative models to predict sequences for structures <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b62">Strokach et al., 2020;</ref><ref type="bibr" target="#b3">Anand-Achim et al., 2021;</ref><ref type="bibr" target="#b28">Jing et al., 2021b)</ref>, generate backbone structures <ref type="bibr" target="#b2">(Anand &amp; Huang, 2018;</ref><ref type="bibr" target="#b16">Eguchi et al., 2020)</ref>, jointly generate structures and sequences <ref type="bibr" target="#b5">(Anishchenko et al., 2021;</ref><ref type="bibr" target="#b70">Wang et al., 2021)</ref>, or model sequences directly <ref type="bibr">(Rives et al., 2021;</ref><ref type="bibr" target="#b39">Madani et al., 2021;</ref><ref type="bibr">Shin et al., 2021;</ref><ref type="bibr" target="#b18">Gligorijevic et al., 2021;</ref><ref type="bibr" target="#b10">Bryant et al., 2021;</ref><ref type="bibr" target="#b13">Dallago et al., 2021)</ref>. The potential to learn the rules of protein design directly from data makes deep generative models a promising alternative to current physics-based energy functions.</p><p>However, the relatively small number of experimentally determined protein structures places a limit on deep learning approaches. Experimentally determined structures cover Augmenting inverse folding with predicted structures. To evaluate the potential for training protein design models with predicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2 <ref type="bibr" target="#b30">(Jumper et al., 2021)</ref>. An autoregressive inverse folding model is trained to perform fixed-backbone protein sequence design. Train and test sets are partitioned at the topology level, so that the model is evaluated on structurally held-out backbones. We compare transformer models having invariant geometric input processing layers, with fully geometric models used in prior work. Span masking and noise is applied to the input coordinates. less than 0.1% of the known space of protein sequences.</p><p>While the UniRef sequence database <ref type="bibr" target="#b63">(Suzek et al., 2015)</ref> has over 50 million clusters at 50% sequence identity; as of January 2022, the Protein Data Bank (PDB) <ref type="bibr" target="#b8">(Berman et al., 2000)</ref> contains structures for fewer than 53,000 unique sequences clustered at the same level of identity.</p><p>Here we explore whether predicted structures can be used to overcome the limitation of experimental data. With progress in protein structure prediction <ref type="bibr" target="#b30">(Jumper et al., 2021;</ref><ref type="bibr" target="#b6">Baek et al., 2021)</ref>, it is now possible to consider learning from predicted structures at scale. Predicting structures for the sequences in large databases can expand the structural coverage of protein sequences by orders of magnitude. To train an inverse model for protein design, we predict structures for 12 million sequences in UniRef50 using AlphaFold2.</p><p>We focus on the problem of predicting sequences from backbone structures, known as inverse folding or fixed backbone design. We approach inverse folding as a sequenceto-sequence problem <ref type="bibr" target="#b24">(Ingraham et al., 2019)</ref>, using an autoregressive encoder-decoder architecture, where the model is tasked with recovering the native sequence of a protein from the coordinates of its backbone atoms.</p><p>We make use of the large number of sequences with unknown structures by adding them as additional training data, conditioning the model on predicted structures when the experimental structures are unknown (Figure <ref type="figure" target="#fig_8">1</ref>). This approach parallels back-translation <ref type="bibr" target="#b54">(Sennrich et al., 2015;</ref><ref type="bibr" target="#b15">Edunov et al., 2018)</ref> in machine translation, where predicted translations in one direction are used to improve a model in the opposite direction. Back-translation has been found to effectively learn from extra target data (i.e. sequences) even when the predicted inputs (i.e. structures) are of low quality.</p><p>We find that existing approaches have been limited by data.</p><p>While current state-of-the-art inverse folding models degrade when training is augmented with predicted structures, much larger models and different model architectures can effectively learn from the additional data, leading to an im-provement of nearly 10 percentage points in the recovery of sequences for structurally held out native backbones.</p><p>We evaluate models on fixed backbone design benchmarks from prior work, and assess the generalization capabilities across a series of tasks including design of complexes and binding sites, partially masked backbones, and multiple conformations. We further consider the use of the models for zero-shot prediction of mutational effects on protein function and stability, complex stability, and binding affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning inverse folding from predicted structures</head><p>The goal of inverse folding is to design sequences that fold to a desired structure. In this work, we focus on the backbone structure without considering side chains. While each of the 20 amino acid has a specific side chain, they share a common set of atoms that make up the amino acid backbone. Among the backbone atoms, we choose the N, Cα (alpha Carbon), and C atom coordinates to represent the backbone.</p><p>Using the structures of naturally existing proteins we can train a model for this task by supervising it to predict the protein's native sequence from the coordinates of its backbone atoms in three-dimensional space. Formally we represent this problem as one of learning the conditional distribution p(Y |X), where for a protein of length n, given a sequence X of spatial coordinates (x 1 , . . . , x i , . . . , x 3n ) for each of the backbone atoms N, Cα, C in the structure, the objective is to predict Y the native sequence (y 1 , . . . , y i , . . . , y n ) of amino acids. This density is modeled autoregressively through a sequence-to-sequence encoder-decoder:</p><formula xml:id="formula_0">p(Y |X) = n i=1 p(y i |y i−1 , . . . , y 1 ; X)<label>(1)</label></formula><p>We train a model by minimizing the negative log likelihood of the data. We can design sequences by sampling, or by finding sequences that maximize the conditional probability given the desired structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>Predicted structures We generate 12 million structures for sequences in UniRef50 to explore how predicted structures can improve inverse folding models. To select sequences for structure prediction we first use MSA Transformer <ref type="bibr" target="#b50">(Rao et al., 2021)</ref> to predict distograms for MSAs of all UniRef50 sequences. We rank the sequences by distogram LDDT scores <ref type="bibr" target="#b53">(Senior et al., 2020)</ref> as a proxy for the quality of the predictions. We take the top 12 million sequences not longer than five hundred amino acids and forward fold them using the AlphaFold2 model with a final Amber <ref type="bibr" target="#b21">(Hornak et al., 2006)</ref> relaxation. This results in a predicted dataset approximately 750 times the size of the training set of experimental structures (Appendix A.1).</p><p>Training and evaluation data We evaluate models on a structurally held-out subset of CATH <ref type="bibr" target="#b44">(Orengo et al., 1997)</ref>.</p><p>We partition CATH at the topology level with an 80/10/10 split resulting in 16153 structures assigned to the training set, 1457 to the validation set, and 1797 to the test set. Particular care is required to prevent leakage of information in the test set via the predicted structures. We use Gene3D topology classification <ref type="bibr" target="#b35">(Lees et al., 2012)</ref> to filter both the sequences used for supervision in training, as well as the MSAs used as inputs for AlphaFold2 predictions (Appendix A.1). We also perform evaluations on a smaller subset of the CATH test set that has been additionally filtered by TM-score using Foldseek <ref type="bibr" target="#b31">(Kim et al., 2021)</ref> to exclude any structures with similarity to those in the training set (Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model architectures</head><p>We study model architectures using Geometric Vector Perceptron (GVP) layers <ref type="bibr" target="#b28">(Jing et al., 2021b</ref>) that learn rotationequivariant transformations of vector features and rotationinvariant transformations of scalar features.</p><p>We present results for three model architectures: (1) GVP-GNN from <ref type="bibr" target="#b28">Jing et al. (2021b)</ref> which is currently state-ofthe-art on inverse folding;</p><p>(2) a GVP-GNN with increased width and depth (GVP-GNN-large); and (3) a hybrid model consisting of a GVP-GNN structural encoder followed by a generic transformer (GVP-Transformer). All models used in evaluations are trained to convergence, with detailed hyperparameters listed in Table <ref type="table">A</ref>.1.</p><p>In inverse folding, the predicted sequence should be independent of the reference frame of the structural coordinates.</p><p>For any rotation and translation T of the input coordinates, we would like for the model's output to be invariant under these transformations, i.e., p(Y |X) = p(Y |T X). Both the GVP-GNN and GVP-Transformer inverse folding models studied in this work are invariant (Appendix A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GVP-GNN</head><p>We start with the GVP-GNN architecture with 3 encoder layers and 3 decoder layers as described in <ref type="bibr" target="#b28">(Jing et al., 2021b)</ref>, with the vector gates described in <ref type="bibr" target="#b27">(Jing et al., 2021a</ref>) (GVP-GNN, 1M parameters). When trained on predicted structures, we find a deeper and wider version of GVP-GNN with 8 encoder layers and 8 decoder layers (GVP-GNN-large, 21M parameters) performs better. Scaling GVP-GNN further did not improve model performance in preliminary experiments (Figure <ref type="figure" target="#fig_6">6c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GVP-Transformer</head><p>We use GVP-GNN encoder layers to extract geometric features, followed by a generic autoregressive encoder-decoder Transformer <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref>. In GVP-GNN, the input features are translation-invariant and each layer is rotation-equivariant. We perform a change of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training</head><p>Combining experimental and predicted data During training, in each epoch we mix the training set of experimentally derived structures (∼16K structures) with a 10% random sample of the AlphaFold2-predicted training set (10% of 12M), resulting in a 1:80 experimental:predicted data ratio. For larger models, a high ratio of predicted data during training helps prevent overfitting on the smaller experimental train set (Figure <ref type="figure" target="#fig_6">6b</ref>).</p><p>The loss is equally weighted for each amino acid in target sequences. We mask out predicted input coordinates with AlphaFold2 confidence score (pLDDT) below 90, around 25% of the predicted coordinates. See Figure <ref type="figure" target="#fig_2">3</ref> for visualization of the pLDDT confidence score. Most often these low confidence regions are at the start and the end of sequences and may correspond to disordered regions. We prepend one token at the beginning of each sequence to indicate whether the structure is experimental or predicted. For each residue we provide the pLDDT confidence score from AlphaFold2 as a feature encoded by Gaussian radial basis functions.</p><p>Adding Gaussian noise at the scale of 0.1 angstroms to the predicted structures during training slightly improves performance ( Span masking To enable sequence design for partially masked backbones, we introduce backbone masking during training. We experiment with both independent random masking and span masking. In natural language processing, span masking improves performance over random masking <ref type="bibr" target="#b29">(Joshi et al., 2020)</ref>. We randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. The communication patterns in the geometric layers are adapted to account for masking with details in Appendix A.2. Span masking improves the performance of GVP-Transformer both on unmasked backbones (Table <ref type="table" target="#tab_0">C</ref>.1) and on masked regions (Figure <ref type="figure" target="#fig_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We evaluate models across a variety of benchmarks in two overall settings: fixed backbone sequence design and zeroshot prediction of mutation effects. For fixed backbone design, we start with evaluation in the standard setting <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b28">Jing et al., 2021b)</ref> of sequence design given all backbone coordinates. Then, we make the sequence design task more challenging along three dimensions:</p><p>(1) introducing masking on coordinates;</p><p>(2) generalization to protein complexes; and (3) conditioning on multiple conformations. Additionally, we show that inverse folding models are effective zero-shot predictors for protein complex stability, binding affinity, and insertion effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fixed backbone protein design</head><p>We begin with the task of predicting the native protein sequence given its backbone atom (N, Cα, C) coordinates. Perplexity and sequence recovery on held-out native sequences are two commonly used metrics for this task. Perplexity  measures the inverse likelihood of native sequences in the predicted sequence distribution (low perplexity for high likelihood). Sequence recovery (accuracy) measures how often sampled sequences match the native sequence at each position. To maximize sequence recovery, the predicted sequences are sampled with low temperature T = 1e−6 from the model. Table <ref type="table">1</ref> compares models using these metrics on the structurally held-out backbones.</p><p>We observe that current state-of-the-art inverse folding models are limited by the CATH training set. Scaling the current 1M parameter model (GVP-GNN) to 21M parameters (GVP-GNN-large) on the CATH dataset results in overfitting with a degradation of sequence recovery from 42.2% to 39.2% (Table <ref type="table">1</ref>). On the other hand, the current model at the 1M parameter scale cannot make use of the predicted structures: training GVP-GNN with predicted structures results in a degradation to 38.6% sequence recovery (Table <ref type="table">1</ref>), with performance worsening with increasing numbers of predicted structures in training (Figure <ref type="figure" target="#fig_6">6a</ref>).</p><p>Larger models benefit from training on the AlphaFold2predicted UniRef50 structures. Training with predicted structures increases sequence recovery from 39.2% to 50.8% for GVP-GNN-large and from 38.3% to 51.6% for GVP-Transformer over training only on the experimentally derived structures. The improvements are also reflected in perplexity. Similar improvements are observed on the test subset filtered by TM-score (Table <ref type="table">B</ref>.1). The best model trained with UniRef50 predicted stuctures, GVP-Transformer, improves sequence recovery by 9.4 percentage points over the best model, GVP-GNN, trained on CATH alone.</p><p>As there are many sequences that can fold to approximately the same structure, even an ideal protein design model will not have 100% native sequence recovery. We observe that the GVP-GNN-large and GVP-Transformer models are wellcalibrated <ref type="bibr">(Figure C.5)</ref>. The substitution matrix between native sequences and model-designed sequences resembles the BLOSUM62 substitution matrix (Figure C.4), albeit noticeably sparser for the amino acid Proline.</p><p>When we break down performance on core residues and surface residues, as expected, core residues are more constrained and have a high native sequence recovery rate of 72%, while surface residues are not as constrained and have a lower sequence recovery of 39% (Figure <ref type="figure" target="#fig_5">5</ref>; top). Generally perplexity increases with the solvent accessible surface area (Figure <ref type="figure" target="#fig_5">5</ref>; bottom). Despite the lower sequence recovery on the surface, sampled sequences do tend not to have hydrophobic residues on the surface <ref type="bibr">(Figure C.6)</ref>.</p><p>As an example of inverse folding of a structurally-remote protein, we re-design the receptor binding domain (RBD) sequence of the SARS-CoV-2 spike protein (PDB: 6XRA and 6VXX; illustrated in Figure <ref type="figure">C</ref>.3) with the two models. The SARS-CoV-2 spike protein has no match to the training data with TM-score above 0.5. Both GVP-GNN and GVP-Transformer achieve high sequence recovery (49.7% and 53.6%) for the native RBD sequence (Table <ref type="table" target="#tab_0">C</ref>.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partially-masked backbones</head><p>We evaluate the models on partial backbones. While masking during training does not significantly change test performance on unmasked backbones (Table <ref type="table" target="#tab_0">C</ref>.1), masking does enable models to non-trivially predict sequences for mask regions. Although GVP-GNN-large has low perplexity on short-length masks, its performance quickly degrades to the perplexity of the background distribution on masks longer than 5 amino acids (Figure <ref type="figure" target="#fig_4">4</ref>). By contrast, the GVP-Transformer model main- tains moderate performance even on longer masked regions, with less degradation if trained with span masking instead of independent random masking (Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>Protein complexes Although the training data only consists of single chains, we find that models generalize to multi-chain protein complexes. We represent complexes by concatenating the chains together with 10 mask tokens between chains, and include all complexes in the test set up to length 1000. For chains that are part of a protein complex, there is a substantial improvement in perplexity of both models when given the full complex coordinates as input, versus only the single chain (Table <ref type="table" target="#tab_0">2 and Figure C</ref>.2), suggesting that both GVP-GNN and GVP-Transformer can make use of inter-chain information from amino acids that are close in 3D structure but far apart in sequence.</p><p>Multiple conformations Multi-state design is of interest for engineering enzymes and biosensors <ref type="bibr" target="#b34">(Langan et al., 2019;</ref><ref type="bibr" target="#b48">Quijano-Rubio et al., 2021)</ref>. Some proteins exist in multiple distinct folded forms in equilibrium, while other proteins may exhibit distinct conformations when binding to partner molecules. For a backbone X, the inverse folding model predicts a conditional distribution p(Y |X) over possi- Table <ref type="table">2</ref>. Sequence design performance on complexes in the CATH topology test split when given the backbone coordinates of only a chain ("Chain" column) and when given all backbone coordinates of the complex ("Complex" column). The perplexity is evaluated on the same chain in the complex for both columns.</p><p>ble sequences Y for the backbone. To design a protein with two states A and B, we would like find sequences that have high likelihoods in the conditional distributions p(Y |A) and p(Y |B) for each of the two states. We use the geometric average of the two conditional likelihoods as a proxy for the desired distribution p(Y |A, B) conditioned on the sequence being compatible with both states.</p><p>We compare single-state and multi-state sequence design performance on 87 test split proteins with multiple conformations in the PDBFlex dataset <ref type="bibr" target="#b22">(Hrabe et al., 2016)</ref>. On locally flexible residues, multi-state design results in lower sequence perplexity than single-state design (Figure <ref type="figure" target="#fig_7">7</ref>). See Appendix C for more details on the PDBFlex data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Zero-shot predictions</head><p>We next show that inverse folding models are effective zeroshot predictors of mutational effects across practical design applications, including prediction of complex stability, binding affinity, and insertion effects. To score the effect of a mutation on a particular sequence, we use the ratio between likelihoods of the mutated and wildtype sequences according to the inverse folding model, given the experimentally determined wildtype structure. Exact likelihood evaluations are possible from both GVP-GNN and GVP-Transformer as they are both based on autoregressive decoders. We then compare these likelihood ratio scores to experimentally-determined fitness values measured on the same set of sequences.</p><p>De novo mini-proteins <ref type="bibr" target="#b52">Rocklin et al. (2017)</ref> performed deep mutational scans across a set of de novo designed miniproteins with 10 different folds measuring the stability in response to point mutations. The likelihoods of inverse folding models have been shown to correlate with experimentally measured stability using this dataset <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b28">Jing et al., 2021b)</ref>. We evaluate the GVP-Transformer and GVP-GNN-large models on the same mutational scans, and observe improvements in stability predictions from using predicted structures as training data for 8 out of 10 folds in the dataset ( Table <ref type="table">3</ref>. Zero-shot performance on binding affinity prediction for the receptor binding domain (RBD) of SARS-CoV-2 Spike, evaluated on ACE2-RBD mutational scan data <ref type="bibr" target="#b59">(Starr et al., 2020)</ref>. The zero-shot predictions are based on the sequence log-likelihood for the receptor binding motif (RBM), which is the portion of the RBD in direct contact with ACE2 <ref type="bibr" target="#b33">(Lan et al., 2020)</ref>. We evaluate in four settings: 1) Given sequence data alone ("No coords"); 2) Given backbone coordinates for both ACE2 and the RBD but excluding the RBM and without sequence ("No RBM coords"); 3) Given the full backbone for the RBD but no information for ACE2 ("No ACE2 coords"); and 4) Given all coordinates for the RBD and ACE2.</p><p>Complex stability We evaluate models on zero-shot prediction of mutational effects on protein complex interfaces, using the Atom3D benchmark <ref type="bibr" target="#b64">(Townshend et al., 2020)</ref> which incorporates binding free energy changes in the SKEMPI database <ref type="bibr" target="#b25">(Jankauskaitė et al., 2019)</ref> as a binary classification task. We find that sequence log-likelihoods from GVP-GNN are effective zero-shot predictors of stability changes of protein complexes even without predicted structures as training data (Table <ref type="table" target="#tab_0">C</ref>.4), performing comparably to the best supervised method which uses transfer learning. While we observe a substantial improvement in perplexity when predicted structures are added to training (Table <ref type="table">2</ref>), this does not further improve complex stability prediction for the single-point mutations in SKEMPI (Table <ref type="table" target="#tab_0">C</ref>.4), indicating potential limitations of evaluating models only on single-point mutations.</p><p>Binding affinity While the SKEMPI dataset features one mutation entry per protein, we also want to evaluate whether inverse folding models can rank different mutations on the same protein, potentially enabling binding-affinity optimization, which is an important task in therapeutic design. We assess whether inverse folding models can predict mutational effects on binding by leveraging a dataset generated by <ref type="bibr" target="#b59">Starr et al. (2020)</ref> in which all single amino acid substitutions to the SARS-CoV-2 receptor binding domain (RBD) were experimentally measured for binding affinity to human ACE2. Given potential applications to interface optimization or design, we focus on mutations within the receptor binding motif (RBM), the portion of the RBD in direct contact with ACE2 <ref type="bibr" target="#b33">(Lan et al., 2020)</ref>. When given all RBD and ACE2 coordinates, the best inverse folding model produces RBD-sequence log-likelihoods that have a Spearman correlation of 0.69 with experimental binding affinity measurements (Table <ref type="table">3</ref>). We observe weaker correlations when not providing the model with ACE2 coordinates, indicating that inverse folding models take advantage of structural information in the binding partner. When masking RBM coordinates (69 of 195 residues, a longer span than masked during model training), we no longer observe correlation between RBD log-likelihood and binding affinity, indicating that the model relies on structural information at the interface to identify interface designs that preserve binding. Zero-shot prediction via inverse folding outperforms methods for sequence-based variant effect prediction, which use the likelihood ratio between the mutant and wildtype amino acids at each position to predict the impact of a mutation on binding affinity. These likelihoods are inferred by masked language models, ESM-1b, ESM-1v, and ESM-MSA-1b, as described by Meier et al. (2021) (Table <ref type="table">3</ref>); additional details are given in Appendix C.</p><p>Sequence insertions Using masked coordinate tokens at insertion regions, inverse folding models can also predict insertion effects. On adeno-associated virus (AAV) capsid variants, we show that relative differences in sequence log-likelihoods correlate with the experimentally measured insertion effects from <ref type="bibr" target="#b10">Bryant et al. (2021)</ref>. As shown in Table <ref type="table" target="#tab_0">C</ref>.5, both GVP-GNN and GVP-Transformer outperform the sequence-only zero-shot prediction baseline ESM-1v <ref type="bibr" target="#b40">(Meier et al., 2021)</ref>. When evaluating on subsets of sequences increasingly further away from the wildtype (≥ 2, ≥ 3, and ≥ 8 mutations), the GVP-GNN-large and GVP-Transformer models trained with predicted structures have increasing advantages compared to GVP-GNN trained without predicted structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>Structure-based protein sequence design Early work on design of protein sequences studied the packing of amino acid side chains to fill the interior space of predetermined backbone structures, either for a fixed backbone conformation <ref type="bibr" target="#b61">(Street &amp; Mayo, 1999;</ref><ref type="bibr" target="#b12">Dahiyat &amp; Mayo, 1997;</ref><ref type="bibr">De-Grado et al., 1991)</ref>, or with flexibility in the backbone conformation <ref type="bibr" target="#b19">(Harbury et al., 1998)</ref>. Since then, the Rosetta energy function <ref type="bibr" target="#b0">(Alford et al., 2017)</ref> has become an established approach for structure-based sequence design. An alternative non-parametric approach involves decomposing the library of known structures into common sequencestructure motifs <ref type="bibr" target="#b75">(Zhou et al., 2020)</ref>.</p><p>Early machine learning approaches in structure-based protein sequence design used fragment-based and energy-based global features derived from structures <ref type="bibr" target="#b37">(Li et al., 2014;</ref><ref type="bibr" target="#b43">O'Connell et al., 2018)</ref>  <ref type="formula">2021</ref>) also studied protein-specific autoregressive models for sequence generation.</p><p>Recently language models have been proposed for modeling large scale databases of protein sequences rather than families of related sequences. Examples include <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019;</ref><ref type="bibr" target="#b1">Alley et al., 2019;</ref><ref type="bibr" target="#b20">Heinzinger et al., 2019;</ref><ref type="bibr" target="#b49">Rao et al., 2019;</ref><ref type="bibr" target="#b38">Madani et al., 2020;</ref><ref type="bibr" target="#b17">Elnaggar et al., 2021;</ref><ref type="bibr">Rives et al., 2021;</ref><ref type="bibr" target="#b50">Rao et al., 2021)</ref>. <ref type="bibr" target="#b40">Meier et al. (2021)</ref> found that the log-likelihoods of large protein language models predict mutational effects. <ref type="bibr" target="#b39">Madani et al. (2021)</ref> study an autoregressive sequence model conditioned on functional annotations and show it can generate functional proteins.</p><p>Structure-agnostic protein sequence design We point the reader to <ref type="bibr" target="#b71">Wu et al. (2021)</ref> for a review of the many machine learning-based sequence design approaches that do not explicitly model protein structures. Additionally, as an alternative to sequence generation models, model-guided algorithms design sequences based on predictive models as oracles <ref type="bibr" target="#b73">(Yang et al., 2019;</ref><ref type="bibr" target="#b4">Angermueller et al., 2019;</ref><ref type="bibr" target="#b9">Brookes et al., 2019;</ref><ref type="bibr" target="#b58">Sinai et al., 2020)</ref>.</p><p>Back-translation For machine translation (MT) in NLP, <ref type="bibr" target="#b54">Sennrich et al. (2015)</ref> studied how to leverage large amounts of monolingual data in the target language, a setting that parallels the situation we consider with protein sequences (the target language in our case). Sennrich et al. found it most effective to generate synthetic source sentences by performing the backwards translation from the target sentence, i.e. back-translation. This parallels the approach we take of predicting structures for sequence targets that have unknown structures. <ref type="bibr" target="#b15">Edunov et al. (2018)</ref> further investigated back-translation for large-scale language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>While there are billions of protein sequences in the largest sequence databases, the number of available experimentally determined structures is on the order of hundreds of thousands, imposing a limit on generative methods that learn from protein structure data. In this work, we explored whether predicted structures from recent deep learning methods can be used in tandem with experimental structures to train models for protein design.</p><p>To this end, we generated structures for 12 million UniRef50 sequences using AlphaFold2. As a result of training with this data we observe improvements in perplexity and sequence recovery by substantial margins, and demonstrate generalization to longer protein complexes, to proteins in multiple conformations, and to zero-shot prediction for mutation effects on binding affinity and AAV packaging. These results highlight that in addition to the geometric inductive biases which have been the major focus for work on inversefolding to date, finding ways to leverage more sources of training data is an equally important path to improved modeling capabilities.</p><p>We also take initial steps toward more general structureconditional protein design tasks. By integrating backbone span masking into the inverse folding task and using a sequence-to-sequence transformer, reasonable sequence predictions can be achieved for short masked spans.</p><p>If ways can be found to continue to leverage predicted structures for generative models of proteins, it may be possible to create models that learn to design proteins from an expanded universe of the billions of natural sequences whose structures are currently unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional details on datasets, training procedures, and model architectures A.1. Details on dataset of predicted structures</head><p>We used training data from two sources: 1) experimental protein structures from the CATH 40% non-redundant chain set, and 2) AlphaFold2-predicted structures from UniRef50 sequences. To evaluate the generalization performance across different protein folds, we split the train, validation, and test data based on the CATH hierarchical classification of protein structures <ref type="bibr" target="#b44">(Orengo et al., 1997)</ref> for both data sources. To achieve that a rigorous structural hold-out, we additionally use foldseek <ref type="bibr" target="#b31">(Kim et al., 2021)</ref> for pairwise TMalign between the test set the train set.</p><p>CATH topology split. Following the structural split methodology in previous work <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b28">Jing et al., 2021b;</ref><ref type="bibr" target="#b62">Strokach et al., 2020)</ref>, we randomly split the CATH v4.3 (latest version) topology classification codes into train, validation, and test sets at a 80/10/10 ratio. The CATH <ref type="bibr" target="#b44">(Orengo et al., 1997)</ref> structural hierarchy, classifies domains in four levels: Class (C), Architecture (A), Topology/fold (T), and Homologous superfamily (H). The topology/fold (T) level roughly corresponds to the SCOP fold classification.</p><p>Experimental structures. We collected full chains up to length 500 for all domains in the CATH v4.3 40% sequence identity non-redundant set. The experimental structure data contained only stand-alone chains and no multichain complexes.</p><p>As each chain may be classified with more than one topology codes, we further removed chains with topology codes spanning different splits, so that there is no overlap in topology codes between train, validation, and test. This results in 16,153 chains in the train split, 1457 chains in the validation split, and 1797 chains in the test split.</p><p>Predicted structures. We curated a new data set of AlphaFold2 <ref type="bibr" target="#b30">(Jumper et al., 2021)</ref>-predicted structures for a selective subset of UniRef50 ( <ref type="formula">202001</ref>) sequences. To prevent information leakage about the test set from the predicted structures, we proceeded in the following steps.</p><p>First, we annotated UniRef50 sequences with CATH classification according to the Gene3D <ref type="bibr" target="#b35">(Lees et al., 2012)</ref> database, also used by Strokach <ref type="bibr" target="#b62">(Strokach et al., 2020)</ref> for data curation. Gene3D represents each CATH classification code as a library of representative profile HMMs. We searched all HMMs associated with the validation and test splits against the UniRef50 sequences using default parameters in hmmsearch <ref type="bibr" target="#b46">(Potter et al., 2018)</ref> and excluded all hits.</p><p>Additionally, as AlphaFold2 predictions use multiple sequence alignments (MSAs) as inputs, we also took precaution to avoid information leakage from sequences in the MSAs. We created a filtered version of UniRef100 by searching all the validation-split and test-split Gene3D HMMs against UniRef100 (202001) and excluding all hits. Then, we constructed our MSAs using hhblits <ref type="bibr" target="#b60">(Steinegger et al., 2019)</ref> on this filtered version of UniRef100.</p><p>As AlphaFold2 predictions are computationally costly, our budget only allowed for predicting structures for a subset of the UniRef50 sequences. We ranked UniRef50 sequences based on the distogram lDDT score (Supplementary Equation 6 in <ref type="bibr" target="#b53">(Senior et al., 2020)</ref>), based on distogram predictions from MSATransformer <ref type="bibr" target="#b50">(Rao et al., 2021)</ref>, as a proxy for the quality of predicted structures. In this order, using AlphaFold2 Model 1 on the filtered UniRef100 MSAs described above, we obtained predicted structures for the top 12 million UniRef50 sequences under length 500, roughly 750 times the CATH train set size.</p><p>We used the publicly released model weights from AlphaFold2 Model 1 for CASP14 as a single model, as opposed the 5-model ensemble in <ref type="bibr" target="#b30">(Jumper et al., 2021)</ref>, to cover more sequences with the same amount of computing resources. We curated the input MSAs from UniRef100 with hhblits, with an additional filtering step as described above. To reduce computational costs, compared to the standard AlphaFold2 protocol, we did not include the UniRef90 jackhmmer MSAs, or the MGnify and BFD metagenomics MSAs, nor the pdb70 templates. Other than a reduced inputs, we followed the default settings in AlphaFold2 open source code, using 3 recycling iterations and the default Amber relaxation protocol. Despite the reduced inputs, the resulting 12 million predicted structures still have high pLDDT scores from AlphaFold, with 75% of residues having pLDDT above 90 (highly confident).</p><p>We found that increasing the predicted data size to up to 1 million structures (75 times the CATH experimental data size) substantially improves model performance. Beyond 1 million structures, models still benefit from more data but with diminished marginal returns (Figure <ref type="figure" target="#fig_6">6a</ref>).</p><p>Noise on AlphaFold2-predicted backbone coordinates. Even after Amber relaxation, the backbone coordinates predicted by AlphaFold2 contain artifacts in the sub-Angstrom scale that may give away amino acid identities. noise on predicted structures, there is a substantial gap between held-out set performance on predicted structures and on experimental structures. To prevent the model from learning non-generalizable AlphaFold2-specific rules, we added Gaussian noise at the 0.1A scale on predicted backbone coordinates. The Gaussian noise improves the invariant Transformer performance but not the GVP-GNN performance (Supplementary Figure C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details on span masking</head><p>We add a binary feature indicating whether each coordinate is masked or not. In GVP-Transformer, we exclude the masked nodes in the GVP-GNN encoder layers, and then impute zeros when passing the GVP-GNN outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model. In GVP-GNN, we impute zeros for the input vector features, and in the input graph connect the masked nodes to the k sequence nearest-neighbors (k = 30) in lieu of the k nearest nodes by spatial distance.</p><p>For span masking, we randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. Such a span masking scheme has shown to improve performance on natural language processing benchmarks <ref type="bibr" target="#b29">(Joshi et al., 2020)</ref>. The span lengths are sampled from a geometric distribution Geo(p) where p = 0.05 (corresponding to an average span length of 1/p = 20). The starting points for the spans are uniformly randomly sampled. Compared to independent random masking, span masking is better for GVP-Transformer but not for GVP-GNN (Table <ref type="table" target="#tab_0">C</ref>.1).</p><p>For the amino acids with masked coordinates, we exclude the corresponding nodes from the input graph to the pre-processing GVP message passing layers, and then impute zeros for the geometric features when passing the GVP outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Details on model architectures</head><p>Invariance to rotation and translation. The input features for both GVP-GNN and GVP-Transformer are translationinvariant, making the overall models also invariant to translations.</p><p>Each GVP-GNN layer is rotation-equivariant, that is, for a vector feature x and any arbitrary rotation T , T f (x) = f (T x).</p><p>With equivariant intermediate layers and an invariant output projection layer, GVP-GNN is overall invariant to rotations, since the composition of an equivariant function f with an invariant function g produces an invariant function g(f (x)).</p><p>The GVP-Transformer architecture is also invariant to rotations and translations. The initial GVP-GNN layers in GVP-Transformer output rotation-invariant scalar features and rotation-equivariant vector features for each amino acid. To make the overall GVP-Transformer invariant, we perform a change of basis on GVP-GNN vector outputs to produce rotation-invariant features for the Transformer. More specifically, for each amino acid, we define a local reference frame based on the N, CA, and C atom positions in the amino acid, following Algorithm 21 in AlphaFold2 <ref type="bibr" target="#b30">(Jumper et al., 2021)</ref>.</p><p>We then perform a change of basis according to this local reference frame, rotating the vector features in GVP-GNN outputs into the local reference frames of each amino acid. We concatenate this rotated "local version" of vector features together with the scalar features as inputs to the Transformer. The concatenated features are invariant to both translations and rotations on the input backbone coordinates, forming a L × E matrix where L is the number of amino acids in the protein backbone and E is the feature dimension. For amino acids with masked or missing coordinates, the features are imputed as zeros.</p><p>Transformer. We closely followed the original autoregressive encoder-decoder Transformer architecture <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> except for using learned positional embeddings instead of sinusoidal positional embeddings, attention dropout, and layer normalization inside the residual blocks ("pre-layernorm"). For model scaling experiments, we followed the model sizes in <ref type="bibr" target="#b66">(Turc et al., 2019)</ref>, and chose the 142-million-parameter model with 8 encoder layers, 8 decoder layers, 8 attention heads, and embedding dimension 512 based on the best validation set performance (Figure <ref type="figure" target="#fig_6">6c</ref> shows test set ablation).</p><p>The GVP-GNN, GVP-GNN-large, and GVP-Transformer models used in the evaluations in this manuscript are all trained to convergence, with detailed hyperparameters listed in Table <ref type="table">A</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TM-score-based test set</head><p>In addition to the CATH topology-based test set following previous work <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b28">Jing et al., 2021b)</ref> Predicting complex stability changes upon mutations. SKEMPI <ref type="bibr" target="#b25">(Jankauskaitė et al., 2019)</ref> is a database of binding free energy changes upon single point mutations within protein complex interfaces. This database is used as a task in the Atom3D benchmark suite <ref type="bibr" target="#b64">(Townshend et al., 2020)</ref> for comparing supervised stability prediction methods. The task is to classify whether the stability of the complex increases as a result of the mutation. We compare zero-shot predictions using inverse folding models to supervised and transfer learning methods <ref type="bibr" target="#b64">(Townshend et al., 2020;</ref><ref type="bibr" target="#b27">Jing et al., 2021a)</ref> on the Atom3D test set. We find that sequence log-likelihoods from GVP-GNN, GVP-GNN-large, and GVP-Transformer models are all effective zero-shot predictors of stability changes of protein complexes (Table <ref type="table" target="#tab_0">C</ref>.4), performing comparably to the best supervised method which uses transfer learning.</p><p>Predicting insertion effects on AAV. Using masked coordinate tokens at insertion regions, inverse folding models can also predict the effects of sequence insertions. Adeno-associated virus (AAV) capsids are a promising gene delivery vehicle, approved by the US Food and Drug Administration for use as gene delivery vectors in humans. Focusing on mutating a 28-amino acid segment, <ref type="bibr" target="#b10">Bryant et al. (2021)</ref> generated more than 200,000 variants of AAV sequences with 12-29 mutations across this region, and measured their ability to package of a DNA payload. This dataset is unique compared to many other mutagenesis datasets in that most sequences feature random insertions in the 28-amino acid segment, as opposed to only random substitutions.</p><p>We use inverse folding models to predict insertion and substitution effects as follows: For each sequence, we input the full backbone coordinates of the wild-type (PDB: 1LP3), and insert one masked token into the input backbone coordinates for each insertion. Then we compare the conditional sequence log-likelihood on this input with masks to the conditional sequence log-likelihood of the wild-type sequence on the wild-type backbone. The difference in these two conditional log-likelihoods are used as the score for predicting packaging ability.</p><p>We report the zero-shot performance on each of the 7 data subsets evaluated in the FLIP <ref type="bibr" target="#b13">(Dallago et al., 2021)</ref> benchmark suite. As shown in Table <ref type="table" target="#tab_0">C</ref>.5, GVP-Transformer trained with predicted structures outperforms the sequence-only zero-shot prediction baseline ESM-1v on 6 out of the 7 data subsets. For ESM-1v, we scored variant sequences based on the independent marginals formula, as described in Equation 1 from <ref type="bibr" target="#b40">Meier et al. (2021)</ref>.</p><p>Confusion matrix. We calculated the substitution scores between native sequences and sampled sequences (sampled with temperature T = 1) by using the same log odds ratio formula as in the BLOSUM62 substition matrix. For two amino acids x and y, the substitution score s(x, y) is s(x, y) = log p(x, y) q(x)q(y) ,</p><p>where p(x, y) is the jointly likelihood that native amino acid x is substituted by sampled amino acid y, q(x) is the marginal likelihood in the native distribution, and q(y) is the marginal likelihood in the sampled distribution.</p><p>Calibration. Calibration curves examines how well the probabilistic predictions of a classifier are calibrated, plotting the true frequency of the label against its predicted probability. When computing the calibration curve, for each amino acid, we bin the predicted probabilities into 10 bins and then compare with the true probability.</p><p>Placement of hydrophobic residues. We define the amino acids IVLFCMA as hydrophobic residues, and inspect the distribution of solvent accessible surface area for both hydrophobic residues and polar (non-hydrophobic) residues. Solvent accessible surface area calculated with the Shrake-Rupley ("rolling probe") algorithm from the biotite package <ref type="bibr" target="#b32">(Kunzmann &amp; Hamacher, 2018)</ref> and summed over all atoms in each amino acid. All models have similar distributions of accessible surface area for hydrophobic residues, also similar to the distribution in native sequences <ref type="bibr">(Figure C.6)</ref>.</p><p>Sampling speed. We profile the sampling speed with PyTorch Profiler, averaging over the sampling time for 30 sequences in each sequence length bucket on a Quadro RTX 8000 GPU with 48GB memory. For the generic Transformer decoder, we use the incremental causal decoding implementation in fairseq <ref type="bibr" target="#b45">(Ott et al., 2019)</ref>. For GVP-GNN, we use the implementation from the gvp-pytorch GitHub repository. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1. Augmenting inverse folding with predicted structures. To evaluate the potential for training protein design models with predicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2<ref type="bibr" target="#b30">(Jumper et al., 2021</ref>). An autoregressive inverse folding model is trained to perform fixed-backbone protein sequence design. Train and test sets are partitioned at the topology level, so that the model is evaluated on structurally held-out backbones. We compare transformer models having invariant geometric input processing layers, with fully geometric models used in prior work. Span masking and noise is applied to the input coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the protein design tasks considered.</figDesc><graphic url="image-21.png" coords="2,80.89,145.02,67.53,67.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example AlphaFold prediction compared with experimental structure for a UniRef50 sequence (UniRef50: P07260; PDB: 1AP8). The experimental structure is shown as pink with transparency. The prediction is coloured by the pLDDT confidence score, with blue in high-confidence regions.</figDesc><graphic url="image-25.png" coords="3,338.14,67.06,170.09,130.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>scheme during training) GVP-GNN-large (random mask) GVP-GNN-large (span mask) GVP-Transformer (random mask) GVP-Transformer (span mask) Random sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. Perplexity on regions of masked coordinates of different lengths. The GVP-GNN architecture degrades to the perplexity of the background distribution for masked regions of more than a few tokens, while GVP-Transformer maintains moderate accuracy on long masked spans, especially when trained on masked spans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of perplexity and sequence recovery by structural context according to two different measures: number of neighbors (top) and solvent accessible surface area (bottom). Top: Breakdown for core and surface residues. Residues are categorized by density of neighboring Cα atoms within 10A of the central residue Cα atom (core: ≥ 24 neighbors; surface: &lt; 16 neighbors).Each box shows the distribution of perplexities for the core or surface residues across different sequences. Bottom: Perplexity and sequence recovery as a function of solvent accessible surface area. Increased sequence recovery for buried residues suggests the model learns dense hydrophobic packing constraints in the core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Ablation studies on training data. (a) Effect of increasing the number of predicted structures. The original GVP-GNN degrades with training on additional data, but GVP-GNN-large and GVP-Transformer improve with increasing numbers of predicted structures. (b) Effect of increasing the mixing ratio during training between predicted and experimental structures. A higher ratio of predicted structures improves performance for both GVP-GNN-large and GVP-Transformer. (c) GVP-GNN and GVP-Transformer model size.</figDesc><graphic url="image-26.png" coords="6,55.44,67.06,486.01,126.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Dual-state design. GVP-Transformer conditioned on two conformations results in lower sequence perplexity at locally flexible residues than single-conformation conditioning for structurally held-out proteins in PDBFlex (see Appendix C for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure B. 1 .</head><label>1</label><figDesc>Figure B.1. An illustrative example of structural overlap between CATH topology splits. The jack bean canavalin (PDB code 1DGW; chain Y; red) and the soybean β-Conglycinin (PDB code 1UIJ; chain B; blue) are assigned different topology codes in CATH (1.10.10 and 2.60.120), but they align with TM-score 0.94 and CA RMSD 0.7A on a segment of 90 residues. The difference in topology classifications likely resulted from CATH annotating only a 37-residue mainly helical segment of the jack bean canavalin as a domain while annotating a longer 176-residue mainly beta sheet segment of the soybean β-Conglycinin as a domain.</figDesc><graphic url="image-27.png" coords="15,200.00,67.06,194.40,155.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure C.2. Fixed backbone sequence design perplexity for protein complexes. The model is evaluated on 796 structurally held-out protein complexes. Comparison of conditioning on the backbone coordinates of individual chains (x-axis) with conditioning on backbone coordinates of the entire complex (y-axis). Note that for both values perplexity is evaluated on the same chain in the complex. The shift to the lower right indicates improved perplexity when the model is given the complete structure of the complex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure</head><label></label><figDesc>Figure C.4. Confusion matrix between native sequence and sampled sequences from the model, compared to BLOSUM62 as reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table C</head><label>C</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Perplexity</cell><cell></cell><cell></cell><cell>Recovery %</cell><cell></cell></row><row><cell>Model</cell><cell>Data</cell><cell cols="2">Short Single-chain</cell><cell>All</cell><cell cols="2">Short Single-chain</cell><cell>All</cell></row><row><cell>Natural frequencies</cell><cell></cell><cell>18.12</cell><cell>18.03</cell><cell>17.97</cell><cell>9.6%</cell><cell>9.0%</cell><cell>9.5%</cell></row><row><cell>Structured GNN</cell><cell>CATH</cell><cell>7.91</cell><cell>6.48</cell><cell>6.49</cell><cell>31.5%</cell><cell>37.1%</cell><cell>37.1%</cell></row><row><cell>GVP-GNN</cell><cell cols="2">CATH + AlphaFold2 8.55 7.14</cell><cell>5.36 6.17</cell><cell>5.43 6.06</cell><cell>34.0% 29.5%</cell><cell>42.7% 38.2%</cell><cell>42.2% 38.6%</cell></row><row><cell>GVP-GNN-large</cell><cell cols="2">CATH + AlphaFold2 6.11 7.68</cell><cell>6.12 4.09</cell><cell>6.17 4.08</cell><cell>32.6% 38.3%</cell><cell>39.4% 50.8%</cell><cell>39.2% 50.8%</cell></row><row><cell>GVP-Transformer</cell><cell cols="2">CATH + AlphaFold2 6.05 8.18</cell><cell>6.33 4.00</cell><cell>6.44 4.01</cell><cell>31.3% 38.1%</cell><cell>38.5% 51.5%</cell><cell>38.3% 51.6%</cell></row><row><cell cols="8">Table 1. Fixed backbone sequence design. Evaluation on the CATH 4.3 topology split test set. Models are compared on the basis of</cell></row><row><cell cols="8">per-residue perplexity (lower is better; lowest perplexity bolded) and sequence recovery (higher is better; highest sequence recovery</cell></row><row><cell cols="8">bolded). Large models can make better use of the predicted UniRef50 structures. The best model trained with predicted structures</cell></row><row><cell cols="8">(GVP-Transformer) improves sequence recovery by 8.9 percentage points over the best model (GVP-GNN) trained on CATH only.</cell></row><row><cell cols="4">Edunov et al. (2018), who observe that backtranslation with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">sampled or noisy synthetic data provides a stronger training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">signal than maximum a posteriori (MAP) predictions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">.1). This finding is consistent with</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table C.2). Further details are in Appendix C.</figDesc><table><row><cell></cell><cell cols="2">Spearman correlation</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">No coords No RBM coords No ACE2 coords All coords</cell></row><row><cell>ESM-1v</cell><cell>0.03</cell><cell></cell><cell></cell></row><row><cell>ESM-1b</cell><cell>0.02</cell><cell></cell><cell></cell></row><row><cell>ESM-MSA-1b (few-shot)</cell><cell>0.51</cell><cell></cell><cell></cell></row><row><cell>GVP-GNN</cell><cell>-0.10</cell><cell>0.50</cell><cell>0.60</cell></row><row><cell>GVP-GNN-large+AF2</cell><cell>-0.05</cell><cell>0.52</cell><cell>0.69</cell></row><row><cell>GVP-Transformer+AF2</cell><cell>-0.06</cell><cell>0.53</cell><cell>0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>So far there has been little work on generative models of structures directly. Interesting examples include<ref type="bibr" target="#b2">Anand &amp; Huang (2018)</ref> who model fixed-length protein backbones with generative adversarial networks (GANs) via pairwise distance matrices, and<ref type="bibr" target="#b16">Eguchi et al. (2020)</ref> who generate antibody structures with variational autoencoders (VAEs).</figDesc><table><row><cell></cell><cell>Language models A large body of work has focused on</cell></row><row><cell></cell><cell>modeling the sequences in individual protein families. Shin</cell></row><row><cell></cell><cell>et al. (2021) show that protein-specific autoregressive se-</cell></row><row><cell>. More recently, convolution-based</cell><cell>quence models trained on related proteins can predict point</cell></row><row><cell>deep learning methods have also been applied to predict</cell><cell>mutation and indel effects and design functional nanobod-</cell></row><row><cell>amino acid propensities given the surrounding local struc-</cell><cell>ies. Trinquier et al. (</cell></row><row><cell>tural environments (Anand-Achim et al., 2021; Boomsma</cell><cell></cell></row><row><cell>&amp; Frellsen, 2017; Shroff et al., 2020; Li et al., 2020; Qi &amp;</cell><cell></cell></row><row><cell>Zhang, 2020; Zhang et al., 2020; Chen et al., 2019; Wang</cell><cell></cell></row><row><cell>et al., 2018). Another recent machine learning approach</cell><cell></cell></row><row><cell>is to leverage structure prediction networks for sequence</cell><cell></cell></row><row><cell>design. Norn et al. (2021) carried out Monte Carlo sampling</cell><cell></cell></row><row><cell>in the sequence space to invert the trRosetta (Yang et al.,</cell><cell></cell></row><row><cell>2020) structure prediction network for sequence design.</cell><cell></cell></row><row><cell>Generative models of proteins The literature on</cell><cell></cell></row><row><cell>structure-based generative models of protein sequences is</cell><cell></cell></row><row><cell>the closest to our work. Ingraham et al. (2019) introduced</cell><cell></cell></row><row><cell>the formulation of fixed-backbone design as a conditional</cell><cell></cell></row><row><cell>sequence generation problem, using invariant features with</cell><cell></cell></row><row><cell>graph neural networks, modeling each amino acid as a node</cell><cell></cell></row><row><cell>in the graph with edges connecting spatially adjacent amino</cell><cell></cell></row><row><cell>acids. Jing et al. (2021b;a) further improved graph neu-</cell><cell></cell></row><row><cell>ral networks for this task by developing architectures with</cell><cell></cell></row><row><cell>translation-and rotation-equivariance to enable geometric</cell><cell></cell></row><row><cell>reasoning, showing that GVP-GNN achieves higher native</cell><cell></cell></row><row><cell>sequence recovery rates than Rosetta on TS50, a bench-</cell><cell></cell></row><row><cell>mark set of 50 protein chains. Strokach et al. (2020) trained</cell><cell></cell></row><row><cell>graph neural networks for conditional generation with the</cell><cell></cell></row><row><cell>masked language modeling objective, adding homologous</cell><cell></cell></row><row><cell>sequences as data augmentation to training.</cell><cell></cell></row><row><cell>Recently models have been proposed to jointly generate</cell><cell></cell></row><row><cell>structures and sequences. Anishchenko et al. (2021) gener-</cell><cell></cell></row><row><cell>ate structures by optimizing sequences through the trRosetta</cell><cell></cell></row><row><cell>structure prediction network to maximize their difference</cell><cell></cell></row><row><cell>from a background distribution. The joint generation ap-</cell><cell></cell></row><row><cell>proach is also being explored in the setting of infilling par-</cell><cell></cell></row><row><cell>tial structures. Contemporary to this work, Wang et al.</cell><cell></cell></row><row><cell>(2021) apply span masking to fine-tune the RosettaFold</cell><cell></cell></row><row><cell>model (Baek et al., 2021) to perform infilling. However</cell><cell></cell></row><row><cell>Wang et al. do not consider inverse folding, and condition</cell><cell></cell></row></table><note>on both coordinates and amino acid identities. Also contemporary to this work,<ref type="bibr" target="#b26">Jin et al. (2021)</ref> develop a conditional generation model for jointly generating sequences and structures for antibody complementarity determining regions (CDRs), conditioned on framework region structures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table A.1. Details on model hyperparameters and training.</figDesc><table><row><cell></cell><cell>GVP-GNN</cell><cell>GVP-GNN-large</cell><cell>GVP-Transformer</cell></row><row><cell>GVP-GNN embedding dim (node)</cell><cell>(100, 16)</cell><cell>(256, 64)</cell><cell>(1024, 256)</cell></row><row><cell>GVP-GNN embedding dim (edge)</cell><cell>(32, 1)</cell><cell>(32, 1)</cell><cell>(32, 1)</cell></row><row><cell>Top K neighbors in GVP-GNN</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>GVP-GNN encoder layers</cell><cell>3</cell><cell>8</cell><cell>4</cell></row><row><cell>GVP-GNN decoder layers</cell><cell>3</cell><cell>8</cell><cell></cell></row><row><cell>Transformer embedding dim</cell><cell></cell><cell></cell><cell>512</cell></row><row><cell>Feedforward embedding dim</cell><cell></cell><cell></cell><cell>2048</cell></row><row><cell>Attention heads</cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>Transformer encoder layers</cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>Transformer decoder layers</cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>Total number of parameters</cell><cell>1M</cell><cell>21M</cell><cell>142M</cell></row><row><cell>Batch size (tokens per GPU)</cell><cell>3072</cell><cell>4096</cell><cell>4096</cell></row><row><cell>GPUs</cell><cell>1</cell><cell>32</cell><cell>32</cell></row><row><cell>CATH:AF2 mixing ratio</cell><cell>1:0</cell><cell>40:1</cell><cell>80:1</cell></row><row><cell>Epochs until convergence</cell><cell>84</cell><cell>368</cell><cell>178</cell></row><row><cell>Train time per epoch (GPU hours)</cell><cell>0.07</cell><cell>24</cell><cell>88</cell></row><row><cell>Total train time (GPU days)</cell><cell>0.2</cell><cell>368</cell><cell>653</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Learning rate schedule</cell><cell cols="3">Constant Inverse square root Inverse square root</cell></row><row><cell>Peak learning rate</cell><cell>1.0E-03</cell><cell>1.0E-03</cell><cell>1.0E-03</cell></row><row><cell>Initial learning rate</cell><cell></cell><cell>1.0E-07</cell><cell>1.0E-07</cell></row><row><cell>Warm-up updates</cell><cell></cell><cell>5000</cell><cell>5000</cell></row><row><cell>Gradient clipping</cell><cell>4.0</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Without adding</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Halil Akin, Sal Candido, Ori Kabeli, Joshua Meier, Ammar Rizvi, and Zhongkai Zhu for feedback on the manuscript and insightful conversations.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code and weights available at https://github.com/facebookresearch/esm.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We constructed a TM-score-based test set of 223 proteins with no TMalign matches (TM-score ≥ 0.5) from the train set, using the foldseek <ref type="bibr" target="#b31">(Kim et al., 2021)</ref> TMalign tool with default parameters for the pairwise search.</p><p>We found that the conclusions about model performance overall remains the same on this </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results and details</head><p>Ablation on noise and masking during training. We found that GVP-Transformer models trained with Gaussian noise during training perform slightly better at test time than those trained without (Table <ref type="table">C</ref>.1). When given full backbone coordinates at test time, training with span masking only very slightly improves model performance compared to no masking or to random masking, even though there is a much larger performance gap between random masking and span masking on regions with masked backbone coordinates (Figure <ref type="figure">4</ref>).</p><p>Dual-state design test set from PDBFlex. We test design performance on multiple conformations by finding test split proteins with distinct conformations in the PDBFlex database. From PDBFlex, we looks for experimental structures of protein sequences in the CATH topology split test set (95% sequence identity or above), and take all paired instances that are at least 5 angstroms apart in overall RMSD between conformations. We report perplexity on locally flexible residues (defined as local RMSD above 1 angstrom). To be more conservative in our evaluation, we show the better of the two conformations to represent single-state perplexity in Figure <ref type="figure">7</ref>.</p><p>Ablation on the number of GVP-GNN encoder layers in GVP-Transformer.  Stability prediction on de novo small proteins. We predict protein stability on an experimentally measured stability dataset for de novo small proteins <ref type="bibr" target="#b52">(Rocklin et al., 2017)</ref>. We use the relative difference in sequence conditional loglikelihoods as a predictor for stability and compute Pearson correlation with the mutation effect following <ref type="bibr" target="#b24">(Ingraham et al., 2019)</ref>, assuming that more stable sequences should score higher in log-likelihoods. For each fold, Rocklin et al. ( <ref type="formula">2017</ref>) starts with a reference protein and generates sequence variants with single amino acid substitutions. We calculate the Pearson correlation between sequence conditional log-likelihood scores and experimental stability measurements for all designed sequences in each fold. With predicted structures as additional training data, the GVP-Transformer model improves the pearson correlation on 8 out of the 10 folds.</p><p>Perplexity and sequence recovery of SARS-CoV-2 RBD. We show perplexity and sequence recovery on the SARS-CoV-2 protein receptor binding domain (RBD) as an example for inverse folding. The RBD can exist in a closed-state with the RBD down or in an open-state with the RBD up <ref type="bibr" target="#b68">(Walls et al., 2020)</ref> 0.71 GVP-GNN-large+AF2 (chain) 0.61 GVP-  0.71 GVP-Transformer+AF2 (chain) 0.60  0.68</p><p>Table <ref type="table">C</ref>.4. Protein complex stability on SKEMPI test set (binary classification of increase in stability on single-point mutations). Although only trained on single chains, the inverse-folding models generalize to protein complexes. Giving the full complex as input, complex, improves performance compared to giving only the chain as input, chain. Zero-shot prediction compared to fully supervised and supervised transfer learning methods from <ref type="bibr" target="#b64">(Townshend et al., 2020)</ref> and <ref type="bibr" target="#b27">(Jing et al., 2021a)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The rosetta allatom energy function for macromolecular modeling and design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jeliazkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>O'meara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kappel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3031" to="3048" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative modeling for protein structures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein sequence design with a learned potential</title>
		<author>
			<persName><forename type="first">N</forename><surname>Anand-Achim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Derry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biorxiv</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-based reinforcement learning for biological sequence design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">De novo protein design by deep network hallucination</title>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pellock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ramelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bafna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Norn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">600</biblScope>
			<biblScope unit="issue">7889</biblScope>
			<biblScope unit="page" from="547" to="552" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kinch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Millán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Degiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ebrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Opperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sagmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buhlheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavkov-Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Rathinaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dalwadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Grishin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.abj8754</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">6557</biblScope>
			<biblScope unit="page" from="871" to="876" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garnett ; Brookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Listgarten</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017. 2019</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
	<note>International conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep diversification of an aav capsid protein by machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kelsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">To improve protein sequence profile prediction through image captioning on pairwise residue distance map</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="391" to="399" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probing the role of packing specificity in protein design</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Dahiyat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="10172" to="10177" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flip: Benchmark tasks in fitness landscape inference for proteins</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">De novo protein design: what are we learning?</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Degrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Raleigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Handel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Structural Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="984" to="993" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ig-vae: generative modeling of immunoglobulin proteins by direct 3d coordinate generation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prottrans: Towards cracking the language of lifes code through selfsupervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3095381</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Function-guided protein design by deep manifold sampling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kelow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-resolution protein design with backbone freedom</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Harbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Plecs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="issue">5393</biblScope>
			<biblScope unit="page" from="1462" to="1467" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of multiple amber force fields and development of improved protein backbone parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hornak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Okur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Strockbine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simmerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="712" to="725" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pdbflex: exploring flexibility in protein structures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hrabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rotkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jaroszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D423" to="D428" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The coming of age of de novo protein design</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">537</biblScope>
			<biblScope unit="issue">7620</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="15794" to="15805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jankauskaitė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiménez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dapkūnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernández-Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Moal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="469" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Iterative refinement graph neural network for antibody sequence-structure co-design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Equivariant graph neural networks for 3d macromolecular structure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from protein structure with geometric vector perceptrons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J L</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Spanbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Kempen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><surname>Foldseek</surname></persName>
		</author>
		<ptr target="https://github.com/steineggerlab/foldseek" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Biotite: a unifying open source computational biology framework in python</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kunzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hamacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structure of the sars-cov-2 spike receptor-binding domain bound to the ace2 receptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">581</biblScope>
			<biblScope unit="issue">7807</biblScope>
			<biblScope unit="page" from="215" to="220" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">De novo design of bioactive protein switches</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Samson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">572</biblScope>
			<biblScope unit="issue">7768</biblScope>
			<biblScope unit="page" from="205" to="210" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gene3d: a domain-based resource for comparative genomics, functional annotation and protein network analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sillitoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Dessailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D465" to="D471" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting changes in protein thermodynamic stability upon point mutation with deep 3d convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e1008291</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Faraggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struc-Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2565" to="2573" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03497</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep neural language modeling enables functional protein generation across families</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Holton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models enable zero-shot prediction of the effects of mutations on protein function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Uniclust databases of clustered and deeply annotated protein sequences and alignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Von Den Driesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D170" to="D176" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Protein sequence design by conformational landscape optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Norn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Wicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Koepnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting sequence profiles from protein structures using deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehzangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Spin2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="629" to="633" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cath-a hierarchic classification of protein domain structures</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Swindells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1109" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Fairseq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">A fast, extensible toolkit for sequence modeling</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hmmer web server: 2018 update</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W200" to="W204" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Densecpd: improving the accuracy of neural-network-based computational protein sequence design with densenet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1245" to="1252" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">De novo design of modular and tunable protein biosensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quijano-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Miranda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">591</biblScope>
			<biblScope unit="issue">7850</biblScope>
			<biblScope unit="page" from="482" to="487" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Evaluating protein transfer learning with tape. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rives</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A. Msa transformer. bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houliston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6347</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kruse</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Protein design and variant prediction using autoregressive generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discovery of novel gain-of-function mutations guided by structure-based deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Annapareddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gollihar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Ellington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS synthetic biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2927" to="2935" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Whatley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Slocum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Locane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kelsic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02141</idno>
		<title level="m">Adalead: A simple and robust adaptive greedy search algorithm for sequence design</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep mutational scanning of sars-cov-2 receptor binding domain reveals constraints on folding and ace2 binding</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Starr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Greaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dingens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tortorici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Walls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1295" to="1310" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hh-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vöhringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Haunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Computational protein design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="R105" to="R109" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast and flexible protein design using deep graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corbi-Verge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez-Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="402" to="411" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">ATOM3D: tasks on molecules in three dimensions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J L</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vögele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Derry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Laloudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<idno>CoRR, abs/2012.04035</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Efficient generative modeling of protein sequences using simple autoregressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Trinquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zamponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03292</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Structure, function, and antigenicity of the sars-cov-2 spike glycoprotein</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tortorici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Veesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Computational protein design with deep learning neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep learning methods for designing proteins scaffolding functional sites</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lisanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Protein sequence design with deep generative models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Chemical Biology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted interresidue orientations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1496" to="1503" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Machine-learningguided directed evolution for protein engineering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="687" to="694" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Protein design using a convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Prodconn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="819" to="829" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A generalpurpose protein design framework based on mining sequence-structure relationships in known protein structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Panaitiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grigoryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1059" to="1068" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
