<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A User-Specific Machine Learning Approach for Improving Touch Accuracy on Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daryl</forename><surname>Weir</surname></persName>
							<email>darylw@dcs.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Rogers</surname></persName>
							<email>simon.rogers@glasgow.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Roderick</forename><surname>Murray-Smith</surname></persName>
							<email>roderick.murray-smith@glasgow.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Löchtefeld</surname></persName>
							<email>markus.loechtefeld@dfki.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science University of Glasgow 18 Lilybank Gardens Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI) Stuhlsatzenhausweg</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>D3-2 D-66123</postCode>
									<settlement>Campus, Saarbrucken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">UIST&apos;12</orgName>
								<address>
									<addrLine>October 7-10</addrLine>
									<postCode>2012</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A User-Specific Machine Learning Approach for Improving Touch Accuracy on Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FDCDF99BB02D7A8C739B04617655DE3A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.2. Information Interfaces and Presentation: Input devices and strategies (e.g. mouse, touchscreen) Touch</term>
					<term>Machine Learning</term>
					<term>Regression</term>
					<term>Gaussian Processes</term>
					<term>Probabilistic Modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a flexible Machine Learning approach for learning user-specific touch input models to increase touch accuracy on mobile devices. The model is based on flexible, non-parametric Gaussian Process regression and is learned using recorded touch inputs. We demonstrate that significant touch accuracy improvements can be obtained when either raw sensor data is used as an input or when the device's reported touch location is used as an input, with the latter marginally outperforming the former. We show that learned offset functions are highly nonlinear and user-specific and that user-specific models outperform models trained on data pooled from several users. Crucially, significant performance improvements can be obtained with a small (≈ 200) number of training examples, easily obtained for a particular user through a calibration game or from keyboard entry data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In this paper, we investigate an alternative, data-driven approach to touch. We treat the problem as a Machine Learning (ML) task where we are interested in learning a function that maps an input (the device's reported touch location or the raw sensor values) to the intended touch location. We show later that the functions learned for individual users are highly userspecific and vary greatly across the touch surface, overcoming the limitations of a device specific offset as in <ref type="bibr" target="#b5">[6]</ref>. We use a flexible non-parametric regression algorithm based on Gaussian Processes (GPs; <ref type="bibr" target="#b11">[12]</ref>) to enable us to model complex Given the increasing ubiquity of touch, there is a clear need for techniques which facilitate accurate input. One of the primary causes of touch error is the so called "fat finger problem" <ref type="bibr" target="#b2">[3]</ref>: through the act of touching, the user occludes the very targets she is trying to touch and the softness of the finger/thumb produces a much larger, more ambiguous, contact region than intended <ref type="bibr" target="#b7">[8]</ref>. This is particularly problematic when users type on a virtual keyboard with their thumbs, both when operating the device one-handed <ref type="bibr" target="#b10">[11]</ref> (holding and typing with the same hand) or typing with two thumbs with the device in landscape mode. The result is a user-specific offset between the touch location reported by the device and that intended by the user.</p><p>There has been considerable research effort directed at creating models to account for this offset. Studies both in the wild <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and in the lab <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> have succeeded in compensating for some of the error inherent in touch based interaction. Unfortunately these approaches are either based on custom hardware or on global compensation methods; none of them focused on the needs of specific users. The approach proposed here is both user-specific and uses only data available on commercial smartphones. Further, our model is fully probabilistic in its predictions and is naturally able to incorporate data from sensors other than the touch screen (e.g. accelerometer readings). This approach provides the basis for future research into propagating uncertainty from input to application and potentially opening rich new channels of interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Even though HCI research focused on touch screens based on capacitive sensing has been conducted for over 25 years <ref type="bibr" target="#b9">[10]</ref>, current error rates suggest there is still room for improvement <ref type="bibr" target="#b5">[6]</ref>. Precise capacitive touch input is clearly a desirable goal, given the increasing ubiquity of touchscreen devices. Such devices are employed in an extremely broad range of settings -even input through fabric enclosure (e.g. inside a pocket) has been successfully explored <ref type="bibr" target="#b15">[16]</ref>.</p><p>In lab studies, <ref type="bibr" target="#b7">[8]</ref> investigated the relationship between finger orientation and touch error and introduced 'Ridgepad'a device that could infer finger orientation through fingerprint scanning. Similarly, <ref type="bibr" target="#b13">[14]</ref> showed that it is possible to infer finger orientation using an array of capacitive sensors and sophisticated Bayesian statistical techniques. Both <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref> showed that there is considerable diversity between users, and between finger and thumb use. This suggests that significant improvements could be made if user-and contextspecific models were built. Unfortunately, the improvements in touch performance described by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref> come at significant hardware and computational costs respectively and therefore do not offer solutions that are currently practical.</p><p>In two recent large-scale studies, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> used a crowd-sourcing approach to collect vast quantities of touch data from a variety of users and devices. They investigated simple tapping as well as text entry with applications deployed in the Android Marketplace. Their data led them to create a device-specific compensation function which they then showed improved accuracy <ref type="bibr" target="#b5">[6]</ref>. For the text-entry scenario a simple shift function helped to decrease the error rate by 9.1% globally <ref type="bibr" target="#b6">[7]</ref>. This approach increases the accuracy of touch input, but we argue that there is still room for improvement. In <ref type="bibr" target="#b8">[9]</ref> it was shown that the personal perceived touch point is important, in addition to the physical characteristics of the touching finger or thumb. Users tend to use visual features of their fingers for their mental model that determines the corresponding point on the screen that their touch hits. Since these mental models can differ between users, personalized models would have a bigger influence than general models. This is a key motivation for our approach -no matter how good the sensing hardware is at converting sensor inputs into a touch location, there will always be user-specific biases. For example, in Figure <ref type="figure" target="#fig_0">1</ref> we show the smoothed sensor values for a touch event (white is high valued, black low). The grey circle shows the touch location reported by the device, which looks very reasonable given the sensor values. However, the white circle represents the point the user was trying to touch. This offset between the intended touch point and the measured touch point will be highly user-specific (see e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>) and it is this error that we aim to reduce in this work.</p><p>In both <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b5">[6]</ref>, the techniques proposed give deterministic values for the intended touch location -there is no information about the uncertainty of predictions. There is a growing body of research showing the value of propagating uncertainty from input to the application level <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="bibr" target="#b14">[15]</ref> explicitly models the uncertainty in a touch input and uses this to negotiate the handover of control between the user and the system. The authors demonstrate a map browsing application which automatically scrolls towards nearby points of interest when user input is uncertain (i.e. when the finger is lifted from the screen slightly.) The work of Schwarz <ref type="bibr" target="#b16">[17]</ref> is also relevant. The authors propose a framework for modelling uncertain interaction which allows a system to consider several possible actions and assign them probabilities based on user input. Definitive action is only taken when the probability for an event reaches a certain threshold. The predictions of our model are Gaussian distributions with full covariance structure, which could be used as input to such a probabilistic interaction framework. Personalized models have been used commonly for key-target resizing on soft keyboards <ref type="bibr" target="#b1">[2]</ref> but the disadvantages of these techniques is that the approach is only applicable for keyboard input since they are based on natural languages technologies. Similarly, in <ref type="bibr" target="#b3">[4]</ref> the authors use machine learning to classify which key a given touch was targetting. Again, this requires specific information about the layout of the keyboard and is not generally applicable to all touches. Our approach combines the personalization and learning process with the power of a model which increases touch accuracy for all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUCH AS A MACHINE LEARNING PROBLEM</head><p>Our approach to overcome the problems discussed above is to introduce a machine learning technique that allows us to create user specific models based on either raw touch sensor data or the device's reported touch location. We are able to access the raw sensor data for the Nokia N9 MeeGo smartphone, which has an array of capacitive sensors laid out in a grid. We base most of our experimentation on this device as it is the only commercial product for which we have access to raw sensor data. Our task is therefore to find a mapping between either a vector of sensor values or a 2-dimensional reported touch location and the corresponding intended 2-dimensional touch location on the 854x480 pixel display. Figure <ref type="figure" target="#fig_0">1</ref> shows the sensor values for a typical touch towards the top of the device (the intended touch point is represented as a white circle). It is clear that a typical touch produces a significant sensor reading in several sensor positions simultaneously. As already mentioned, the exact pattern of sensor activation for a particular touch will depend on the user and on how they are performing the touch -with a finger or a thumb say -and it is not clear what form this function will take. We therefore turn to a flexible non-parametric regression algorithm: Gaussian Process regression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Process Regression</head><p>Gaussian Processes (GPs) <ref type="bibr" target="#b11">[12]</ref> are a popular statistical technique for regression and classification. In this work, we are interested in learning a function that can map between inputs s and intended 2D touch location (x, y): (x, y) = f (s). In our experiments, the input s will be either the raw sensor values, or the touch location given by the device. GPs allow us to do this without making any parametric assumptions about the form of f (s). Instead, we supply a mean function µ(s) (which is set to zero in all of our experiments) and a covariance function, C(s n , s m ), which defines how similar the nth and mth outputs should be based on inputs s n and s m . Readers are pointed to <ref type="bibr" target="#b11">[12]</ref> for extensive details on GP regression and classification.</p><p>Given N training examples, x 1 , . . . , x n , . . . , x N and associated locations, (x 1 , y 1 ), . . . , (x n , y n ), . . . , (x N , y N ), our GP regression approach proceeds as follows. Firstly, we turn the problem from a two-dimensional regression into a onedimensional regression by stacking all of the locations into a single vector: z = [x 1 , . . . , x n , . . . , x N , y 1 , . . . , y n , . . . , y N ]</p><p>T .</p><p>Note that this does not preclude us from modelling possible dependencies between x n and y n (see below). We now build an N × N covariance matrix, C, where the n, mth element is calculated by evaluating the covariance function C(s n , s m ). This matrix is then stacked up to produce the full 2N × 2N covariance matrix, C:</p><formula xml:id="formula_0">C = C αC αC C ,</formula><p>where α controls the strength of the dependence between x n and y n . Formally, the covariance between x n and x m (or y n and y m ) is given by C(s n , s m ) whilst the covariance between x n and y m is given by αC(s n , s m ). If α = 0, we are effectively using independent regression models for the x and y locations.</p><p>Finally, we assume a small amount of additive Gaussian noise (with variance σ 2 ). This overcomes possible problems resulting from trying to map very similar input sensor values to different intended touch locations.</p><p>Our aim is to be able to predict (x * , y * ) for a new set of input values s * . To do this, we create a vector comprising the covariance function evaluated between s * and the N input vectors in the training set:</p><formula xml:id="formula_1">c = [C(s * , s 1 ), . . . , C(s * , s N )],</formula><p>which is then stacked up similarly to the training covariance matrix:</p><formula xml:id="formula_2">c = c αc αc c .</formula><p>The GP prediction is a 2-dimensional Gaussian with mean and covariance given by:</p><formula xml:id="formula_3">(x * , y * ) ∼ N (µ, Σ) µ = c C + σ 2 I -1 z Σ = C(s * , s * ) -c C + σ 2 I -1 c T .</formula><p>Note that the inversion of the 2N × 2N matrix is not prediction specific and can therefore be done just once after training data has been collected. Note that if α = 0, this predictive Gaussian will have no covariance between the two dimensions (i.e. the top right and bottom left elements of Σ will be equal to zero). Both α and σ 2 are parameters that need to be optimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of covariance function</head><p>Many different covariance functions have been used for GP regression. In this work, we have used the following, standard, combination of a linear and Gaussian covariance:</p><formula xml:id="formula_4">C(s n , s m ) = b as T n s m + (1 -a) exp -γ||s n -s m || 2 2 ,</formula><p>where a controls the relative influence of the linear and Gaussian terms and γ controls the length scale of the Gaussian. Cross validation on a gives an indication of the non-linearity of the mapping from the input space to the intended touch location. We shall see later that using the sensor values as input results in a highly non-linear function, whereas using the device's reported touch location as our input gives an even balance of the linear and non-linear terms. This covariance function provided excellent performance, but an exploration of other covariance functions is an avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTAL SETUP</head><p>We built a simple data collector in Python and PyGame that ran natively on the Nokia N9. The software displayed crosshairs on the screen that the users had to touch. The crosshairs were randomly located in the area that is normally occupied by the landscape keyboard of the N9 to simulate text entry in landscape mode. For each touch on a crosshair, the system recorded the intended location (i.e. the screen location of the crosshair), the values of the capacitive sensors and the location reported by the N9 for the touch event. After the user lifted his finger from the screen another crosshair would be displayed. We obtained data from a total of 8 participants (3 female), aged between 23 and 34, all of whom performed 1000 touches holding the device in landscape mode, using either thumb to touch (see Figure <ref type="figure" target="#fig_10">2</ref>). All but one of our participants owned smartphones and therefore were used to operating a touch-screen device.</p><p>For all our experiments, we rescaled the touch locations such that they were in the unit square centered on the origin, with no loss of generality. This was done so that the same model parameters can be used across different devices with different native resolutions. We transform the data back to a real world scale when computing virtual button accuracy and RMS error on our predictions.         Comparison of performance between the N9 touch events (N9) and our Gaussian Processes predictions using the raw sensor values as input (GP). Top row: Accuracy for different virtual button sizes (800 training points). Bottom row: Learning curves. Subject 1 represents an average user, subject 3 a user that already performs far above average already with the N9 screen and subject 8 represents a user that doesn't own a touch-screen based phone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>In our first set of experiments, we use the raw values from the capacitive sensor to predict the true touch location. We predict the location directly rather than an offset, since in principle this algorithm could take the place of whatever technique is currently used to predict device location. 5-fold crossvalidation was performed for subject 1 over a small range of values for a, γ, b, α, σ 2 and these values were then used across all subjects. It is important to note that these parameters control the GP covariance function, which defines how smooth the resulting predictive functions can be, rather than defining the functions themselves. In practice this means that the results for subject 1 are likely to be slightly optimistic, and those for all other users pessimistic. Ideally, a crossvalidation would be performed on a per-user basis but the necessary computation is an unreasonable demand in a mobile setting -any deployment of the system would have to use predetermined covariance parameters such as these. The chosen parameter values were: γ = 0.05, b = 5, a = 0.1, α = 0.9, σ 2 = 10 -3 . The sensor data is pre-processed prior to use by first setting to zero any values that are less than 150 (we found this was the typical background noise in the absence of touch input) and then normalising each touch so that the sum of the sensor values is equal to 1. The mean of the predictive Gaussian provided by the GP is used as the inferred touch location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting from raw sensor values</head><p>We begin by looking at virtual button accuracy, following <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref>. Assuming a circular button is placed around the centre of the crosshair at which the user was aiming, we can compute the proportion of touches that would fall within this circle. In Figure <ref type="figure" target="#fig_7">3</ref> we show the improvement in proportion for the GP regression over the N9s native location sensing. Positive values mean the model is an improvement over the N9s native algorithm. The data were analysed for three different virtual button sizes (2mm radius, 3mm, 4mm) and two different sizes of training set (400 and 800 touches). In each case, we repeated the experiment 10 times. For each repetition, we randomly sampled N = 400 or N = 800 touches from the 1000 touches for each user and used the remaining (600 or 200) samples for testing. Note that as we are interested in personalised touch models, separate regression models are trained for each user. The average reductions in error rate across all users are 23.47% (2mm buttons), 14.20% (3mm) and 4.70% (4mm).</p><p>For all users except subject 3, we see notable performance increases for buttons of radius 2mm and 3mm. These improvements are all statistically significant (paired t-test, p &lt; 0.05), with the exception of subject 3. An increase of 0.2 corresponds to a potential decrease in error rate of 20%. Improvements are lower for 4mm buttons reflecting the acceptable N9 performance at this radius. For 4mm buttons the improvements are statistically significant for subjects other than 1, 3 and 6. To place these results in context, in Figure <ref type="figure" target="#fig_9">4</ref> we show more detailed results from subjects 1,3 and 8 (representing a wide range of performances). The top plots ((a)-(c)) show the button radius results for buttons of radius 1mm to 7mm. The solid curve is the performance of the GP regression and the dashed curve that of the N9s native algorithm. For all subjects, results are indistinguishable at 6mm but large improvements are evident at smaller button sizes for subjects 1 and 8. Subject 3 is the only subject for which we see no improvement. Inspecting Figure <ref type="figure" target="#fig_9">4</ref>(b), we can see that both models perform very well for this subject -both the N9s algorithm and the GP perform well, and so very little improvement is possible. It is also interesting to note that subject 8 is the only user in the study who does not own a touch-screen device and is a user for which we see very large improvements.</p><p>The bottom row in Figure <ref type="figure" target="#fig_9">4</ref> shows how the root mean squared error between the intended and inferred touch location (in mm) varies as the training set size increases. The solid line gives the GP performance and the shaded area shows plus and minus one standard deviation. This was obtained by randomly taking a subset of touches for training and then testing on the remainder. The N9 performance is averaged over the same set of test points (variance in N9 performance was too small to visualise). We notice that for subjects 1 and 8, there is a large drop up to 400 training examples and then performance appears to plateau. In <ref type="bibr" target="#b5">[6]</ref>, participants recorded on average ∼ 1000 touches each and therefore our proposed approach could be trained using data collected in a similar manner. Subject 3 also plateaus at around 400 training examples, but does not show any improvement. Again however, this can be explained by the excellent N9 performance for this user (a root mean squared error value of less than 2mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction from device location</head><p>In our second set of experiments, we used the N9's reported touch location as input instead of the raw sensor values. This is motivated by the fact that it is not currently possible to obtain raw sensor data on most touchscreen devices. This approach is similar to the offsets computed in <ref type="bibr" target="#b5">[6]</ref> in that it is adding a post-processing step to the device's reported touch location but differs in two respects. Firstly, we are learning a continuous, user-specific offset function which can, in theory, have a different value at any input location and secondly, the GP regression does not restrict us to any particular parametric function family, such as the 5th order polynomials used by Henze et al. We once again optimise the GP covariance parameters by performing a cross-validation (us-       Comparison of performance between the N9 touch events (N9), a simple linear regression model (Linear) (e.g. as in <ref type="bibr" target="#b5">[6]</ref> and our Gaussian Process' predictions using the N9's reported touch location as input (GP). Top row: Accuracy for different virtual button sizes (800 training points). Bottom row: Learning curves. Subject 1 represents an average user, subject 3 a user that already performs far above average already with the N9 screen and subject 8 represents a user that never used a touch screen based phone before. ing user 5; randomly chosen). The optimal parameters were: γ = 100, b = 1, a = 0.5, α = 0.3, σ 2 = 10 -3 .</p><p>Figure <ref type="figure" target="#fig_16">5</ref> shows the improvements in performance offered by this method over the N9's reported touch location. We found that similar improvements can be made with fewer training examples than are required when using sensor data as input and so show the performance for N = 200 and N = 600 for buttons of radius 2mm, 3mm and 4mm. Once again, we notice significant (paired t-test, p &lt; 0.05) performance improvements for all subjects except subject 3. These improvements are for just 200 training examples -collection of which would represent a very low overhead for the user. Assuming that an increase in performance of 0.1 corresponds to a reduction in error rate of 10%, the average reduction in error rate across all users is 23.79% (2mm buttons), 14.79% (3mm) and 5.11% (4mm).</p><p>The work in <ref type="bibr" target="#b8">[9]</ref> suggests a theoretical lower limit of 2.15mm for the radius of a button which can be acquired with 95% accuracy. Their technique can acquire targets of radius 2.7mm, but doing so requires an overhead camera and an algorithm to extract salient features from the outline of the finger. Clearly these are not reasonable restrictions in a mobile setting. Our method achieves 95% accuracy with targets of radius between 2.8mm and 4mm, depending on the user. For all subjects bar one, this still represents an improvement over commercially available sensing technology of the N9.</p><p>In Figure <ref type="figure" target="#fig_17">6</ref> we show more detailed results for subjects 1, 3 and 8. We also give the performance of a simple linear regression model (see e.g. <ref type="bibr" target="#b12">[13]</ref> p. 22) trained on the same data. For subject 3 we again see no significant improvement but for subjects 1 and 8 we see that both the linear regression and the GP outperform the N9's native algorithm and that the GP outperforms the linear regression, suggesting that the touch offset is not a linear function of the touch location. Notice also that for subjects 1 and 8, the performance is never worse than that of the N9, whereas in predicting the intended location from the raw sensor values, the performance was worse when there was little training data (see Figure <ref type="figure" target="#fig_9">4(d)</ref> and<ref type="figure">(f)</ref>). This can be explained by the GP prior mean function, which is set to zero. When very little data is available, the GP will produce an output close to zero. For the offset prediction, this will give reasonable results as the model output will not vary much from the N9 output. However, when predicting the location rather than the offset, it will give very poor performance unless the touch happens to be close to the origin.</p><p>When using the N9 touch location as an input, it is possible to visualise the offset functions that are learned across the 2D input space. These are shown for subjects 1, 3 and 8 in Figure <ref type="figure" target="#fig_20">7</ref>. The left hand panels show the predicted offsets across the input space, binned into a 10 × 5 grid. The size and direction of the offset for each touch which falls in a given box is shown relative to the center of that box. This binning is done only for ease of visualisation -all calculations were done with the continuous offset functions. The centre and right panels show the continuous horizontal and vertical offset functions. White corresponds to a large positive value (to the right or up for horizontal and vertical offsets respectively) and black a large negative value (to the left and down). We see very small offsets for subject 3, reflecting the fact that the N9 input is already very accurate. For users 1 and 8, it is immediately clear that the offset functions are highly non-linear. The horizontal offset functions have two reasonably flat areas separated by a steep ridge. Recall that the user is operating the system with two thumbs (see Figure <ref type="figure" target="#fig_10">2</ref>) and this ridge represents the point at which the thumb they are using changes. The vertical offsets are more complex, likely reflecting the change in thumb orientation as it reaches up and down.</p><p>To show the relative limitations of the linear model, Figure <ref type="figure" target="#fig_22">8</ref> shows the linear offsets learned for subject 8. Comparing these with the bottom row in Figure <ref type="figure" target="#fig_20">7</ref>, we can clearly see the subtleties lost by the linear model. In particular, the horizontal offset in the linear model changes smoothly from left to right, completely missing the sharp change required towards the centre of the device where the user changes thumbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictive covariance</head><p>The Gaussian Process prediction consists of a Gaussian density. Thus far we have just used the mean of this Gaussian as a point estimate. The covariance of the Gaussian is also useful as it describes the uncertainty in the prediction and could therefore be used at the application level. In Figure <ref type="figure" target="#fig_18">9</ref> we visualise the log determinant of the predictive covariance matrix (higher corresponds to greater uncertainty) for subject 1 when the N9 touch location is used as an input. The highest uncertainty is at the edges, representing the limit of the recorded data. There are also areas of higher uncertainty down the centre and in the centre left and right areas. The centre area can most likely be explained by the user changing thumbs in this area -they will not always change at exactly the same horizontal position and therefore uncertainty in the required offsets is natural. This information about the uncertainty is a further advantage over the polynomial function of <ref type="bibr" target="#b5">[6]</ref>, which only gives a point estimate of the offset. We discuss other uses of predictive uncertainty below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalising Results</head><p>To ensure that the results described above were not specific to a particular device and orientation combination, we car-  ried out a number of additional experiments. First, we had two subjects (1 and 6) each generate 1000 additional touches, with the N9 in a portrait orientation. Users held the device in one hand and touched using the thumb on that hand. The touch targets were generated in the region corresponding to the device's portrait virtual keyboard. While significant conclusions cannot be drawn for a sample of two, the results of this probe are included to contextualise our core results.</p><p>Figure <ref type="figure" target="#fig_24">10</ref> shows our results. Again, we computed the virtual button accuracy as our performance metric ((a) and (c)). We used the raw sensor values of the touches as the input to the GP. For both users, we see that the GP is more accurate than the N9's native algorithm. These GPs were based on the optimal parameters found from the cross validation on the landscape data, showing that the covariance parameters generalise well across orientation. The learning curves (10(b) and (d))</p><p>show that the GP outperforms the N9 after approximately 300 training points are generated. An interesting future research direction is the development of a model which can predict touch location regardless of device orientation. This is potentially possible since even when touching the same location, the sensor readings are likely to be different across the different orientations.</p><p>We also replicated our experiment on a Google Nexus One running Android. Since this device does not expose the raw sensor values in the screen, we were restricted to considering the prediction problem using the device's reported touch location as input. We had two subjects (1 and 3) generate 1000 touches in landscape mode. The protocol was identical to the original experiment. Virtual button accuracies and learning curves for the GP, Android phone and a linear model are shown in Figure <ref type="figure" target="#fig_28">11</ref>. Once again we see that the GP outperforms both competing models for all button sizes. Note also that one of the users shown is subject 3, for whom the GP offered no improved performance on the N9. On the Android phone, we see a large improvement for this user when using the GP. As with the N9, the learning curves are below the device performance for even very small quantities of training data. This can again be attributed to the zero mean of our GP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of user-specificity</head><p>In Figure <ref type="figure" target="#fig_20">7</ref> we saw that the offset functions learned varied greatly across users. To quantify what this means in terms of performance, we carried out an additional experiment where we trained on a mixture of data from all subjects and evaluated the test performance of the resulting model on the data for each individual. Figure <ref type="figure" target="#fig_26">12</ref> shows our results. For each subject, two box plots are shown. The first shows the improvement in accuracy for a 2mm button over the N9 when training on 600 of that subject's touches. The second shows the performance improvement when the model is trained on    600 touches drawn at random from the data for all subjects. This experiment was done using the device location as the GP input. Improvements were also seen for 3mm and 4mm buttons (results not shown).</p><p>In all cases, the performance improvement for the userspecific models is higher than that for the model trained on all subjects. The differences are statistically significant for all subjects (paired t-test, p &lt; 0.05). For subject 3, whose touch behaviour is significantly different from the norm (as evidenced by the earlier results), training on the other subjects' data results in a notable decrease in performance compared to the N9. Thus it is clear that there is significant variation between users, and that training user-specific models is advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND FUTURE WORK Data Collection</head><p>Key to this approach is the collection of training data. There are several ways in which this could be done with minimal effort from the user, described below. Investigating the practical feasibility of these approaches is an area for future research.</p><p>One method for generating the data would be to make use of a calibration game <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b5">[6]</ref> a similar approach has been applied and was very successful. With such a tapping game it would be easily possible to generate the required number of touches in a recreational setting. Note that in <ref type="bibr" target="#b5">[6]</ref>, users provided, on average, many more touches than needed for our approach. An alternative would be to use keyboard entry as a source of training data. Since the approach described in this paper only requires approximately 200 touches (when predicting from the device's reported touch location) to perform  significantly better than the native touch location, writing one long email might already provide enough input data to train. Such an approach could be used in conjunction with the automatic keyboard optimisation proposed by <ref type="bibr" target="#b1">[2]</ref>. In addition, the success of <ref type="bibr" target="#b6">[7]</ref> showed that it is possible to combine typing and a calibration game.</p><p>Another possibility would be to introduce a new unlock method that collects the required data. Most mobile phones can be secured by a four digit PIN needed to unlock the device and this can serve as a data collection point. By arranging the numbers randomly on the screen the user  would need to touch on different places each time he wants to unlock his device. This could generate the needed touches in approximately 50 unlock procedures. Furthermore the approach would be safe against so-called smudge attacks <ref type="bibr" target="#b0">[1]</ref> -reconstruction of frequently used patterns through oily residues on the screen -since the positions of the unlock digits would vary and no common pattern should emerge. The only drawback of this mechanism would be that users would not be able to generate a muscle memory to quickly unlock the device.</p><p>For longitudinal usage an iterative recalibration might be necessary since users might generate certain habits in touching or holding the device. A constant update e.g. using the keyboard approach as discussed above could be a feasible way of adapting the touch accuracy that might actually be capable of coping with temporary impairments such as a sprained thumb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictive variance and uncertainty handover</head><p>The GP regression is naturally probabilistic, providing as its prediction a Gaussian density. Whilst we have only used the mean value in our accuracy calculations, the covariance could be useful from both a design perspective and an interaction perspective. Knowing which areas of the input space are the hardest to model (for example, the vertical strip in the centre when the user is using two thumbs in landscape mode, see Figure <ref type="figure" target="#fig_18">9</ref>) can help in deciding where to place different sized buttons. Being able to model this in a user-specific manner could lead to devices that could optimise layout for accuracy as more data is acquired. From an interaction point of view, including uncertainty in input opens up the possibility of building schemes where autonomy can be transferred from the user to the device as shown in <ref type="bibr" target="#b14">[15]</ref>. For example, uncertainty in touch locations could be used to produce a distribution over which key the user intended to press when operating a keyboard. This distribution could be fed forward into a probabilistic language model allowing for potential improvements in text entry accuracy. In <ref type="bibr" target="#b16">[17]</ref> Schwarz et al presented a model which used uncertainty information to select between several small, closely packed buttons. The authors used a Gaussian centered on the detected touch area to represent the uncertain touch, but it is clear that the touch area is not necessarily a good indication of the intended target. The distributions produced by the GP automatically account for the offset and allow the full covariance structure to be learned for all parts of the space -using this information in application frameworks is a promising research direction which could open up new channels for rich touch interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational complexity</head><p>Key to the success of a method like this is the ability to deploy it in real time on a mobile device. The GP regression approach has two distinct phases: training, involving inverting a square matrix of size 2 × N (where N is the number of training touches) and predicting, which involves creating a covariance matrix and performing a matrix multiplication for each new touch event. The multiplications required for predictions will scale with the number of training examples.</p><p>In our experiments, we found significant improvements for 400 training points (when the sensor data is used as input) and 200 (when the device's reported touch location is used). With matrices of this size, the operations are easily feasible on all modern mobile devices. In addition, our experiments used randomly placed training locations and it is highly likely that smarter position of these (perhaps using an active learning strategy), coupled with a sparse regression algorithm (e.g. <ref type="bibr" target="#b18">[19]</ref>) could reduce the size of training sets further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating additional data</head><p>GP regression provides a principled and flexible framework for incorporating additional data into the prediction task through combinations of covariance functions (see e.g. <ref type="bibr" target="#b11">[12]</ref>). For example, one could include the current output from the accelerometers built into most mobile devices. This could allow the model to automatically adapt to different device orientations or different device usage contexts. For example, the model could adapt to give greater uncertainty in the predictions when the user is walking. In the textentry scenario described above, the increased uncertainty would automatically give more influence to the probabilistic language model. The ability of the GP to probabilistically incorporate streams of data from multiple sensors is a key strength of our approach, and makes the GP a powerful tool for modelling uncertain interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple users on one device</head><p>One drawback of a user-specific model is when multiple users use the same device. One way to overcome this would be to allow the user to switch the adaptive model on or off, depending on who was using the device. Alternatively, it may be possible to automatically infer the particular user from the touch characteristics (particularly if raw sensor data is used) or estimated keyboard error rates (e.g. if the user starts making many mistakes) and switch the predictive model on and off accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>In this paper we demonstrated the feasibility of a user-specific machine learning approach based on GP regression to increase user specific touch accuracy. Using raw sensor input from a capacitive touch screen we were able to improve touch accuracy on average by 23.47% for a 2mm button, 14.20% for a 3mm and 4.7% for a 4mm button. We also demonstrated that the approach using the touch location reported by the device as input resulted in slightly better performance, with accuracy increasing on average by 23.79% for a 2mm, 14.79% for a 3mm and 5.11% for a 4mm button. This has the potential to both improve the user experience and allow for smaller sized buttons.</p><p>We found that our approach worked in both portrait and landscape modes and across two different devices. As shown in <ref type="bibr" target="#b5">[6]</ref>, user-specific offsets exist in all devices studied and our improvements are therefore unlikely to be restricted to the two devices studied here.</p><p>A key finding in this research is that optimal offset functions are highly nonlinear, both horizontally and vertically and vary dramatically over the touch area and over users. For most users, the proposed GP approach considerably outperforms a linear model trained on the same data. User-specific models also outperformed models trained on pooled data, highlighting the diversity in user behaviour.</p><p>This work provides the basis for a wide range of further research. We have a flexible GP regression framework with which we can incorporate data from multiple sensors to intelligently deal with the inherent uncertainties of touch interaction. With this rich predictive model, we can easily build improved systems for important tasks such as text entry. Further, we can use the predictive uncertainty to respond appropriately and automatically to a range of usage contexts, or to negotiate the handover of autonomy between user and device.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sensor outputs (black = low; white = high) for a touch aimed at the white circle. The N9's reported touch location is indicated with a grey circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>over N9Training with 400, button radius 2mm (a) N = 400, 2mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>over N9Training with 400, button radius 3mm (b) N = 400, 3mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>over N9Training with 400, button radius 4mm (c) N = 400, 4mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>over N9Training with 800, button radius 2mm (d) N = 800, 2mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>over N9Training with 800, button radius 3mm (e) N = 800, 3mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>over N9Training with 800, button radius 4mm (f) N = 800, 4mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Improvements in accuracy rate for different virtual button radii and numbers of training examples when the raw sensor values are used as an input. The value on the y-axis is the proportion of touches inside the button for the GP model minus the proportion computed for the N9's native touch location. An increase of say 0.1 corresponds to a 10% decrease in error rate. Boxplots show the distribution of improvement over 10 repetitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of performance between the N9 touch events (N9) and our Gaussian Processes predictions using the raw sensor values as input (GP). Top row: Accuracy for different virtual button sizes (800 training points). Bottom row: Learning curves. Subject 1 represents an average user, subject 3 a user that already performs far above average already with the N9 screen and subject 8 represents a user that doesn't own a touch-screen based phone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Experimental setup: participants held the phone in both hands and used either thumb to touch.</figDesc><graphic coords="5,78.17,62.36,194.43,145.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>over N9 Training with 200, button radius 2mm (a) N = 200, 2mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>over N9 Training with 200, button radius 4mm (c) N = 200, 4mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>over N9Training with 600, button radius 2mm (d) N = 600, 2mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>over N9Training with 600, button radius 3mm (e) N = 600, 3mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>over N9Training with 600, button radius 4mm (f) N = 600, 4mm button radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Improvements in accuracy rate for different virtual button radii and numbers of training examples when the N9's touch location is used as an input. The value on the y-axis is the proportion of touches inside the button for the GP model minus the proportion computed for the N9's reported touch location. An increase of say 0.1 corresponds to a 10% decrease in error rate. Boxplots show the distribution of improvement over 10 repetitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison of performance between the N9 touch events (N9), a simple linear regression model (Linear) (e.g. as in<ref type="bibr" target="#b5">[6]</ref> and our Gaussian Process' predictions using the N9's reported touch location as input (GP). Top row: Accuracy for different virtual button sizes (800 training points). Bottom row: Learning curves. Subject 1 represents an average user, subject 3 a user that already performs far above average already with the N9 screen and subject 8 represents a user that never used a touch screen based phone before.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Log determinant of the predictive covariance matrix for subject 1 when the N9's touch location is used as an input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>(a) Subject 1 binned offsets. (b) Subject 1 horizontal offset function. (c) Subject 1 vertical offset function. (d) Subject 3 binned offsets. (e) Subject 3 horizontal offset function. (f) Subject 3 vertical offset function. (g) Subject 8 binned offsets. (h) Subject 8 horizontal offset function. (i) Subject 8 vertical offset function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Offsets learned for subjects 1, 3 and 8. All panels represent the lower half of the N9 screen when in landscape mode. The left hand plot shows the offsets inferred by the GP for the actual touch data and the right two the continuous horizontal and vertical offset functions learned by the GP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>(a) Subject 8 binned offsets. (b) Subject 8 horizontal offset function. (c) Subject 8 vertical offset function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Offsets learned by linear regression for subject 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Virtual button accuracy and learning curves for portrait touches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Improvements in accuracy rate for 2mm virtual buttons when using a user-specific model (left hand box plot in each case) and a model trained on all users (right hand box plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Performance on Android phone</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Daryl Weir is supported by a PhD studentship from the Scottish Informatics and Computer Science Alliance (SICSA). Nokia provided equipment and funding to support this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smudge attacks on smartphone touch screens</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Aviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mossop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WOOT &apos;10</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards online adaptation and personalization of key-target resizing for mobile devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI &apos;12</title>
		<imprint>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Back-of-device interaction allows creating very small touch devices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;09</title>
		<imprint>
			<biblScope unit="page" from="1923" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Personalized Input: Improving Ten-Finger Touchscreen Typing through Automatic Adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Findlater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Calibration games: making calibration tasks enjoyable by adding motivating game elements</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Flatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Nacke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mandryk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;11</title>
		<imprint>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">000 taps: Analysis and improvement of touch performance in the large</title>
		<author>
			<persName><forename type="first">N</forename><surname>Henze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rukzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>In MobileHCI &apos;11</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Observational and experimental investigation of typing behaviour using virtual keyboards on mobile devices</title>
		<author>
			<persName><forename type="first">N</forename><surname>Henze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rukzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In CHI&apos;12</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The generalized perceived input point model and how to double touch accuracy by extracting fingerprints</title>
		<author>
			<persName><forename type="first">C</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;10</title>
		<imprint>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding touch</title>
		<author>
			<persName><forename type="first">C</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;11</title>
		<imprint>
			<biblScope unit="page" from="2501" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-touch three dimensional touch-sensitive tablet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;85</title>
		<imprint>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Target size study for one-handed thumb use on small touchscreen devices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Parhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A First Course in Machine Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Chapman and Hall / CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anglepose: robust, precise capacitive touch tracking via 3d orientation estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;11</title>
		<imprint>
			<biblScope unit="page" from="2575" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fingercloud: uncertainty and autonomy handover in capacitive sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;10</title>
		<imprint>
			<biblScope unit="page" from="577" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pockettouch: through-fabric capacitive touch input</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Saponas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;11</title>
		<imprint>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A framework for robust and flexible handling of inputs with uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;10</title>
		<imprint>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monte carlo methods for managing interactive state, action and feedback under uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;11</title>
		<imprint>
			<biblScope unit="page">235</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the Relevance Vector Machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Continuous Uncertain Interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
