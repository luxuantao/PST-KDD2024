<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TUNING LANGUAGE MODELS AS TRAINING DATA GENERATORS FOR AUGMENTATION-ENHANCED FEW-SHOT LEARNING</title>
				<funder ref="#_3XpMneM #_jm9zNxJ #_fpAMwRN #_2vKtVbt #_ugEDCC3 #_F6J6y5S #_GDNvAmm #_sJZ6VZS #_nz6hvq5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-06">6 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<email>yumeng5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Michalski</surname></persName>
							<email>martinm6@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
							<email>jiaxinh3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TUNING LANGUAGE MODELS AS TRAINING DATA GENERATORS FOR AUGMENTATION-ENHANCED FEW-SHOT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-06">6 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.03044v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing fewshot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent research has demonstrated the appealing few-shot learning potential of pretrained language models (PLMs) <ref type="bibr" target="#b6">(Brown et al., 2020;</ref><ref type="bibr" target="#b9">Clark et al., 2020;</ref><ref type="bibr" target="#b13">Devlin et al., 2019;</ref><ref type="bibr" target="#b22">He et al., 2021;</ref><ref type="bibr" target="#b42">Liu et al., 2019;</ref><ref type="bibr" target="#b47">Meng et al., 2021;</ref><ref type="bibr">2022b)</ref> on natural language understanding (NLU) tasks <ref type="bibr" target="#b72">(Wang et al., 2019;</ref><ref type="bibr">2018)</ref>: Instead of relying on abundant task-specific annotations, PLMs can effectively leverage a small set of training samples to quickly learn a new task. Such training data efficiency is usually achieved by formulating downstream tasks as prompts <ref type="bibr" target="#b6">(Brown et al., 2020;</ref><ref type="bibr" target="#b17">Gao et al., 2021;</ref><ref type="bibr" target="#b59">Scao &amp; Rush, 2021;</ref><ref type="bibr">Schick &amp; Sch?tze, 2021a;</ref><ref type="bibr">d)</ref> which allow the PLM to adapt its language modeling ability acquired through pretraining to new downstream tasks.</p><p>The success of prompt-based methods has stimulated numerous explorations along the line of effective few-shot learning with PLMs: The training samples converted to natural language prompts can be used to directly fine-tune PLMs <ref type="bibr" target="#b17">(Gao et al., 2021;</ref><ref type="bibr">Schick &amp; Sch?tze, 2021a)</ref> or as in-context demonstrations to facilitate better inference <ref type="bibr" target="#b6">(Brown et al., 2020;</ref><ref type="bibr">Liu et al., 2022b)</ref>. More recent approaches aim to automate the design of prompts by gradient-based searching <ref type="bibr" target="#b65">(Shin et al., 2020)</ref> or parameterizing prompts as continuous learnable embeddings <ref type="bibr" target="#b36">(Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b84">Zhang et al., 2022;</ref><ref type="bibr" target="#b86">Zhong et al., 2021)</ref>. Other studies investigate and address specific issues in promptbased few-shot learning <ref type="bibr">(Liu et al., 2022a;</ref><ref type="bibr" target="#b69">Tam et al., 2021;</ref><ref type="bibr" target="#b85">Zhao et al., 2021)</ref>. While remarkable, the model performance still has a nontrivial gap from fully supervised models trained on massive labeled data. Indeed, training deep models is inherently data demanding-model generalization usually benefits from more training samples <ref type="bibr" target="#b2">(Baum &amp; Haussler, 1988)</ref>.</p><p>In this work, we study few-shot learning with PLMs from a different perspective: Instead of proposing new methods for fine-tuning on few-shot samples, we focus on the generation of quality training data  </p><formula xml:id="formula_0">v G z 6 Q 2 6 d a G V g h b H U J c C a q r 8 n M h w p N Y k C 3 V l c r O a 9 Q v z P 6 6 c Q X n o Z E 0 k K V J D Z o j D l F s R W E Y g 1 Y J I S 4 B N N M J F M 3 2 q R E Z a Y g I 6 t q k N</formula><formula xml:id="formula_1">N j c C u S F V q z m K w q K T G L k l S e J Y Z h C R W e B p f f K 3 8 0 0 s 0 V q b 6 O 0 0 y 7 C c w 1 n I k B Z C T B v U v O 5 G G W E G U A J 0 L U M V R O S g i w m s q r n b G q M t y U G / 4 z f 2 9 d i t s c 7 / p + 5 2 g F V S k 1 Q l 3 Q x 4 4 p U K D T X E 8 q P + O h q n I E 9 Q k F F j b C / y M + g U Y k k J h W Y t y i x m I C x h j z 1 E N C d p + 8 f R K y b e c M u S j 1 L j S x J / U l x M F J N Z O k t h 1 V h f b t 1 4 l v u f 1 c h r t 9 Q u p s 5 x Q i + d F o 1 x x S n m V C x 9 K g 4 L U x B E Q R r p b u T g H A 4</formula><p>J c e j U X w r 9 P + f / J S a s Z t J v h t 7 B x E E 7 j W G S f 2 S b b Z g H r s A N 2 y I 5 Z l w l 2 w + 7 Y P X v w b r 0 f 3 q P 3 8 7 l 1 x p v O b L B X 8 H 7 9 B a Q j m 4 E = &lt; / l a t e x i t &gt; rL w-gen &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x A c T 6 6 e h S 1 i</p><formula xml:id="formula_2">W G f 3 A p J e x v Y 9 8 U 9 o = " &gt; A A A B 6 n i c d V D L S s N A F L 2 p r 1 p f V Z d u B o v g K i Q 2 t e m u 4 M Z l R f u A N p T J d N I O T i Z h Z q K U 0 k 9 w 4 0 I R t 3 6 R O / / G 6 U N Q 0 Q M X D u f c y 7 3 3 h C l n S j v O h 5 V b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H L Z V k k t A m S X g i O y F W l D N B m 5 p p T j u p p D g O O W 2 H t x c z v 3 1 H p W K J u N H j l A Y x H g o W M Y K 1 k a 7 v + 2 6 / W H J s z 3 c q v o M c u 1 L 2 y 9 W y I b W a X / G r y L W d O U q w R K N f f O 8 N E p L F V G j C s V J d 1 0 l 1 M M F S M 8 L p t N D L F E 0 x u c V D 2 j V U 4 J i q Y D I / d Y p O j D J A U S J N C Y 3 m 6 v e J C Y 6 V G s e h 6 Y y x H q n f 3 k z 8 y + t m O v K D C R N p p q k g i 0 V R x p F O 0 O x v N G C S E s 3 H h m A i m b k V k R G W m G i T T s G E 8 P U p + p + 0 z m z 3 3 P a u v F L d W 8 a R h y M 4 h l N w o Q p 1 u I Q G N I H A E B 7 g C Z 4 t b j 1 a L 9 b r o j V n L W c O 4 Q e s t 0 + Y 8 I 3 + &lt; / l a t e x i t &gt; w 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T J S 9 t a 5 i + J 8  </p><formula xml:id="formula_3">9 d 9 O Z D 3 t 2 Q 9 Z N c E Q = " &gt; A A A B 6 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w F S Z p s O m u 4 M Z l R f u A N p T J d N I O n U z C z E Q p o Z / g x o U i b v 0 i d /</formula><formula xml:id="formula_4">x R q V g i b v U 0 p U G M R 4 J F j G B t p J v 7 g T s o V 5 D t V m s I O R D Z V c e p + b 4 h 9 b r v V j 3 o 2 G i B C l i h O S i / 9 4 c J y W I q N O F Y q Z 6 D U h 3 k W G p G O J 2 V + p m i K S Y T P K I 9 Q w W O q Q r y x</formula><formula xml:id="formula_5">F + i o X 0 I = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h Z 0 Q 8 r g F v H i M a B 6 Q L G F 2 M p s M m Z 1 d Z m a V s O Q T v H h Q x K t f 5 M 2 / c T a J o K I F D U V V N 9 1 d f i y 4 N q 7 7 4 a y t b 2 x u b e d 2 8 r t 7 + w e H h a P j j o 4 S R V m b R i J S P Z 9 o J r h k b c O N Y L 1 Y M R L 6 g n X 9 6 W X m d + + Y 0 j y S t 2 Y W M y 8 k Y 8 k D T o m x 0 s 3 9 U A 4 L R b f k u i 7 G G G U E 1 6 q u J Y 1 G v Y z r C G e W R R F W a A 0 L 7 4 N R R J O Q S U M F 0 b q P 3 d h 4 K V G G U 8 H m + U G i W U z o l I x Z 3 1 J J Q q a 9 d H H q H J 1 b Z Y S C S N m S B i 3 U 7 x M p C b W e h b 7 t D I m Z 6 N 9 e J v 7 l 9 R M T 1 L 2 U y z g x T N L l o i A R y E Q o + x u N u G L U i J k l h C p u b 0 V 0 Q h S h x q a T t y F 8 f Y r + J 5 1 y C V d L l e t K s V l Z x Z G D U z i D C 8 B Q g y Z c Q Q v a Q G E M D / A E</formula><p>z 4 5 w H p 0 X 5 3 X Z u u a s Z k 7 g B 5 y 3 T 7 J M j g 0 = &lt; / l a t e x i t &gt; w n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 L u 7 0 n k E T q y Q 5      <ref type="bibr">et al., 2019)</ref>), fine-grained control (e.g., specific words or phrases <ref type="bibr" target="#b7">(Chan et al., 2021)</ref>) or both <ref type="bibr" target="#b28">(Khalifa et al., 2021)</ref>; at inference time, control over desired attributes can also be enforced without updating the PLM parameters <ref type="bibr" target="#b12">(Dathathri et al., 2020;</ref><ref type="bibr" target="#b31">Krause et al., 2021;</ref><ref type="bibr" target="#b32">Kumar et al., 2021;</ref><ref type="bibr">Liu et al., 2021a;</ref><ref type="bibr" target="#b54">Pascual et al., 2021;</ref><ref type="bibr" target="#b80">Yang &amp; Klein, 2021)</ref>. More specifically related to the idea of generating training data with language models, early methods in topic classification use bag-of-words or LSTM-based language models <ref type="bibr" target="#b45">(Meng et al., 2018;</ref><ref type="bibr">2019)</ref> to generate class-conditioned texts as training data.</p><formula xml:id="formula_6">k P T 6 A A t p T C s j n o = " &gt; A A A C C X i c d V A 9 S w N B E N 3 z M 8 a v q K X N Y h B s D H c h x K Q L 2 F h Y K J g Y y I U w t 9 n o 4 t 7 e s T s n h u N a G / + K j Y U i t v 4 D O / + N e z G C i j 4 Y e L w 3 w 8 y 8 I J b C o O u + O z O z c / M L i 4 W l 4 v L K 6 t p 6 a W O z Y 6 J E M 9 5 m k Y x 0 N w D D p V C 8 j Q I l 7 8 a a Q x h I f h 5 c H e b + + T X X R k T q D M c x 7 4 d w o c R I M E A r D U p 0 3 1 c Q S P B D w E s G M j 3 O B q m P / A b T o T A s y w a l s l t x L e p 1 m h O v 4 X q W N J u N a r V J v Y n l u m U y x c m g 9 O Y P I 5 a E X C G T Y E z P c 2 P s p 6 B R M M m z o p 8 Y H g O 7 g g v e s 1 R B y E 0 / n X y S 0 V 2 r D O k o 0 r Y U 0 o n 6 f S K F 0 J h x G N j O / G D z 2 8 v F v 7 x e g q N G P x U q T p A r 9 r l o l E i K E c 1 j o U O h O U M 5 t g S Y F v Z W y i 5 B A 0 M b X t G G 8 P U p / Z 9 0 q h W v X q m d 1 s q t 2 j S O A t k m O</formula><formula xml:id="formula_7">= " &gt; A A A C A 3 i c b V A 9 S w N B E N 3 z M 8 a v q J 0 2 h 0 G w M d x J U M u A j Y V F B P M B S T j 2 N n P J k r 2 9 Y 3 d O D c e B j X / F x k I R W / + E n f / G z U e h i Q 8 G H u / N M D P P j w X X 6 D j f 1 s L i 0 v L K a m 4 t v 7 6 x u b V d 2 N m t 6 y h R D G o s E p F q + l S D 4 B J q y F F A M 1 Z A Q 1 9 A w x 9 c j v z G H S j N I 3 m L w x g 6 I e 1 J H n B G 0 U h e Y b 8 d U u w z K t L r z E v b C A + Y 3 p / 0 Q G a Z V y g 6 J W c M e 5 6 4 U 1 I k U 1 S 9 w l e 7 G 7 E k B I l M U K 1 b r h N j J 6 U K O R O Q 5 d u J h p i y A e 1 B y 1 B J Q 9 C d d P x D Z h 8 Z p W s H k T I l 0 R 6 r v y d S G m o 9 D H 3 T O b p Y z 3 o j 8 T + v l W B w 0 U m 5 j B M E y S a L g k T Y G N m j Q O w u V 8 B Q D A 2 h T H F z q 8 3 6 V F G G J r a 8 C c G d f X m e 1 E 9 L 7 l m p f F M u V s r T O H L k g B y S Y + K S c 1 I h V 6 R K a o S R R / J M X</formula><formula xml:id="formula_8">k o U g x q L R K S a A d U g u I Q a c h T Q j B X Q M B D Q C I a X Y 7 9 x D 0 r z S N 7 i K A Y / p H 3 J e 5 x R N F L H P m y H F A e M i v Q 6 6 6 R t h A d M + y C z 7 K 7 U s Q t u 0 Z 3 A W S T e j B T I D N W O / d X u R i w J Q S I T V O u W 5 8 b o p 1 Q h Z w K y f D v R E F M 2 p H 1 o G S p p C N p P J z 9 k z o l R u k 4 v U q Y k O h P 1 9 0 R K Q 6 1 H Y W A 6 x x f r e W 8 s / u e 1 E u x d + C m X c Y I g 2 X R R L x E O R s 4 4 E K f L F T A U I 0 M o U 9 z c 6 r A B V Z S h i S 1 v Q v D m X 1 4 k 9 V L R O y u W b 8 q F S n k W R</formula><formula xml:id="formula_9">G 0 U h + 8 a A T U h w w K t L r z E 8 7 C A + Y 9 k F m 2 Z 3 r F 0 t O 2 Z n A X i T u j J T I D D W / + N X p R i w J Q S I T V O u 2 6 8 T o p V Q h Z w K y Q i f R E F M 2 p H 1 o G y p p C N p L J z 9 k 9 r F R u n Y v U q Y k 2 h P 1 9 0 R K Q 6 1 H Y W A 6 x x f r e W 8 s / u e 1 E + x d e C m X c Y I g 2 X R R L x E 2 R v Y 4 E L v L F T A U I 0 M o U 9 z c a r M B V Z S h i a 1 g Q n D n X 1 4 k j d O y e 1 a u 3 F R K 1 c o s j j w 5 J E f k h L j k n F T J F a m R O m H k k T y T V / J m P V k v 1 r v 1 M W 3 N W b O Z f f I H 1 u c P A z y Y X Q = = &lt;</formula><formula xml:id="formula_10">X V t f x 6 Y W N z a 3 u n u L v X 0 G G s G N R Z K E L V 8 q k G w S X U k a O A V q S A B r 6 A p j + 6 z P z m P S j N Q 3 m L 4 w i 8 g A 4 k 7 3 N G 0 U j d 4 k E n o D h k V C T X a T f p I D x g M g C Z p n f G L D l l Z w J 7 k b g z U i I z 1 L r F r 0 4 v Z H E A E p m g W r d d J 0 I v o Q o 5 E 5 A W O r G G i L I R H U D b U E k D 0 F 4 y + S G 1 j 4 3 S s / u h M i X R n q i / J x I a a D 0 O f N O Z X a z n v U z 8 z 2 v H 2 L / w E i 6 j G E G y 6 a J + L G w M 7 S w Q u 8 c V M B R j Q y h T 3 N x q s y F V l K G J r W B C c O d f X i S N 0 7 J 7 V q 7 c V E r V y i y O P D k k R + S E u O S c V M k V q Z E 6 Y e S R</formula><p>Recently, a few studies explore fine-tuning autoregressive PLMs <ref type="bibr" target="#b0">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b81">Yang et al., 2020)</ref> with the standard language modeling objective on the training set or using label-specific prompts <ref type="bibr">(Meng et al., 2022a;</ref><ref type="bibr">Schick &amp; Sch?tze, 2021c;</ref><ref type="bibr" target="#b74">Wang et al., 2021;</ref><ref type="bibr" target="#b82">Ye et al., 2022)</ref> to steer text generation towards the desired label.</p><p>Meta-Learning for Sample Weighting. The idea of weighting training samples in the loss calculation originates from the class imbalance <ref type="bibr" target="#b73">(Wang et al., 2017)</ref> and noisy label <ref type="bibr" target="#b23">(Hendrycks et al., 2018)</ref> learning scenarios-By assigning higher weights to the samples from minority classes or lower weights to the noisy samples, the learning process is less impacted by the imbalance/label noise issues. Meta-learning <ref type="bibr" target="#b1">(Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b15">Finn et al., 2017;</ref><ref type="bibr" target="#b16">Franceschi et al., 2018;</ref><ref type="bibr" target="#b78">Wu et al., 2018)</ref> is one way to automatically learn the weight for each sample. Specifically, a meta objective, usually defined as the loss on a clean unbiased validation set <ref type="bibr" target="#b58">(Ren et al., 2018;</ref><ref type="bibr" target="#b66">Shu et al., 2019)</ref>, can be used to learn the sample weights which become hyperparameters that control the optimization of model parameters. Our work has a different motivation and formulation of the meta objective for token-wise weighted training: Not all tokens in a training sample are equally label-discriminative. We thus design a meta objective to emphasize distinction across different labels (instead of using the validation loss as the meta objective) for learning the token weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PRELIMINARIES</head><p>Overview. We consider the strict few-shot learning setting <ref type="bibr" target="#b55">(Perez et al., 2021)</ref> Text Generation with Autoregressive PLMs. In standard fine-tuning for text generation, an autoregressive PLM G ? is trained via the maximum likelihood generation loss of each token in a sequence x conditioned on previous tokens:</p><formula xml:id="formula_11">min ? - 1 n n j=1 log p ? (x j |x &lt;j ), p ? (x j |x &lt;j ) = exp(e j h j ) |V | j =1 exp(e j h j )</formula><p>.</p><p>where the token generation probability p ? (?) is usually parameterized using token embeddings e and hidden states h of a Transformer <ref type="bibr" target="#b70">(Vaswani et al., 2017)</ref>   <ref type="formula" target="#formula_13">2</ref>) the generation models for different labels can share the same backbone Transformer parameters with only the prefix vectors being different, significantly reducing the memory requirement for multi-class classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LABEL-DISCRIMINATIVE TEXT GENERATOR TUNING WITH META WEIGHTS</head><p>Motivation. To model the conditional text generation probability p(x|y l ) on different labels, a straightforward way is to parameterize a generation model G ?p l for each label y l via a set of prefix vectors ? p l so that p(x|y l ) = p ?p l (x), and then tune ? p l on the training samples x with label y l : However, such an approach only optimizes the generative likelihood p(x|y l ) without accounting for label discriminativeness p(y l |x) which is essential for generating unambiguous training samples to benefit the final classification task. Indeed, we find that optimizing p(x|y l ) separately for each label does not necessarily make the generators aware of the distinction over different labels. As shown in Fig. <ref type="figure" target="#fig_10">2</ref>, L disc (defined in Eq. ( <ref type="formula" target="#formula_13">2</ref>)) can even increase during training-It is possible that the dominating patterns in the training samples are label-indiscriminate (e.g., a movie review dataset may frequently mention "the movie"), making the generators of different labels eventually converge to similar distributions, especially when there are limited training samples per label.</p><formula xml:id="formula_12">min ?p l L gen , L gen (? p l ) = - 1 n n j=1 log p ?p l (x j |x &lt;j ).<label>(1</label></formula><p>To promote the generation of label-discriminative texts, we hope to generate each token x j so that the probability of the so far generated text sequence x ?j is maximized towards label y l via L disc :</p><formula xml:id="formula_13">L disc = - 1 n n j=1 log p ?p (y l |x ?j ) p ?p (y l |x ?j ) = p(x j |y l , x &lt;j )p(y l ) L l =1 p(x j |y l , x &lt;j )p(y l ) = p ?p l (x j |x &lt;j ) L l =1 p ?p l (x j |x &lt;j )<label>(2)</label></formula><p>where ? p = {? p l }| L l=1 , and uniform label prior (i.e., p(y l ) = 1/L) is assumed; for non-uniform prior, the result will be up to some scaling.</p><p>Although one can directly combine L disc with L gen to train G ?p to enforce distinction across different labels, doing so will result in two undesirable consequences: (1) A hyperparameter needs to be  <ref type="table"></ref>and<ref type="table" target="#tab_1">? (0)  for t ? [0, 1,</ref> </p><formula xml:id="formula_14">. . . , T -1] do B ? Sample a minibatch from D train ?(t) p ? (t) ? Take one gradient step to descend L w-gen ? (t) p ; ? (t) on B ? (t+1) ? Take one gradient step to descend L disc ?(t) p ? (t) on B ? (t+1) p ? Take one gradient step to descend L w-gen ? (t) p ; ? (t+1) on B end return ? p = ? (T ) p</formula><p>introduced to balance the weights of the two losses, whose optimal value is likely to vary by task; and (2) the generation-irrelevant loss L disc will unavoidably interfere the language modeling process, making the resulting model prone to generating less fluent and coherent texts.</p><p>Weighted Maximum Likelihood Generator Tuning. To preserve the generative learning of G ?p while emphasizing label-discriminative tokens, we assume each token is associated with a weight in the maximum likelihood loss. Intuitively, when our goal is to generate distinctive texts across different labels as training samples, not all tokens should contribute equally to generator training. For example, for sentiment classification tasks, one would expect "good/bad" to be more label-discriminative than "the movie", and the former should be paid more attention to during training. It is thus natural to revise L gen from Eq. ( <ref type="formula" target="#formula_12">1</ref>) to L w-gen as in Eq. ( <ref type="formula">3</ref>) by assuming a weight w j is given for each token.</p><formula xml:id="formula_15">min ?p l L w-gen , L w-gen (? p l ; w) = - n j=1 w j log p ?p l (x j |x &lt;j ), n j=1 w j = 1.</formula><p>(3)</p><p>Note that in L w-gen , w is assumed to be the hyperparameter under which ? p l is optimized. While it is possible to manually design weighting rules for setting w to promote label-discriminative learning, they will likely necessitate task-specific knowledge and nontrivial tuning. To facilitate the automatic learning of these weights w, we propose to parameterize them as learnable hyperparameters using the idea of meta-learning.</p><p>Meta Objective. The general idea of meta-learning is to formulate a meta objective to enable automatic learning of hyperparameters. When w is a learnable variable, the optimal ? * p l (w) = arg min ?p l L w-gen (? p l ; w) will be a function of w. Since our goal is to encourage label discriminativeness, we require the optimal ? * p l (w) obtained under w to minimize L disc as the meta objective:</p><formula xml:id="formula_16">min w L disc , L disc (? * p (w)) = - 1 n n j=1 log p ? * p (w) (y l |x ?j ).</formula><p>Training Setup and Algorithm. The weight w needs to characterize the discriminativeness of each token and thus we parameterize it via a weighting network ? taking the value of L disc as input:</p><formula xml:id="formula_17">w j (?) = exp ?(L j disc ) n j =1 exp ?(L j disc ) , L j disc = -log p ?p (y l |x ?j ).</formula><p>Following <ref type="bibr" target="#b66">Shu et al. (2019)</ref>, we instantiate ? to be a feedforward network with only one 100dimension hidden layer. Instead of solving the optimal ? * and ? * p via nested optimization loops, we use an online optimization strategy <ref type="bibr" target="#b66">(Shu et al., 2019)</ref> for training efficiency. It also guarantees convergence to the critical points of both L w-gen and L disc under mild conditions. Similar to the observations in Li &amp; Liang (2021), we find it beneficial to initialize the prefix vectors ? p using task-descriptive tokens. The initialization prompts can be found in Appendix B. The overall training procedure is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLASSIFIER FINE-TUNING</head><p>With the trained generator G ?p , we can synthesize novel training samples D gen that augment D train for fine-tuning a PLM C ? for classification. The major challenge to effectively leverage D gen is that the label noise (i.e., some generated samples may not accurately pertain to the corresponding label) may deteriorate model performance if standard supervised learning is directly used. We propose a simple noise-robust training procedure to improve the generalization and stability of training: First fine-tune C ? on D train with standard supervised training, and then continue fine-tuning it on D gen by applying label smoothing <ref type="bibr" target="#b68">(Szegedy et al., 2016)</ref> and temporal ensembling <ref type="bibr" target="#b34">(Laine &amp; Aila, 2017)</ref> as regularization. Specifically, given a training sample ( x, ?) ? D gen , we minimize the following classification loss:</p><formula xml:id="formula_18">min ? L class , L class (?) = - L l=1 q l log(p ? ( x) l ) -? L l=1 zl log p ? ( x) l zl ,<label>(4)</label></formula><p>where q l = 1(l = ?)(1 -) + /L and is the label smoothing weight; p ? ( x) is the model prediction on x; ? is a regularization weight for temporal ensembling; and z is the accumulated moving-average model predictions. We also use the ensembled prediction z to filter out noisy synthesized samples:</p><p>We only include those samples for training where z strongly agrees with the label ? (i.e., z? &gt; ? where ? &gt; 0 is a threshold parameter). In Eq. ( <ref type="formula" target="#formula_18">4</ref>), the first classification term is the cross-entropy loss with smoothed labels; the second regularization term corresponds to temporal ensembling, which requires the current model prediction to be close to its past accumulated predictions. This not only neutralizes the fluctuation in model predictions for better training stability when label noise is present <ref type="bibr" target="#b52">(Nguyen et al., 2020)</ref> but also helps prevent catastrophic forgetting <ref type="bibr" target="#b30">(Kirkpatrick et al., 2017)</ref> of the information learned previously from the few-shot training set D train . Please refer to Appendix B for details about the temporal ensembling implementation. The overall procedure of classifier fine-tuning is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>Downstream Tasks and Metrics. We conduct evaluation on all tasks of the GLUE benchmark <ref type="bibr" target="#b71">(Wang et al., 2018)</ref>  for training C ? . For simplicity, we use the most basic manual prompt version of LM-BFF <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>. The only exception is CoLA for which we use the standard fine-tuning since the input data might be out of the distribution of C ? <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>. The hyperparameter tuning is performed on D dev . More details are in Appendix B.</p><p>Compared Methods. No-augmentation baselines include zero-shot prompting, standard finetuning, in-context learning, and the following strong few-shot learning methods: Four versions of LM-BFF <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>, P-Tuning <ref type="bibr">(Liu et al., 2021b)</ref> and DART <ref type="bibr" target="#b84">(Zhang et al., 2022)</ref>. We also compare FewGen with data augmentation methods for few-shot learning: Using back translation systems to generate paraphrases (UDA-style <ref type="bibr" target="#b79">(Xie et al., 2020)</ref> augmentation), GPT3Mix <ref type="bibr" target="#b83">(Yoo et al., 2021)</ref> and standard fine-tuning of generator on the few-shot samples with prompts. All augmentation methods use LM-BFF (Man.) for fine-tuning the RoBERTa Large classifier. More details about data augmentation baselines can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MAIN RESULTS</head><p>We present the results of FewGen and baselines in Table <ref type="table" target="#tab_1">1</ref>. FewGen achieves overall better performance across the GLUE tasks, on average 5+ points higher than the previous best few-shot method without augmentation, and 3+ points better than GPT3Mix<ref type="foot" target="#foot_1">2</ref> (Yoo et al., 2021) which uses a 100 times larger generator model (175B) than FewGen. The promising results confirm the effectiveness of our proposed FewGen method in generating quality training data and leveraging them in combination with the few-shot training set for fine-tuning the classification model.</p><p>Comparison with Back Translation. Using back translation to paraphrase the few-shot samples does not improve the results, even with prompt-based fine-tuning to train the classifier -this is probably because it does not produce samples that are sufficiently different from the few-shot training set. The success of UDA <ref type="bibr" target="#b79">(Xie et al., 2020)</ref> is grounded in the augmentations from abundant unlabeled data that improve the classifier generalization. However, under the strict few-shot learning setup, there is no access to additional task-specific unlabeled data <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>  paraphrases of the few-shot samples, as the generator is trained via prefix-tuning to preserve the PLM's pretraining knowledge, based on which novel training samples can be synthesized.</p><p>Comparison with GPT3Mix. The gigantic size of GPT3 makes it challenging for tuning on fewshot samples. Therefore, GPT3Mix (Yoo et al., 2021) uses few-shot samples as demonstrations for creating the augmentations. Such an approach suffers from two limitations: (1) Without any parameter update to the PLM, its learning ability is not fully leveraged to adapt to the few-shot training set. (2) The PLM can only use a small subset of the few-shot samples at a time for creating each augmentation, as the number of demonstrations received by the model is bounded by its maximum input sequence length. This makes the quality of the created augmentations more sensitive to the randomly drawn training samples. Our FewGen method, on the other hand, can use the entire few-shot set for tuning the PLM and achieves overall even better classification results with a much smaller PLM (&lt; 1% the size of the GPT3 model) which can be deployed much more easily in practice. We further analyze the effectiveness of each important component in FewGen. Specifically, we compare FewGen with the following ablations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDIES</head><p>(1) Using the standard L gen in Eq. (1) instead of our proposed L w-gen in Eq. (3) for generator tuning (w. L gen ); (2) using the directly combined L gen and L disc for generator tuning (w.</p><p>L gen + L disc ); (3) without applying label smoothing in Eq. (4) (-label smooth); (4) without applying temporal ensembling in Eq. ( <ref type="formula" target="#formula_18">4</ref>) (-temporal ensemble). As shown in Table <ref type="table" target="#tab_2">2</ref>, (1) &amp; (2) using the standard maximum likelihood loss or the combination of generation and discrimination losses to tune the generator both yield lower-quality training data and lead to degraded classification performance; (3) &amp; ( <ref type="formula" target="#formula_18">4</ref>) not applying regularization techniques for fine-tuning the classifier is more prone to label noise in the generated samples. To study the impact of the amount of generated training samples on the model performance, we plot the MNLI-m accuracy (mean and standard deviation) with different sizes of D gen in Fig. <ref type="figure">3</ref>. Both the average model performance and stability improve with more generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSES OF LOSS FUNCTIONS FOR GENERATOR TUNING</head><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, the choice of generator loss has a significant impact on the synthesized data quality and thus the final model performance. We conduct further analyses to compare the training processes of the generator under the following three loss functions and the resulting generated samples: (1) L gen which is the standard language modeling loss; (2) L gen + L disc which directly adds the discriminative loss to generator training; and (3) L w-gen which is our meta-weighted objective. Fig. <ref type="figure" target="#fig_14">4</ref> shows the discriminative loss L disc and the standard language modeling loss on the held-out development set throughout training. Although using L gen + L disc helps reduce the discriminative loss, it comes at the cost of hindering language modeling-the generator loss on the development set is high. Using our meta-weighted objective L w-gen for tuning the generator not only encourages discriminativeness but also mitigates overfitting, yielding the lowest validation set loss. This is The NIMD found that hair samples from 1,137 Taiji residents had mercury in their hair. (neutral) neutral The NIMD found that there was no evidence of a link between exposure to high levels of mercury and thyroid cancer. (neutral) contradiction There was no evidence of mercury in hair samples from Taiji. (neutral)</p><formula xml:id="formula_19">Lgen + Ldisc entailment</formula><p>The number of hairs in a sample is equal to the number of people who lived in Taiji. (neutral) neutral</p><p>The results showed that there was no significant difference in levels of mercury. (neutral) contradiction Hair samples from 1,137 Taiji residents were not tested. (contradiction)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lw-gen entailment</head><p>The NIMD tested hair samples from 1,137 residents of Taiji. (entailment) neutral There was no significant difference in levels between people who lived near a nickel mine and those living far away. (neutral) contradiction The NIMD did not test any of the hair samples. (contradiction) probably because the model receives contrastive information from other labels which facilitates more accurate modeling of the texts with the target label. We also showcase concrete generation results for the three labels of MNLI by models trained with the three different loss functions in Table <ref type="table" target="#tab_3">3</ref>. The model trained with L gen produces fluent and coherent sentences, but the generated sentences do not accurately pertain to the desired label (i.e., the "entailment" and "contradiction" generation results are in fact neutral with respect to the given sentence), lacking label discriminativeness. When L gen + L disc is used, the generated samples of different labels are more distinctive, but also become less natural and coherent due to the model's language modeling ability being hampered. The generator tuned with L w-gen produces both coherent and label-discriminative samples which can serve as quality training data. We also visualize the token weights w automatically learned and used in L w-gen in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSIONS AND CONCLUSIONS</head><p>Ethical Considerations. Despite the impressive text generation and representation power of PLMs, they can also come with the risk <ref type="bibr" target="#b4">(Bender et al., 2021;</ref><ref type="bibr" target="#b3">Bender &amp; Koller, 2020;</ref><ref type="bibr" target="#b6">Brown et al., 2020)</ref> of generating disinformation <ref type="bibr" target="#b53">(Pagnoni et al., 2021)</ref> or exacerbating biases <ref type="bibr" target="#b56">(Prabhumoye et al., 2018)</ref>. Instead of improving upon PLM architectures or generation techniques, our work focuses on using existing PLMs to create training data for NLU tasks. Therefore, our method can be combined with any bias reduction and correction strategies <ref type="bibr" target="#b18">(Gehman et al., 2020;</ref><ref type="bibr" target="#b44">Ma et al., 2020)</ref> in practice to reduce the adverse effects of PLMs.</p><p>Limitations. Compared to few-shot learning methods that directly train classification models on the small training set, FewGen requires tuning a generator PLM and using it to synthesize novel training samples, resulting in higher computation costs and longer running time. Still, we believe that our method may bring more good than harm-when the small training data size becomes the performance bottleneck for NLU tasks, a simple yet costly solution is to obtain more human annotations. Our method may replace or reduce the human efforts in such training data creation processes.</p><p>Conclusions. In this work, we propose FewGen, which leverages few-shot training samples to tune a generator PLM for synthesizing novel training data. The generated data can be then used in combination with few-shot samples to fine-tune a classification model for better generalization.</p><p>To emphasize label-discriminative information during generator tuning, we propose a weighted maximum likelihood objective where the token weights are automatically learned via a discriminative meta objective. Since the generated samples may contain label noise, we propose a simple training procedure that first trains classifiers on the few-shot training set and then on the generated set by applying temporal ensembling for noise-robustness. Across seven classification tasks from the GLUE benchmark, FewGen significantly outperforms existing approaches under the same few-shot learning setting. The effectiveness of each important component in FewGen is validated via ablation studies. Future work directions may include: Using larger PLMs as the generator and the classifier, jointly training both models with each other's high-confident predictions, and developing systematic metrics for evaluating the quality of generated training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A GLUE TASKS</head><p>We provide the details of the seven classification tasks included in the GLUE benchmark.</p><p>MNLI: Multi-genre Natural Language Inference <ref type="bibr" target="#b77">(Williams et al., 2018)</ref> requires predicting whether a given premise sentence entails, contradicts or neutral with respect to a given hypothesis sentence.</p><p>QQP: Quora Question Pairs <ref type="bibr" target="#b64">(Shankar et al., 2017)</ref> requires judging whether a pair of questions asked are semantically equivalent. QNLI: Question Natural Language Inference requires predicting whether a given sentence contains the answer to a given question sentence.</p><p>SST-2: Stanford Sentiment Treebank <ref type="bibr" target="#b67">(Socher et al., 2013)</ref> requires determining if a movie review has positive or negative sentiment.</p><p>CoLA: Corpus of Linguistic Acceptability <ref type="bibr" target="#b75">(Warstadt et al., 2019)</ref> requires determining whether a given sentence is linguistically acceptable or not. RTE: Recognizing Textual Entailment <ref type="bibr" target="#b5">(Bentivogli et al., 2009;</ref><ref type="bibr" target="#b11">Dagan et al., 2005;</ref><ref type="bibr" target="#b19">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b20">Haim et al., 2006)</ref> requires predicting whether a given premise sentence entails a given hypothesis sentence or not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS</head><p>Details of Initialization Prompts Used for Generator Tuning on Different Tasks. For generator tuning, we find it beneficial to initialize the prefix vectors with task-descriptive prompts, and the prefix lengths (i.e., number of trained prefix token positions) are equal to the number of tokens in the prompts. We present details about the prompts used for initializing the prefix vectors for different tasks in Table <ref type="table" target="#tab_4">4</ref>. For sequence-pair tasks, an additional infix prompt is used between the two sequences, and we also tune the embeddings of the infix (i.e., prompt-tuning <ref type="bibr" target="#b36">(Lester et al., 2021)</ref>) for generator training.</p><p>Details of Generator Tuning. In Algorithm 1, we use SGD with 2e -2 as the learning rate for the first gradient update (i.e., optimizing ?(t)</p><p>p ? (t) ); we use SGD with 1e -2 as the learning rate for the second gradient update (i.e., optimizing ? (t+1) ); we use Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref> with 5e -3 as the learning rate for the third gradient update (i.e., optimizing ? (t+1) p</p><p>). We set batch size to be 2 and training epoch to be 20. For all tasks, we generate 5, 000 samples per label.</p><p>For SST-2, we use one of the following tokens to start generation: "a", "one", "the", "this", "that", "i", "you", "it", "what". For CoLA, we use a random stop word to start generation.</p><p>We apply repetition penalty <ref type="bibr" target="#b27">(Keskar et al., 2019)</ref> to the logits of tokens that have already appeared in the sequence. Overall, the token probability distribution is post-processed as follows before conducting sampling:</p><formula xml:id="formula_20">p ? (x i |x &lt;i ) = exp(e i h i /?) |V | j=1 exp(e j h i /?) , ? = ? ? x i ? x &lt;i ? else ,</formula><p>where ? is the temperature hyperparameter, and ? is the repetition penalty hyperparameter. For labels that Preprint favor token repetitions between the first and the second sequences (e.g., paraphrase or entailment), we set ? to be a smaller value (e.g., 1.0), and vice versa.</p><p>The hyperparameter values for training data generation on all tasks can be found in Table <ref type="table" target="#tab_5">5</ref>.</p><p>Hyperparameters for Fine-Tuning Classifier PLMs. For fine-tuning on the few-shot training samples D train , we search among the following hyperparameter ranges based on development set (D dev ) model performance and pick the best performing model for futher fine-tuning on synthesized data: Learning rate in [1e -5, 2e -5] and batch size in <ref type="bibr">[4,</ref><ref type="bibr">8]</ref>. The number of training steps is fixed to be 1000. We also find it beneficial to apply label smoothing (smoothing weight set to 0.15) for fine-tuning on the few-shot training set.</p><p>For fine-tuning on the synthesized training samples D gen , we use the following hyperparameters: 5e -6 as the learning rate; 16 as the batch size; label smoothing weight = 0.15 ; temporal ensemble momentum ? = 0.9; temporal ensemble loss weight ? = 20; training steps T = 6, 000.</p><p>Details of Temporal Ensembling for Fine-Tuning Classifier PLMs on Synthetic Data. We update ensembled predictions z as follows where p ? is the current model prediction, ? is the momentum parameter, ? is the accumulated model prediction before bias correction, z is the accumulated model prediction after bias correction, and t is the number of updates z has received:</p><formula xml:id="formula_21">? ? ? ? + (1 -?)p ? , z ? ?/(1 -? t ).</formula><p>The accumulated model prediction ? has a zero initialization; the division (1? t ) is for bias correction <ref type="bibr" target="#b34">(Laine &amp; Aila, 2017)</ref>. After each update of ?, it will be compared to a threshold value ?; each synthesized sample ( x, ?) will be included in training only if z? &gt; ?.</p><p>We update the ensembled predictions z on all samples in D gen every 200 steps, and set the threshold value for sample filtering ? = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Environment.</head><p>The experiments are conducted on NVIDIA A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DATA AUGMENTATION BASELINE DETAILS</head><p>Details About Back Translation. We use two trained Marian <ref type="bibr" target="#b26">(Junczys-Dowmunt et al., 2018)</ref> models to perform data augmentation via back translation. We translate our labeled examples from English to French, and then back to English. As in UDA <ref type="bibr" target="#b79">(Xie et al., 2020)</ref>, we employ random sampling with a tunable temperature to generate a diverse set of derivative examples. We generate 32 examples from each few-shot training example and let the synthesized samples share the same label with the original few-shot training sample. After combining with the original examples, we fine-tune the classifier and observe performance.</p><p>Details About GPT3Mix <ref type="bibr" target="#b83">(Yoo et al., 2021)</ref>. We use the 175B GPT3 model for generating the augmentations. For creating each augmentation, we randomly sample k = 4 (the optimal setting according to GPT3Mix) examples from the few-shot training set as demonstrations. The prompts follow the suggested format proposed in the original paper <ref type="bibr" target="#b83">(Yoo et al., 2021)</ref> and are shown in Table <ref type="table">6</ref>. We create 5, 000 augmented samples per label to make the resulting training set size equal to that of FewGen. After obtaining the augmented examples and their pseudo labels (the probability predictions over all labels by GPT3), we use them along with the real few-shot samples for fine-tuning the classifier, following the setting in GPT3Mix <ref type="bibr" target="#b83">(Yoo et al., 2021)</ref>.</p><p>Details About Standard Generator Fine-Tuning. We fine-tune the same 1.6B CTRL <ref type="bibr" target="#b27">(Keskar et al., 2019)</ref> model as used in FewGen with the standard maximum likelihood objective. Different from previous studies <ref type="bibr" target="#b0">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b33">Kumar et al., 2020)</ref> that prepend categorical labels to the training samples, we enhance the generator fine-tuning with label-descriptive prompts (shown in Table <ref type="table" target="#tab_4">4</ref>) used in FewGen. We create 5, 000 augmented samples per label to make the resulting training set size equal to that of FewGen.</p><p>Table <ref type="table">6</ref>: Prompts used for GPT3Mix augmentation. For sequence-pair tasks, x 1 and x 2 denote the first and second input sequence, respectively. For single-sequence tasks, x denotes the input sequence. y denotes the label name. Only one example is shown in the template for clarity; in practice, we concatenate k = 4 samples according to the optimal setting in GPT3Mix <ref type="bibr" target="#b83">(Yoo et al., 2021)</ref>.</p><formula xml:id="formula_22">Task Template Label name SST-2</formula><p>Each item in the following list contains a movie review and the respective sentiment. positive: positive The sentiment is one of 'positive' or 'negative'.</p><p>negative: negative Movie review: x (Sentiment: y) . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoLA</head><p>Each item in the following list contains a text and the respective grammar.</p><p>grammatical: correct The grammar is one of 'correct' or 'incorrect'.</p><p>not grammatical: incorrect Text: x (Grammar: y) . . . Sentence 1: But prophecy is always strongest when based on coincidence--that is a prime rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNLI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label: Contradiction</head><p>Sentence 1: But Rodgers did tell Lewis that he despises Amelio because Amelio supported ddddddClinton, so it is Rodgers' mistake, not our author's, that we are correcting.</p><formula xml:id="formula_23">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 6 2 T D p O n d L o A Q U 4 h g e k l W g M h q K o = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s V B 9 W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J a l Y 3 A &lt; / l a t e x i t &gt; 0.03 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R W P 8 v M v A S + r X K l o m A 2 B K U a C F k p Q = " &gt; A A A B 7 H i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W H S h p r s C m 5 c V j C 2 0 I Y y m U 7 a o Z N J m J k I J f Q b 3 L h Q x K 0 f 5 M 6 / c f o Q V P T A h c M 5 9 3 L v P V H G m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 d K f S X B I a k J S n s h t h R T k T N N B M c 9 r N J M V J x G k n m l z N / c 4 9 l Y q l 4 l Z P M x o m e C R Y z A j W R g q Q j e p w U K 0 h 2 / c 8 H 3 k Q 2 S 5 C r u c b 0 v B 9 v 9 m A j o 0 W q I E V 2 o P q e 3 + Y k j y h Q h O O l e o 5 K N N h g a V m h N N Z p Z 8 r m m E y w S P a M 1 T g h K q w W B w 7 g 2 d G G c I 4 l a a E h g v 1 + 0 S B E 6 W m S W Q 6 E 6 z H 6 r c 3 F / / y e r m O v b B g I s s 1 F W S 5 K M 4 5 1 C m c f w 6 H T F K i + d Q Q T C Q z t 0 I y x h I T b f K p m B C + P o X / k 7 u 6 7 T R t 9 8 a t t S 5 W c Z T B C T g F 5 8 A B l 6 A F r k E b B I A A B h 7 A E 3 i 2 h P V o v V i v y 9 a S t Z o 5 B j 9 g v X 0 C r z u N 6 Q = = &lt; / l a t e x i t &gt; 0.02</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P c f 2 Y i y s w 9 O p w T v t D T 9 g w y w</p><formula xml:id="formula_24">I F s = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s x x l W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J Z E o 2 / &lt; / l a t e x i t &gt; 0.11</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 5 z g v j 4 r R 8 d h z a 7 s s s</p><formula xml:id="formula_25">V G o 3 3 Q e S U = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s 1 B h W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J f I Y 3 D &lt; / l a t e x i t &gt; 0.06 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 2 W n J o p 1 i f 5 + 3 2 S 1 K s D 3 e C W 2 E Q c = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 4 r P V V d e l m s A g u J E x s r M m u 4 M Z l B f u A N p T J d N I O n U n C z E Q o o b / g x o U i b v 0 h d / 6 N k 7 a C i h 6 4 c D j n X u 6 9 J 0 w 5 U x q h D 2 t l d W 1 9 Y 7 O 0 V d 7 e 2 d 3 b r x w c t l W S S U J b J O G J 7 I Z Y U c 5 i 2 t J M c 9 p N J c U i 5 L Q T T q 4 L v 3 N P p W J J f K e n K Q 0 E H s U s Y g T r Q k I 2 u h x U q s j 2 P c 9 H H k S 2 i 5 D r + Y b U f N + v 1 6 B j o z m q Y I n m o P L e H y Y k E z T W h G O l e g 5 K d Z B j q R n h d F b u Z 4 q m m E z w i P Y M j b G g K s j n t 8 7 g q V G G M E q k q V j D u f p 9 I s d C q a k I T a f A e q x + e 4 X 4 l 9 f L d O Q F O Y v T T N O Y L B Z F G Y c 6 g c X j c M g k J Z p P D c F E M n M r J G M s M d E m n r I J 4 e t T + D 9 p X 9 h O 3 X Z v 3 W r j f B l H C R y D E 3 A G H H A F G u A G N E E L E D A G D + A J P F v C e r R e r N d F 6 4 q 1 n D k C P 2 C 9 f Q J d n Y 3 C &lt; / l a t e x i t &gt;</formula><p>0.05 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t g S 9 O E 1 q y 8 V w + g S z F 7 7 n W I D L r n 8 = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0</p><formula xml:id="formula_26">V w I W F q Q k 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s x x l W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y Y 4 v u 8 3 H F i 3 0 Q I 1 s E J r W H 0 f j B K S x V R o w r F S / T p K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o</formula><p>J 4 e t T + D / p X N r 1 h u 3 e u r X m x S q O M j g B p + A c 1 M E V a I I b 0 A J t Q M A E P I A n 8 G z F 1 q P 1 Y r 0 u W 0 v W a u Y Y / I D 1 9 g l f J I 3 D &lt; / l a t e x i t &gt; 0.33 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 5 z g v j 4 r R 8 d h z a 7 s s s  s Y q j D E 7 A K T g H D r g C T X A D W q A N C J i A B / A E n q 3 Y e r R e r N d l a 8 l a z R y D H 7 D e P g F i K Y 3 F &lt; / l a t e x i t &gt; 0.08 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m Q y K r e p r p p d w o X / b a d K Z / n R b M t o = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 4 r P V V d e l m s A g u J E x s a J N d w Y 3 L C v Y B b S i T 6 a Q d O p m E m Y l Q Q n / B j Q t F 3 P p D 7 v w b J 2 0 F F T 1 w 4 X D O v d x 7 T 5 h y p j R C H 9 b a + s b m 1 n Z p p 7 y 7 t 3 9 w W D k 6 7 q g k k 4 S 2 S c I T 2 Q u x o p w J 2 t Z M c 9 p L J c V x y G k 3 n F 4 X f v e e S s U S c a d n K Q 1 i P B Y s Y g T r Q k I 2 a g w r V W T 7 n u c j D y L b R c j 1 f E N q v u / X a 9 C x 0 Q J V s E J r W H k f j B K S x V R o w r F S f Q e l O s i x 1 I x w O i 8 P M k V T T K Z 4 T P u G C h x T F e S L W + f w 3 C g j G C X S l N B w o X 6 f y H G s 1 C w O T W e M 9 U T 9 9 g r x L 6 + f 6 c g L c i b S T F N B l o u i j E O d w O J x O G K S E s 1 n h m A i m b k V k g m W m G g T T 9 m E 8 P U p / J 9 0 r m y n b r u 3 b r V 5 u Y q j B E 7 B G b g A D m i A J r g B L d A G B E z A A 3 g C z 1 Z s P V o v 1 u u y d c 1 a z Z y A H 7 D e P g F g p Y 3 E &lt; / l a t e x i t &gt; 0.07  </p><formula xml:id="formula_27">V G o 3 3 Q e S U = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s 1 B h W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J f I Y 3 D &lt; / l a t e x i t &gt; 0.06 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h B W F + 7 C Q N M Z m a v l q r k V O V P f / i m w = " &gt; A A A B 6 3 i c d V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B g 4 R N G 2 p y K 3 j x W M H a Q h v K Z r t p l + 4 m Y X c j l N C / 4 M W D I l 7 9 Q 9 7 8 N 2 7 a C i r 6 Y O D x 3 g w z 8 8 K U M 6 U R + r B K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w 6 E 4 l m S S 0 Q x K e y F 6 I F e U s p h 3 N N K e 9 V F I s Q k 6 7 4 f S q 8 L v 3 V C q W x L d 6 l t J A 4 H H M I k a w L i R k 1 5 1 h t Y Z s 3 / N 8 5 E F k u w i 5 n m 9 I w / f 9 Z g M 6 N l q g B l Z o D 6 v v g 1 F C M k F j T T h W q u + g V A c 5 l p o R T u e V Q a Z o i s k U j 2 n f 0 B g L q o J 8 c e s c n h l l B K N E m o o 1 X K j f J 3 I s l J q J 0 H Q K r C f q t 1 e I f 3 n 9 T E d e k L M 4 z T S N y X J R l H G o E 1 g 8 D k d M U q L 5 z B B M J D O 3 Q j L B E h N t 4 q m Y E L 4 + h f + T u 7 r t N G 3 3 x q 2 1 L l Z x l M E J O A X n w A G X o A W u Q R t 0 A A E T 8 A C e w L M l r E f r x X p d t p a s 1 c w x + A H r 7 R N a l 4 3 A &lt; / l a t e x i t &gt; 0.21 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 6 2 T D p O n d L o A Q U 4 h g e k l W g M h q K o = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s V B 9 W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J a l Y 3 A &lt; / l a t e x i t &gt; 0.03 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 6 2 T D p O n d L o A Q U 4 h g e k l W g M h q K o = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s V B 9 W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J a l Y 3 A &lt; / l a t e x i t &gt; 0.03 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 6 2 T D p O n d L o A Q U 4 h g e k l W g M h q K o = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s V B 9 W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J a l Y 3 A &lt; / l a t e x i t &gt; 0.03 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 5 z g v j 4 r R 8 d h z a 7 s s s V G o 3 3 Q e S U = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s 1 B h W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J f I Y 3 D &lt; / l</formula><formula xml:id="formula_28">o L Y T U O Q A A = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s x x 1 W a 8 j 2 P c 9 H H k S 2 i 5 D r + Y b U f d 9 v 1 K F j o w V q Y I X W s P o + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 Z Z I q m m E z x m P Y N F T i m K s g X t 8 7 h m V F G M E q k K a H h Q v 0 + k e N Y q V k c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VISUALIZATION OF TOKEN WEIGHT LEARNING</head><p>To gain intuitive understanding of what tokens are assigned more weight during generator tuning, we visualize the learned weights in Fig. <ref type="figure">5</ref>. The tokens with higher weights (e.g., "weak" in the first example and "hates" in the second example) are learned to be important tokens that decide the relation of the second sentence to the first sentence (i.e., the label of the training sample). With such tokens emphasized during training, the generator is encouraged to capture label-discriminative information that facilitates the generation of unambiguous training samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " f F H m Q i a N P G 9 Z d c S B I O u 1 E g 7 y A t E = " &gt; A A A C A 3 i c b V B N S 8 N A E N 3 4 W e t X 1 J t e g k X w V B I p 6 r G g B 4 8 V 7 A c 0 I W y 2 m 3 b p Z h N 2 J 2 I J A S / + F S 8 e F P H q n / D m v 3 H T 5 q C t D w Y e 7 8 0 w M y 9 I O F N g 2 9 / G 0 v L K 6 t p 6 Z a O 6 u b W 9 s2 v u 7 X d U n E p C 2 y T m s e w F W F H O B G 0 D A 0 5 7 i a Q 4 C j j t B u O r w u / e U 6 l Y L O 5 g k l A v w k P B Q k Y w a M k 3 D 9 0 I w 4 h g n l 3 n f u Y C f Y A M J G Y i z 3 2 z Z t f t K a x F 4 p S k h k q 0 f P P L H c Q k j a g A w r F S f c d O w M u w B E Y 4 z a t u q m i C y R g P a V 9 T g S O q v G z 6 Q 2 6 d a G V g h b H U J c C a q r 8 n M h w p N Y k C 3 V l c r O a 9 Q v z P 6 6 c Q X n o Z E 0 k K V J D Z o j D l F s R W E Y g 1 Y J I S 4 B N N M J F M 3 2 q R EZ a Y g I 6 t q k N w 5 l 9 e J J 2 z u n N e b 9 w 2 a s 1 G G U c F H a F j d I o c d I G a 6 A a 1 U B s R 9 I i e 0 S t 6 M 5 6 M F + P d + J i 1 L h n l z A H 6 A + P z B 3 e Q m K o = &lt; / l a t e x i t &gt;D trainStep 1: Supervised Training on&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f F H m Q i a N P G 9 Z d c S B I O u 1 E g 7 y A t E = " &gt; A A A C A 3 i c b V B N S 8 N A E N 3 4 W e t X 1 J t e g k X w V B I p 6 r G g B 4 8 V 7 A c 0 I W y 2 m 3 b p Z h N 2 J 2 I J A S / + F S 8 e F P H q n / D m v 3 H T 5 q C t D w Y e 7 8 0 w M y 9 I O F N g 2 9 / G 0 v L K 6 t p 6 Z a O 6 u b W 9 s 2 v u 7 X d U n E p C 2 y T m s e w F W F H O B G 0 D A 0 5 7 i a Q 4 C j j t B u O r w u / e U 6 l Y L O 5 g k l A v w k P B Q k Y w a M k 3 D 9 0 I w 4 h g n l 3 n f u Y C f Y A M J G Y i z 3 2 z Z t f t K a x F 4 p S k h k q 0 f P P L H c Q k j a g A w r F S f c d O w M u w B E Y 4 z a t u q m i C y R g P a V 9 T g S O q v G z 6 Q 2 6 d a G V g h b H U J c C a q r 8 n M h w p N Y k C 3 V l c r O a 9 Q v z P 6 6 c Q X n o Z E 0 k K V J D Z o j D l F s R W E Y g 1 Y J I S 4 B N N M J F M 3 2 q R EZ a Y g I 6 t q k N w 5 l 9 e J J 2 z u n N e b 9 w 2 a s 1 G G U c F H a F j d I o c d I G a 6 A a 1 U B s R 9 I i e 0 S t 6 M 5 6 M F + P d + J i 1 L h n l z A H 6 A + P z B 3 e Q m K o = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " l w / C y x S 9 B F w G b l z 0 k b C S y j d q / E 4 = " &gt; AA A C A X i c b V B N S 8 N A E N 3 4 W e t X 1 Y v g J V g E T y W R o h 4 L e v B Y w X 5 A G 8 J m O 2 2 X b j Z h d y K W E C / + F S 8 e F P H q v / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y + I B d f o O N / W 0 v L K 6 t p 6 Y a O 4 u b W 9 s 1 v a 2 2 / q K F E M G i w S k W o H V I P g E h r I U U A 7 V k D D Q E A r G F 1 N / N Y 9 K M 0 j e Y f j G L y Q D i T v c 0 b R S H 7 p s B t S H D I q 0 u v M T 7 s I D 5 g O Q G a Z X y o 7 F W c K e 5 G 4 O S m T H H W / 9 N X t R S w J Q S I T V O u O 6 8 T o p V Q h Z w K y Y j f R E F M 2 o g P o G C p p C N p L p x 9 k 9 o l R e n Y / U q Y k 2 l P 1 9 0 R K Q 6 3 H Y W A 6 J / f q e W 8 i / u d 1 E u x f e i m X c Y I g 2 W x R P x E 2 R v Y k D r v H F T A U Y 0 M o U 9 z c a r M h V Z S h C a 1 o Q n D n X1 4 k z b O K e 1 6 p 3 l b L t W o e R 4 E c k W N y S l x y Q W r k h t R J g z D y S J 7 J K 3 m z n q w X 6 9 3 6 m L U u W f n M A f k D 6 / M H v s i X s g = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " f F H m Q i a N P G 9 Z d c S B I O u 1 E g 7 y A t E = " &gt; A A A C A 3 i c b V B N S 8 N A E N 3 4 W e t X 1 J t e g k X w V B I p 6 r G g B 4 8 V 7 A c 0 I W y 2 m 3 b p Z h N 2 J 2 I J A S / + F S 8 e F P H q n / D m v 3 H T 5 q C t D w Y e 7 8 0 w M y 9 I O F N g 2 9 / G 0 v L K 6 t p 6 Z a O 6 u b W 9 s 2 v u 7 X d U n E p C 2 y T m s e w F W F H O B G 0 D A 0 5 7 i a Q 4 C j j t B u O r w u / e U 6 l Y L O 5 g k l A v w k P B Q k Y w a M k 3 D 9 0 I w 4 h g n l 3 n f u Y C f Y A M J G Y i z 3 2 z Z t f t K a x F 4 p S k h k q 0 f P P L H c Q k j a g A w r F S f c d O w M u w B E Y 4 z a t u q m i C y R g P a V 9 T g S O q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>w 5 l 9 e J J 2 z u n N e b 9 w 2 a s 1 G G U c F H a F j d I o c d I G a 6 A a 1 U B s R 9 I i e 0 S t 6 M 5 6 M F + P d + J i 1 L h n l z A H 6 A + P z B 3 e Q m K o = &lt; / l a t e x i t &gt; D train &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 v s C g g + 5 B h b N I v I l D n W S W 0 k O 1 1 o = " &gt; A A A C C n i c d V A 9 T x t B E N 2 D 8 B G H D w N l m g 0 W E g 3 W n T n Z 0 C G l o a A g U g x I P s u a W 4 / N i r 2 9 0 + 4 c Y J 2 u p u G v 0 K Q A o b T 5 B e n y b 7 I H j g S I P G m k p / d m N D M v z p S 0 5 P t / v J n Z D 3 P z C 4 s f a 5 + W l l d W 6 2 v r J z b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 N 0 4 e g o g c u H M 6 5 l 3 v v C V P O l E b o w y q s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t k o y S W i L J D y R 3 R A r y p m g L c 0 0 p 9 1 U U h y H n H b C y e X c 7 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a k z e G a U I Y w S a U p o u F C / T + Q 4 V m o a h 6 Y z x n q s f n t z 8 S + v l + n I D 3 I m 0 k x T Q Z a L o o x D n c D 5 3 3 D I J C W a T w 3 B R D J z K y R j L D H R J p 2 S C e H r U / g / a b u 2 c 2 F 7 1 1 6 l 4 a 3 i K I I T c A r O g Q N q o A G u Q B O 0 A A E j 8 A C e w L P F r U f r x X p d t h a s 1 c w x + A H r 7 R N 3 p 4 3 n &lt; / l a t e x i t &gt; w 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 9 Y m e 9 N a o o s Y r + v Z H d 5 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 S P e O S A t M g R O S F t w s g t u S e P 5 M m 5 c x 6 c Z + f l s 3 X G m c 5 s k R 9 w X j 8 A E J q b N w = = &lt; / l a t e x i t &gt; rL disc &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l N o d w l g G 4 b 3 p 0 R n + x 4 x z F 5 / z y c A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>s m b 9 W S 9 W O / W x 6 R 1 w Z r O 7 J E / s D 5 / A C H 1 m H I = &lt; / l a t e x i t &gt; L w-gen &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 9 Q 3 4 t + m X r w P m z / Q 6 r r Z a Q M N u N g = " &gt; A A A C A 3 i c b V A 9 S w N B E N 3 z M 8 a v U z t t D o N g F e 5 C U M u A j Y V F B P M B y R n 2 N p N k y d 7 e s T s n h u P A x r 9 i Y 6 G I r X / C z n / j 5 q P Q x A c D j / d m m J k X x I J r d N 1 v a 2 l 5 Z X V t P b e R 3 9 z a 3 t m 1 9 / b r O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>4 4 c k W N y S j x y T i r k i l R J j T D y S J 7 J K 3 m z n q w X 6 9 3 6 m L Y u W b O Z A / I H 1 u c P B M C Y X g = = &lt; / l a t e x i t &gt; L 2 gen &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a X 5 k f S e / 0 F X Q y p 7 Y H Y 3 P 9 H E r w Z k = " &gt; A A A C A 3 i c b V A 9 S w N B E N 2 L X z F + R e 2 0 O Q y C V b i T o J Y B G w u L C O Y D k v P Y 2 0 y S J X t 7 x + 6 c G I 4 D G / + K j Y U i t v 4 J O / + N m 4 9 C E x 8 M P N 6 b Y W Z e E A u u 0 X G + r d z S 8 s r q W n 6 9 s L G 5 t b 1 T 3 N 1 r 6 C h R D O o s E p F q B V S D 4 B L q y F F A K 1 Z A w 0 B A M x h e j v 3 m P S j N I 3 m L o x i 8 k P Y l 7 3 F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " l s h Z G e 8 e F 7 7 w 4 T o F V e J D M D i 7 j v I = " &gt; A A A C A 3 i c b V A 9 S w N B E N 2 L X z F + R e 2 0 O Q y C V b i T o J Y B G w u L C O Y D k j P s b S b J k r 2 9 Y 3 d O D M e B j X / F x k I R W / + E n f / G v S S F J j 4 Y e L w 3 w 8 w 8 P x J c o + N 8 W 7 m l 5 Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>genFigure 1 :</head><label>1</label><figDesc>Figure 1: Overview of FewGen. A generator PLM is first tuned on the few-shot samples with our proposed meta-weighted maximum likelihood objective and then used to synthesize new training samples. A classification PLM is finally trained on both the few-shot and the generated samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>:</head><label></label><figDesc>The training set D train = {(x, y) i } consists of K training samples per label where x = [x 1 , x 2 , . . . , x n ] is a text sequence with n tokens. The development set D dev is of the same size as D train . There is no access to additional task-specific unlabeled data. The number of training samples K is assumed to be very small (e.g., K = 16), making it challenging to train a classification model C ? that generalizes well to unseen data. To mitigate such a training data scarcity issue, we propose to first train an autoregressive PLM on D train , and then use it as a generator G ? to synthesize a large amount of novel samples D gen = {( x, ?) i } that augment the original training set. Finally, a classification PLM C ? is fine-tuned on both D train and D gen to perform the task. An overview of our FewGen method is shown in Fig.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (On MNLI) Training the generator only via L gen does not automatically decrease L disc .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Algorithm 1</head><label>1</label><figDesc>Meta-Weighted Generator Tuning. Input: D train : Few-shot training set. Parameter: T : Number of training steps. Output: ? p : Prefix parameters for all labels. Initialize ? (0) p (with task-descriptive prompts)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>PreprintAlgorithm 2</head><label>2</label><figDesc>Classification model fine-tuning on D train and D gen . Input: D train : Few-shot training set; D gen : Synthesized training set. Parameter: T : Number of training steps. Output: ?: Trained classification model parameters. ? (0) ? Train on D train with standard supervised learning z ? 0 // Ensembled prediction initialization for t ? [0, 1, . . . , T -1] do B ? Sample a minibatch from D gen ? (t+1) ? Take one gradient step to descend L class in Eq. (4) on B z ? Accumulate the current model prediction Update D gen to exclude noisy samples based on z end return ? = ? (T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>, making it challenging for paraphrase-based methods to create sufficiently diverse training samples only based on the small few-shot set. The new training samples produced by our FewGen method are not limited to the Preprint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FigureFigure 4 :</head><label>4</label><figDesc>Figure 3: MNLI-m accuracy with different amounts of generated training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>MRPC:</head><label></label><figDesc>Microsoft Research Paraphrase Corpus (Dolan &amp; Brockett, 2005) requires predicting whether two sentences are semantically equivalent or not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Each item in the following list contains a premise, a hypothesis and their logical relation. entailment: entailment The logical relation is one of 'entailment', 'neutral' or 'contradiction'. neutral: neutral Premise: x1 Hypothesis: x2 (Logical relation: y) . . . contradiction: contradiction QNLI Each item in the following list contains a question, an answer and their logical relation. entailment: entailment The logical relation is one of 'entailment' or 'neutral'. not entailment: neutral Question: x1 Answer: x2 (Logical relation: y) . . . RTE Each item in the following list contains a premise, a hypothesis and their logical relation. entailment: entailment The logical relation is one of 'entailment' or 'neutral'. not entailment: neutral Premise: x1 Hypothesis: x2 (Logical relation: y) . . . MRPC Each item in the following list contains two sentences and their semantic relation. equivalent: equivalent The semantic relation is one of 'equivalent' or 'different'. not equivalent: different Sentence 1: x1 Sentence 2: x2 (Semantic relation: y) . . . QQP Each item in the following list contains two questions and their semantic relation. equivalent: equivalent The semantic relation is one of 'equivalent' or 'different'. not equivalent: different Question 1: x1 Question 2: x2 (Semantic relation: y) . . .Sentence 2: Prophecies based on coincidences are widely known to be weak and unreliable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " s L a c a W P l x 3 E r e f d 1 m a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q kJ Z r P D M F E M n M r J B M s M d E m n o o J 4 e t T + D / p X N p O w 3 Z v 3 V r z Y h V H G Z y A U 3 A O H H A F m u A G t E A b E D A B D + A J P F u x 9 W i 9 W K / L 1 p K 1 m j k G P 2 C 9 f Q J d n o 3 C &lt; / l a t e x i t &gt; 0.14 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 0 z 2 S f / 6 H C e m i I w w J x 9 U 8 U X 8 d / M = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w I W F i Q 0 1 2 B T c u K 9 g H t K F M p p N 2 6 G Q S Z i Z C C f 0 F N y 4 U c e s P u f N v n L Q V V P T A h c M 5 9 3 L v P W H K m d I I f V i l t f W N z a 3 y d m V n d 2 / / o H p 4 1 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s 5 A 2 r N W T 7 n u c j D y L b R c j 1 f E P q v u 8 3 6 t C x 0 Q I 1 s E J r W H 0 f j B K S x V R o w r F S f Q e l O s i x 1 I x w O q 8 M M k V T T K Z 4 T P u G C h xT F e S L W + f w z C g j G C X S l N B w o X 6 f y H G s 1 C w O T W e M 9 U T 9 9 g r x L 6 + f 6 c g L c i b S T F N B l o u i j E O d w O J x O G K S E s 1 n h m A i m b k V k g m W m G g T T 8 W E 8 P U p / J 9 0 L m 2 n Y b u 3 b q 1 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " p C M A L U 1 e 8 h c h G b W 2 C h 4 f L P e B X K 4 = " &gt; A A A B 6 3 i c d V D L S g M x F M 3 4 r P V V d e k m W A Q X M m T s 0 M 7 s C m 5 c V r A P a I e S S T N t a C Y z J B m h D P 0 F N y 4 U c e s P u f N v z L Q V V P R A 4 O S c e 7 n 3 n j D l T G m E P q y 1 9 Y 3 N r e 3 S T n l 3 b / / g s H J 0 3 F F J J g l t k 4 Q n s h d i R T k T t K 2 Z 5 r S X S o r j k N N u O L 0 u / O 4 9 l Y o l 4 k 7 P U h r E e C x Y x A j W h Y R s t z G s V J H t e 5 6 P P G j + C L m e b 0 j N 9 / 1 6 DT o 2 W q A K V m g N K + + D U U K y m A p N O F a q 7 6 B U B z m W m h F O 5 + V B p m i K y R S P a d 9 Q g W O q g n y x 6 x y e G 2 U E o 0 S a J z R c q N 8 7 c h w r N Y t D U x l j P V G / v U L 8 y + t n O v K C n I k 0 0 1 S Q 5 a A o 4 1 A n s D g c j p i k R P O Z I Z h I Z n a F Z I I l J t r E U z Y h f F 0 K /y e d K 9 u p 2 + 6 t W 2 1 e r u I o g V N w B i 6 A A x q g C W 5 A C 7 Q B A R P w A J 7 A s x V b j 9 a L 9 b o s X b N W P S f g B 6 y 3 T 2 a 5 j c g = &lt; / l a t e x i t &gt; 0.47 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z V m z 4 c B s t k n V G N d M e F e y Y W S y P 1 8 = " &gt; A A A B 6 3 i c d V D L S s N A F J 3 4 r P V V d e l m s A g u J E x s a J N d w Y 3 L C v Y B b S i T 6 a Q d O p m E m Y l Q Q n / B j Q t F 3 P p D 7 v w b J 2 0 F F T 1 w 4 X D O v d x 7 T 5 h y p j R C H 9 b a + s b m 1 n Z p p 7 y 7 t 3 9 w W D k 6 7 q g k k 4 S 2 S c I T 2 Q u x o p w J 2 t Z M c 9 p L J c V x y G k 3 n F 4 X f v e e S s U S c a d n K Q 1i P B Y s Y g T r Q k K 2 0 x h W q s j 2 P c 9 H H k S 2 i 5 D r + Y b U f N + v 1 6 B j o w W q Y I X W s P I + G C U k i 6 n Q h G O l + g 5 K d Z B j q R n h d F 4 e Z I q m m E z x m P Y N F T i m K s g X t 8 7 h u V F G M E q k K a H h Q v 0 + k e N Y q V k c m s 4 Y 6 4 n 6 7 R X i X 1 4 / 0 5 E X 5 E y k m a a C L B d F G Y c 6 g c X j c M Q k J Z r P D M F E M n M r J B M s M d E m nr I J 4 e t T + D / p X N l O 3 X Z v 3 W r z c h V H C Z y C M 3 A B H N A A T X A D W q A N C J i A B / A E n q 3 Y e r R e r N d l 6 5 q 1 m j k B P 2 C 9 f Q J i K o 3 F &lt; / l a t e x i t &gt; 0.17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>&lt; 08 Figure 5 :</head><label>085</label><figDesc>Figure 5: Visualization of learned token weights on two samples from MNLI's few-shot training set. The generator is trained given the first sentence to generate the second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>model. After training, G ? can be used to generate novel texts by iteratively sampling tokens from its generation probability distribution. Prefix-Tuning. Unlike fine-tuning which updates all model parameters ? of a PLM, prefixtuning (Li &amp; Liang, 2021) freezes all pretrained Transformer parameters and only optimizes prefix vectors ? p that are prepended to each Transformer layer. We use prefix-tuning for training G ?p on D</figDesc><table /><note><p>train because (1) it offers better effectiveness than fine-tuning for small datasets (Li &amp; Liang, 2021) and (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>(more details in Appendix A) except STS-B which is a regression task. We follow the same data split and evaluation protocol as<ref type="bibr" target="#b17">Gao et al. (2021)</ref>: Both D train and D dev contain 16 samples per label and are sampled from the original training set with 5 different random seeds. The original development sets are used for testing. For all reported results, we include the average and standard deviation over the 5 different D train /D dev splits. F1 score is used as the metric for QQP and MRPC, Matthews correlation for CoLA, and accuracy for the remaining tasks. Results on seven classification tasks of the GLUE benchmark. We report average and standard deviation (as subscripts) performance over 5 different D train /D dev splits defined in<ref type="bibr" target="#b17">Gao et al. (2021)</ref>. ? : Results from<ref type="bibr" target="#b17">Gao et al. (2021)</ref>. ? : Results from<ref type="bibr" target="#b84">Zhang et al. (2022)</ref>. Methods that use additional models apart from the final classification model are marked.</figDesc><table><row><cell>Method</cell><cell>MNLI-(m/mm) (Acc.)</cell><cell>QQP (F1)</cell><cell>QNLI (Acc.)</cell><cell>SST-2 (Acc.)</cell><cell>CoLA (Matt.)</cell><cell>RTE (Acc.)</cell><cell>MRPC (F1)</cell><cell>AVG</cell></row><row><cell cols="8">Methods without Augmentation: Few-shot samples are directly used for classifier tuning or as demonstrations for inference</cell><cell></cell></row><row><cell>Prompting  ?</cell><cell>50.8/51.7</cell><cell>49.7</cell><cell>50.8</cell><cell>83.6</cell><cell>2.0</cell><cell>51.3</cell><cell>61.9</cell><cell>50.1</cell></row><row><cell>Fine-Tuning  ?</cell><cell>45.86.4/47.86.8</cell><cell cols="4">60.74.3 60.26.5 81.43.8 33.914.3</cell><cell>54.43.9</cell><cell>76.62.5</cell><cell>59.1</cell></row><row><cell>In-Context  ?</cell><cell>52.00.7/53.40.6</cell><cell cols="4">36.15.2 53.80.4 84.81.3 -1.52.4</cell><cell>60.41.4</cell><cell>45.76.0</cell><cell>47.4</cell></row><row><cell>LM-BFF (Man.)  ?</cell><cell>68.32.3/70.51.9</cell><cell cols="3">65.55.3 64.54.2 92.70.9</cell><cell>9.37.3</cell><cell>69.13.6</cell><cell>74.55.3</cell><cell>63.6</cell></row><row><cell>+ demonstration  ?</cell><cell>70.71.3/72.01.2</cell><cell cols="3">69.81.8 69.21.9 92.60.5</cell><cell>18.78.8</cell><cell>68.72.3</cell><cell>77.82.0</cell><cell>66.9</cell></row><row><cell>LM-BFF (Auto)  ? (w. 2.9B T5)</cell><cell>68.32.5/70.12.6</cell><cell cols="4">67.03.0 68.37.4 92.31.0 14.014.1</cell><cell>73.92.2</cell><cell>76.22.3</cell><cell>65.8</cell></row><row><cell>+ demonstration  ? (w. 2.9B T5)</cell><cell>70.03.6/72.03.1</cell><cell cols="4">67.75.8 68.55.4 93.00.6 21.815.9</cell><cell>71.15.3</cell><cell>78.13.4</cell><cell>67.3</cell></row><row><cell>P-Tuning  ?</cell><cell>61.52.1/-</cell><cell cols="3">65.63.0 64.32.8 92.20.4</cell><cell>-</cell><cell>-</cell><cell>74.57.6</cell><cell>-</cell></row><row><cell>DART  ?</cell><cell>67.52.6/-</cell><cell cols="3">67.83.2 66.73.7 93.50.5</cell><cell>-</cell><cell>-</cell><cell>78.34.5</cell><cell>-</cell></row><row><cell cols="7">Methods with Augmentation: Few-shot samples are used for creating synthesized samples and for classifier tuning</cell><cell></cell><cell></cell></row><row><cell>Back Translation (w. trained Marian)</cell><cell>66.94.6/68.33.8</cell><cell cols="3">59.84.6 67.84.9 91.11.9</cell><cell>7.53.7</cell><cell>62.45.3</cell><cell cols="2">68.011.2 60.6</cell></row><row><cell>GPT3Mix (w. 175B GPT3)</cell><cell>61.53.2/62.62.2</cell><cell cols="3">70.41.9 69.20.3 93.60.6</cell><cell>48.91.9</cell><cell cols="3">70.410.0 69.912.4 69.2</cell></row><row><cell cols="2">Generator Fine-Tuning (w. 1.6B CTRL) 68.95.1/70.85.3</cell><cell cols="4">60.48.7 70.94.1 91.21.2 18.810.0</cell><cell>66.14.4</cell><cell>60.815.4</cell><cell>62.6</cell></row><row><cell>FewGen (w. 1.6B CTRL)</cell><cell>75.71.6/77.11.0</cell><cell cols="3">71.51.7 76.34.4 93.10.8</cell><cell>40.07.5</cell><cell>71.22.4</cell><cell>81.12.5</cell><cell>72.8</cell></row><row><cell>Fully Supervised Fine-Tuning  ?</cell><cell>89.8/89.5</cell><cell>81.7</cell><cell>93.3</cell><cell>95.0</cell><cell>62.6</cell><cell>80.9</cell><cell>91.4</cell><cell>84.9</cell></row></table><note><p><p><p><p><p>Models, Training Settings and Hyperparameters. FewGen is a training data generation method and can be used with any fine-tuning method on any classification model. We use moderatesized PLMs to ensure our results are reproducible on typical research hardware: CTRL (1.6B parameters)</p><ref type="bibr" target="#b27">(Keskar et al., 2019)</ref> </p>as the generator G ? and RoBERTa Large (356M parameters)</p><ref type="bibr" target="#b42">(Liu et al., 2019)</ref> </p>as the classifier C ? . We use prefix-tuning for training G ? and prompt-based fine-tuning</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies by removing (-) or switching (w.) one component of FewGen.</figDesc><table><row><cell>Method</cell><cell cols="2">MNLI-(m/mm) SST-2</cell></row><row><cell>FewGen</cell><cell>75.7 1.6 /77.1 1.0</cell><cell>93.1 0.8</cell></row><row><cell cols="2">w. L gen w. L gen + L disc -label smooth -temporal ensemble 72.2 2.5 /74.0 2.2 74.9 1.0 /76.2 1.0 74.6 1.6 /76.0 1.5 75.0 1.3 /76.2 1.0</cell><cell>92.5 0.7 92.4 0.8 92.7 0.7 92.1 1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>(For MNLI)  Examples of generated second sequence (hypothesis) by generators tuned with three different objectives conditioned on a given first sequence (premise) "In 2009, hair samples from 1,137 Taiji residents were tested for mercury by the National Institute for Minamata Disease (NIMD)". The true label of the generated sequence is marked at the end of the sequence (if the generated sequence correctly pertains to the target label, it is marked in blue; otherwise, it is in red).</figDesc><table><row><cell>Objective</cell><cell>Label</cell><cell>Generated Second Sequence</cell></row><row><cell></cell><cell>entailment</cell><cell></cell></row><row><cell>Lgen</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Prompts used for initializing the prefix vectors and control codes (required byCTRL (Keskar  et al., 2019)) used in generator training. The control codes are selected to approximiate the domain of the task. For single-sequence tasks, x denotes the training sample; for sequence-pair tasks, x 1 and x 2 denote the first and second sequence in the training sample, respectively.</figDesc><table><row><cell>Task</cell><cell>Task Type</cell><cell cols="2">Control Code Label</cell><cell>Initialization Prompt</cell></row><row><cell>SST-2</cell><cell cols="2">single-sequence Reviews</cell><cell>positive negative</cell><cell>Rating: 5.0 positive movie review: x Rating: 1.0 negative movie review: x</cell></row><row><cell>CoLA</cell><cell cols="2">single-sequence Links</cell><cell cols="2">grammatical not grammatical Linguistically incorrect sentence: x Linguistically correct sentence: x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>entailment</cell><cell>Sentence 1 implies Sentence 2. Sentence 1: x1 Sentence 2: x2</cell></row><row><cell>MNLI</cell><cell>sequence-pair</cell><cell>Wikipedia</cell><cell>neutral</cell><cell>Sentence 2 supplements Sentence 1. Sentence 1: x1 Sentence 2: x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>contradiction</cell><cell>Sentence 2 contradicts Sentence 1. Sentence 1: x1 Sentence 2: x2</cell></row><row><cell>QNLI</cell><cell>sequence-pair</cell><cell>Links</cell><cell>entailment not entailment</cell><cell>Paragraph is relevant to Question. Question: x1 Paragraph: x2 Paragraph is irrelevant to Question. Question: x1 Paragraph: x2</cell></row><row><cell>RTE</cell><cell>sequence-pair</cell><cell>Wikipedia</cell><cell>entailment not entailment</cell><cell>Sentence 1 implies Sentence 2. Sentence 1: x1 Sentence 2: x2 Sentence 2 supplements Sentence 1. Sentence 1: x1 Sentence 2: x2</cell></row><row><cell cols="2">MRPC sequence-pair</cell><cell>Wikipedia</cell><cell>equivalent not equivalent</cell><cell>Sentence 1 is equivalent to Sentence 2. Sentence 1: x1 Sentence 2: x2 Sentence 1 is different from Sentence 2. Sentence 1: x1 Sentence 2: x2</cell></row><row><cell>QQP</cell><cell>sequence-pair</cell><cell>Links</cell><cell>equivalent not equivalent</cell><cell>Question 1 is equivalent to Question 2. Question 1: x1 Question 2: x2 Question 1 is different from Question 2. Question 1: x1 Question 2: x2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters for generating training data of different tasks. ? : Temperature during sampling (? = 0 means greedy sampling); ?: Repetition penalty.</figDesc><table><row><cell>Task</cell><cell>Label</cell><cell>?</cell><cell>?</cell></row><row><cell>SST-2</cell><cell>positive negative</cell><cell>0.5</cell><cell>1.1 1.1</cell></row><row><cell>CoLA</cell><cell cols="3">grammatical not grammatical 10 1.1 0.3 1.1</cell></row><row><cell></cell><cell>entailment</cell><cell></cell><cell>1.1</cell></row><row><cell>MNLI</cell><cell>neutral</cell><cell>0</cell><cell>1.5</cell></row><row><cell></cell><cell>contradiction</cell><cell></cell><cell>1.1</cell></row><row><cell>QNLI</cell><cell>entailment not entailment</cell><cell>0</cell><cell>1.0 1.5</cell></row><row><cell>RTE</cell><cell>entailment not entailment</cell><cell>0</cell><cell>1.0 1.5</cell></row><row><cell>MRPC</cell><cell>equivalent not equivalent</cell><cell>0</cell><cell>1.0 1.5</cell></row><row><cell>QQP</cell><cell>equivalent not equivalent</cell><cell>0</cell><cell>1.0 1.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code can be found at https://github.com/yumeng5/FewGen.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The CoLA results reported in the original GPT3Mix paper use accuracy as the metric instead of Matthews correlation; our reimplemented GPT3Mix achieves 79.40.6 on CoLA if measured by accuracy.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>g w q 4 V 5 8 <rs type="projectName">S I M o a u R c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 m k q M e C F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 2 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 Z u a 3 H 1 F p H s s H M 0 n Q j + h Q 8 p A z a q x 0 / 9 T 3 + u W K W 3 X n I K v E y 0 k F c j T 6 5 a / e I G Z p h N I w Q b X u e m 5 i / I w q w 5 n A a a m X a k w o G 9 M h d i 2 V N E L t Z / N T p + T M K g M S x s q W N G S u / p 7 I a K T 1 J A p s Z 0 T N S C 9 7 M / E / r 5 u a 8 N r P u E x S g 5 I t F o W p I C Y m s 7 / J g C t k R k w s o U x x e y t h I 6 o o M z a d k g 3 B W 3 5 5 l b Q u q t 5 l t X Z X q 9 R r e R x F O I F T O A c P r q A O t 9 C A J j A Y w j O 8 w p s j n B f n 3 f l Y t B a c f O Y Y / s D 5 / A E K a I 2 b &lt; / l a t e x i t &gt;</p><p>x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y B z 0 4 + z + E T s A g w q 4 V 5 8 <rs type="projectName">S I M o a u R c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 m k q M e C F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 2 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H</p><p>q j 4 Y t E g l S 7 F 7 u x 9 t y 8 0 c p I T S x j X w t 7 q 8 h H T j J M N q W h D 8 J d f X i X N y 4 p / V a n e V 8 u 1 a h 5 H A U 7 h D C 7 A h 2 u o w R 3 U o Q E c F D z D K 7 w 5 x n l x 3 p 2 P R e u a k 8 / G s F S N 8 q 9 1 7 8 Z E b 9 8 a c Q L z w m 4 Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l U S K e i x 4 8 V j B f m A b y m Y 7 b Z d u N m F 3 I p T Q f + H F g y J e / T f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F i R S G P O / b W V v f 2 N z a L u w U d / f 2 D w 5 L R 8 d N E 6 e a Y 4 P H M t b t k B m U Q m G D B E l s J x p Z F E p s h e P b m d 9 6 Q m 1 E r B 5 o k m A Q s a E S A 8 E Z W e m x S y M k 1 s u S a a 9 U 9 i r e H O 4 q 8 X N S h h z 1 X u m r 2 4 9 5 G q</p><p>G a I J t f P H X P r d J 3 B 7 G 2 p c i d q 7 8 n M h Y Z M 4 l C 2 x k x G p l l b y b + 5 3 V S G t w E m V B J S q j 4 Y t E g l S 7 F 7 u x 9 t y 8 0 c p I T S x j X w t 7 q 8 h H T j J M N q W h D 8 J d f X i X N y 4 p / V a n e V 8 u 1 a h 5</p><p>4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8</p><p>4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8</p><p>w 8 u I I G 3 E E T W s D g E Z 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w e d g 1 w <rs type="projectName">V A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 l K U Y 8 F L x 4 r 2 l Z o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b</p><p>z + M o w g m c w j l 4 c A k N u I E m t I D B E J 7 h F d 4 c 6 b w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 8 L 7 I 2 c &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " <rs type="programName">+ F X i S Q i Y w l N 0 o E E E d S O O e d g 1 w V A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1</rs> q / q h 6 9 L B b B U 0 l K U Y 8 F L x 4 r 2 l Z o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U N n G q G W + x W M b 6 I a C G S 6 F 4 C w V K / p B o T q N A 8 k 4 w v p 7 5 n U e u j Y j V P U 4 S 7 k d 0 q</p><p>z + M o w g m c w j l 4 c A k N u I E m t I D B E J 7 h F d 4 c 6 b w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 8 L 7 I 2 c &lt; / l a t e x i t &gt;</p><p>x 2</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z L e q v M q E y 8 J w f m d 5 J B G m</p><p>x n</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z L e q v M q E y 8</p><p>x n</p></div>
<div><head>? ? ?</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8</p><p>w 8 u I I G 3 E E T W s D g E Z 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8 = " <rs type="projectName">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 W e h Q j O o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P J n N / W W v c <rs type="programName">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</rs> W s D g E Z 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8 = " <rs type="projectName">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 W e h Q j O o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P J n N / W W v c <rs type="programName">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</rs> W s D g E Z 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A c P H c 2 / G m s e U v Y w 3 9 t e t y l S l 3 A 8 = " <rs type="projectName">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</rs> U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 W e h Q j O o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P J n N / W W v c <rs type="programName">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</rs> W s D g E Z 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 A f O 5 w + u B Y 8 u &lt; / l a t e x i t &gt; Classification PLM &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l w / C y x S 9 B F w G b l z 0 k b C S y j d q / E 4 = " <rs type="projectName">&gt; A A A C A X i c b V B N S 8 N A E N 3 4</rs> W e t X 1 Y v g J V g E T y W R o h 4 L e v B Y w X 5 A G 8 J m O 2 2 X b j Z h d y K W E C / + F S 8 e F P H q v / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y + I B d f o O N / W 0 v L K 6 t p 6 Y a O 4 u b W 9 s 1 v a 2 2 / q K F E M G i w S k W o H V I P g <rs type="programName">E h r I U U A 7 V k D D Q E A r G F</rs> <rs type="grantNumber">1 N / N Y 9 K M 0</rs> j e Y f j G L y Q D i T v c 0 b R S H 7 p s B t S H D I q 0 u v M T 7 s I D 5 g O Q G a Z X y o 7 F W c K e 5 G 4 O S m T H H W / 9 N X t R S w J Q S I T V O u O 6 8 T o p V Q h Z w K y Y j f R E F M 2 o g P o G C p p C N p L p x 9 k 9 o l R e n Y / U q Y k 2 l P <rs type="grantNumber">1 9 0 R K Q 6 3 H Y W A 6 J /</rs> f q e W 8 i / u d 1 E u x f e i m X c Y I g 2 W x R P x E 2 R v Y k D r v H F T A U Y 0 M o U 9 z c a r M h V Z S h C a 1 o Q n D n X 1 4 k z b O K e 1 6 p 3 l b L t W o e R 4 E c k W N y S l x y Q W r k h t R J g z D y S J 7 J K 3 m z n q w X 6 9 3 6 m L U u W f n M A f k D 6 / M H v s i X s g = = &lt; / l a t e x i t &gt;</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_3XpMneM">
					<orgName type="project" subtype="full">S I M o a u R c = &quot; &gt; A A A B 6 n i c b V B N S 8 N A E J 3</orgName>
				</org>
				<org type="funded-project" xml:id="_jm9zNxJ">
					<orgName type="project" subtype="full">S I M o a u R c = &quot; &gt; A A A B 6 n i c b V B N S 8 N A E J 3</orgName>
				</org>
				<org type="funded-project" xml:id="_fpAMwRN">
					<orgName type="project" subtype="full">V A = &quot; &gt; A A A B 6 n i c b V B N S 8 N A E J 3</orgName>
				</org>
				<org type="funding" xml:id="_2vKtVbt">
					<orgName type="program" subtype="full">+ F X i S Q i Y w l N 0 o E E E d S O O e d g 1 w V A = &quot; &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1</orgName>
				</org>
				<org type="funded-project" xml:id="_ugEDCC3">
					<orgName type="project" subtype="full">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</orgName>
					<orgName type="program" subtype="full">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</orgName>
				</org>
				<org type="funded-project" xml:id="_F6J6y5S">
					<orgName type="project" subtype="full">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</orgName>
					<orgName type="program" subtype="full">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</orgName>
				</org>
				<org type="funded-project" xml:id="_GDNvAmm">
					<orgName type="project" subtype="full">&gt; A A A B 7 X i c b V B N S 8 N A E J 3</orgName>
					<orgName type="program" subtype="full">F H G U 4 Q R O 4 R w 8 u I I G 3 E E T</orgName>
				</org>
				<org type="funded-project" xml:id="_sJZ6VZS">
					<idno type="grant-number">1 N / N Y 9 K M 0</idno>
					<orgName type="project" subtype="full">&gt; A A A C A X i c b V B N S 8 N A E N 3 4</orgName>
					<orgName type="program" subtype="full">E h r I U U A 7 V k D D Q E A r G F</orgName>
				</org>
				<org type="funding" xml:id="_nz6hvq5">
					<idno type="grant-number">1 9 0 R K Q 6 3 H Y W A 6 J /</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do not have enough data? deep learning to the rescue! In AAAI</title>
		<author>
			<persName><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zwerdling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What size net gives valid generalization</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CoCon: A self-supervised approach for controlled text generation</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Pung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Mixtext</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prototypical verbalizer for prompt-based few-shot tuning</title>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longtao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Paraphrasing (IWP)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Findings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WARP: Word-level adversarial reprogramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeBERTa: Decoding-enhanced BERT with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demo</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CTRL: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A distributional approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GeDi: Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Rajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Controlled text generation as continuous optimization with multiple constraints</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Life-long Learning for Spoken Language Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural data augmentation via example extrapolation</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01335</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DExperts: Decoding-time controlled text generation with experts and anti-experts</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2022a</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What makes good in-context examples for GPT-3?</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.10385</idno>
	</analytic>
	<monogr>
		<title level="j">GPT understands</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PowerTransformer: Unsupervised controllable revision for biased language correction</title>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weakly-supervised neural text classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-supervised hierarchical text classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">COCO-LM: Correcting and contrasting text sequences for language model pretraining</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Generating training data with language models: Towards zero-shot language understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pretraining text encoders with adversarial mixture of training signal generators</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2022b</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Noisy channel language model prompting for few-shot text classification</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SELF: Learning to filter noisy labels with self-ensembling</title>
		<author>
			<persName><forename type="first">Duc</forename><surname>Tam Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaithanya</forename><surname>Kumar Mummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi-Phuong-Nhung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics</title>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A plug-andplay method for controlled text generation</title>
		<author>
			<persName><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B?ni</forename><surname>Egressy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Style transfer through back-translation</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Shrimai Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Few-shot text generation with natural language instructions</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating datasets with pretrained language models</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">First Quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Iyer</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandekar</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csernai</forename><surname>Korn?l</surname></persName>
		</author>
		<ptr target="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Eliciting knowledge from language models using automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Meta-weightnet: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improving and simplifying pattern exploiting training</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Rakesh R Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop BlackboxNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Towards zero-label language learning</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.09193</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Learning to teach with dynamic loss functions</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Preprint Lijun Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">FUDGE: Controlled text generation with future discriminators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">G-daug: Generative data augmentation for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Yiben</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji Ping</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Findings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Zerogen: Efficient zero-shot learning via dataset generation</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qintong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.07922</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">GPT3Mix: Leveraging large-scale language models for text augmentation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do-Hyoung</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woomyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Fine-tuning language models from human preferences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.08593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
