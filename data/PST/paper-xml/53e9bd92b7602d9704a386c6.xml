<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency</title>
				<funder ref="#_5DVBhqp">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
				</funder>
				<funder ref="#_AWS6Rar">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiming</forename><surname>Liu</surname></persName>
							<email>hmliu@cs.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
							<email>mferdman@ece.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
							<email>jaehyuk.huh@amd.com</email>
						</author>
						<author>
							<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
							<email>dburger@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Elec. &amp; Comp. Eng</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96% of the dead blocks with a 96% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8% to 17% and the L2 efficiency from 17% to 27%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12% at the L1 and by 13% at the L2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Prior studies have shown that data caches have low efficiency <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b25">[26]</ref>; only a small fraction of cache lines actually hold data that will be referenced before eviction. Traditionally, a cache line that will be referenced again before eviction is called a live block; otherwise it is called a dead block. Cache efficiency can be improved if more live blocks are stored in the cache without increasing its capacity. Improved cache efficiency reduces cache-miss rate and improves system performance.</p><p>The root cause of low cache efficiencies is that blocks die, reside in the cache for a long period of time with no accesses, and then are finally evicted. With LRU replacement, upon the last access to a block, multiple replacements to that set must occur before the dead block is evicted <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which can take thousands of cycles. This last access may occur after only several accesses, or worse, immediately upon the first time the block is loaded into the cache.</p><p>To achieve better efficiency, dead blocks should be identified early. The earlier a dead block is identified, the more opportunity there is to improve cache efficiency. A block turns dead on its last access before its eviction from the cache. The identification of a dead block should be done between the last access to the block and its eviction from the cache. Since the hardware does not know with certainty which access to a block is the last access, the identification of a block as dead is a speculative action called dead-block prediction.</p><p>Three approaches for dead-block prediction have been proposed: trace-based, counting-based, and time-based. <ref type="bibr">Lai et al.</ref> were the first to propose the concept of dead-block prediction and a trace-based predictor <ref type="bibr" target="#b15">[16]</ref>, which predicts a block dead once it has been accessed by a certain sequence of instructions. They use the predictor to trigger prefetches into the L1 data cache. Hu et al. later proposed a time-based predictor <ref type="bibr" target="#b6">[7]</ref>, also to trigger prefetches into the L1 data cache. Time-based predictors predict a block dead once it has not been accessed for a certain number of cycles. <ref type="bibr">Kharbutli et al.</ref> proposed a counting-based predictor <ref type="bibr" target="#b13">[14]</ref>, which predicts a block dead once it has been accessed a certain number of times. They use the predictor to optimize cache replacement policy and to bypass zero-reuse blocks.</p><p>Most prior dead-block predictors predict the death of a block immediately after the block is accessed, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), which shows a sequence of accesses to three blocks, A, B, and C, in the same set of a two-way associative cache. P(A) in the figure indicates a prediction about whether Predicting dead blocks at different times block A has died. While this approach identifies dead blocks as early as possible, it sacrifices prediction accuracy and coverage because a block just accessed may be accessed again soon. There is a tradeoff between the timeliness and accuracy/coverage of dead-block prediction. The earlier the prediction is made, the more useful it is. On the other hand, the later the prediction is made, the less likely it is to mispredict. In this paper, we quantify this tradeoff by making dead-block predictions at different points during the dead time of a block. Making dead-block predictions when a block just becomes non-MRU, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), gives the best tradeoff between timeliness and prediction accuracy/coverage. Prior dead-block predictors also update the history of a block every time the block is referenced. A prediction about whether a block has died is made based on the individual reference history of each block. However, how a block is accessed in the L1 cache may depend on the control-flow path the program takes, the value or offset of the referenced data in the block, and other parameters. These variations can cause the individual reference history of a block to be irregular and cause problems for existing dead-block predictors. To address this problem, we propose a new class of dead-block predictors for the L1 cache that predict dead blocks using the cache burst history of each block. A cache burst begins when a block moves into the MRU position and ends when it moves out of the MRU position. In these new deadblock prediction schemes, the contiguous references a block receives in the MRU position are grouped into one cache burst. In Figure <ref type="figure" target="#fig_0">1</ref>, block A receives two cache bursts. A prediction about whether a block has died is made only when it becomes non-MRU, using the block's cache burst history. Because cache burst history hides the irregularity in individual references, it is easier to predict than individual reference history for L1 caches. The downside of this approach is that dead-block predictions are made later than the point at which blocks actually die (the last reference). Cache bursts can be used with trace-based, counting-based, or time-based predictors. This paper evaluates cache bursts as a strategy for improving counting-based and trace-based predictors.</p><p>Cache bursts only work well at the L1 cache. For the L2, counting-based predictors work best. We improve upon a previously proposed counting-based prediction scheme by addressing its limitations caused by reference count variation.</p><p>Compared to prior schemes, the new predictors show significant improvement in prediction accuracy and coverage while only lose approximately 1/n th of the dead time, where n is the associativity of the cache. When used in a two-way L1 cache, a trace-based cache burst predictor can correctly identify 96% of the dead blocks with a 96% accuracy and a counting-based cache burst predictor can correctly identify 86% of the dead blocks, also with a 96% accuracy. For a 16-way L2 cache, the improved counting-based predictor can identify 67% of the dead blocks with a 89% accuracy.</p><p>These improved dead-block predictors are used in several ways to improve cache efficiency. Like prior work, they are used for replacement optimization and for bypassing zeroreuse blocks at the L2 cache and for prefetching into dead blocks at the L1 cache. They are also used for prefetching into dead blocks at the L2 cache, which has not been studied by prior work. The results show bypassing and replacement optimization offer mostly overlapped benefit, with both techniques achieving similar results of approximately 5% performance improvement on the same set of applications. In contrast, prefetching into dead blocks increases the L1 efficiency from 8% to 17% and the L2 efficiency from 17% to 27%. The improved cache efficiency translates into higher overall performance: prefetching into dead blocks outperform the same prefetch scheme without dead block prediction by 12% at the L1 and by 13% at the L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work on Dead Block Prediction</head><p>Dead block prediction can be performed in software <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> or in hardware <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Software solutions pass hints about dead-block information collected through profiling or compiler analysis <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> to the hardware. They are more accurate but usually have lower coverage. Hardware solutions can be classified into two categories: dataaddress based <ref type="bibr" target="#b6">[7]</ref> and PC based <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Compared to data-address based approaches, PC-based approaches require much lower storage overhead. Based on the state the predictor maintains to make predictions, hardware solutions can be classified into three categories: trace-based, counting-based, and time-based.</p><p>Lai et al. were the first to propose the concept of dead-block prediction <ref type="bibr" target="#b15">[16]</ref> and a trace-based dead-block predictor for the L1 cache, called DBP. Because we use DBP in this paper to refer to dead-block prediction in general, to avoid confusion, we use the name Reference Trace Predictor (RefTrace) to denote this predictor. <ref type="foot" target="#foot_0">1</ref> RefTrace records the sequence of instructions that have referenced a block by hashing the PCs of these instructions together. A history table is used to learn which trace values (sequences of references) result in dead blocks by observing the trace value of each evicted block. Blocks brought into the cache by the same instruction but referenced along different paths will have different trace values upon eviction. The different sequences of references conceptually form a tree embedded in the history table, with the root of the tree being the instruction that caused the miss and each leaf indicating dead blocks. Each entry in the history table indicates the likelihood that the corresponding trace value will result in a dead block. Aliasing can occur if one sequence, which results in dead blocks in some cases, is a prefix of other longer sequences.</p><p>Kharbutli and Solihin later proposed a counting-based dead-block predictor called Live Time Predictor <ref type="bibr" target="#b13">[14]</ref>, for L2 caches. In this paper, we use the name RefCount to denote that it is a counting-based predictor. In RefCount, each block in the cache is augmented with a counter that records both how many times the block has been referenced and the PC of the instruction that first missed on the block. When the counter reaches a threshold value, the block is predicted dead. The threshold is dynamically learned using a history table by observing the reference count and recorded PC of each evicted block. Compared to RefTrace, RefCount uses only the PC of the instruction that brought a block into the cache to make predictions, and can not distinguish blocks that are brought into the cache by the same instruction but are referenced by different instruction sequences.</p><p>Hu et al. proposed a time-based dead-block predictor, Timekeeping (TK) <ref type="bibr" target="#b6">[7]</ref>, for the L1 cache. TK dynamically learns the number of cycles a block stays alive and if the block is not accessed in more than twice this number of cycles, it is predicted dead. Abella et al. proposed <ref type="bibr" target="#b0">[1]</ref> another timebased predictor to turn off dead blocks dynamically in the L2 cache. They observed that both the inter-access time between hits to the same block and the dead time correlate with the reference counts of a block. They also predict a block dead if it has not been accessed in a certain number of cycles, but the cycle count is derived from how many times the block has been accessed. Compared to time-based predictors, tracebased and counting-based predictors are easier to implement in hardware and incur less overhead. Also, the traces and reference counts of blocks are more closely correlated to the memory-reference behavior of a program than the cycle count between accesses to the same block.</p><p>These predictors are used in various cache optimizations, including prefetching, replacement, bypassing, power reduction, and coherence protocol optimizations.</p><p>Prefetching: Lai et al. <ref type="bibr" target="#b15">[16]</ref> and Hu et al. <ref type="bibr" target="#b6">[7]</ref> used deadblock prediction to trigger prefetches into dead blocks in the L1 data cache. They found triggering prefetches on dead-block predictions improves the timeliness of prefetching compared to triggering prefetches on cache misses. Ferdman and Falsafi later extended the work in <ref type="bibr" target="#b15">[16]</ref> to store correlation patterns off-chip and stream them on-chip as needed <ref type="bibr" target="#b3">[4]</ref>, which makes it possible to perform correlation-prefetching with large correlation tables.</p><p>Replacement: Kharbutli and Solihin <ref type="bibr" target="#b13">[14]</ref> used deadblock prediction to improve the LRU algorithm by replacing dead blocks first, and also for bypassing the cache. Other approaches optimize LRU replacement without dead-block prediction: Wong and Baer modified the LRU algorithm by replacing blocks with no temporal locality first <ref type="bibr" target="#b28">[29]</ref>, Kampe et al. proposed an Self-Correcting LRU algorithm <ref type="bibr" target="#b11">[12]</ref> to correct LRU replacement mistakes, whereas Qureshi et al. proposed to adaptively place missing blocks into the LRU instead of the MRU position when the working set is larger than the capacity of the cache <ref type="bibr" target="#b19">[20]</ref>.</p><p>Bypassing: Prior work has also used bypassing <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref> to improve cache efficiency. Tyson et al. proposed bypassing based on the hit rate of the missing load/store instruction <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr">Johnson et al.</ref> proposed bypassing based on the reference frequency of the data being referenced <ref type="bibr" target="#b10">[11]</ref> but put bypassed blocks in a separate buffer parallel to the cache. Jalminger and Stenstr?m proposed bypassing based on the reuse distance of the missing block <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr">Gonz?lez et al.</ref> proposed to bypass L1 data cache blocks with low temporal locality <ref type="bibr" target="#b5">[6]</ref>.</p><p>Power reduction: Dead block prediction has also been used to reduce leakage by turning off dead blocks. Kaxiras et al. used dead-block prediction to turn off blocks in the L1 D-cache <ref type="bibr" target="#b12">[13]</ref>. Abella et al. proposed to turn off blocks in the L2 cache dynamically <ref type="bibr" target="#b0">[1]</ref>. Both schemes predict how many cycles have to pass before a block can be turned off without affecting performance. Dead block prediction can also be used in drowsy caches <ref type="bibr" target="#b4">[5]</ref>, to decide which blocks should switch to the drowsy state.</p><p>Coherence protocol optimization: Cache coherence protocols can also benefit from dead-block prediction. Lebeck and Wood proposed dynamic self-invalidation <ref type="bibr" target="#b16">[17]</ref> to reduce the overhead of the cache coherence protocol by invalidating some of the shared cache blocks early. Lai and Falsafi later proposed a last-touch predictor <ref type="bibr" target="#b14">[15]</ref> that uses PC traces to predict when shared cache blocks should be invalidated. Somogyi et al. studied using PC-traces to identify last stores to cache blocks <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cache Efficiency</head><p>The concept of cache efficiency was first proposed by Burger et al. in <ref type="bibr" target="#b1">[2]</ref>, where cache efficiency is defined as the average fraction of the cache blocks that store live data. For any cycle during the execution of a program, some fraction of the blocks in the cache are live. The arithmetic mean of these live block fractions across the execution time of a program, or cache efficiency, can be computed as:</p><formula xml:id="formula_0">E = A?S-1 i=0 U i N ? A ? S<label>(1)</label></formula><p>In Equation <ref type="formula" target="#formula_0">1</ref>, A is the associativity of the cache, S is the number of sets, N is the execution time in cycles, and U i is the total number of cycles for which cache block i is live. Cache efficiency measures the portion of the cache that actually holds useful data; the remaining portion holds useless data and can be vacated to store useful data. Table <ref type="table" target="#tab_0">1</ref> shows the cache efficiency of a set of SPEC2000 benchmarks and several other benchmarks measured using sim-alpha <ref type="bibr" target="#b2">[3]</ref>, which models an Alpha 21264 processor. The geometric mean of the baseline cache efficiency for the L1 data cache and L2 cache, shown in the columns labeled "Baseline" of Table <ref type="table" target="#tab_0">1</ref>, is only 0.08 and 0.17 respectively, indicating the poor utilization of the caches and significant opportunities for improvement.</p><p>The reason cache efficiency is low is that the time a block stays alive in the cache is usually much shorter than the time that it is dead. The interval between the last access to a block and its eviction from the cache is called the dead time of the block. Likewise, the interval between the first access to a block, i.e., the access which brings the block into the cache, and the last access before its eviction, is called the live time. Prior work has shown that the dead time is usually at least one order of magnitude longer than the live time <ref type="bibr" target="#b6">[7]</ref>.</p><p>To improve cache efficiency, a cache must identify dead blocks early and replace them with useful blocks. By identifying dead blocks early with the best dead-block predictors we evaluated for the L1 and L2 caches and replacing them with new blocks through prefetching (discussed later in this paper), the L1 efficiency more than doubles and the L2 efficiency improves by 60%, as shown in the columns labeled "Optimized" in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Identifying Dead Blocks Based on Cache Bursts</head><p>Accesses to the L1 and L2 caches have different characteristics. For example, accesses to the L2 cache are filtered by the L1 so L2 accesses have little spatial locality within a block while L1 accesses can have high spatial locality. These differences should be considered when designing dead-block predictors for each cache level. In this section, we propose new dead-block predictors for the L1 and L2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cache Burst Predictors: Tolerating Irregularity of Individual References in the L1 Cache</head><p>All prior dead-block predictors try to find regular patterns in the individual reference history of each block. However, individual reference histories can be volatile and irregular because how a block is accessed may depend on the control flow path the program takes, the value or offset of the referenced data in the block, and other parameters, all of which can change dynamically and may not show any regular patterns (RefTrace can handle control flow dependence to some extent). This is especially true for the L1 cache because the irregularity can be filtered out by the L1 cache and may not be observed by the L2 cache. Figure <ref type="figure" target="#fig_1">2</ref> shows two examples of reference variance.</p><p>Figure <ref type="figure" target="#fig_1">2</ref>(a) shows how control-flow irregularity can lead to irregular reference history. Suppose the first access to p? value always misses and p?value will not be referenced after the iteration. Depending on whether p?value is zero, the block which has p?value can be accessed either once or twice. However, it is not possible to find a regular pattern in the individual reference history of each block because some of the blocks are referenced only by the load instruction while others are referenced by both the load and the store. </p><formula xml:id="formula_1">? ? ?? ? ??? ? ??? ??? ??? ??? ?? ? ! ? ? ?? ? ??? " #### ?? ?$ ?% # ?&amp; #### ?''( ? ?) '% " ?0 ' " ?0 ! ) ? 1 ?2 $( ? ??$ 3 45 2 ? ( 3 45 2 ? '' % 34? 6 ? ! ) ? Figure 2.</formula><p>Examples of irregular accesses to L1 blocks eviction. Otherwise, the block that has A[i].a will only be accessed once. Again, it is not possible to find a regular pattern in the individual reference history of each block that has A[i].a because some blocks will be accessed only by one load instruction and others will be accessed by both loads.</p><p>This irregularity in individual reference history can cause problems for existing dead-block predictors: neither RefCount nor RefTrace can handle the two examples in Figure <ref type="figure" target="#fig_1">2</ref> well because neither can predict exactly after which access a block becomes dead.</p><p>The problem with trying to find regular patterns in the individual reference history of each block is that the predictor observes events at excessively fine granularity. Because L1 cache accesses tend to be bursty in the sense that several accesses to the same block are usually clustered in a short interval, an effective strategy is to predict dead blocks by cache bursts instead of individual references. We formally define cache bursts as follows:</p><p>Definition A cache burst is the contiguous group of cache accesses a block receives while it is in the MRU position of its cache set with no intervening references to any other block in the same set.</p><p>Although the references within a cache burst may be irregular, the cache-burst history can still be regular. Examining the two examples using bursts, there still is a regular pattern. In Figure <ref type="figure" target="#fig_1">2</ref>(a), the block containing p?value will become dead after exactly one cache burst, regardless of whether p? value is zero. In Figure <ref type="figure" target="#fig_1">2</ref> Based on this observation, we propose a new class of dead-block predictors that predict based on cache bursts, not references, of each block. Cache bursts begin when a block moves into the MRU position and end when it moves out of the MRU position, at which point a dead block prediction is made, typically 1/n th into the dead time, where n is the set associativity.</p><p>A Burst Counting Predictor (BurstCount) uses the same structure as a reference counting predictor except that it counts cache bursts instead of individual references. When a block is filled into the MRU position of its set, its burst count is set to 0. The burst count is incremented only when the block moves from a non-MRU position into the MRU position. If the block is accessed in the MRU position, the burst count does not change. A prediction is made only when a block becomes non-MRU.</p><p>Similarly, a Burst Trace Predictor (BurstTrace) uses the same structure as a reference trace predictor. The difference is that BurstTrace predicts dead blocks based on the sequence of cache bursts a block has received. In BurstTrace, the trace of a block is updated only when the block moves into the MRU position. If it is accessed in the MRU position, the Figure <ref type="figure" target="#fig_3">3</ref> shows that burst history is more regular than reference history in the L1 cache. Figure <ref type="figure" target="#fig_3">3(a)</ref> shows the reference count distribution of the blocks brought into the L1 D-cache by the same instruction in sphinx. This particular instruction causes the most misses in the L1 D-cache. The X axis is the reference count. The Y axis shows for a given reference count, what percentage of the blocks (out of all the blocks brought into the cache by this instruction) die after that number of references. Figure <ref type="figure" target="#fig_3">3(b)</ref> shows the corresponding burst count distribution for the same instruction. The figures indicate burst count is much more predictable than reference count in the L1 D-cache.</p><p>Besides the higher dead-block prediction accuracy and coverage, burst-based predictors also have much lower power overhead than reference-based predictors. A reference-based predictor needs to read the history table and update the state of the accessed block on every cache access. In contrast, a burst-based predictor only reads the history table when a block becomes non-MRU and updates the state when it becomes MRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By references By bursts Trace</head><p>RefTrace <ref type="bibr" target="#b15">[16]</ref> BurstTrace Prediction Counting RefCount <ref type="bibr" target="#b13">[14]</ref> BurstCount metric Time TimeKeeping <ref type="bibr" target="#b6">[7]</ref>, IATAC <ref type="bibr" target="#b0">[1]</ref> Future work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. A taxonomy of dead-block prediction schemes</head><p>The introduction of cache bursts adds a new dimension to the design space of dead-block predictors. Based on the metric used to make dead-block predictions, dead-block predictors can be classified into trace-based, counting-based, and time-based. Based on how the state of a block is updated, dead-block predictors can be classified into reference-based and burst-based. Table <ref type="table">2</ref> classifies the possible dead-block predictors using this taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tuning Reference Counting for the L2 Cache</head><p>While burst-based predictors work well for the L1 cache, they do not benefit the L2 cache because most of the irregularity in individual references has already been filtered out by the L1. Prior work <ref type="bibr" target="#b13">[14]</ref> found counting-based predictors are better suited for the L2 than trace-based predictors because the filtering effect of the L1 prevents trace-based predictors from seeing the complete reference history of a block. One problem with counting-based dead-block predictors is reference count variation: blocks brought into the cache by the same instruction can receive different number of references in the cache.</p><p>To handle reference count variation, RefCount uses a confidence bit in each entry of the history table: when a block is evicted from the cache, its reference count is compared with the threshold stored in the history table. The confidence bit is set if the new reference count equals the old threshold and cleared otherwise. The threshold in the history table will not be used for prediction if the confidence bit is cleared.</p><p>One problem with this mechanism is that it can clear the confidence bit unnecessarily: the confidence bit will be cleared whenever a smaller reference count follows a larger reference count. In an extreme case where the reference count alternates between two different values, the confidence bit will never be set. A better way to handle such cases is to continue to use the larger reference count as the threshold without clearing the confidence bit, if the smaller reference count is only temporary. This can be achieved by an additional counter, filter cnt, and a saturating counter, sat cnt. A smaller reference count is first stored in filter cnt and changes the threshold only when sat cnt saturates.</p><p>Another issue is how to keep the history information current. As shown in Figure <ref type="figure">4</ref>(a), in RefCount, each block copies the threshold and confidence bit from the history table when the block is filled into the cache and uses the copied information to make predictions thereafter. However, the threshold and confidence bit stored in each block can become outdated as the history table gets updated. A better approach is to remove the threshold and confidence bit stored in each block. Instead, when the predictor predicts, it uses the threshold and confidence bit from the history table, which has the most up-to-date information. This optimization reduces the area overhead but increases the frequency the history table is accessed. The increased accesses to the history table adds little energy overhead because the L2 cache is accessed only when L1 caches miss. Additionally, when used at the L1 With the inclusion of these two changes, we call the resulting predictor RefCount+, as shown in Figure <ref type="figure">4</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Timeliness vs. Accuracy/Coverage: When to Predict</head><p>One question not answered by prior work is the best time to make dead-block predictions. The dead time of a block begins with the last access to the block and ends with its eviction from the cache. Dead block prediction can be made at any point in this interval. Almost all prior dead-block prediction schemes predict whether a block has died immediately after it is referenced, when the block is still in the MRU position. Higher prediction accuracy and coverage can be achieved if dead-block predictions are made later because it is less likely to make premature predictions. At the same time, predictions made closer to the end of a block's dead time are less useful because they leave the majority of the dead time exposed.</p><p>Figure <ref type="figure">5</ref> shows the accuracy and coverage of the Ref-Count+ predictor when dead-block predictions are made at different depths of the LRU stack after a block's last access. The results are obtained using sim-alpha with a 4-way, 64KB L1 cache. Other parameters of the simulation are listed in Table <ref type="table" target="#tab_4">4</ref>. The X axis shows the average number of cycles between the last access to a block and its movement into each position of the LRU stack. The last number on the X axis is the average number of cycles between the last access to a block and its eviction from the cache, i.e., the dead time. As expected, accuracy increases as predictions are made later. Coverage also increases because delaying the prediction does not miss any opportunity to identify dead blocks and the increase in accuracy causes more dead blocks to be correctly identified. The "knee" of the curves is located at way one of the LRU stack, indicating that predicting when a block just becomes non-MRU gives the best tradeoff between timeliness and accuracy/coverage. The same study of a 16way L2 cache shows a similar trend except that the difference in prediction accuracy and coverage between way one and the LRU position is larger because of the higher associativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation: Dead Block Prediction Accuracy and Coverage</head><p>We compare the prediction accuracy and coverage of various dead block predictors. Coverage is measured as the number of blocks evicted from the cache that are correctly predicted dead divided by the total number of cache evictions. Accuracy is measured as the number of correct dead-block predictions divided by the total number of dead-block predictions ever made by each predictor. The evaluation uses both  single-threaded benchmarks running on a single processor and multi-threaded benchmarks running on a CMP. We first compare the overhead of each predictor, listed in Table <ref type="table" target="#tab_3">3</ref>. The overhead of each predictor includes the history table and the extra bits added to each block. The size of RefCount is scaled down from <ref type="bibr" target="#b13">[14]</ref> to make it comparable with other predictors. It uses a 2K-entry history table; the index into the table is a hash with 8 bits from the PC and 3 bits from the block address. When calculating the predictor overhead, we assume a 64KB L1 D-cache and a 1MB L2 cache, both with 64-byte blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Single-threaded workloads.</head><p>The results for singlethreaded workloads are collected using sim-alpha <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table" target="#tab_4">4</ref> shows the configuration of the simulated machine.</p><p>Besides the 11 benchmarks from SPEC 2000, we also use two benchmarks from Versabench <ref type="bibr" target="#b20">[21]</ref> (corner turn and vpenta), a speech recognition application (sphinx), and stream <ref type="bibr" target="#b17">[18]</ref>. For each benchmark, we simulate up to 2 billion instructions identified by SimPoint <ref type="bibr" target="#b23">[24]</ref>.</p><p>Table <ref type="table" target="#tab_5">5</ref> lists the prediction coverage and accuracy of each predictor used at the L1 D-cache. We can draw several conclusions from Table <ref type="table" target="#tab_5">5</ref>. First, both burst-based predictors (BurstTrace, BurstCount) significantly outperform the corresponding reference-based predictors (RefTrace, RefCount+): BurstTrace makes 50% more correct predictions than Ref-Trace with higher accuracy, and BurstCount makes 25% more correct predictions than RefCount+ with the same accuracy. The improvement in dead-block prediction comes with much reduced power consumption and no increase in area. Second, the optimizations to RefCount lead to better prediction coverage with higher accuracy: RefCount+ makes 13% more correct predictions than RefCount, with higher accuracy (96% vs. 91%). Third, of the five predictors listed in Table <ref type="table" target="#tab_5">5</ref>, BurstTrace incurs the smallest overhead but has the best coverage and accuracy, making it the best predictor for the L1 D-cache.</p><p>Table <ref type="table">6</ref> shows the coverage and accuracy of the L2 dead-block predictors. We only compare reference-based predictors because burst-based predictors do not work as well at the L2 cache. Here, the two counting-based predictors (RefCount, RefCount+) both outperform the trace-based predictor (RefTrace), corroborating the findings in <ref type="bibr" target="#b13">[14]</ref>. Of the two counting-based predictors (RefCount, RefCount+), RefCount+ has significantly higher accuracy (89% vs. 64%) and also higher coverage because of its ability to handle reference count variation better, as discussed in subsection 4.2. Additionally, RefCount+ also incurs the smallest overhead, making it the best choice for the L2 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Multi-threaded workloads.</head><p>We also present results for a set of server and parallel workloads collected using MP-sauce <ref type="bibr" target="#b8">[9]</ref>. MP-sauce is an execution-driven, full-system simulator derived from IBM's SimOS-PPC. The timing model is based on sim-outorder in SimpleScalar with additional changes to model CMPs. The main parameters of the simulated machine are listed in Table <ref type="table" target="#tab_6">7</ref>.</p><p>We evaluate three commercial applications (SPECWeb99, TPC-W, and SPECjbb) and five scientific applications from SPLASH-2 <ref type="bibr" target="#b29">[30]</ref>. Because of the cache coherence protocol, the definition of prediction coverage for multi-threaded workloads differs slightly from the definition used for single-threaded workloads. Multiprocessor coverage is measured by the total number of blocks predicted dead when evicted from the cache or invalidated by the coherence protocol divided by the total number of evictions and invalidations.</p><p>Table <ref type="table" target="#tab_8">8</ref> shows the prediction coverage and accuracy of each predictor used at the L1 data cache. For multi-threaded benchmarks, the benefit of using burst history over individ-ual access history is more pronounced: BurstTrace makes 70% more correct predictions than RefTrace and BurstCount makes 40% more correct predictions than RefCount+, both with higher accuracy. Again, RefCount+ significantly outperforms RefCount with higher coverage and accuracy because of its ability to handle reference count variation better. Of the five predictors, BurstTrace is still the best choice because of its highest coverage, lowest overhead, and close to highest accuracy.</p><p>Table <ref type="table" target="#tab_7">9</ref> shows the prediction coverage and accuracy of the L2 dead-block predictors. For the two counting-based predictors (RefCount, RefCount+), the prediction coverage and accuracy are much lower compared to those for the single-threaded workloads. This effect results from cache invalidations caused by the coherence protocol, which makes prediction harder. Although RefCount+ still has the highest accuracy and significantly outperforms RefCount, its coverage is only about 27%. Another phenomenon is that RefTrace has the highest coverage of the three predictors. This effect is also caused by the cache coherence protocol: the L2 caches in a CMP sees more accesses (upgrade requests, for example) from the L1 which would otherwise be filtered by the L1 cache in a single processor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Using Dead Block Prediction To Improve Performance</head><p>There are several distinct ways to use dead-block prediction to improve performance. A conservative approach, including replacement optimization and bypassing, only evicts dead blocks early to give other blocks more opportunities to get reused. A more aggressive approach prefetches new blocks into dead blocks to reduce future demand misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evicting Dead Blocks Early</head><p>With LRU replacement, blocks with poor locality can stay in the cache too long and cause blocks with good locality to be replaced. Once a block is dead, it can be evicted from the cache before it becomes LRU. Early dead-block eviction gives other blocks that are located lower on the LRU stack more opportunities to get reused. However, if these blocks do not receive additional references, evicting dead blocks early does not improve performance.</p><p>One form of early dead-block eviction, replacement optimization, has been studied by prior work <ref type="bibr" target="#b13">[14]</ref>. In replacement optimization, on a cache miss, the hardware first checks if any block in the set has already been predicted dead. If so, the dead block that is closest to the LRU is picked for replacement. If no dead block in the set is found, the LRU block is replaced.</p><p>A more aggressive form of early dead-block eviction, cache bypassing, targets blocks that will not be referenced if they are placed into the cache. Programs that exhibit poor locality or have a working set larger than the capacity of the cache have many such zero-reuse blocks. 32% of the blocks brought into the L1 cache and 40% of the blocks brought into the L2 cache for the single-threaded benchmarks studied in this paper are never reused. Dead block prediction can be used to identify these zero-reuse blocks; if a block causing a miss is predicted dead, it will not be written into the cache.</p><p>We evaluate several early dead-block eviction schemes, the speedups of which are shown in Figure <ref type="figure" target="#fig_4">6</ref>. Like <ref type="bibr" target="#b13">[14]</ref>, all schemes are applied to the L2 cache because evicting dead blocks early at the L1 cache gives only marginal  speedups. The "RefCount:Replace+Bypass" scheme, as described in <ref type="bibr" target="#b13">[14]</ref>, uses the RefCount dead block predictor for replacement and bypassing: on a miss, it first tries to find a dead block for eviction; if no dead block exists, it bypasses the missing block if it is predicted dead; otherwise, the LRU block is chosen for eviction. The "RefCount+:Replace" scheme uses the RefCount+ dead-block predictor just for replacement: on a miss, it first tries to find a block that is predicted dead; if no such block exists, the LRU block is replaced. The "RefCount+:Bypass" scheme uses RefCount+ just for bypassing: if a missing block is predicted dead, it is not written into the cache. The "RefCount+:Bypass+Replace" scheme is similar to "RefCount:Replace+Bypass" but uses RefCount+ for both replacement and bypassing. Figure <ref type="figure" target="#fig_4">6</ref> indicates the four schemes achieve similar speedups of approximately 5% on average. The figure also indicates that the benefits of using dead-block prediction for bypassing and replacement are mostly overlapped: if a program benefits from bypassing, it also benefits similarly from the replacement optimization. Doing bypassing and replacement optimization at the same time does not bring much additional performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improving Prefetching</head><p>Early dead-block eviction by itself has the limitation that successful bypassing or early replacement of dead blocks does not always reduce the cache-miss rate. For programs that do not benefit from early dead-block eviction, a more aggressive technique, which replaces dead blocks with prefetched blocks, can be used.</p><p>While prefetching can be performed without dead-block prediction, using dead-block prediction to trigger prefetches has two benefits. First, dead blocks provide ideal space to store prefetched blocks without causing pollution. Second, the long dead time gives sufficient time for prefetched blocks to arrive at the cache before they are referenced.</p><p>One issue ignored by prior work that uses dead-block prediction for prefetching is how to track prefetched blocks so that the dead-block predictor can predict when these blocks become dead. The prefetch engine can bring many blocks into the cache and these prefetched blocks are not associated with any instruction in a program. Since all the dead-block predictors we study in this work use the PC to make predictions, prefetched blocks will not be predicted dead, preventing further prefetches from being triggered. To address this problem, an extra bit, pc valid, is added to each block to differentiate prefetched blocks from blocks that are caused by demand misses. For prefetched blocks, the pc valid bit is initially set to zero. When a prefetched block is accessed for the first time, its pc valid bit is set to one and the PC of the current instruction is used to update the hashed PC stored along with the block.</p><p>Next, we investigate using dead-block prediction to prefetch into dead blocks at the L1 and L2 caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Baseline prefetch engine.</head><p>We use an existing prefetching scheme, tag correlating prefetching (TCP) <ref type="bibr" target="#b7">[8]</ref> as the baseline prefetch engine. TCP is a correlating prefetcher that was proposed to reduce the penalty of L1 misses but places prefetched data in the L2 cache to avoid polluting the L1 cache. With dead-block prediction at the L1 cache, prefetched data can be directly placed into the L1 cache. Figure <ref type="figure">7</ref> shows how TCP works. Each set maintains the two most recent tags that caused misses to the set. On a miss, a hash of the two tags in the miss history of the accessed set is used as index into the correlation table. If a match is found, the predicted tag is used with the index of the set to form a prefetch address. The correlation table is updated on every cache miss.</p><p>As a correlating prefetcher, TCP can learn arbitrary repetitive miss patterns. TCP also exploits the property that the same sequence of tags are often accessed in different sets, an effect known as constructive aliasing. Constructive aliasing enables TCP to learn access patterns more quickly.</p><p>The prefetch engine is configured as follows: both the L1 and L2 correlation tables are 2-way associative. The L1  <ref type="figure">8</ref> compares the speedups of three L1 prefetching schemes. The baseline TCP prefetches on L1 misses and places prefetched blocks into the LRU position. The second scheme uses the RefTrace dead-block predictor with the baseline TCP. It prefetches when blocks become dead and places prefetched blocks into the space of the dead blocks. This scheme resembles the DBCP scheme because it uses the same dead-block predictor but differs from DBCP in the prefetch engine. The third scheme is similar to the second one except it uses BurstTrace, which works best at  <ref type="figure">8</ref> shows using dead-block prediction to trigger prefetches improves performance for almost all the applications. It also shows BurstTrace outperforms RefTrace when used with the baseline prefetch engine, because of its better dead-block prediction capability. On average, the baseline prefetch engine improves performance by 11%, adding RefTrace improves performance by 16%, and adding BurstTrace improves performance by 23%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Using Dead Block Prediction to Improve L2</head><p>Prefetching. Dead block prediction can also be used to trigger prefetches into L2 caches, which has not been evaluated in prior dead-block prediction work. Applying dead-block prediction to L2 prefetching differs from L1 prefetching in several ways. First, the L2 cache is more tolerant of pollution but L2 misses are much more expensive. Therefore L2 prefetching should be more aggressive. Second, dead-block prediction at the L2 cache has much lower coverage (66%) than at the L1 (96%). This means one third of the dead blocks are not identified by dead-block prediction and triggering prefetches only when dead blocks are identified will miss many opportunities to prefetch. Therefore, besides issuing prefetches when dead blocks are identified in the L2 cache, additional prefetches are issued when the L2 cache misses, to cover the otherwise missed opportunities of those dead blocks that are not identified by dead-block prediction.</p><p>Figure <ref type="figure">9</ref> shows the speedups of two L2 prefetching schemes: the baseline TCP, which prefetches on L2 misses, and the baseline TCP augmented with RefCount, which prefetches both when L2 misses and when blocks become dead in the L2 cache. The figure shows using RefCount to trigger additional prefetches improves performance by 23% compared to the performance improvement of 10% by the baseline prefetch engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The efficacy of the cache is determined by the amount of useful data it stores, not the capacity of the cache. In this paper, we propose several dead-block predictors that identify dead blocks with better accuracy and coverage than prior schemes and use these predictors to eliminate dead blocks and increase the efficacy of the cache.</p><p>For both the L1 and L2 caches, predicting the death of a block when it becomes non-MRU, not immediately after it is accessed, gives the best tradeoff between timeliness and prediction accuracy/coverage. Because of the differences in L1 and L2 accesses, a dead-block predictor should maintain different state in each block to make better dead-block predictions at the L1 and L2 cache.</p><p>For the L1 cache, a dead-block predictor should maintain state about cache bursts, not individual references, to make predictions. Cache bursts are more predictable because they hide the irregularity of individual references. Therefore, burst-based predictors can correctly identify more dead blocks while making fewer predictions. The best burst-based predictor can identify 96% of the dead blocks in the L1 D-cache with a 96% accuracy.</p><p>For the L2 cache, a dead-block predictor should maintain state about reference counts to make predictions. To cope with reference count variation, we optimize an existing predictor by using more up-to-date history information to increase prediction accuracy and filtering out sporadic smaller reference counts to increase prediction coverage. The improved predictor can identify 66% of the dead blocks in the L2 cache with a 89% accuracy.</p><p>We used dead-block prediction to improve performance through replacement optimization, bypassing, and prefetching. Replacement optimization and bypassing eliminate dead blocks only on demand misses whereas prefetching aims to eliminate dead blocks whenever they are identified. On average, replacement optimization or bypassing improves performance by 5% while prefetching into dead blocks brings a 12% performance improvement over the baseline prefetching scheme for the L1 cache and a 13% performance improvement over the baseline prefetching scheme for the L2 cache. These results indicate that it is possible to increase cache efficiency by storing useful data in the space of dead blocks. On the other hand, even after these optimizations, the average cache efficiency is still low (17% for the L1 and 27% for the L2), due to the following reasons: dead blocks identified too late, wrong dead-block predictions, dead blocks not identified, the time spent waiting for correctly prefetched blocks to arrive, and useless prefetches. It remains to be seen how better prefetching schemes can push the cache efficiency even higher.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Predicting dead blocks at different times</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>b) shows how data alignment variation can cause the same problem. Suppose the cache block size is 64 bytes and the access to A[i].a always misses. Because of data alignment differences, A[i].a and A[i].b can be located in the same block or in two adjacent blocks. If they are located in the same block, the block will be accessed twice before</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b), the block containing A[i].a will also become dead after exactly one cache burst, regardless of whether A[i].b is located in the same block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Reference count distribution; (b) Burst count distribution trace does not change. A prediction is made only when the block becomes non-MRU.Figure3shows that burst history is more regular than reference history in the L1 cache. Figure3(a)shows the reference count distribution of the blocks brought into the L1 D-cache by the same instruction in sphinx. This particular instruction causes the most misses in the L1 D-cache. The X axis is the reference count. The Y axis shows for a given reference count, what percentage of the blocks (out of all the blocks brought into the cache by this instruction) die after that number of references. Figure3(b)shows the corresponding burst count distribution for the same instruction. The figures indicate burst count is much more predictable than reference count in the L1 D-cache.Besides the higher dead-block prediction accuracy and coverage, burst-based predictors also have much lower power overhead than reference-based predictors. A reference-based predictor needs to read the history table and update the state of the accessed block on every cache access. In contrast, a burst-based predictor only reads the history table when a block becomes non-MRU and updates the state when it becomes MRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Speedups of using dead-block prediction for replacement/bypassing with a 1MB L2 cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Application</cell><cell cols="2">DL1 efficiency Baseline Optimized</cell><cell cols="2">L2 efficiency Baseline Optimized</cell></row><row><cell>swim</cell><cell>0.02</cell><cell>0.36</cell><cell>0.06</cell><cell>0.10</cell></row><row><cell>mgrid</cell><cell>0.08</cell><cell>0.24</cell><cell>0.18</cell><cell>0.23</cell></row><row><cell>applu</cell><cell>0.07</cell><cell>0.23</cell><cell>0.03</cell><cell>0.07</cell></row><row><cell>gcc</cell><cell>0.05</cell><cell>0.10</cell><cell>0.34</cell><cell>0.55</cell></row><row><cell>art</cell><cell>0.01</cell><cell>0.10</cell><cell>0.12</cell><cell>0.69</cell></row><row><cell>mcf</cell><cell>0.04</cell><cell>0.07</cell><cell>0.05</cell><cell>0.14</cell></row><row><cell>ammp</cell><cell>0.08</cell><cell>0.14</cell><cell>0.05</cell><cell>0.08</cell></row><row><cell>lucas</cell><cell>0.01</cell><cell>0.06</cell><cell>0.01</cell><cell>0.04</cell></row><row><cell>parser</cell><cell>0.33</cell><cell>0.33</cell><cell>0.32</cell><cell>0.33</cell></row><row><cell>perlbmk</cell><cell>0.40</cell><cell>0.40</cell><cell>0.17</cell><cell>0.21</cell></row><row><cell>gap</cell><cell>0.07</cell><cell>0.12</cell><cell>0.07</cell><cell>0.09</cell></row><row><cell>sphinx</cell><cell>0.09</cell><cell>0.10</cell><cell>0.34</cell><cell>0.52</cell></row><row><cell>corner turn</cell><cell>0.02</cell><cell>0.12</cell><cell>0.04</cell><cell>0.05</cell></row><row><cell>stream</cell><cell>0.01</cell><cell>0.21</cell><cell>0.03</cell><cell>0.16</cell></row><row><cell>vpenta</cell><cell>0.01</cell><cell>0.01</cell><cell>0.80</cell><cell>0.80</cell></row><row><cell>GeoMean</cell><cell>0.08</cell><cell>0.17</cell><cell>0.17</cell><cell>0.27</cell></row></table><note><p>. Cache efficiency of a 64KB, 2-way DL1 and a 1MB, 16-way L2 measured using sim-alpha</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Overhead of different dead-block predictors</figDesc><table><row><cell>Issue width</cell><cell>6-way out of order(4 integer, 2 floating point)</cell></row><row><cell>Inst. window</cell><cell>80-entry reorder buffer, 32-entry Load/Store queue</cell></row><row><cell></cell><cell>each</cell></row><row><cell>L1 I-cache</cell><cell>64KB, 2-way LRU, 64B cacheline, 1-cycle w/ set</cell></row><row><cell></cell><cell>prediction</cell></row><row><cell>L1 D-cache</cell><cell>64KB, 2-way LRU, 64B cacheline, 3-cycle</cell></row><row><cell>L2 cache</cell><cell>1MB, 16-way LRU, 64B cacheline, 12-cycle</cell></row><row><cell>Main memory</cell><cell>200-cycle, 16B bus width</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Configuration of simulated SP machine</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Coverage and accuracy of DL1 DBPs (Single-threaded workloads)</figDesc><table><row><cell cols="2">Application</cell><cell cols="3">RefTrace coverage accuracy</cell><cell cols="2">BurstTrace coverage accuracy</cell><cell cols="2">RefCount coverage accuracy</cell><cell cols="2">RefCount+ coverage accuracy</cell><cell>BurstCount coverage accuracy</cell></row><row><cell>swim</cell><cell></cell><cell>0.90</cell><cell>0.96</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>0.78</cell><cell>1.00</cell><cell>0.97</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>mgrid</cell><cell></cell><cell>0.68</cell><cell>0.82</cell><cell></cell><cell>0.98</cell><cell>0.97</cell><cell>0.65</cell><cell>0.96</cell><cell>0.83</cell><cell>1.00</cell><cell>0.91</cell><cell>0.99</cell></row><row><cell>applu</cell><cell></cell><cell>0.45</cell><cell>0.74</cell><cell></cell><cell>0.98</cell><cell>0.96</cell><cell>0.75</cell><cell>0.93</cell><cell>0.78</cell><cell>1.00</cell><cell>0.95</cell><cell>0.99</cell></row><row><cell>gcc</cell><cell></cell><cell>0.65</cell><cell>0.94</cell><cell></cell><cell>0.97</cell><cell>0.99</cell><cell>0.69</cell><cell>0.97</cell><cell>0.74</cell><cell>1.00</cell><cell>0.93</cell><cell>0.99</cell></row><row><cell>art</cell><cell></cell><cell>0.96</cell><cell>0.95</cell><cell></cell><cell>0.99</cell><cell>0.99</cell><cell>0.91</cell><cell>1.00</cell><cell>0.90</cell><cell>1.00</cell><cell>0.97</cell><cell>0.99</cell></row><row><cell>mcf</cell><cell></cell><cell>0.75</cell><cell>0.82</cell><cell></cell><cell>0.99</cell><cell>0.97</cell><cell>0.47</cell><cell>0.98</cell><cell>0.54</cell><cell>0.99</cell><cell>0.93</cell><cell>0.98</cell></row><row><cell>ammp</cell><cell></cell><cell>0.54</cell><cell>0.69</cell><cell></cell><cell>0.94</cell><cell>0.90</cell><cell>0.55</cell><cell>0.95</cell><cell>0.68</cell><cell>0.95</cell><cell>0.77</cell><cell>0.95</cell></row><row><cell>lucas</cell><cell></cell><cell>0.90</cell><cell>0.92</cell><cell></cell><cell>0.97</cell><cell>0.98</cell><cell>0.99</cell><cell>1.00</cell><cell>0.96</cell><cell>1.00</cell><cell>0.88</cell><cell>0.99</cell></row><row><cell>parser</cell><cell></cell><cell>0.17</cell><cell>0.45</cell><cell></cell><cell>0.85</cell><cell>0.84</cell><cell>0.20</cell><cell>0.69</cell><cell>0.29</cell><cell>0.78</cell><cell>0.54</cell><cell>0.83</cell></row><row><cell>perlbmk</cell><cell></cell><cell>0.28</cell><cell>0.85</cell><cell></cell><cell>0.85</cell><cell>0.92</cell><cell>0.54</cell><cell>0.57</cell><cell>0.57</cell><cell>0.80</cell><cell>0.60</cell><cell>0.77</cell></row><row><cell>gap</cell><cell></cell><cell>0.42</cell><cell>0.77</cell><cell></cell><cell>0.96</cell><cell>0.98</cell><cell>0.39</cell><cell>0.97</cell><cell>0.41</cell><cell>1.00</cell><cell>0.88</cell><cell>0.99</cell></row><row><cell>sphinx</cell><cell></cell><cell>0.47</cell><cell>0.66</cell><cell></cell><cell>0.95</cell><cell>0.93</cell><cell>0.27</cell><cell>0.81</cell><cell>0.37</cell><cell>0.89</cell><cell>0.79</cell><cell>0.92</cell></row><row><cell cols="2">corner turn</cell><cell>1.00</cell><cell>0.96</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>0.98</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>stream</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>vpenta</cell><cell></cell><cell>0.98</cell><cell>1.00</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>0.79</cell><cell>0.99</cell><cell>0.98</cell><cell>1.00</cell><cell>0.99</cell><cell>1.00</cell></row><row><cell cols="2">GeoMean</cell><cell>0.61</cell><cell>0.82</cell><cell></cell><cell>0.96</cell><cell>0.96</cell><cell>0.61</cell><cell>0.91</cell><cell>0.69</cell><cell>0.96</cell><cell>0.86</cell><cell>0.96</cell></row><row><cell>Application</cell><cell cols="2">RefTrace cov. accu.</cell><cell cols="2">RefCount cov. accu.</cell><cell cols="2">RefCount+ cov. accu.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>swim</cell><cell>0.61</cell><cell>0.75</cell><cell>0.94</cell><cell>0.99</cell><cell>0.96</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mgrid</cell><cell>0.69</cell><cell>0.80</cell><cell>0.74</cell><cell>0.93</cell><cell>0.85</cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>applu</cell><cell>0.67</cell><cell>0.80</cell><cell>0.82</cell><cell>0.97</cell><cell>0.88</cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gcc</cell><cell>0.23</cell><cell>0.20</cell><cell>0.40</cell><cell>0.34</cell><cell>0.47</cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>art</cell><cell>0.91</cell><cell>0.97</cell><cell>0.89</cell><cell>0.89</cell><cell>0.92</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mcf</cell><cell>0.51</cell><cell>0.72</cell><cell>0.61</cell><cell>0.93</cell><cell>0.73</cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ammp</cell><cell>0.58</cell><cell>0.54</cell><cell>0.52</cell><cell>0.43</cell><cell>0.51</cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lucas</cell><cell>0.73</cell><cell>0.68</cell><cell>0.95</cell><cell>1.00</cell><cell>0.98</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>parser</cell><cell>0.18</cell><cell>0.21</cell><cell>0.15</cell><cell>0.14</cell><cell>0.23</cell><cell>0.56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>perlbmk</cell><cell>0.88</cell><cell>0.69</cell><cell>0.80</cell><cell>0.91</cell><cell>0.85</cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gap</cell><cell>0.38</cell><cell>0.46</cell><cell>0.98</cell><cell>0.99</cell><cell>0.98</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sphinx</cell><cell>0.28</cell><cell>0.54</cell><cell>0.40</cell><cell>0.33</cell><cell>0.37</cell><cell>0.79</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>corner turn</cell><cell>0.41</cell><cell>0.40</cell><cell>0.55</cell><cell>0.43</cell><cell>0.40</cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stream</cell><cell>0.78</cell><cell>0.76</cell><cell>0.98</cell><cell>1.00</cell><cell>0.99</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vpenta</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeoMean</cell><cell>0.51</cell><cell>0.55</cell><cell>0.63</cell><cell>0.64</cell><cell>0.66</cell><cell>0.89</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 6. Coverage and accuracy of L2 DBPs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(Single-threaded workloads)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># of processors</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Issue width</cell><cell></cell><cell cols="2">4-way out of order</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Instruction window</cell><cell cols="5">64-entry RUU, 32-entry Load/store queue</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1 I-cache</cell><cell></cell><cell cols="5">64KB, 2-way LRU, 64B cacheline, 2-cycle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1 D-cache</cell><cell></cell><cell cols="5">64KB, 2-way LRU, 64B cacheline, 2-cycle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L2 cache</cell><cell></cell><cell cols="5">1MB private per core, 8-way LRU, 64B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">cacheline, 13-cycle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Coherence protocol</cell><cell cols="3">Snoop-based MOESI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Main memory</cell><cell></cell><cell>200-cycle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Configuration of simulated MP machine</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Coverage and accuracy of L2 DBPs (Multi-threaded workloads)</figDesc><table><row><cell>Application</cell><cell cols="2">RefTrace cov. accu.</cell><cell cols="2">RefCount cov. accu.</cell><cell cols="2">RefCount+ cov. accu.</cell></row><row><cell>SPECweb</cell><cell>0.59</cell><cell>0.78</cell><cell>0.35</cell><cell>0.44</cell><cell>0.22</cell><cell>0.86</cell></row><row><cell>SPECjbb</cell><cell>0.46</cell><cell>0.58</cell><cell>0.32</cell><cell>0.40</cell><cell>0.23</cell><cell>0.67</cell></row><row><cell>TPC-W</cell><cell>0.45</cell><cell>0.74</cell><cell>0.31</cell><cell>0.38</cell><cell>0.27</cell><cell>0.89</cell></row><row><cell>barnes</cell><cell>0.32</cell><cell>0.69</cell><cell>0.20</cell><cell>0.09</cell><cell>0.19</cell><cell>0.81</cell></row><row><cell>FFT</cell><cell>0.38</cell><cell>0.58</cell><cell>0.10</cell><cell>0.29</cell><cell>0.37</cell><cell>0.57</cell></row><row><cell>lu</cell><cell>0.62</cell><cell>0.69</cell><cell>0.52</cell><cell>0.10</cell><cell>0.37</cell><cell>0.92</cell></row><row><cell>ocean</cell><cell>0.50</cell><cell>0.83</cell><cell>0.33</cell><cell>0.55</cell><cell>0.34</cell><cell>0.92</cell></row><row><cell>radix</cell><cell>0.42</cell><cell>0.59</cell><cell>0.11</cell><cell>0.38</cell><cell>0.23</cell><cell>0.54</cell></row><row><cell>GeoMean</cell><cell>0.46</cell><cell>0.68</cell><cell>0.25</cell><cell>0.28</cell><cell>0.27</cell><cell>0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Coverage and accuracy of DL1 DBPs (Multi-threaded workloads)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>table has 1024 sets and the L2 table has 8192 sets. Each entry in the table is 36 bits. Hence, the L1 correlation table is 9KB and the L2 correlation table is 72KB. Table 4 shows the parameters of the simulated machine.</figDesc><table><row><cell></cell><cell cols="2">Per-set tag miss history</cell><cell>New missing tag</cell><cell></cell></row><row><cell></cell><cell>Tag0</cell><cell>Tag1</cell><cell>Tag2</cell><cell></cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tag</cell><cell>Next_tag Cnt</cell><cell>Tag</cell><cell>Next_tag Cnt</cell><cell>2-way tag correlation table</cell></row><row><cell>=?</cell><cell></cell><cell>=?</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prefetch tag</cell><cell></cell></row><row><cell cols="5">Figure 7. Baseline Tag Correlating Prefetch Engine</cell></row><row><cell cols="5">5.2.2. Using Dead Block Prediction to Improve L1</cell></row><row><cell cols="5">Prefetching. Using dead-block prediction to trigger</cell></row><row><cell cols="5">prefetches into the L1 cache was first proposed by Lai et</cell></row><row><cell cols="5">al. in a scheme called Dead Block Correlating Prefetching</cell></row><row><cell cols="5">(DBCP) [16]. DBCP triggers prefetches when dead blocks</cell></row><row><cell cols="5">are identified in the L1 cache, not when the L1 cache</cell></row><row><cell cols="5">misses, to reduce pollution and improve the timeliness of</cell></row><row><cell cols="5">prefetching. DBCP requires a large correlation table (a 2MB</cell></row><row><cell cols="5">table for a 32KB directly-mapped L1 D-cache) because it</cell></row><row><cell cols="5">records correlation of full block addresses, compared to</cell></row><row><cell cols="5">Tag Correlating Prefetching, which records correlation of</cell></row><row><cell cols="5">cache-line tags and needs a much smaller table (9KB for a</cell></row><row><cell cols="5">64KB two-way L1 cache). Tag correlation is smaller because</cell></row><row><cell cols="5">one entry of tag correlation can represent multiple entries</cell></row><row><cell cols="5">of full block address correlation, at the cost of potential</cell></row><row><cell>aliasing.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>RefTrace was evaluated on directly mapped caches in<ref type="bibr" target="#b15">[16]</ref>. This paper uses it on set-associative caches. In contrast to this study which evaluates RefTrace in the MRU position,<ref type="bibr" target="#b3">[4]</ref> evaluated RefTrace in the LRU position.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Babak Falsafi</rs>, who contributed a lot to this work. The authors also thank members of the <rs type="institution">CART research group</rs> and the anonymous reviewers for their valuable feedback on this paper. This work is supported by the <rs type="funder">Defense Advanced Research Projects Agency</rs> under contract <rs type="grantNumber">F33615-03-C-4106</rs> and by <rs type="funder">NSF</rs> grant <rs type="grantNumber">EIA-0303609</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5DVBhqp">
					<idno type="grant-number">F33615-03-C-4106</idno>
				</org>
				<org type="funding" xml:id="_AWS6Rar">
					<idno type="grant-number">EIA-0303609</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IATAC: a smart predictor to turn-off L2 cache lines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F P</forename><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The declining effectiveness of dynamic caching for general-purpose microprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>K?gi</surname></persName>
		</author>
		<idno>1261</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Madison</publisher>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences Department, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sim-alpha: a validated, execution-driven alpha 21264 simulator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno>TR-01-23</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences Department, University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Last-touch correlated data streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>the International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Drowsy caches: simple techniques for reducing leakage power</title>
		<author>
			<persName><forename type="first">K</forename><surname>Flautner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A data cache with multiple caching strategies tuned to different types of locality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliagas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Supercomputing</title>
		<meeting>the 9th International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Timekeeping in the memory system: Predicting and optimizing memory behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Computer Architecture</title>
		<meeting>the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TCP: Tag correlating prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Symposium on High Performance Computer Architecture</title>
		<meeting>the 9th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hardware techniques to reduce communication costs in multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Sciences, University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel approach to cache block reuse prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jalminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 International Conference on Parallel Processing</title>
		<meeting>the 2003 International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Run-time cache bypassing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1338" to="1354" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-correcting LRU replacement policies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st conference on Computing frontiers</title>
		<meeting>the 1st conference on Computing frontiers</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cache decay: Exploiting generational behavior to reduce cache leakage power</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
		<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Counter-based cache replacement and bypassing algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective, accurate, and timely self-invalidation using last-touch prediction</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Symposium on Computer Architecture</title>
		<meeting>the 27th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead block correlating prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
		<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic self-invalidation: Reducing coherence overhead in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 22nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory bandwidth and machine balance in current high performance computers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccalpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Technical Committee on Computer Architecture (TCCA) Newsletter</title>
		<imprint>
			<date type="published" when="1995-12">December 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling live and dead lines in cache memory systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thi?baut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high-performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Versatility and versabench: A new metric and a benchmark suite for flexible architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rabbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<idno>MIT-LCS-TM-646</idno>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
		<respStmt>
			<orgName>MIT Techincal Memo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Utilizing resue information in data cache management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Supercomputing</title>
		<meeting>the 12th International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cooperative caching with keep-me and evict-me</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkiteswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th IEEE Annual Workshop on the Interaction between Compilers and Computer Architectures</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Basic block distribution analysis to find periodic behavior and simulation points in applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 International Conference on Parallel Architecture and Compilation Techniques</title>
		<meeting>the 2001 International Conference on Parallel Architecture and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory coherence activity prediction in commercial workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on Memory Performance Issues</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chip-multiprocessing and beyond</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote at 12th International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A modified approach to data cache management</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Microarchitecture</title>
		<meeting>the 28th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using the compiler to improve cache replacement decisions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Weems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modified LRU policies for improving second-level cache behavior</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Symposium on High Performance Computer Architecture</title>
		<meeting>6th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The SPLASH-2 programs: Characterization and methodological considerations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 22nd Annual International Symposium on Computer Architecture</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
