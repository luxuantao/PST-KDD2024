<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yusu</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Urwa</forename><surname>Muaz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><surname>Won</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tandon School of Engineering New York University</orgName>
								<address>
									<addrLine>6 MetroTech Center Brooklyn</addrLine>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tandon School of Engineering New York University</orgName>
								<address>
									<addrLine>6 MetroTech Center Brooklyn</addrLine>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<addrLine>60 Fifth Avenue New York</addrLine>
									<postCode>10012</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<addrLine>251 Mercer St</addrLine>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gender bias exists in natural language datasets which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity by much. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach, and show that it outperforms existing strategies in all bias evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Processing (NLP) models are shown to capture unwanted biases and stereotypes found in the training data which raise concerns about socioeconomic, ethnic and gender discrimination when these models are deployed for public use <ref type="bibr" target="#b6">(Lu et al., 2018;</ref><ref type="bibr" target="#b10">Zhao et al., 2018)</ref>.</p><p>There are numerous studies that identify algorithmic bias in NLP applications. <ref type="bibr" target="#b5">Lapowsky (2018)</ref> showed ethnic bias in Google autocomplete suggestions whereas <ref type="bibr" target="#b4">Lambrecht and Tucker (2018)</ref> found gender bias in advertisement delivery systems. Additionally, <ref type="bibr" target="#b10">Zhao et al. (2018)</ref> demonstrated that coreference resolution systems exhibit gender bias.</p><p>Language modelling is a pivotal task in NLP with important downstream applications such as text generation <ref type="bibr" target="#b8">(Sutskever et al., 2011)</ref>. Recent * Yusu Qian and Urwa Muaz contributed equally to the paper.</p><p>studies by <ref type="bibr" target="#b6">Lu et al. (2018)</ref> and <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref> have shown that this task is vulnerable to gender bias in the training corpus. Two prior works focused on reducing bias in language modelling by data preprocessing <ref type="bibr" target="#b6">(Lu et al., 2018)</ref> and word embedding debiasing <ref type="bibr" target="#b1">(Bordia and Bowman, 2019)</ref>. In this study, we investigate the efficacy of bias reduction during training by introducing a new loss function which encourages the language model to equalize the probabilities of predicting gendered word pairs like he and she. Although we recognize that gender is non-binary, for the purpose of this study, we focus on female and male words.</p><p>Our main contributions are summarized as follows: i) to our best knowledge, this study is the first one to investigate bias alleviation in text generation by direct modification of the loss function; ii) our new loss function effectively reduces gender bias in the language models during training by equalizing the probabilities of male and female words in the output; iii) we show that end-to-end debiasing of the language model can achieve word embedding debiasing; iv) we provide an interpretation of our results and draw a comparison to other existing debiasing methods. We show that our method, combined with an existing method, counterfactual data augmentation, achieves the best result and outperforms all existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, the study of bias in NLP applications has received increasing attention from researchers. Most relevant work in this domain can be broadly divided into two categories: word embedding debiasing and data debiasing by preprocessing. <ref type="bibr" target="#b0">Bolukbasi et al. (2016)</ref> introduced the idea of gender subspace as low dimensional space in an embedding that cap-tures the gender information. <ref type="bibr" target="#b0">Bolukbasi et al. (2016)</ref> and <ref type="bibr" target="#b9">Zhao et al. (2017)</ref> defined gender bias as a projection of gender-neutral words on a gender subspace and removed bias by minimizing this projection. <ref type="bibr" target="#b2">Gonen and Goldberg (2019)</ref> proved that bias removal techniques based on minimizing projection onto the gender space are insufficient. They showed that male and female stereotyped words cluster together even after such debiasing treatments. Thus, gender bias still remains in the embeddings and is easily recoverable. <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref> introduced a cooccurrence based metric to measure gender bias in texts and showed that the standard datasets used for language model training exhibit strong gender bias. They also showed that the models trained on these datasets amplify bias measured on the model-generated texts. Using the same definition of embedding gender bias as <ref type="bibr" target="#b0">Bolukbasi et al. (2016)</ref>, <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref> introduced a regularization term that aims to minimize the projection of neutral words onto the gender subspace. Throughout this paper,we refer to this approach as REG. They found that REG reduces bias in the generated texts for some regularization coefficient values. But, this bias definition is shown to be incomplete by <ref type="bibr" target="#b2">Gonen and Goldberg (2019)</ref>. Instead of explicit geometric debiasing of the word embedding, we implement a loss function that minimizes bias in the output and thus adjust the whole network accordingly. For each model, we analyze the generated word embedding to understand how it is affected by output debiasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embedding Debiasing</head><p>Data Debiasing <ref type="bibr" target="#b6">Lu et al. (2018)</ref> showed that gender bias in coreference resolution and language modelling can be mitigated through a data augmentation technique that expands the corpus by swapping the gender pairs like he and she, or father and mother. They called this Counterfactual Data Augmentation (CDA) and concluded that it outperforms the word embedding debiasing strategy proposed by <ref type="bibr" target="#b0">Bolukbasi et al. (2016)</ref>. CDA doubles the size of the training data and increases time needed to train language models. In this study, we intend to reduce bias during training without requiring an additional data preprocessing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>For the training data, we use Daily Mail news articles released by <ref type="bibr" target="#b3">Hermann et al. (2015)</ref>. This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational <ref type="bibr" target="#b1">(Bordia and Bowman, 2019)</ref>. For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Model</head><p>We use a pre-trained 300-dimensional word embedding, GloVe, by <ref type="bibr" target="#b7">Pennington et al. (2014)</ref>. We apply random search to the hyperparameter tuning of the LSTM language model. The best hyperparameters are as follows: 2 hidden layers each with 300 units, a sequence length of 35, a learning rate of 20 with an annealing schedule of decay starting from 0.25 to 0.95, a dropout rate of 0.25 and a gradient clip of 0.25. We train our models for 150 epochs, use a batch size of 48, and set early stopping with a patience of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>Language models are usually trained using crossentropy loss. Cross-entropy loss at time step t is</p><formula xml:id="formula_0">L CE (t) = − w∈V y w,t log (ŷ w,t ) ,</formula><p>where V is the vocabulary, y is the one hot vector of ground truth and ŷ indicates the output softmax probability of the model.</p><p>We introduce a loss term L B , which aims to equalize the predicted probabilities of gender pairs such as woman and man.</p><formula xml:id="formula_1">L B (t) = 1 G G i log ŷf i ,t ŷm i ,t</formula><p>f and m are a set of corresponding gender pairs, G is the size of the gender pairs set, and ŷ indicates the output softmax probability. We use gender pairs provided by <ref type="bibr" target="#b9">Zhao et al. (2017)</ref>. By considering only gender pairs we ensure that only gender information is neutralized and distribution over semantic concepts is not altered. For example, it will try to equalize the probabilities of congressman with congresswoman and actor with actress but distribution of congressman, congresswoman versus actor, actress will not affected. Overall loss can be written as</p><formula xml:id="formula_2">L = 1 T T t=1 L CE (t) + λL B (t) ,</formula><p>where λ is a hyperparameter and T is the corpus size. We observe that among the similar minima of the loss function, L B encourages the model to converge towards a minimum that exhibits the lowest gender bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Evaluation</head><p>Language models are evaluated using perplexity, which is a standard measure of performance for unseen data. For bias evaluation, we use an array of metrics to provide a holistic diagnosis of the model behavior under debiasing treatment. These metrics are discussed in detail below. In all the evaluation metrics requiring gender pairs, we use gender pairs provided by <ref type="bibr" target="#b9">Zhao et al. (2017)</ref>. This list contains 223 pairs, all other words are considered gender-neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Co-occurrence Bias</head><p>Co-occurrence bias is computed from the modelgenerated texts by comparing the occurrences of all gender-neutral words with female and male words. A word is considered to be biased towards a certain gender if it occurs more frequently with words of that gender. This definition was first used by <ref type="bibr" target="#b9">Zhao et al. (2017)</ref> and later adapted by <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref>. Using the definition of gender bias similar to the one used by <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref>, we define gender bias as</p><formula xml:id="formula_3">B N = 1 N w∈N log c(w, m) c(w, f ) ,</formula><p>where N is a set of gender-neutral words, and c(w, g) is the occurrences of a word w with words of gender g in the same window. This score is designed to capture unequal co-occurrences of neutral words with male and female words. Cooccurrences are computed using a sliding window of size 10 extending equally in both directions. Furthermore, we only consider words that occur more than 20 times with gendered words to exclude random effects.</p><p>We also evaluate a normalized version of B N which we denote by conditional co-occurrence bias, B N c . This is defined as</p><formula xml:id="formula_4">B N c = 1 N w∈N log P (w|m) P (w|f ) ,</formula><p>where</p><formula xml:id="formula_5">P (w|g) = c(w, g) c(g) .</formula><p>B N c is less affected by the disparity in the general distribution of male and female words in the text. The disparity between the occurrences of the two genders means that text is more inclined to mention one over the other, so it can also be considered a form of bias. We report the ratio of occurrence of male and female words in the model generated text, GR, as</p><formula xml:id="formula_6">GR = c(m) c(f ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Causal Bias</head><p>Another way of quantifying bias in NLP models is based on the idea of causal testing. The model is exposed to paired samples which differ only in one attribute (e.g. gender) and the disparity in the output is interpreted as bias related to that attribute. <ref type="bibr" target="#b10">Zhao et al. (2018)</ref> and <ref type="bibr" target="#b6">Lu et al. (2018)</ref> applied this method to measure bias in coreference resolution and <ref type="bibr" target="#b6">Lu et al. (2018)</ref> also used it for evaluating gender bias in language modelling. Following the approach similar to Lu et al. ( <ref type="formula">2018</ref>), we limit this bias evaluation to a set of gender-neutral occupations. We create a list of sentences based on a set of templates. There are two sets of templates used for evaluating causal occupation bias (Table <ref type="table" target="#tab_0">1</ref>). The first set of templates is designed to measure how the probabilities of occupation words depend on the gender information in the seed. Below is an example of the first set of templates:</p><formula xml:id="formula_7">[Gendered word] is a | [occupation] .</formula><p>Here, the vertical bar separates the seed sequence that is fed into the language models from the target occupation, for which we observe the output softmax probability. We measure causal occupation bias conditioned on gender as</p><formula xml:id="formula_8">CB|g = 1 |O| 1 G o∈O G i log p(o|f i ) p(o|m i ) ,</formula><p>where O is a set of gender-neutral occupations and G is the size of the gender pairs set. For example, P (doctor|he) is the softmax probability of Causal occupation bias conditioned on occupation is represented as</p><formula xml:id="formula_9">CB|o = 1 |O| 1 G o∈O G i log p(f i |o) p(m i |o) ,</formula><p>where O is a set of gender-neutral occupations and G is the size of the gender pairs set. For example, P (man|doctor) is the softmax probability of man where the seed sequence is The doctor is a.</p><p>We believe that both CB|g and CB|o contribute to gender bias in the model-generated texts. We also note that CB|o is more easily influenced by the general disparity in male and female word probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Word Embedding Bias</head><p>Our debiasing approach does not explicitly address the bias in the embedding layer. Therefore, we use gender-neutral occupations to measure the embedding bias to observe if debiasing the output layer also decreases the bias in the embedding. We define the embedding bias, EB d , as the difference between the Euclidean distance of an occupation word to male words and the distance of the occupation word to the female counterparts. This definition is equivalent to bias by projection described by <ref type="bibr" target="#b0">Bolukbasi et al. (2016)</ref>. We define EB d as</p><formula xml:id="formula_10">EB d = o∈O G i | E(o) − E(m i ) 2 − E(o) − E(f i ) 2 | ,</formula><p>where O is a set of gender-neutral occupations, G is the size of the gender pairs set and E is the word-to-vector dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Existing Approaches</head><p>We apply CDA where we swap all the gendered words using a bidirectional dictionary of gender pairs described by <ref type="bibr" target="#b6">Lu et al. (2018)</ref>. This creates a dataset twice the size of the original data, with exactly the same contextual distributions for both genders and we use it to train the language models.</p><p>We also implement the bias regularization method of <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref> which debiases the word embedding during language model training by minimizing the projection of neutral words on the gender axis. We use hyperparameter tuning to find the best regularization coefficient and report results from the model trained with this coefficient. We later refer to this strategy as REG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Initially, we measure the co-occurrence bias in the training data. After training the baseline model, we implement our loss function and tune for the λ hyperparameter. We test the existing debiasing approaches, CDA and REG, as well but since <ref type="bibr" target="#b1">Bordia and Bowman (2019)</ref> reported that results fluctuate substantially with different REG regularization coefficients, we perform hyperparameter tuning and report the best results in Table <ref type="table" target="#tab_1">2</ref>. Additionally, we implement a combination of our loss function and CDA and tune for λ. Finally, bias evaluation is performed for all the trained models. Causal occupation bias is measured directly from the models using template datasets discussed above and co-occurrence bias is measured from the model-generated texts, which consist of 10,000 documents of 500 words each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Results for the experiments are listed in Table <ref type="table" target="#tab_1">2</ref>. It is interesting to observe that the baseline model amplifies the bias in the training data set as measured by B N and B N c . From measurements using the described bias metrics, our method effectively mitigates bias in language modelling with- We notice that all methods result in GR around 1, indicating that there are near equal amounts of female and male words in the generated texts. In our experiments we note that with increasing λ, the bias steadily decreases and perplexity tends to slightly increase. This indicates that there is a trade-off between bias and perplexity.</p><p>REG is not very effective in mitigating bias when compared to other methods, and fails to achieve the best result in any of the bias metrics that we used. But REG results in the best perplexity and even does better than the baseline model in this respect. This indicates that REG has a slight regularization effect. Additionally, it is interesting to note that our loss function outperforms REG in EB d even though REG explicitly aims to reduce gender bias in the embeddings. Although our method does not explicitly attempt geometric debiasing of the word embedding, the results show that it results in the most debiased embedding as compared to other methods. Furthermore, <ref type="bibr" target="#b2">Gonen and Goldberg (2019)</ref> emphasizes that geometric gender bias in word embeddings is not completely understood and existing word embedding debiasing strategies are insufficient. Our approach provides an appealing end-to-end solution for model debiasing without relying on any measure of bias in the word embedding. We believe this concept is generalizable to other NLP applications.</p><p>Our method outperforms CDA in CB|g, CB|o, and EB d . While CDA achieves slightly better results for co-occurrence biases, B N and B N c , and results in a better perplexity. With a marginal differences, our results are comparable to those of CDA and both models seem to have similar bias mitigation effects. However, our method does not require a data augmentation step and allows training of an unbiased model directly from biased datasets. For this reason, it also requires less time to train than CDA since its training data has a smaller size without data augmentation. Furthermore, CDA fails to effectively mitigate occupation bias when compared to our approach. Although the training data for CDA does not contain gender bias, the model still exhibits some gender bias when measured with our causal occupation bias metrics. This reinforces the concept that some model-level constraints are essential to debiasing a model and dataset debiasing alone cannot be trusted.</p><p>Finally, we note that the combination of CDA and our loss function outperforms all the methods in all measures of biases without compromising perplexity. Therefore, it can be argued that a cascade of these approaches can be used to optimally debias the language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this research, we propose a new approach for mitigating gender bias in neural language models and empirically show its effectiveness in reducing bias as measured with different evaluation metrics. Our research also highlights the fact that debiasing the model with bias penalties in the loss function is an effective method. We emphasize that loss function based debiasing is powerful and gen-eralizable to other downstream NLP applications. The research also reinforces the idea that geometric debiasing of the word embedding is not a complete solution for debiasing the downstream applications but encourages end-to-end approaches to debiasing.</p><p>All the debiasing techniques experimented in this paper rely on a predefined set of gender pairs in some way. CDA used gender pairs for flipping, REG uses it for gender space definition and our technique uses them for computing loss. This reliance on pre-defined set of gender pairs can be considered a limitation of these methods. It also results in another concern. There are gender associated words which do not have pairs, like pregnant. These words are not treated properly by techniques relying on gender pairs. Future work includes designing a context-aware version of our loss function which can distinguish between the unbiased and biased mentions of the gendered words and only penalize the biased version. Another interesting direction is exploring the application of this method in mitigating racial bias which brings more challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example templates of two types of occupation bias the word doctor where the seed sequence is He is a. The second set of templates like below, aims to capture how the probabilities of gendered words depend on the occupation words in the seed.</figDesc><table><row><cell>He is a | She is a | s 1 s 2</cell><cell>doctor log P (t|s 1 ) P (t|s 2 ) t</cell><cell>s The doctor is a |</cell><cell>man t 1 woman t 2</cell><cell>log P (t 1 |s) P (t 2 |s)</cell></row><row><cell cols="2">(a) Occupation bias conditioned on gendered words</cell><cell cols="3">(b) Occupation bias conditioned on occupations</cell></row></table><note>T he [occupation] is a | [gendered word] .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for models trained on Daily Mail and their generated texts out a significant increase in perplexity. At λ value of 1, it reduces B N by 58.95%, B N c by 45.74%, CB|o by 100%, CB|g by 98.52% and EB d by 98.98%. Compared to the results of CDA and REG, it achieves the best results in both occupation biases, CB|g and CB|o, and EB d .</figDesc><table><row><cell>Model</cell><cell>B N</cell><cell>B N</cell><cell>GR</cell><cell>P pl.</cell><cell>CB|o</cell><cell>CB|g</cell><cell>EB d</cell></row><row><cell>Dataset</cell><cell cols="2">0.340 0.213</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="7">0.531 0.282 1.415 117.845 1.447 97.762 0.528</cell></row><row><cell>REG</cell><cell cols="7">0.381 0.329 1.028 114.438 1.861 108.740 0.373</cell></row><row><cell>CDA</cell><cell cols="5">0.208 0.149 1.037 117.976 0.703</cell><cell>56.82</cell><cell>0.268</cell></row><row><cell>λ 0.01</cell><cell cols="5">0.492 0.245 1.445 118.585 0.111</cell><cell>9.306</cell><cell>0.077</cell></row><row><cell>λ 0.1</cell><cell cols="5">0.459 0.208 1.463 118.713 0.013</cell><cell>2.326</cell><cell>0.018</cell></row><row><cell>λ 0.5</cell><cell cols="5">0.312 0.173 1.252 120.344 0.000</cell><cell>1.159</cell><cell>0.006</cell></row><row><cell>λ 0.8</cell><cell cols="5">0.226 0.151 1.096 119.792 0.001</cell><cell>1.448</cell><cell>0.002</cell></row><row><cell>λ 1</cell><cell cols="5">0.218 0.153 1.049 120.973 0.000</cell><cell>0.999</cell><cell>0.002</cell></row><row><cell>λ 2</cell><cell cols="5">0.221 0.157 1.020 123.248 0.000</cell><cell>0.471</cell><cell>0.000</cell></row><row><cell cols="6">λ 0.5 + CDA 0.205 0.145 1.012 117.971 0.000</cell><cell>0.153</cell><cell>0.000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>We are grateful to Sam Bowman for helpful advice, Shikha Bordia, Cuiying Yang, Gang Qian, Xiyu Miao, Qianyi Fan, Tian Liu, and Stanislav Sobolevsky for discussions, and reviewers for detailed feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16 Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4356" to="4364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Identifying and reducing gender bias in word-level language models</title>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ArXiv:1904.03035</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>ArXiv:1903.03862</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;15 Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithmic bias? an empirical study into apparent gender-based discrimination in the display of stem career ads</title>
		<author>
			<persName><forename type="first">Anja</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">E</forename><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Google autocomplete still makes vile suggestions</title>
		<author>
			<persName><forename type="first">Issie</forename><surname>Lapowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Kaiji</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangjing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preetam</forename><surname>Amancharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<idno>ArXiv:1807.11714v1</idno>
		<title level="m">Gender bias in neural natural language processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;11 Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning gender-neutral word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Kaiwei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
