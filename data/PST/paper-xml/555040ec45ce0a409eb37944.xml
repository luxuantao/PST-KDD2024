<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qirong</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ming</forename><surname>Dong</surname></persName>
							<email>mdong@cs.wayne.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhengwei</forename><surname>Huang</surname></persName>
							<email>zhengwei.hg@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Yongzhao</forename><surname>Zhan</surname></persName>
							<email>yzzhan@mail.ujs.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State Univer-sity</orgName>
								<address>
									<postCode>48202</postCode>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Sci-ence and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<postCode>212013</postCode>
									<settlement>Zhenjiang</settlement>
									<region>Jiangsu Province</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State Univer-sity</orgName>
								<address>
									<postCode>48202</postCode>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<postCode>212013</postCode>
									<settlement>Zhenjiang</settlement>
									<country>Jiangsu Province, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">103F3826F4CC324EA5149B6215FE4B84</idno>
					<idno type="DOI">10.1109/TMM.2014.2360798</idno>
					<note type="submission">received March 13, 2014; revised June 27, 2014 and September 16, 2014; accepted September 22, 2014. Date of publication September 29, 2014; date of current version November 13, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affective-salient discriminative feature analysis</term>
					<term>convolutional neural networks</term>
					<term>feature learning</term>
					<term>speech emotion recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on finding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the first stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>next generation of computer system, in which a natural human machine interface enables the automated provision of services that require a good appreciation of the emotional state of a user.</p><p>Although advances have been made recently in automatic SER in terms of speech emotion feature extraction and emotion recognition, robust and accurate SER is still a challenging problem due to complex factors such as the variations of speakers and contents, and environment distortion <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In SER, one of the central research issues is how to extract discriminative, affect-salient features from speech signals <ref type="bibr" target="#b23">[24]</ref>. In this direction, a number of speech emotion features have been proposed in the literature, and they can be roughly classified into four categories <ref type="bibr" target="#b19">[20]</ref>: 1) acoustic features, 2) linguistic features (words and discourse), 3) context information (e.g., subject, gender, and turn-level features representing local and global aspects of the dialogue) <ref type="bibr" target="#b23">[24]</ref>, and 4) hybrid features that combine acoustic features with other information. However, it is unclear if these hand-tuned feature sets can sufficiently and efficiently characterize the emotional content of speech <ref type="bibr" target="#b23">[24]</ref>. Moreover, their performance varies greatly in different scenarios. Finally, automatic extraction of some of these features can be difficult. For example, existing automatic speech recognition (ASR) systems cannot reliably recognize all the verbal content of emotional speech <ref type="bibr" target="#b0">[1]</ref>. Extracting semantic discourse information is even more challenging, which, in many cases, has to be performed manually <ref type="bibr" target="#b43">[44]</ref>.</p><p>Thus, in SER, it is important to explore new strategies that can obtain the optimal feature set that is invariant to nuisance factors while maintaining discrimination with respect to the task of emotion recognition. In this work, we introduce feature learning in SER. Our idea here is inspired by the recent development of deep learning. Deep learning is part of a broader family of machine learning methods based on learning feature representations. It addresses the problem of what makes better representations and how to learn them. In many situations where labeled data is limited or not available, deep learning is shown to have the capability to generate good features, for example, for facial expression recognition <ref type="bibr" target="#b30">[31]</ref> and ASR <ref type="bibr" target="#b42">[43]</ref>.</p><p>In the paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). In CNN, simple features are learned in the lower layers, and affect-salient, discriminative features are obtained in the higher layers. More specifically, CNN has two learning phases. In the first stage, unlabeled samples are used to learn local invariant features (LIF) by a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, the local invariant features are used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features. We propose a novel objective function in SDFA by encouraging feature saliency, orthogonality, and discrimination for SER. Our experimental results on several benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker variation and noise), and outperforms several well-established SER features. The preliminary version of this work was first presented in a shortened form as a conference abstract <ref type="bibr" target="#b13">[14]</ref>. The major contributions of this paper are:</p><p>1) To our best knowledge, this is the first paper introducing feature learning to SER, in which the optimal feature set can be effectively and automatically learned by CNN with a few labeled samples. 2) By introducing a novel objective function in SDFA, we can extract affect-salient features for SER by disentangling emotions from other factors such as speakers and noise. Specifically, the LIF from unsupervised learning are divided into two blocks, related to emotion and other remaining factors, respectively. The emotion-related features are discriminative and robust, leading to great performance improvement on SER. The rest of the paper is organized as follows. We introduce the related work in Section II. Section III presents our CNNbased feature learning algorithm in detail. Section IV describes SER benchmark datasets and reports our experimental results. Conclusions and future directions are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>A typical SER system consists of two components: 1) a front-end processing unit that extracts the appropriate features from the speech data, and 2) a classifier that decides the emotion of the speech utterance. In the following, we first briefly review the classification strategies, and then focus on feature extraction methods, as they are more related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classifiers</head><p>Various types of classifiers have been used for the task of SER, including hidden Markov model <ref type="bibr" target="#b25">[26]</ref>, Gaussian mixture model <ref type="bibr" target="#b37">[38]</ref>, support vector machine (SVM) <ref type="bibr" target="#b22">[23]</ref>, artificial neural networks <ref type="bibr" target="#b3">[4]</ref>, k-nearest neighbor <ref type="bibr" target="#b27">[28]</ref> and many others <ref type="bibr" target="#b28">[29]</ref>. Among these methods, SVM and HMM are widely used in almost all speech-related applications <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, experiments show that each classifier has its own advantages and limitations. In order to combine the merits of different classifiers, aggregating a group of classifiers has also been recently studied <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>Two research issues must be considered in feature extraction for SER. The first one is regarding the Region of Interest (ROI) used for feature extraction. One possible approach is to divide the speech signal into many small intervals, i.e., frames, and construct a local feature vector for each frame. For example, prosodic speech features such as pitch and energy, can be extracted from each interval and are considered as local features <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>. On the other hand, global features such as statistics, can be obtained from the whole speech utterance <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b15">[16]</ref>, which typically have a lower dimension than the local ones, leading to less computing time <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>The second research issue is to determine the most suitable type of features for SER. In general, speech features can be grouped into four categories: 1) acoustic features, 2) linguistic features, 3) context information, and 4) hybrid features that combining acoustic features with other information. Specifically, acoustic features can be further classified into four groups <ref type="bibr" target="#b23">[24]</ref>: continuous features, qualitative features, spectral features, and Teager Energy Operator (TEO)-based features, and all of them have been intensively studied in SER. Koolagudi et al. <ref type="bibr" target="#b14">[15]</ref> examined pitch-related features in Berlin emotion speech corpus. Four emotions from Chinese natural emotional speech corpus including anger, joy, sadness, and neutral are discriminated by combining prosody and voice quality features in <ref type="bibr" target="#b45">[46]</ref>. In <ref type="bibr" target="#b46">[47]</ref>, the amplitude of emotional speech marginal spectrum is extracted. The discrimination ability of TEO for SER is studied in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Linguistic content of the spoken utterance is also an important part of the conveyed emotion. In <ref type="bibr" target="#b33">[34]</ref>, a spotting algorithm that searches for emotional keywords or phrases in the utterances was employed. An alternative procedure for detecting emotions using lexical information is found in <ref type="bibr" target="#b17">[18]</ref>. The context information generally includes subject, gender, and turn-level features representing local and global aspects of the dialogue. They have been investigated in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b10">[11]</ref> on audio affective recognition. More recently, there has been a focus on hybrid features, i.e., the integration of acoustic, linguistic, and context features <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b41">[42]</ref> Although linguistic content of the spoken utterance can help the acoustic emotion features improve the accuracy of SER, current ASR systems still cannot reliably recognize all the verbal content of emotional speech. In addition, context information or transcripts are simply not available in many cases. Thus, the most popular feature representation for SER are acoustic features such as prosodic features (e.g., pitch-related feature, energy-related features and speech rate) and spectral features (e.g., Mel frequency cepstral coefficients (MFCC) and cepstral features). However, due to the tight coupling of speech emotion and other factors of variation such as speaker and other environment distortion, it is unclear if these hand-tuned acoustic features can sufficiently and efficiently characterize the emotional content of speech.</p><p>Recently, it has been shown that unsupervised feature leaning is very helpful for ASR <ref type="bibr" target="#b42">[43]</ref> and image understanding <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For SER, Stuhlsatz et al. <ref type="bibr" target="#b35">[36]</ref> used generatively pre-trained artificial neural networks (ANNs) to learn discriminative features of low dimension and found improvement in both weighted and unweighted recall on multiple emotion corpora. Schmidt and Kim <ref type="bibr" target="#b32">[33]</ref> used deep belief networks (DBNs) to learn high-level features directly from magnitude spectra and achieved good performances on emotional music recognition compared to other feature extraction schemes. More recently, Le et al. <ref type="bibr" target="#b16">[17]</ref> investigated dynamic frame-level modeling with hybrid DBN-HMM classifiers on the FAU Aibo spontaneous emotion corpus <ref type="bibr" target="#b34">[35]</ref> and achieved state-of-the-art results on the 5-class problem. In <ref type="bibr" target="#b5">[6]</ref>, a common emotion-specific mapping rule is learnt from a small set of labeled data in a target domain. Then, newly reconstructed data are obtained by applying this rule on the emotion-specific data in a different domain. Wollmer et al. <ref type="bibr" target="#b40">[41]</ref> presents a method to systematically investigate the number of past and future utterance-level observations that are considered to generate an emotion prediction for a given utterance, and to examine to what extent this temporal bidirectional context contributes to the overall performance.</p><p>While these previous works studied the problem of feature learning for SER using various techniques, their focus is mainly on learning discriminative features from the input. How to learn affect-salient features by disentangling factors of variation, a very important aspect in SER, is not yet addressed. In this paper, we introduce a feature learning framework to SER using CNN, in which we directly deal with the issue of entangle factors of variation through a novel object function that integrates feature saliency, discrimination, orthogonality, and reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEARNING SALIENT FEATURES FOR SER</head><p>In this section, we present our feature learning algorithm using CNN in SER. The architecture of CNN is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which has an input layer, one convolutional layer, one fully connected layer, and a SVM classifier. We use the spectrogram of the speech signal as the input of CNN. The main idea of feature learning is to learn high-level representations from the low-level raw features, and the spectrogram is well-suited for this task. As a low-level feature, spectrogram is widely used in speech recognition and audio-based speaker and gender recognition. For example, in <ref type="bibr" target="#b18">[19]</ref>, the spectrogram is used together with convolutional deep belief networks for various audio classification tasks, e.g., speaker identification, gender classification, and phone classification. In <ref type="bibr" target="#b42">[43]</ref>, spectrogram features are used to conduct speech recognition and achieved solid performance.</p><p>Following the hierarchy of CNN, the features learned at each layer become increasingly invariant to nuisance factors while maintaining affect-salience with respect to the goal of SER. Specifically, the training can be broken down into the following three steps: First, we use a sparse auto-encoder to learn kernels with different scales. Then, the entire emotional spectrogram fragment is convolved with the learned kernels to form a series of feature maps. These feature maps are then subsampled through mean-pooling and stacked into one feature vector as the final output of the convolutional layer. 2) Salient Discriminative Feature Analysis: The local invariant features obtained through the auto-encoder are used as the input to the fully connected layer, in which they are divided into two blocks. While all features are trained to cooperate to reconstruct the input, the affect-salient feature block is also trained to predict the emotion classes based on the labeled samples. Our objective in segregating the features is to disentangle the affect-salient features that learn to encode useful information about speech emotion from nuisance features (that are complementary but not affect-salient). To this end, we propose a novel objective function in SDFA that locally encourages the affect-salient features and non-discriminative features to encode distinct directions of variation in the input space. 3) SVM Training: Finally, the affect-salient features are used as the input to train a linear SVM based on the labeled training data. More specifically, the input layer of CNN in our system has 900 neurons to receive normalized (in range of [0,1]) spectrogram fragments of size . Generally speaking, normalizing the input will help the network converge quicker. The convolutional layer consists of three consecutive operations: convolution with kernels, non-linear activation function, and pooling. The convolutional layer contains 120 kernels with two fixed sizes of and , respectively. The kernel is pretrained by unsupervised auto-encoder feature learning patchwise. The size of the feature maps in the convolutional layer is and , respectively. The feature is given as follows:</p><p>(</p><p>where is the weight matrix (kernel), is a spectrogram fragment ( is the set of the input), and is the bias. In feature pooling, one of the frequently used functions is down-sampling. We perform down-sampling using a mean operation with a window size <ref type="bibr" target="#b1">(2)</ref> where is the size of the pooling window. The output of the convolutional layer is employed as the input of the fullyconnected layer to disentangle factors of variation. The affectsalient feature block of the final feature vector is passed to a SVM classifier to determine the emotion class of the speech utterance. The output layer has 600 fully connected neurons, each neuron corresponding to one feature.</p><p>CNN extracts local invariant features from the input by using SAE and disentangles affect-salient factors from others by using SDFA. In local invariant feature learning, to preserve the neighborhood relations, a neuron in the current layer is only connected to the neurons in a small neighborhood of the previous layer. In early layers, neurons can extract fine-grained features such as upward-slanting or intensity pattern in the spectrogram. In the later layers, higher level features (global features) are learned based more on their relations to the other features and less on their exact locations. In the pooling step, the saliency of the output features to emotion is increased while their sensitivity to speaker variation and environment distortion is reduced. In SDFA, the affect-salient features are learned by encouraging the weight saliency in the local invariant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Invariant Feature Learning</head><p>Unsupervised feature learning has shown impressive results in many applications, e.g., image classification. An auto-encoder is commonly used to learn a compressed representation for a set of data. The auto-encoder has two parts: encoder and decoder <ref type="bibr" target="#b31">[32]</ref>. The encoder learns a function to map an input to a feature vector , and the decoder reconstructs the input by minimizing the reconstruction error. Usually, the learning is done by training each layer individually and using the current layer codes to feed the next layer.</p><p>In our system, the encoder function maps the input spectrogram fragment to a latent representation . The encoder function can be written in the form <ref type="bibr" target="#b2">(3)</ref> where is a nonlinear activation function, typically a logistic sigmoid <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, is the weight matrix, and is the bias vector.</p><p>The decoder function reconstructs based on the feature vector and is written as <ref type="bibr" target="#b3">(4)</ref> where is the decoder's activation function, typically either the identity (yielding linear reconstruction) or the sigmoid. In our work, is selected as a logistic sigmoid (</p><p>).</p><p>is the weight matrix shared with the encoder and is the bias vector. Auto-encoder trains the network by adjusting the parameters on the collected training set to minimize the total reconstruction error <ref type="bibr" target="#b4">(5)</ref> where is the reconstruction error that is computed using squared error . In order to encourage units to maintain a low average activation, an additional penalty term is added into (5) <ref type="bibr" target="#b5">(6)</ref> where is the sparse penalty term, is the average activation of hidden unit (averaged over the training set), denotes the number of active units, and denotes the number of training samples. Non-negative parameter is the sparsity level, and controls the weight of the sparsity penalty term. They are set at 0.05 and 1 respectively in our experiments.</p><p>Kernel Learning: To capture the structure of the input spectrogram at different scales, we consider kernels with multiple sizes. Instead of generating kernel randomly, we pre-train kernels by sparse auto-encoder. Sparse auto-encoder is trained with patches extracted randomly at different locations, the size of which matches that of the convolutional kernels being learned. Assuming that we have different kernel sizes denoted as -by-, , we can get the kernel ( ) after we pre-train independently a sparse auto-encoder for each kernel size using (6).</p><p>In our system, we employ two kernel sizes: and . Fig. <ref type="figure" target="#fig_2">2</ref> shows examples of the learned kernels, in which the hor-izontal axis represents time, and the vertical axis denotes frequency. The kernels are log scaled to the full range of the color map before the visualization. Each kernel corresponds to a rectangular area in the image, and the color of the patch is determined by the element value based on the color index. From Fig. <ref type="figure" target="#fig_2">2</ref>, we can clearly observe the distribution of learned kernels. In general, kernels with a higher frequency have a larger value (indicated by a warmer color), and thus they play a more important role in feature reconstruction.</p><p>Feature Mapping: After we get the kernel ( ), we compute the corresponding feature maps on the whole spectrogram by applying ( ) to each -by-patch of the input spectrogram <ref type="bibr" target="#b6">(7)</ref> where denotes the convolution operation. Then, we perform down-sampling using mean operation with window size and get the feature map: , where denotes the th window. All the pooled features for patch and kernel are stacked into one feature vector with length <ref type="bibr" target="#b7">(8)</ref> where is the number of windows associated with kernel . Since each utterance may contain a different number of patches (segments), we calculate the mean and variance of over all the patches of a given utterance. The result is a feature vector , which has a fixed length for all the utterances (9)</p><p>Finally, we concatenate for all the kernels. That is, the final feature vector generated by the convolutional layer is <ref type="bibr" target="#b9">(10)</ref> and it is used as the input of SDFA to disentangle emotion-salient factors from others. Note that in this stage, CNN is trained with unlabeled data, abundant in real-world speech applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Salient Discriminative Feature Analysis</head><p>Unsupervised learning provides a network for speech signal reconstruction. In SER, the network needs to be trained to learn good features to identify speech emotion classes. SDFA is more task-related than the initial unsupervised training as some training samples are labeled according to the emotion classes. SDFA promotes the disentangling of emotion discriminative factors of variation in the data from other prominent factors that may well dominate the discriminative factors, and separates the factors of the speech that are discriminative with respect to the SER task from factors that characterize speakers and environment distortion.</p><p>While the data is encoded into a single feature vector after unsupervised feature learning, an input is mapped into two distinct blocks of features: one ( ) that encodes affect-salient factors of its input, and one ( ) that encodes all other factors. Both feature blocks are trained to cooperate to reconstruct their common input with a reconstruction loss function <ref type="bibr" target="#b10">(11)</ref> where is the weight matrix in the fully connected network, and is an offset to capture the mean value of .</p><p>Given , the labeled training set with input spectrogram fragment and emotion label , the block is also trained to predict the emotion label when the label is available. The class prediction is given by the logistic regression of the discriminative block , which is learned by the sigmoid function over an affine transformation of the block <ref type="bibr" target="#b11">(12)</ref> where the weight matrix maps the block to prediction for class , and is the class specific bias. The corresponding discriminant component of the overall loss function is <ref type="bibr" target="#b12">(13)</ref> where is the number of emotion classes, and and are the ground truth and the logistic regression output, respectively.</p><p>To encourage and to present different directions of variation in the input , we ask each sensitivity vector of the th discriminant feature to prefer being orthogonal to every sensitivity vector associated with the th non-discriminant feature . This penalty component is denoted as <ref type="bibr" target="#b13">(14)</ref> Since a salient feature for SER is usually a sensitive feature for reconstruction error or discrimination error, the features responding strongly to this property tend to be more important. We measure the saliency for each input as the sum of its weight saliency. Specifically, the saliency for input is defined as <ref type="bibr" target="#b14">(15)</ref> where is the set of weights connected input and is the th weight. In <ref type="bibr" target="#b14">(15)</ref>, denotes the mean squared error. For features in , both the reconstruction and discrimination errors are taken into consideration, while for features in , only the reconstruction error is considered. The cost function is given as <ref type="bibr" target="#b15">(16)</ref> where the first term encourages salient features in and to reduce reconstruction error, and the second term encourages affect-salient features in to reduce discriminative error. This objective is achieved through weight suppression during training.</p><p>Putting all the components of the loss function together we get <ref type="bibr" target="#b16">(17)</ref> The coefficients and weigh the contribution of the saliency penalty and the orthogonality penalty to the overall loss function, respectively, and they are empirically set as and . The set of parameters involved in SDFA is . To train the network, we applied a common network training technique, back-propagation. After each sample passes the network, the error is calculated based on the loss function in <ref type="bibr" target="#b16">(17)</ref> and weights are updated to minimize the error. All samples are cycled through until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Experimental Setup</head><p>Our affect-salient feature learning method was evaluated on four public emotional speech databases with different languages. The first one is Surrey Audio-Visual Expressed Emotion (SAVEE) Database <ref type="bibr" target="#b11">[12]</ref>, which contains emotional speech utterances covering seven emotions (i.e., anger, disgust, fear, happiness, sadness, surprise, and neutral) deliberately displayed by four English speakers. The sampling rate is 44.1 kHz. The second is Berlin Emotional Database (Emo-DB) <ref type="bibr" target="#b2">[3]</ref>, which also includes seven emotions (i.e., anger, disgust, fear, joy, sadness, boredom and neutral) displayed by ten German actors. The sample rate is 48 kHz. The third is Danish Emotional Speech database (DES) <ref type="bibr" target="#b6">[7]</ref>, which includes emotional speech utterances covering five emotions (i.e., anger, joy, surprise, sadness and neutral) displayed by four Danish actors. The sample rate is 48 kHz. The last is Mandarin Emotional Speech database (MES) <ref type="bibr" target="#b8">[9]</ref>, which contains five emotions (i.e., anger, joy, surprise, sadness and disgust) displayed by seven Mandarin actors. The sample rate is 11.025 kHz.</p><p>Except for the experiments conducted with language variance (Section IV-C4), we handle each database separately. That is, features are learned and evaluated using the samples from the same emotional speech database. Specifically, we first split the data into the training set and the testing set. In the unsupervised feature learning stage, we train kernels using one third of randomly selected data in the training dataset of each database. The labels are removed and not used in this stage. In the second stage (SDFA), we train with all the speech utterances in the training dataset.</p><p>In our experiments, we first convert the time-domain signals into spectrograms. The spectrogram has a ms window size with a ms overlap. The spectrogram was further processed using principal component analysis (PCA) whitening (with 60 components) to reduce its dimensionality. Thus, the data we feed into the unsupervised feature learning network consist of 60 channels of one-dimensional vectors of length . We then pre-train the unsupervised auto-encoder patch-wise to get the kernels which are used later to perform convolution. The unsupervised auto-encoder is trained with patches extracted randomly at different locations, the size of which matches that of the convolutional kernels being learned ( and ). Based on the kernels, we form 600 local invariant features with kernel size and 360 features with kernel size . Then, SDFA is used to extract 300 affect-salient features and 300 non-discriminative features in which the former ones (across all the frames in the window) are fed into a linear SVM for the final emotion classification on each emotional speech database.</p><p>We evaluate affect-salient features based on the classification accuracy and compare it with several other well-established feature representations: spectrogram representation ("RAW" features), TEO <ref type="bibr" target="#b36">[37]</ref>, acoustic features extracted in <ref type="bibr" target="#b22">[23]</ref> (A1) and <ref type="bibr" target="#b7">[8]</ref> (A2), and local invariant features (LIF). We also compare features obtained in our system for two different cases: 1) with and without affect-salient penalty [the third term in ( <ref type="formula">17</ref>)] and 2) with and without orthogonality penalty [the fourth term in <ref type="bibr" target="#b16">(17)</ref>]. More specifically, the "RAW" feature contains statistics, i.e., mean, max, min, and standard deviation, computed for each channel over all frames of the spectrogram. TEO features were quantified using seven statistics (i.e., the mean, median, minimum, maximum, standard deviation, range, and inter-quartile) across all frames of an emotional speech utterance (see <ref type="bibr" target="#b36">[37]</ref> for details). The acoustic feature set (A1) in <ref type="bibr" target="#b22">[23]</ref> contains 101 widely-used emotional speech features for SER such as pitch-related features, energy-related features, speech rate, and MFCC, while the acoustic emotion feature vectors (A2) in <ref type="bibr" target="#b7">[8]</ref> contains 6552 features extracted by the openEAR toolkit. LIF is the local invariant features learned by unsupervised feature learning. Finally, we denote the features learned by SDFA without saliency penalty and without orthogonality penalty as SDFA (no_s) and SDFA (no_or), respectively. The features learned by SDFA with all the penalty terms in <ref type="bibr" target="#b16">(17)</ref> are denoted as SDFA.</p><p>These feature representations are first evaluated for SER in four public emotional speech databases. Except for the speakerindependent experiments reported in Table <ref type="table" target="#tab_0">I</ref>, recognition accuracy on all the expressions of each database are reported using five-fold cross-validation with a SVM classifier. The speakerindependent experiments reported in Table <ref type="table" target="#tab_0">I</ref> are conducted with two-fold cross-validation. Here, the test speaker's utterances are excluded in the unsupervised training of CNN. To determine the parameters and for SVM, we randomly selected 100 speech utterances from the dataset of SAVEE to form an independent validation set, based on which a grid search is performed in the range of and for and , respectively. The pair of parameters ( and ) that gives the best results on the validation set is chosen as the one used for cross-validation. Besides overall classification accuracy, we also evaluate robustness of our method with respect to the common disturbing   factors in SER, i.e., the speaker variation, environment distortion, and language variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Visualization</head><p>In order to provide intuitive understanding of the learned features by SDFA, we first visualize several examples obtained with emotion, speaker, language, and environmental variations. Specifically, Fig. <ref type="figure" target="#fig_4">3(a)</ref> shows the features associated with the same emotion but different speakers. The first column provides the raw spectrogram of sample speech fragments of "Surprise" for two different speakers. The raw signal is preprocessed by PCA whitening (the second column) in order to reduce its dimensionality. In the third and fourth columns, we visualize affect-salient features and non-discriminative features learned for the two speakers in the first and the second row, respectively.</p><p>In addition, the difference between the features in the first and the second row is shown in the last row of the corresponding column. We first observe that the affect-salient features contain a higher energy (a larger value) in the entire spectrogram than the non-discriminative feature . As the weights with larger values are considered as more important features for SER, this indicates that we achieved our goal of disentangling emotions from other noisy factors. More importantly, when two utterances have the same emotion but from different speakers, the affect-salient feature changes much less than the non-discriminative one, as shown clearly by the difference images in the last row.  observe that changes on is much smaller than on , which indicates that affect-salient features are robust to environment distortion. Fig. <ref type="figure" target="#fig_4">3(c</ref>) shows the features associated with the emotion "Neutral" but with different languages (English vs. German). Clearly, our method is also robust to the language variation. Finally, Fig. <ref type="figure" target="#fig_4">3(d)</ref> shows the features obtained with two different emotions "Anger" and "Disgust" while all other factors are kept as the same. Obviously, we get the opposite results this time: the affect-salient feature has a larger change than the non-discriminative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation</head><p>Accuracy and Convergence on Public Emotional Speech Databases: In this section, we report the recognition accuracy on the four public emotional speech databases using 5-fold cross-validation based on features learned in different stages of CNN: single non-convolutional one-layer kernels (Kernel( ), Kernel ( )), LIF, affect-salient features learned with random kernels ( ), affect-salient features learned with auto-encoder ( ), and non-discriminative features . In all cases, SVM classifier is used for the emotion classification. The results are shown in Fig. <ref type="figure" target="#fig_5">4</ref>. These results clearly show that each successive layer in CNN helps to disentangle discriminative features, yielding better classification performance. Notice that on all the databases, the accuracy obtained by affect-salient features is much higher than that obtained by the non-discriminative features, both are obtained in the last layer of CNN. Furthermore, on all the databases, the accuracy of affect-salient features with auto-encoder (</p><p>) is much higher than that obtained by using random kernels. Our experiments also show that auto-encoder can help speed up the convergence of CNN training. Fig. <ref type="figure" target="#fig_6">5</ref> compares the training error of CNN on Emo-DB with respect to the training epoch for (</p><p>) and (</p><p>) using the same learning rate. Clearly, the network with the auto-encoder converges faster (and achieves a lower error) than that with random kernels.</p><p>Robustness to Speaker Variation: A major source of variability in SER is the variance across speakers. The performance of SER methods degenerates greatly if the speakers in the training set are not the same as those in the testing set. In this section, we further evaluate the features learned by SDFA by comparing it with other well-established feature representations with respect to speaker variance. In Table <ref type="table" target="#tab_0">I</ref>, single speaker means that the emotional speech utterances come from the same speaker both in the training and the testing set. Speaker-dep means that the person whose emotional speech utterances appear in the testing set also has speech utterances that were used to train the model. Finally, speaker-indep means that the person whose emotional speech utterances appear in the testing set did not have any speech utterance that was used to train the model. The average accuracy and the standard deviation for RAW, TEO, A1 <ref type="bibr" target="#b22">[23]</ref>, A2 <ref type="bibr" target="#b7">[8]</ref>, LIF, SDFA(no_or), SDFA(no_s), and SDFA on four public emotional speech databases are reported.</p><p>The results clearly show that the learned features (i.e., LIF, SDFA(no_or), SDFA(no_s), and SDFA) generally outperform the baseline ones (RAW, TEO and A1) and achieve comparable results with the well-established acoustic feature set A2. SDFA gets the highest accuracy in all the cases (1% to 6% higher than A2) except that it is second to SDFA(no_or) for the case of speaker-dep on SAVEE. Notice that in the case of speaker-independent, that is, the speakers in training set are mismatched with those in the testing set, SDFA has the smallest standard deviation on most of the databases (SAVEE, Emo-DB and DES), which indicates that SDFA is able to learn a feature representation that is salient to emotion while being robust to speaker variation.</p><p>Robustness to Environment Distortion: In SER, there are often cases where significant mismatch between training and test data persists. Environmental factors, e.g., ambient noise, reverberation, microphone type and capture device, are common sources of such mismatch. In this section, we evaluate the extent to which invariance can be obtained with respect to distortions caused by the environment.</p><p>In Table <ref type="table" target="#tab_2">II</ref>, noise means that the utterances in the test set are corrupted by the Gaussian noise of 20 dB SNR. Channel means we do not use a consistent sampling rate between the training and test samples. Specifically, the sampling frequency of the utterances in the test set of four public emotional speech databases is 16 kHz, which is different from that of the samples in the training set.</p><p>Table <ref type="table" target="#tab_2">II</ref> compares the accuracy and the standard deviation for RAW, TEO, A1 <ref type="bibr" target="#b22">[23]</ref>, A2 <ref type="bibr" target="#b7">[8]</ref>, LIF, SDFA(no_or), SDFA(no_s),   and SDFA on the four emotional speech databases, respectively.</p><p>Clearly, among all the methods SDFA achieves the highest and the most stable accuracy (the smallest standard deviation) in 14 out of 16 cases, and is the second best in the remaining two cases. In addition, the learned features (i.e., LIF, SDFA(no_or), SDFA(no_s), and SDFA) generally outperform the baseline ones (RAW, TEO and A1).</p><p>Robustness to Language Variance: The goal of this experiment is to evaluate whether the features learned by SDFA can achieve competitive performance with respect to language variance. We divide the four public emotional speech databases into two groups: one group is SAVEE and Emo-DB, and the other one is DES and MES. We conduct language variance experiment in each group separately as the databases in one group have the similar categories of emotions. Specifically, in each group, the affect-salient features are learned by SDFA using one database, but the SVM classifier is trained and tested by all the samples in the other database, after which we exchange these two databases and conduct the experiment again. In this experiment, we only use the emotion categories included in both databases of each group. Specifically, in the group of SAVEE and Emo-DB, the common emotion categories (anger, disgust, fear, happiness, sadness and neutral) are used, and in the group of DES and MES, only anger, joy, surprise and sadness are used.</p><p>Table <ref type="table" target="#tab_3">III</ref> compares the average recognition accuracy and standard deviation of the eight feature extraction methods on each experiment using 5-fold cross-validation. Even though the features are learned from a database with a different language, com-petitive classification performance is still achieved by SDFA. Clearly, SDFA achieves the highest accuracy and the smallest standard deviation compared with all other feature representations for all the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>Learning salient, discriminative features is an important research issue for SER. The main contribution of this work is two-fold. First, we introduce feature learning to SER, in which the optimal feature set for SER are learned automatically by CNN through two-stage training: SAE and SDFA. Second, in SDFA, we propose a novel objective function that encourages the feature saliency, orthogonality, and discrimination. Consequently, our method can disentangle affect-salient features from other noisy factors such as speakers and language. Experimental results on public emotional speech databases show superior performance of the learned features with respect to speaker variation, environment distortion, and language variation when compared with several well-established feature representations.</p><p>In this paper, our focus is on the recognition of prototypic expressions of several basic emotions based on displayed emotional utterances in laboratory settings. Subtle, continuous, and context-specific interpretations of affective utterances recorded in naturalistic and real-world settings are clearly more important and more difficult research problems. Feature learning, as an advanced technique to learn a transformation of raw inputs to a representation that can be effectively exploited by a classifier, is well-suited for addressing these challenges. In the future, we plan to extend the proposed method in this paper and evaluate its performance on naturalistic speech data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System pipeline. Left: Input spectrogram at two different resolutions. The next stage is the local invariant feature learning containing the output of one long feature vector. The salient discriminative feature learning produces the last stage of affect-salient features and nuisance features , and the former is then fed to a linear SVM for SER.</figDesc><graphic coords="3,116.04,64.14,358.92,138.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Local Invariant Feature Learning: We use a sparse autoencoder to learn local invariant features from emotional speech signal at multiple scales in an unsupervised fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Convolutional kernels learned by sparse auto-encoder. The horizontal axis represents time, and the vertical axis denotes frequency. Each kernel corresponds to a rectangular area in the image, and the color of the kernel is determined by the element value based on the color index. Left:. Right: .</figDesc><graphic coords="4,127.02,64.14,340.98,124.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Visualization of the features learned by SDFA. (a) Features with speaker variation, (b) features with environmental distortion, (c) features with language variation, and (d) features with emotion variation. In each panel, the first column provides the raw spectrogram, the second column shows the results after PCA whitening, and the third and the fourth column visualize affect-salient features and non-discriminative features, respectively. The difference images with various conditions (e.g., emotion, speaker, language, and environmental factors) are provided in the last row.</figDesc><graphic coords="7,115.02,271.14,360.96,244.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 (</head><label>3</label><figDesc>b) shows the features associated with the same emotion "Anger" but under different environmental factors (clean vs. noisy background with different sampling rates). Again, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Average recognition accuracy and standard deviation (%) on the four public emotional speech emotion databases with features learned in different stages of CNN. In the figure, kernel1 and kernel2 denote the kernel with the size of and , respectively.</figDesc><graphic coords="8,63.00,63.12,204.00,147.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Convergence speed on Emo-DB with the same learning rate, auto-encoder versus random kernels .</figDesc><graphic coords="8,352.98,64.14,153.00,118.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,94.98,108.12,400.03,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SER</head><label>I</label><figDesc>ACCURACY AND STANDARD DEVIATION ON THE FOUR PUBLIC EMOTIONAL SPEECH DATABASES WITH SPEAKER VARIATION (SINGLE SPEAKER, SPEAKER-DEPENDENT, AND SPEAKER-INDEPENDENT). THE RECOGNITION ACCURACY IS REPORTED IN %, AND THE HIGHEST ONE IS HIGHLIGHTED IN BOLD. IN THE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TABLE, SPEAKER-DEP AND SPEAKER-INDEP DENOTE SPEAKER-DEPENDENT AND SPEAKER-INDEPENDENT, RESPECTIVELY</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SER</head><label>II</label><figDesc>ACCURACY AND STANDARD DEVIATION ON FOUR PUBLIC EMOTIONAL SPEECH DATABASE WITH ENVIRONMENTAL DISTORTION (NOISE, CHANNEL, AND ). THE RECOGNITION ACCURACY IS REPORTED IN %, AND THE HIGHEST ONE IS HIGHLIGHTED IN BOLD. IN THE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SER</head><label>III</label><figDesc>TABLE, ED DENOTES ENVIRONMENT DISTORTION ACCURACY AND STANDARD DEVIATION ON FOUR PUBLIC EMOTIONAL SPEECH DATABASES WITH LANGUAGE VARIANCE. THE RECOGNITION ACCURACY IS REPORTED IN %, AND THE HIGHEST ONE IS HIGHLIGHTED IN BOLD. IN THE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>TABLE, FE-DATABASE DENOTES THE DATABASE USED TO LEARN THE FEATURES,</figDesc><table /><note><p>AND CL-DATABASE DENOTES THE DATABASE USED TO PERFORM AND EVALUATE SER</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Nature Science Foundation of China under Grants 61272211 and 61170126, and by the Six Talent Peaks Foundation of Jiangsu Province under Grant DZXX-027.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ASR for emotional speech: Clarifying the issues and enhancing performance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Athanaselis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakamidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dologlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowiea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="437" to="444" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A database of German Emotional Speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
			<biblScope unit="page" from="1517" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using FCBF feature selection method and GA-optimized fuzzy ARTMAP neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Davood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alireza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2115" to="2126" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing sharedhidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>39th IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4818" to="4822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse autoencoder-based feature transfer learning for speech emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Biannu</title>
		<meeting>5th Biannu<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Design, recording and verification of a Danish emotional speech database</title>
		<author>
			<persName><forename type="first">I</forename><surname>Engberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dalsgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Eur</title>
		<meeting>5th Eur<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-09">Sep. 1997</date>
			<biblScope unit="page" from="1695" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">openEAR: Introducing the Munich Open-Source Emotion and Affect Recognition Toolkit</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Affective Comput. Intell. Interaction</title>
		<meeting>Affective Comput. Intell. Interaction<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">2009. 1996</date>
			<biblScope unit="page" from="576" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech emotion recognition based on parametric filter and fractal dimension</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2324" to="2326" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An unsupervised hierarchical feature learning framework for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="632" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using a backward context</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 39th Appl. Imagery Pattern Recog. Workshop</title>
		<meeting>IEEE 39th Appl. Imagery Pattern Recog. Workshop<address><addrLine>Washington, D.C., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker-dependent audiovisual emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Edge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Auditory-Visual Speech Process</title>
		<meeting>Int. Conf. Auditory-Visual Speech ess<address><addrLine>Norwich, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="53" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusion of global statistical and segmental spectral features for speech emotion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using CNN</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech using sub-syllabic and pitch synchronous spectral features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Krothapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech using source, system, and prosodic features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="289" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion recognition from spontaneous speech using hidden markov models with deep belief networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Automac. Speech Recog. Understand</title>
		<meeting>Automac. Speech Recog. Understand<address><addrLine>Olomouc, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="216" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward detecting emotions in spoken dialogs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="303" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Advances Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1096" to="1104" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature analysis and evaluation for automatic emotion identification in speech</title>
		<author>
			<persName><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining classifiers with diverse feature sets for robust speaker independent emotion recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Signal Process. Conf</title>
		<meeting>Eur. Signal ess. Conf<address><addrLine>Trivandrum, Kerala, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="1225" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modular neural-SVM scheme for speech emotion recognition using ANOVA feature selection method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Davood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech emotion recognition method based on improved decision tree and layered feature selection</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">R</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Humanoid Robot</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Moataz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fakhri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ensemble methods for spoken emotion recognition in call-centres</title>
		<author>
			<persName><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="112" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Nwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C D</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-centred intelligent human-computer interaction ( ): How far are we from attaining it?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Autonomous and Adaptive Commun. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="187" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study of different weighting schemes on KNN-based emotion recognition in Mandarin speech</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Intell. Comput. Theories and Appl. With Aspects of Theoretical and Methodological</title>
		<imprint>
			<biblScope unit="volume">4681</biblScope>
			<biblScope unit="page" from="997" to="1005" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech emotion recognition approaches in human-computer interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M M E</forename><surname>Emary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telecommun. Syst</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1467" to="1478" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech using global and local prosodic features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Vempada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision</title>
		<meeting>Eur. Conf. Comput. Vision<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="808" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning emotion-based acoustic features with deep belief networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.</title>
		<imprint>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2011">2011</date>
			<pubPlace>New Paltz, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="577" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic classification of emotion related user states in spontaneous children&apos;s speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dept., Univ. Erlangen</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Nuremberg; Erlangen, Germany</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic emotion recognition: Raising the benchmarks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlsatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zieike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoustics, Speech Signal Process</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5688" to="5691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating glottal parameters and Teager energy operators in emotion recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Affective Comput. Intell. Interaction</title>
		<imprint>
			<biblScope unit="volume">6975</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Loss-scaled large-margin Gaussian mixture models for speech emotion classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sungrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="585" to="598" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speech emotion analysis: Exploring the role of context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="502" to="509" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Emotional speech recognition: Resources, features and methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ververidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1162" to="1181" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analyzing the memory of BLSTM neural networks for enhanced emotion classification in dyadic spoken interactions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>37th Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4157" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2011-06">Jan.-Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature learning in deep neural networks -Studies on speech recognition tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Conf. Learn. Representations</title>
		<meeting>1st Int. Conf. Learn. Representations<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Audio-visual affect recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="424" to="428" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emotion recognition in Chinese natural speech by combining prosody and voice quality features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Symp</title>
		<meeting>5th Int. Symp<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Based on EEMD-HHT marginal spectrum of speech emotion recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput</title>
		<meeting>Int. Conf. Comput<address><addrLine>Taiyuan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="91" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<title level="m">M&apos;12) received the M.S. and Ph.D. degrees in computer application technology from Jiangsu University</title>
		<meeting><address><addrLine>Zhenjiang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002 and 2009</date>
		</imprint>
	</monogr>
	<note>respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
