<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-25">25 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
							<email>qizhex@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-25">25 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1904.12848v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A fundamental weakness of deep learning is that it typically requires a lot of labeled data to work well. Semi-supervised learning (SSL) <ref type="bibr">(Chapelle et al., 2009)</ref> is one of the most promising paradigms of leveraging unlabeled data to address this weakness. The recent works in SSL are diverse but those that are based on consistency training <ref type="bibr" target="#b1">(Bachman et al., 2014;</ref><ref type="bibr" target="#b32">Rasmus et al., 2015;</ref><ref type="bibr">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b41">Tarvainen &amp; Valpola, 2017)</ref> have shown to work well on many benchmarks.</p><p>In a nutshell, consistency training methods simply regularize model predictions to be invariant to small noise applied to either input examples <ref type="bibr" target="#b23">(Miyato et al., 2018;</ref><ref type="bibr" target="#b34">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b6">Clark et al., 2018)</ref> or hidden states <ref type="bibr" target="#b1">(Bachman et al., 2014;</ref><ref type="bibr">Laine &amp; Aila, 2016)</ref>. This framework makes sense intuitively because a good model should be robust to any small change in an input example or hidden states. Under this framework, different methods in this category differ mostly in how and where the noise injection is applied. Typical noise injection methods are additive Gaussian noise, dropout noise or adversarial noise.</p><p>In this work, we investigate the role of noise injection in consistency training and observe that advanced data augmentation methods, specifically those work best in supervised learning <ref type="bibr" target="#b39">(Simard et al., 1998;</ref><ref type="bibr">Krizhevsky et al., 2012;</ref><ref type="bibr">Cubuk et al., 2018;</ref><ref type="bibr" target="#b49">Yu et al., 2018)</ref>, also perform well in semisupervised learning. There is indeed a strong correlation between the performance of data augmentation operations in supervised learning and their performance in consistency training. We, hence, propose to substitute the traditional noise injection methods with high quality data augmentation methods in order to improve consistency training. To emphasize the use of better data augmentation in consistency training, we name our method Unsupervised Data Augmentation or UDA.</p><p>We evaluate UDA on a wide variety of language and vision tasks. On six text classification tasks, our method achieves significant improvements over state-of-the-art models. Notably, on IMDb, UDA with 20 labeled examples outperforms the state-of-the-art model trained on 1250x more labeled data. On standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all existing semi-supervised learning methods by significant margins and achieves an error rate of 5.43 and 2.72 with 250 labeled examples respectively. Finally, we also find UDA to be beneficial when there is a large amount of supervised data. For instance, on ImageNet, UDA leads to improvements of top-1 accuracy from 58.84 to 68.78 with 10% of the labeled set and from 78.43 to 79.05 when we use the full labeled set and an external dataset with 1.3M unlabeled examples.</p><p>Our key contributions and findings can be summarized as follows:</p><p>• First, we show that state-of-the-art data augmentations found in supervised learning can also serve as a superior source of noise under the consistency enforcing semi-supervised framework.</p><p>See results in Table <ref type="table" target="#tab_2">1 and Table 2.</ref> • Second, we show that UDA can match and even outperform purely supervised learning that uses orders of magnitude more labeled data. See results in Table <ref type="table" target="#tab_5">4</ref> and Figure <ref type="figure" target="#fig_2">3</ref>. State-of-the-art results for both vision and language tasks are reported in Table <ref type="table" target="#tab_5">3 and 4</ref>. The effectiveness of UDA across different training data sizes are highlighted in Figure <ref type="figure" target="#fig_2">3</ref> and 6. • Third, we show that UDA combines well with transfer learning, e.g., when fine-tuning from BERT (see Table <ref type="table" target="#tab_5">4</ref>), and is effective at high-data regime, e.g. on ImageNet (see Table <ref type="table" target="#tab_6">5</ref>).</p><p>• Lastly, we also provide a theoretical analysis of how UDA improves the classification performance and the corresponding role of the state-of-the-art augmentation in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">UNSUPERVISED DATA AUGMENTATION (UDA)</head><p>In this section, we first formulate our task and then present the key method and insights behind UDA. Throughout this paper, we focus on classification problems and will use x to denote the input and y * to denote its ground-truth prediction target. We are interested in learning a model p θ (y | x) to predict y * based on the input x, where θ denotes the model parameters. Finally, we will use p L (x) and p U (x) to denote the distributions of labeled and unlabeled examples respectively and use f * to denote the perfect classifier that we hope to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BACKGROUND: SUPERVISED DATA AUGMENTATION</head><p>Data augmentation aims at creating novel and realistic-looking training data by applying a transformation to an example, without changing its label. Formally, let q(x | x) be the augmentation transformation from which one can draw augmented examples x based on an original example x.</p><p>For an augmentation transformation to be valid, it is required that any example x ∼ q(x | x) drawn from the distribution shares the same ground-truth label as x. Given a valid augmentation transformation, we can simply minimize the negative log-likelihood on augmented examples.</p><p>Supervised data augmentation can be equivalently seen as constructing an augmented labeled set from the original supervised set and then training the model on the augmented set. Therefore, the augmented set needs to provide additional inductive biases to be more effective. How to design the augmentation transformation has, thus, become critical.</p><p>In recent years, there have been significant advancements on the design of data augmentations for NLP <ref type="bibr" target="#b49">(Yu et al., 2018</ref><ref type="bibr">), vision (Krizhevsky et al., 2012;</ref><ref type="bibr">Cubuk et al., 2018) and</ref><ref type="bibr">speech (Hannun et al., 2014;</ref><ref type="bibr" target="#b27">Park et al., 2019)</ref> in supervised settings. Despite the promising results, data augmentation is mostly regarded as the "cherry on the cake" which provides a steady but limited performance boost because these augmentations has so far only been applied to a set of labeled examples which is usually of a small size. Motivated by this limitation, via the consistency training framework, we extend the advancement in supervised data augmentation to semi-supervised learning where abundant unlabeled data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UNSUPERVISED DATA AUGMENTATION</head><p>As discussed in the introduction, a recent line of work in semi-supervised learning has been utilizing unlabeled examples to enforce smoothness of the model. The general form of these works can be summarized as follows: This procedure enforces the model to be insensitive to the noise and hence smoother with respect to changes in the input (or hidden) space. From another perspective, minimizing the consistency loss gradually propagates label information from labeled examples to unlabeled ones.</p><p>In this work, we are interested in a particular setting where the noise is injected to the input x, i.e., x = q(x, ), as considered by prior works <ref type="bibr" target="#b34">(Sajjadi et al., 2016;</ref><ref type="bibr">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b23">Miyato et al., 2018)</ref>. But different from existing work, we focus on the unattended question of how the form or "quality" of the noising operation q can influence the performance of this consistency training framework. Specifically, to enforce consistency, prior methods generally employ simple noise injection methods such as adding Gaussian noise, simple input augmentations to noise unlabeled examples. In contrast, we hypothesize that stronger data augmentations in supervised learning can also lead to superior performance when used to noise unlabeled examples in the semi-supervised consistency training framework, since it has been shown that more advanced data augmentations that are more diverse and natural can lead to significant performance gain in the supervised setting.</p><p>Following this idea, we propose to use a rich set of state-of-the-art data augmentations verified in various supervised settings to inject noise and optimize the same consistency training objective on unlabeled examples. When jointly trained with labeled examples, we utilize a weighting factor λ to balance the supervised cross entropy and the unsupervised consistency training loss, which is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Formally, the full objective can be written as follows:</p><formula xml:id="formula_0">min θ J (θ) = E x∼p L (x) [− log p θ (f * (x) | x)] + λE x∼p U (x) E x∼q(x|x) CE p θ (y | x) p θ (y | x) (1)</formula><p>where CE denotes cross entropy, q(x | x) is a data augmentation transformation and θ is a fixed copy of the current parameters θ indicating that the gradient is not propagated through θ, as suggested by VAT <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref>. We set λ to 1 for most of our experiments and use different batch sizes for the labeled data and the unlabeled data. In practice, we use a larger batch size for the objective function on unlabeled data.</p><p>In the vision domain, simple augmentations including cropping and flipping are applied to labeled examples. To minimize the discrepancy between supervised training and prediction on unlabeled examples, we apply the same simple augmentations to unlabeled examples for computing p θ (y | x).</p><p>Discussion. Before detailing the augmentation operations used in this work, we first provide some intuitions on how more advanced data augmentations can provide extra advantages over simple ones used in earlier works from three aspects:</p><p>• Valid noise: Advanced data augmentation methods that achieve great performance in supervised learning usually generate realistic augmented examples that share the same ground-truth labels with the original example. Thus, it is safe to encourage the consistency between predictions on the original unlabeled example and the augmented unlabeled examples. • Targeted inductive biases: Different tasks require different inductive biases. Data augmentation operations that work well in supervised training essentially provides the missing inductive biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">AUGMENTATION STRATEGIES FOR DIFFERENT TASKS</head><p>We now detail the augmentation methods, tailored for different tasks, that we use in this work.</p><p>RandAugment for Image Classification. We use a data augmentation method called RandAugment <ref type="bibr">(Cubuk et al., 2019)</ref>, which is inspired by <ref type="bibr" target="#b8">AutoAugment (Cubuk et al., 2018)</ref>. AutoAugment uses a search method to combine all image processing transformations in the Python Image Library (PIL) to find a good augmentation strategy. In RandAugment, we do not use search, but instead uniformly sample from the same set of augmentation transformations in PIL. In other words, Ran-dAugment is simpler and requires no labeled data as there is no need to search for optimal policies.</p><p>Back-translation for Text Classification. When used as an augmentation method, backtranslation <ref type="bibr" target="#b37">(Sennrich et al., 2015;</ref><ref type="bibr" target="#b14">Edunov et al., 2018)</ref> refers to the procedure of translating an existing example x in language A into another language B and then translating it back into A to obtain an augmented example x. As observed by <ref type="bibr" target="#b49">(Yu et al., 2018)</ref>, back-translation can generate diverse paraphrases while preserving the semantics of the original sentences, leading to significant performance improvements in question answering. In our case, we use back-translation to paraphrase the training data of our text classification tasks.<ref type="foot" target="#foot_1">2</ref> </p><p>We find that the diversity of the paraphrases is important. Hence, we employ random sampling with a tunable temperature instead of beam search for the generation. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the paraphrases generated by back-translation sentence are diverse and have similar semantic meanings.</p><p>More specifically, we use WMT'14 English-French translation models (in both directions) to perform back-translation on each sentence. To facilitate future research, we have open-sourced our back-translation system together with the translation checkpoints.</p><p>Back-translation Given the low budget and production limitations, this movie is very good.</p><p>Since it was highly limited in terms of budget, and the production restrictions, the film was cheerful. There are few budget items and production limitations to make this film a really good one.</p><p>Due to the small dollar amount and production limitations the ouest film is very beautiful. Word replacing with TF-IDF for Text Classification. While back-translation is good at maintaining the global semantics of a sentence, there is little control over which words will be retained. This requirement is important for topic classification tasks, such as DBPedia, in which some keywords are more informative than other words in determining the topic. We, therefore, propose an augmentation method that replaces uninformative words with low TF-IDF scores while keeping those with high TF-IDF values. We refer readers to Appendix A.2 for a detailed description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandAugment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ADDITIONAL TRAINING TECHNIQUES</head><p>In this section, we present additional techniques targeting at some commonly encountered problems.</p><p>Confidence-based masking. We find it to be helpful to mask out examples that the current model is not confident about. Specifically, in each minibatch, the consistency loss term is computed only on examples whose highest probability among classification categories is greater than a threshold β.</p><p>We set the threshold β to a high value. Specifically, β is set to 0.8 for CIFAR-10 and SVHN and 0.5 for ImageNet.</p><p>Sharpening Predictions. Since regularizing the predictions to have low entropy has been shown to be beneficial <ref type="bibr" target="#b15">(Grandvalet &amp; Bengio, 2005;</ref><ref type="bibr" target="#b23">Miyato et al., 2018)</ref>, we sharpen predictions when computing the target distribution on unlabeled examples by using a low Softmax temperature τ . When combined with confidence-based masking, the loss on unlabeled examples</p><formula xml:id="formula_1">E x∼p U (x) E x∼q(x|x) CE p θ (y | x) p θ (y | x) on a minibatch B is computed as: 1 |B| x∈B I(max y p θ (y | x) &gt; β)CE p (sharp) θ (y | x) p θ (y | x) p (sharp) θ (y | x) = exp(z y /τ )</formula><p>y exp(z y /τ ) where I(•) is the indicator function, z y is the logit of label y for example x. We set τ to 0.4 for CIFAR-10, SVHN and ImageNet. Domain-relevance Data Filtering. Ideally, we would like to make use of out-of-domain unlabeled data since it is usually much easier to collect, but the class distributions of out-of-domain data are mismatched with those of in-domain data, which can result in performance loss if directly used <ref type="bibr" target="#b26">(Oliver et al., 2018)</ref>. To obtain data relevant to the domain for the task at hand, we adopt a common technique for detecting out-of-domain data. We use our baseline model trained on the in-domain data to infer the labels of data in a large out-of-domain dataset and pick out examples that the model is most confident about. Specifically, for each category, we sort all examples based on the classified probabilities of being in that category and select the examples with the highest probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS</head><p>In this section, we theoretically analyze why UDA can improve the performance of a model and the required number of labeled examples to achieve a certain error rate. Following previous sections, we will use f * to denote the perfect classifier that we hope to learn, use p U to denote the marginal distribution of the unlabeled data and use q(x | x) to denote the augmentation distribution.</p><p>To make the analysis tractable, we make the following simplistic assumptions about the data augmentation transformation:</p><p>• In-domain augmentation: data examples generated by data augmentation have non-zero probability under p U , i.e., p U (x) &gt; 0 for x ∼ q(x | x), x ∼ p U (x). • Label-preserving augmentation: data augmentation preserves the label of the original example, i.e., f</p><formula xml:id="formula_2">* (x) = f * (x) for x ∼ q(x | x), x ∼ p U (x).</formula><p>• Reversible augmentation: the data augmentation operation can be reversed, i.e., if q</p><formula xml:id="formula_3">(x | x) &gt; 0 then q(x | x) &gt; 0 .</formula><p>As the first step, we hope to provide an intuitive sketch of our formal analysis. Let us define a graph G p U where each node corresponds to a data sample x ∈ X and an edge (x, x) exists in the graph if and only if q(x | x) &gt; 0. Due to the label-preserving assumption, it is easy to see that examples with different labels must reside on different components (disconnected sub-graphs) of the graph G p U . Hence, for an N -category classification problems, the graph has N components (sub-graphs) when all examples within each category can be traversed by the augmentation operation. Otherwise, the graph will have more than N components.</p><p>Given this construction, notice that for each component C i of the graph, as long as there is a single labeled example in the component, i.e. (x * , y * ) ∈ C i , one can propagate the label of the node to the rest of the nodes in C i by traversing C i via the augmentation operation q(x | x). More importantly, if one only performs supervised data augmentation, one can only propagate the label information to the directly connected neighbors of the labeled node. In contrast, performing unsupervised data augmentation ensures the traversal of the entire sub-graph C i . This provides the first high-level intuition how UDA could help.</p><p>Taking one step further, in order to find a perfect classifier via such label propagation, it requires that there exists at least one labeled example in each component. In other words, the number of components lower bounds the minimum amount of labeled examples needed to learn a perfect classifier. Importantly, number of components is actually decided by the quality of the augmentation operation: an ideal augmentation should be able to reach all other examples of the same category given a starting instance. This well matches our discussion of the benefits of state-of-the-art data augmentation methods in generating more diverse examples. Effectively, the augmentation diversity leads to more neighbors for each node, and hence reduces the number of components in a graph.</p><p>With the intuition described, we state our formal results. Without loss of generality, assume there are k components in the graph. For each component C i (i = 1, . . . , k), let P i be the total probability mass that an observed labeled example fall into the i-th component, i.e., P i = x∈Ci p L (x). The following theorem characterizes the relationship between UDA error rate and the amount of labeled examples.</p><p>Theorem 1. Under UDA, let P r(A) denote the probability that the algorithm cannot infer the label of a new test example given m labeled examples from P L . P r(A) is given by</p><formula xml:id="formula_4">P r(A) = i P i (1 − P i ) m .</formula><p>In addition, O(k/ ) labeled examples can guarantee an error rate of O( ), i.e.,</p><formula xml:id="formula_5">m = O(k/ ) =⇒ P r(A) = O( ).</formula><p>Proof. Please see Appendix. C for details.</p><p>From the theorem, we can see the number of components, i.e. k, directly governs the amount of labeled data required to reach a desired performance. As we have discussed above, the number of components effectively relies on the quality of an augmentation function, where better augmentation functions result in fewer components. This echoes our discussion of the benefits of state-of-the-art data augmentation operations in generating more diverse examples. Hence, with state-of-the-art augmentation operations, UDA is able to achieve good performance using fewer labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate UDA on a variety of language and vision tasks. For language, we rely on six text classification benchmark datasets, including IMDb, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification <ref type="bibr" target="#b19">(Maas et al., 2011;</ref><ref type="bibr">Zhang et al., 2015)</ref>.</p><p>For vision, we employ two smaller datasets <ref type="bibr">CIFAR-10 (Krizhevsky &amp; Hinton, 2009)</ref>, SVHN <ref type="bibr" target="#b25">(Netzer et al., 2011)</ref>, which are often used to compare semi-supervised algorithms, as well as Ima-geNet <ref type="bibr" target="#b12">(Deng et al., 2009)</ref> of a larger scale to test the scalability of UDA. For details of the labeled and unlabeled data and experiment details, we refer readers to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CORRELATION BETWEEN SUPERVISED AND SEMI-SUPERVISED PERFORMANCES</head><p>As the first step, we try to verify the fundamental idea of UDA, i.e., there is a positive correlation of data augmentation's effectiveness in supervised learning and semi-supervised learning. Based on Yelp-5 (a language task) and CIFAR-10 (a vision task), we compare the performance of different data augmentation methods in either fully supervised or semi-supervised settings. For Yelp-5, apart from back-translation, we include a simpler method Switchout <ref type="bibr" target="#b44">(Wang et al., 2018)</ref> which replaces a token with a random token uniformly sampled from the vocabulary. For CIFAR-10, we compare RandAugment with two simpler methods: (1) cropping &amp; flipping augmentation and (2) Cutout.  Based on this setting, Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_2">2</ref> exhibit a strong correlation of an augmentation's effectiveness between supervised and semi-supervised settings. This validates our idea of stronger data augmentations found in supervised learning can always lead to more gains when applied to the semi-supervised learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ALGORITHM COMPARISON ON VISION SEMI-SUPERVISED LEARNING BENCHMARKS</head><p>With the correlation established above, the next question we ask is how well UDA performs compared to existing semi-supervised learning algorithms. To answer the question, we focus on the most commonly used semi-supervised learning benchmarks CIFAR-10 and SVHN.</p><p>Vary the size of labeled data. Firstly, we follow the settings in <ref type="bibr" target="#b26">(Oliver et al., 2018)</ref> and employ Wide-ResNet-28-2 <ref type="bibr" target="#b50">(Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr">He et al., 2016)</ref> as the backbone model and evaluate UDA with varied supervised data sizes. Specifically, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial Gaussian noise on input, and (2) MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019)</ref>, a parallel work that combines previous advancements in semi-supervised learning. The comparison is shown in Figure <ref type="figure" target="#fig_2">3</ref> with two key observations.</p><p>• First, UDA consistently outperforms the two baselines given different sizes of labeled data.</p><p>• Moreover, the performance difference between UDA and VAT shows the superiority of data augmentation based noise. The difference of UDA and VAT is essentially the noise process. While the noise produced by VAT often contain high-frequency artifacts that do not exist in real images, data augmentation mostly generates diverse and realistic images.   <ref type="bibr" target="#b41">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 ± 0.28 3.95 ± 0.19 VAT + EntMin <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 ± 0.05 3.86 ± 0.11 SNTG <ref type="bibr" target="#b17">(Luo et al., 2018)</ref> Conv-Large 3.1M 10.93 ± 0.14 3.86 ± 0.27 VAdD <ref type="bibr" target="#b28">(Park et al., 2018)</ref> Conv-Large 3.1M 11.32 ± 0.11 4.16 ± 0.08 Fast-SWA <ref type="bibr" target="#b0">(Athiwaratkun et al., 2018)</ref> Conv-Large 3.1M 9.05 -ICT <ref type="bibr" target="#b43">(Verma et al., 2019)</ref> Conv • First, even with very few labeled examples, UDA can offer decent or even competitive performances compared to the SOTA model trained with full supervised data. Particularly, on binary sentiment analysis tasks, with only 20 supervised examples, UDA outperforms the previous SOTA trained with full supervised data on IMDb and is competitive on Yelp-2 and Amazon-2.</p><p>• Second, UDA is complementary to transfer learning / representation learning. As we can see, when initialized with BERT and further finetuned on in-domain data, UDA can still significantly reduce the error rate from 6.50 to 4.20 on IMDb.</p><p>• Finally, we also note that for five-category sentiment classification tasks, there still exists a clear gap between UDA with 500 labeled examples per class and BERT trained on the entire supervised set. Intuitively, five-category sentiment classifications are much more difficult than their binary counterparts. This suggests a room for further improvement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SCALABILITY TEST ON THE IMAGENET DATASET</head><p>Then, to evaluate whether UDA can scale to problems with a large scale and a higher difficulty, we now turn to the ImageNet dataset with ResNet-50 being the underlying architecture. Specifically, we consider two experiment settings with different natures:</p><p>• We use 10% of the supervised data of ImageNet while using all other data as unlabeled data. As a result, the unlabeled exmaples are entirely in-domain. • In the second setting, we keep all images in ImageNet as supervised data. Then, we use the domain-relevance data filtering method to filter out 1.3M images from JFT <ref type="bibr">(Hinton et al., 2015;</ref><ref type="bibr">Chollet, 2017)</ref>. Hence, the unlabeled set is not necessarily in-domain.</p><p>The results are summarized in Table <ref type="table" target="#tab_6">5</ref>. In both 10% and the full data settings, UDA consistently brings significant gains compared to the supervised baseline. This shows UDA is not only able to scale but also able to utilize out-of-domain unlabeled examples to improve model performance. In parallel to our work, S4L <ref type="bibr">(Zhai et al., 2019b)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Existing works in consistency training does make use of data augmentation (Laine &amp; Aila, 2016; <ref type="bibr" target="#b34">Sajjadi et al., 2016)</ref>; however, they only apply weak augmentation methods such as random translations and cropping. In parallel to our work, ICT <ref type="bibr" target="#b43">(Verma et al., 2019)</ref> and MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019)</ref> also show improvements for semi-supervised learning. These methods employ mixup <ref type="bibr">(Zhang et al., 2017)</ref> on top of simple augmentations such as flipping and cropping; instead, UDA emphasizes on the use of state-of-the-art data augmentations, leading to significantly better results on CIFAR-10 and SVHN. In addition, UDA is also applicable to language domain and can also scale well to more challenging vision datasets, such as ImageNet.</p><p>Other works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble <ref type="bibr" target="#b1">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type="bibr" target="#b23">(Miyato et al., 2018;</ref><ref type="bibr">2016)</ref> defines the noise by approximating the direction of change in the input space that the model is most sensitive to; Cross-view training <ref type="bibr" target="#b6">(Clark et al., 2018)</ref> masks out part of the input data. Apart from enforcing consistency on the input examples and the hidden representations, another line of research enforces consistency on the model parameter space. Works in this category include Mean Teacher <ref type="bibr" target="#b41">(Tarvainen &amp; Valpola, 2017)</ref>, fast-Stochastic Weight Averaging <ref type="bibr" target="#b0">(Athiwaratkun et al., 2018)</ref> and Smooth Neighbors on Teacher Graphs <ref type="bibr" target="#b17">(Luo et al., 2018)</ref>. For a complete version of related work, please refer to Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we show that data augmentation and semi-supervised learning are well connected: better data augmentation can lead to significantly better semi-supervised learning. Our method, UDA, employs state-of-the-art data augmentation found in supervised learning to generate diverse and realistic noise and enforces the model to be consistent with respect to these noise. For text, UDA combines well with representation learning, e.g., BERT. For vision, UDA outperforms prior works by a clear margin and nearly matches the performance of the fully supervised models trained on the full labeled sets which are one order of magnitude larger. We hope that UDA will encourage future research to transfer advanced supervised augmentation to semi-supervised setting for different tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXTENDED METHOD DETAILS</head><p>In this section, we present some additional details used in our method. We introduce Training Signal Annealing in Appendix A.1 and details for augmentation strategies in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 TRAINING SIGNAL ANNEALING FOR LOW-DATA REGIME</head><p>In semi-supervised learning, we often encounter a situation where there is a huge gap between the amount of unlabeled data and that of labeled data. Hence, the model often quickly overfits the limited amount of labeled data while still underfitting the unlabeled data. We consider three increasing schedules of η t with different application scenarios. Let T be the total number of training steps, the three schedules are shown in Figure <ref type="figure" target="#fig_3">4</ref>. Intuitively, when the model is prone to overfit, e.g., when the problem is relatively easy or the number of labeled examples is very limited, the exp-schedule is most suitable as the supervised signal is mostly released at the end of training. In contrast, when the model is less likely to overfit (e.g., when we have abundant labeled examples or when the model employs effective regularization), the log-schedule can serve well. </p><formula xml:id="formula_6">t = α t * (1 − 1 K ) + 1 K . α t is set to 1 − exp(− t T * 5), t</formula><p>T and exp(( t T − 1) * 5) for the log, linear and exp schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 EXTENDED AUGMENTATION STRATEGIES FOR DIFFERENT TASKS</head><p>Discussion on Trade-off Between Diversity and Validity for Data Augmentation. Despite that state-of-the-art data augmentation methods can generate diverse and valid augmented examples as discussed in section 2.2, there is a trade-off between diversity and validity since diversity is achieved by changing a part of the original example, naturally leading to the risk of altering the ground-truth label. We find it beneficial to tune the trade-off between diversity and validity for data augmentation methods. For text classification, we tune the temperature of random sampling. On the one hand, when we use a temperature of 0, decoding by random sampling degenerates into greedy decoding and generates perfectly valid but identical paraphrases. On the other hand, when we use a temperature of 1, random sampling generates very diverse but barely readable paraphrases. We find that setting the Softmax temperature to 0.7, 0.8 or 0.9 leads to the best performances.</p><p>RandAugment Details. In our implementation of RandAugment, each sub-policy is composed of two operations, where each operation is represented by the transformation name, probability, and magnitude that is specific to that operation. For example, a sub-policy can be [(Sharpness, 0.6, 2), (Posterize, 0.3, 9)].</p><p>For each operation, we randomly sample a transformation from 15 possible transformations, a magnitude in [1, 10) and fix the probability to 0.5. Specifically, we sample from the following 15 transformations: Invert, Cutout, Sharpness, AutoContrast, Posterize, ShearX, TranslateX, TranslateY, ShearY, Rotate, Equalize, Contrast, Color, Solarize, Brightness. We find this setting to work well in our first try and did not tune the magnitude range and the probability. Tuning these hyperparameters might result in further gains in accuracy.</p><p>TF-IDF based word replacing Details. Ideally, we would like the augmentation method to generate both diverse and valid examples. Hence, the augmentation is designed to retain keywords and replace uninformative words with other uninformative words. We use BERT's word tokenizer since BERT first tokenizes sentences into a sequence of words and then tokenize words into subwords although the model uses subwords as input.</p><p>Specifically, Suppose IDF(w) is the IDF score for word w computed on the whole corpus, and TF(w) is the TF score for word w in a sentence. We compute the TF-IDF score as TFIDF(w) = TF(w)IDF(w). Suppose the maximum TF-IDF score in a sentence x is C = max i TFIDF(x i ).</p><p>To make the probability of having a word replaced to negatively correlate with its TF-IDF score, we set the probability to min(p(C − TFIDF(x i ))/Z, 1), where p is a hyperparameter that controls the magnitude of the augmentation and Z = i (C − TFIDF(x i ))/|x| is the average score. p is set to 0.7 for experiments on DBPedia.</p><p>When a word is replaced, we sample another word from the whole vocabulary for the replacement. Intuitively, the sampled words should not be keywords to prevent changing the ground-truth labels of the sentence. To measure if a word is keyword, we compute a score of each word on the whole corpus. Specifically, we compute the score as S(w) = freq(w)IDF(w) where freq(w) is the frequency of word w on the whole corpus. We set the probability of sampling word w as (max w S(w ) − S(w))/Z where Z = w max w S(w ) − S(w) is a normalization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXTENDED EXPERIMENTS B.1 ABLATION STUDIES</head><p>Ablation Studies for Unlabeled Data Size Here we present an ablation study for unlabeled data sizes. As shown in Table <ref type="table" target="#tab_10">6 and Table 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations Studies on RandAugment</head><p>We hypothesize that the success of RandAugment should be credited to the diversity of the augmentation transformations, since RandAugment works very well for multiple different datasets while it does not require a search algorithm to find out the most effective policies. To verify this hypothesis, we test UDA's performance when we restrict the number of possible transformations used in RandAugment. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the performance gradually improves as we use more augmentation transformations. As shown in Table <ref type="table">8</ref>, on Yelp-5, where there is a lot more unlabeled data than labeled data, TSA reduces the error rate from 50.81 to 41.35 when compared to the baseline without TSA. More specifically, the best performance is achieved when we choose to postpone releasing the supervised training signal to the end of the training, i.e, exp-schedule leads to the best performance. Teacher <ref type="bibr" target="#b41">(Tarvainen &amp; Valpola, 2017)</ref>. Fully supervised learning using 50,000 examples achieves an error rate of 4.23 and 5.36 with or without RandAugment. The performance of the baseline models are reported by MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019)</ref>.</p><p>To make sure that the performance reported by MixMatch and our results are comparable, we reimplement MixMatch in our codebase and find that the results in the original paper is comparable but slightly better than our reimplementation, which results in a more competitive comparison for UDA. For example, our reimplementation of MixMatch achieves an error rate of 7.00 ± 0.59 and 7.39 ± 0.11 with 4,000 and 2,000 examples. MixMatch uses a different model implementation and employs exponential moving average (EMA) on the model parameters, while we do not use EMA for our implementations.  Proof. Let x be the sampled test example. Then the probability of event A is</p><formula xml:id="formula_7">P r(A) = i P r(A and x ∈ C i ) = i P i (1 − P i ) m</formula><p>To bound the probability, we would like to find the maximum value of i P i (1 − P i ) m . We can define the following optimization function:</p><formula xml:id="formula_8">min P − ci P i (1 − P i ) m s.t. ci P i = 1</formula><p>The problem is a convex optimization problem and we can construct its the Lagrangian dual function:</p><formula xml:id="formula_9">L = i P i (1 − P i ) m − λ( i P i − 1)</formula><p>Using the KKT condition, we can take derivatives to P i and set it to zero. Then we have</p><formula xml:id="formula_10">λ = (1 − mP i )(1 − P i ) m−1</formula><p>Hence P i = P j for any i = j. Using the fact that i P i = 1, we have</p><formula xml:id="formula_11">P i = 1 k</formula><p>Plugging the result back into P r(A) = i P i (1 − P i ) m , we have</p><formula xml:id="formula_12">P r(A) ≤ (1 − 1 k ) m = exp(m log(1 − 1 k )) ≤ exp(− m k )</formula><p>Hence when m = O( k ), we have P r(A) = O( )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXTENDED RELATED WORK</head><p>Semi-supervised Learning. Due to the long history of semi-supervised learning (SSL), we refer readers to <ref type="bibr">(Chapelle et al., 2009)</ref> for a general review. More recently, many efforts have been made to renovate classic ideas into deep neural instantiations. For example, graph-based label propagation <ref type="bibr">(Zhu et al., 2003)</ref> has been extended to neural methods via graph embeddings <ref type="bibr" target="#b45">(Weston et al., 2012;</ref><ref type="bibr" target="#b46">Yang et al., 2016)</ref> and later graph convolutions <ref type="bibr">(Kipf &amp; Welling, 2016)</ref>. Similarly, with the variational auto-encoding framework and reinforce algorithm, classic graphical models based SSL methods with target variable being latent can also take advantage of deep architectures <ref type="bibr">(Kingma et al., 2014;</ref><ref type="bibr" target="#b18">Maaløe et al., 2016;</ref><ref type="bibr" target="#b47">Yang et al., 2017)</ref>. Besides the direct extensions, it was found that training neural classifiers to classify out-of-domain examples into an additional class <ref type="bibr" target="#b36">(Salimans et al., 2016)</ref> works very well in practice. Later, <ref type="bibr" target="#b11">Dai et al. (2017)</ref> shows that this can be seen as an instantiation of low-density separation.</p><p>Apart from enforcing consistency on the noised input examples and the hidden representations, another line of research enforces consistency under different model parameters, which is complementary to our method. For example, Mean Teacher <ref type="bibr" target="#b41">(Tarvainen &amp; Valpola, 2017</ref>) maintains a teacher model with parameters being the ensemble of a student model's parameters and enforces the consistency between the predictions of the two models. Recently, <ref type="bibr" target="#b0">Athiwaratkun et al. (2018)</ref> propose fast-SWA that improves Mean Teacher by encouraging the model to explore a diverse set of plausible parameters. In addition to parameter-level consistency, SNTG <ref type="bibr" target="#b17">(Luo et al., 2018)</ref>  There are also recent works on generating diverse translations <ref type="bibr">(He et al., 2018;</ref><ref type="bibr" target="#b38">Shen et al., 2019;</ref><ref type="bibr">Kool et al., 2019)</ref> that might lead to further improvements when used as data augmentations.</p><p>Unsupervised Representation Learning. Apart from semi-supervised learning, unsupervised representation learning offers another way to utilize unsupervised data. <ref type="bibr" target="#b7">Collobert &amp; Weston (2008)</ref> demonstrated that word embeddings learned by language modeling can improve the performance significantly on semantic role labeling. Later, the pre-training of word embeddings was simplified and substantially scaled in Word2Vec <ref type="bibr" target="#b21">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref>  <ref type="formula">2018</ref>) have shown that pre-training using language modeling and denoising auto-encoding leads to significant improvements on many tasks in the language domain. There is also a growing interest in self-supervised learning for vision <ref type="bibr">(Zhai et al., 2019b;</ref><ref type="bibr">Hénaff et al., 2019;</ref><ref type="bibr" target="#b42">Trinh et al., 2019)</ref>.</p><p>Consistency Training in Other Domains. Similar ideas of consistency training has also been applied in other domains. For example, recently, enforcing adversarial consistency on unsupervised data has also been shown to be helpful in adversarial robustness <ref type="bibr" target="#b40">(Stanforth et al., 2019;</ref><ref type="bibr" target="#b51">Zhai et al., 2019a;</ref><ref type="bibr" target="#b24">Najafi et al., 2019;</ref><ref type="bibr" target="#b3">Carmon et al., 2019)</ref>. Enforcing consistency w.r.t data augmentation has also been shown to work well for representation learning <ref type="bibr" target="#b47">(Hu et al., 2017;</ref><ref type="bibr" target="#b48">Ye et al., 2019)</ref>. Invariant representation learning <ref type="bibr" target="#b16">(Liang et al., 2018;</ref><ref type="bibr" target="#b35">Salazar et al., 2018)</ref> applies the consistency loss not only to the predicted distributions but also to representations and has been shown significant improvements on speech recognition. Nevertheless, we find it works well to use all the unlabeled data.</p><p>Preprocessing. We find the sequence length to be an important factor in achieving good performance. For all text classification datasets, we truncate the input to 512 subwords since BERT is pretrained with a maximum sequence length of 512. Further, when the length of an example is greater than 512, we keep the last 512 subwords instead of the first 512 subwords as keeping the latter part of the sentence lead to better performances on IMDb.</p><p>Fine-tuning BERT on in-domain unsupervised data. We fine-tune the BERT model on in-domain unsupervised data using the code released by BERT. We try learning rate of 2e-5, 5e-5 and 1e-4, batch size of 32, 64 and 128 and number of training steps of 30k, 100k and 300k. We pick the fine-tuned models by the BERT loss on a held-out set instead of the performance on a downstream task.</p><p>Random initialized Transformer. For the experiments with randomly initialized Transformer, we adopt hyperparameters for BERT base except that we only use 6 hidden layers and 8 attention heads.</p><p>We also increase the dropout rate on the attention and the hidden states to 0.2, When we train UDA with randomly initialized architectures, we train UDA for 500k or 1M steps on Amazon-5 and Yelp-5 where we have abundant unlabeled data.</p><p>BERT hyperparameters. Following the common BERT fine-tuning procedure, we keep a dropout rate of 0.1, and try learning rate of 1e-5, 2e-5 and 5e-5 and batch size of 32 and 128. We also tune the number of steps ranging from 30 to 100k for various data sizes.</p><p>UDA hyperparameters. We set the weight on the unsupervised objective λ to 1 in all of our experiments. We use a batch size of 32 for the supervised objective since 32 is the smallest batch size on v3-32 Cloud TPU Pod. We use a batch size of 224 for the unsupervised objective when the Transformer is initialized with BERT so that the model can be trained on more unlabeled data. We find that generating one augmented example for each unlabeled example is enough for BERT FINETUNE .</p><p>All experiments in this part are performed on a v3-32 Cloud TPU Pod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 SEMI-SUPERVISED LEARNING BENCHMARKS CIFAR-10 AND SVHN</head><p>Hyperparameters for Wide-ResNet-28-2. We train our model for 500K steps. We apply Exponential Moving Average to the parameters with a decay rate of 0.9999. We use a batch size of 64 for labeled data and a batch size of 448 for unlabeled data. The softmax temperature τ is set to 0.4. The confidence threshold β is set to 0.8. We use a cosine learning rate decay schedule: cos( 7t 8T * π 2 ) where t is the current step and T is the total number of steps. We use a SGD optimizer with nesterov momentum with the momentum hyperparameter set to 0.9. In order to reduce training time, we generate augmented examples before training and dump them to disk. For CIFAR-10, we generate 100 augmented examples for each unlabeled example. Note that generating augmented examples in an online fashion is always better or as good as using dumped augmented examples since the model can see different augmented examples in different epochs, leading to more diverse samples. We report the average performance and the standard deviation for 10 runs. Experiments in this part are performed on a Tesla V100 GPU.</p><p>Hyperparameters for Shake-Shake and PyramidNet. For the experiments with Shake-Shake, we train UDA for 300k steps and use a batch size of 128 for the supervised objective and use a batch size of 512 for the unsuperivsed objective. For the experiments with PyramidNet+ShakeDrop, we train UDA for 700k steps and use a batch size of 64 for the supervised objective and a batch size of 128 for the unsupervised objective. For both models, we use a learning rate of 0.03 and use a cosine learning decay with one annealing cycle following AutoAugment. Experiments in this part are performed on a v3-32 Cloud TPU v3 Pod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 IMAGENET</head><p>10% Labeled Set Setting. Unless otherwise stated, we follow the standard hyperparameters used in an open-source implementation of ResNet.<ref type="foot" target="#foot_5">6</ref> For the 10% labeled set setting, we use a batch size of 512 for the supervised objective and a batch size of 15,360 for the unsupervised objective. We use a base learning rate of 0.3 that is decayed by 10 for four times and set the weight on the unsupervised objective λ to 20. We mask out unlabeled examples whose highest probabilities across categories are less than 0.5 and set the Softmax temperature to 0.4. The model is trained for 40k steps. Experiments in this part are performed on a v3-64 Cloud TPU v3 Pod.</p><p>Full Labeled Set Setting. For experiments on the full ImageNet, we use a batch size of 8,192 for the supervised objective and a batch size of 16,384 for the unsupervised objective. The weight on the unsupervised objective λ is set to 1. We use entropy minimization to sharpen the prediction. We use a base learning rate of 1.6 and decay it by 10 for four times. Experiments in this part are performed on a v3-128 Cloud TPU v3 Pod.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training objective for UDA, where M is a model that predicts a distribution of y given x.• Given an input x, compute the output distribution p θ (y | x) given x and a noised version p θ (y |x, ) by injecting a small noise . The noise can be applied to x or hidden states. • Minimize a divergence metric between the two distributions D (p θ (y | x) p θ (y | x, )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Augmented examples using back-translation and RandAugment.</figDesc><graphic url="image-1.png" coords="4,156.23,456.05,76.44,57.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison with two semi-supervised learning methods on CIFAR-10 and SVHN with varied number of labeled examples. Vary model architecture. Next, we directly compare UDA with previously published results under different model architectures. Following previous work, 4k and 1k labeled examples are used for CIFAR-10 and SVHN respectively. As shown in Table3, given the same architecture, UDA outperforms all published results by significant margins and nearly matches the fully supervised performance, which uses 10x more labeled examples. This shows the huge potential of state-of-theart data augmentations under the consistency training framework in the vision domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three schedules of TSA. We set ηt = α t * (1 − 1 K ) + 1 K . α t is set to 1 − exp(− t T * 5),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Error rate of UDA on CIFAR-10 with different numbers of possible transformations in RandAugment. UDA achieves lower error rate when we increase the number of possible transformations, which demonstrates the importance of a rich set of augmentation transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy on IMDb and Yelp-2 with different number of labeled examples. In the largedata regime, with the full training set of IMDb, UDA also provides robust gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>In our semi-supervised setting, we randomly sampled labeled examples from the full supervised set 4 and use the same number of examples for each category. For unlabeled data, we use the whole training set for DBPedia, the concatenation of the training set and the unlabeled set for IMDb and external data for Yelp-2, Yelp-5, Amazon-2 and Amazon-5<ref type="bibr" target="#b20">(McAuley et al., 2015)</ref> 5 . Note that for Yelp and Amazon based datasets, the label distribution of the unlabeled set might not match with that of labeled datasets since there are different number of examples in different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Diverse noise: Advanced data augmentation can generate a diverse set of examples since it can make large modifications to the input example without changing its label, while simple Gaussian noise only make local changes. Encouraging consistency on a diverse set of augmented examples can significantly improve the sample efficiency.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Error rates on CIFAR-10.</figDesc><table><row><cell>Augmentation</cell><cell>Sup</cell><cell>Semi-Sup</cell><cell>Augmentation</cell><cell>Sup</cell><cell>Semi-sup</cell></row><row><cell cols="2">(# Sup examples) (50k)</cell><cell>(4k)</cell><cell cols="2">(# Sup examples) (650k)</cell><cell>(2.5k)</cell></row><row><cell>Crop &amp; flip</cell><cell>5.36</cell><cell>10.94</cell><cell></cell><cell>38.36</cell><cell>50.80</cell></row><row><cell>Cutout</cell><cell>4.42</cell><cell>5.43</cell><cell>Switchout</cell><cell>37.24</cell><cell>43.38</cell></row><row><cell>RandAugment</cell><cell>4.23</cell><cell>4.32</cell><cell>Back-translation</cell><cell>36.71</cell><cell>41.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Error rate on Yelp-5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>, given the same architecture, UDA outperforms all published results by significant margins and nearly matches the fully supervised performance, which uses 10x more labeled examples. This shows the huge potential of state-of-theart data augmentations under the consistency training framework in the vision domain.</figDesc><table><row><cell>4.3 EVALUATION ON TEXT CLASSIFICATION DATASETS</cell></row><row><cell>Next, we further evaluate UDA in the language domain. Moreover, in order to test whether UDA can</cell></row><row><cell>be combined with the success of unsupervised representation learning, such as BERT (Devlin et al.,</cell></row><row><cell>2018), we further consider four initialization schemes: (a) random Transformer; (b) BERT BASE ;</cell></row><row><cell>(c) BERT LARGE ; (d) BERT</cell></row></table><note>FINETUNE : BERT LARGE fine-tuned on in-domain unlabeled data 3 . Under each of these four initialization schemes, we compare the performances with and without UDA.The results are presented in Table4where we would like to emphasize three observations:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between methods using different models where PyramidNet is used with ShakeDrop regularization. On CIFAR-10, with only 4,000 labeled examples, UDA matches the performance of fully supervised Wide-ResNet-28-2 and PyramidNet+ShakeDrop, where they have an error rate of 5.4 and 2.7 respectively when trained on 50,000 examples without RandAugment. On SVHN, UDA also matches the performance of our fully supervised model trained on 73,257 examples without RandAugment, which has an error rate of 2.84.</figDesc><table><row><cell>-Large</cell><cell>3.1M</cell><cell>7.29 ± 0.02</cell><cell>3.89 ± 0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Error rates on text classification datasets. In the fully supervised settings, the pre-BERT SO-TAs include ULMFiT (Howard &amp; Ruder, 2018) for Yelp-2 and Yelp-5, DPCNN (Johnson &amp; Zhang, 2017) for Amazon-2 and Amazon-5, Mixed VAT (Sachan et al., 2018) for IMDb and DBPedia. All of our experiments use a sequence length of 512.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>and CPC (Hénaff et al., 2019) also show significant improvements on ImageNet. Top-1 / top-5 accuracy on ImageNet with 10% and 100% of the labeled set. We use image size 224 and 331 for the 10% and 100% experiments respectively.</figDesc><table><row><cell>Methods</cell><cell>SSL</cell><cell>10%</cell><cell>100%</cell></row><row><cell>ResNet-50</cell><cell></cell><cell cols="2">55.09 / 77.26 77.28 / 93.73</cell></row><row><cell>w. RandAugment</cell><cell></cell><cell cols="2">58.84 / 80.56 78.43 / 94.37</cell></row><row><cell>UDA (RandAugment)</cell><cell></cell><cell cols="2">68.78 / 88.80 79.05 / 94.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. Ryuichiro Hataya and Hideki Nakayama. Unifying semi-supervised and robust learning by mixup. ICLR The 2nd Learning from Limited Labeled Data (LLD) Workshop, 2019. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S 4 l: Self-supervised semisupervised learning. In Proceedings of the IEEE international conference on computer vision, 2019b. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.</figDesc><table><row><cell>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-</cell></row><row><cell>nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. sification. In Advances in neural information processing systems, pp. 649-657, 2015.</cell></row><row><cell>770-778, 2016. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian</cell></row><row><cell>Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. Sequence to sequence mixture model for fields and harmonic functions. In Proceedings of the 20th International conference on Machine</cell></row><row><cell>diverse machine translation. arXiv preprint arXiv:1810.07391, 2018. learning (ICML-03), pp. 2003.</cell></row><row><cell>Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient</cell></row><row><cell>image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.</cell></row><row><cell>Alex Hernández-García and Peter König. Data augmentation instead of explicit regularization. arXiv</cell></row><row><cell>preprint arXiv:1806.03852, 2018.</cell></row><row><cell>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv</cell></row><row><cell>preprint arXiv:1503.02531, 2015.</cell></row><row><cell>Jacob Jackson and John Schulman. Semi-supervised learning by label gradient alignment. arXiv</cell></row><row><cell>preprint arXiv:1902.02336, 2019.</cell></row><row><cell>Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text categorization.</cell></row><row><cell>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</cell></row><row><cell>(Volume 1: Long Papers), volume 1, pp. 562-570, 2017.</cell></row><row><cell>Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised</cell></row><row><cell>learning with deep generative models. In Advances in neural information processing systems, pp.</cell></row><row><cell>3581-3589, 2014.</cell></row><row><cell>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-</cell></row><row><cell>works. arXiv preprint arXiv:1609.02907, 2016.</cell></row><row><cell>Wouter Kool, Herke van Hoof, and Max Welling. Stochastic beams and where to find</cell></row><row><cell>them: The gumbel-top-k trick for sampling sequences without replacement. arXiv preprint</cell></row><row><cell>arXiv:1903.06059, 2019.</cell></row><row><cell>Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-</cell></row><row><cell>nical report, Citeseer, 2009.</cell></row><row><cell>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-</cell></row><row><cell>lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,</cell></row><row><cell>2012.</cell></row></table><note>Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 328-339, 2018. Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1558-1567. JMLR. org, 2017. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, volume 3, pp. 2, 2013.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>To tackle this difficulty, we introduce a new training technique, called Training Signal Annealing (TSA), which gradually releases the "training signals" of the labeled examples as training progresses.Intuitively, we only utilize a labeled example if the model's confidence on that example is lower than a predefined threshold which increases according to a schedule. Specifically, at training step t, if the model's predicted probability for the correct category p θ (y * | x) is higher than a threshold η t , we remove that example from the loss function. Suppose K is the number of categories, by gradually increasing η t from 1</figDesc><table /><note>Kto 1, the threshold η t serves as a ceiling to prevent over-training on easy labeled examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>, given the same number of labeled examples, reducing the number of unsupervised examples clearly leads to worse performance. In fact, having abundant unsupervised examples is more important than having more labeled examples since reducing the unlabeled data amount leads to worse performance than reducing the labeled data by the same ratio. Error rate (%) for CIFAR-10 with different amounts of labeled data and unlabeled data.</figDesc><table><row><cell># Unsup / # Sup</cell><cell>250</cell><cell>500</cell><cell>1,000</cell><cell>2,000</cell><cell>4,000</cell></row><row><cell>50,000</cell><cell>5.43 ± 0.96</cell><cell>4.80 ± 0.09</cell><cell>4.75 ± 0.10</cell><cell>4.73 ± 0.14</cell><cell>4.32 ± 0.08</cell></row><row><cell>20,000</cell><cell>11.01 ± 1.01</cell><cell>9.46 ± 0.14</cell><cell>8.57 ± 0.14</cell><cell>7.65 ± 0.17</cell><cell>7.31 ± 0.24</cell></row><row><cell>10,000</cell><cell cols="5">23.17 ± 0.71 18.43 ± 0.43 15.46 ± 0.58 12.52 ± 0.13 10.32 ± 0.20</cell></row><row><cell>5,000</cell><cell cols="5">35.41 ± 0.75 28.35 ± 0.60 22.06 ± 0.71 17.36 ± 0.15 13.19 ± 0.12</cell></row><row><cell># Unsup / # Sup</cell><cell>250</cell><cell>500</cell><cell>1,000</cell><cell>2,000</cell><cell>4,000</cell></row><row><cell>73,257</cell><cell>2.72 ± 0.40</cell><cell>2.27 ± 0.09</cell><cell cols="3">2.23 ± 0.07 2.20 ± 0.06 2.28 ± 0.10</cell></row><row><cell>20,000</cell><cell>5.59 ± 0.74</cell><cell>4.43 ± 0.15</cell><cell cols="3">3.81 ± 0.11 3.86 ± 0.14 3.64 ± 0.20</cell></row><row><cell>10,000</cell><cell>17.13 ± 12.85</cell><cell>7.59 ± 1.01</cell><cell cols="3">5.76 ± 0.29 5.17 ± 0.12 5.40 ± 0.12</cell></row><row><cell>5,000</cell><cell>31.58 ± 7.39</cell><cell cols="4">12.66 ± 0.81 6.28 ± 0.25 8.35 ± 0.36 7.76 ± 0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Error rate (%) for SVHN with different amounts of labeled data and unlabeled data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Error rate (%) for CIFAR-10.Results with varied label set sizes on SVHN In Table10, we similarly show results for compared methods of Figure3band results of methods mentioned above. Fully supervised learning using 73,257 examples achieves an error rate of 2.28 and 2.84 with or without RandAugment. The performance of the baseline models are reported by MixMatch<ref type="bibr" target="#b2">(Berthelot et al., 2019)</ref>. Our reimplementation of MixMatch also resulted in comparable but higher error rates than the reported ones.</figDesc><table><row><cell>Methods / # Sup</cell><cell>250</cell><cell>500</cell><cell>1,000</cell><cell>2,000</cell><cell>4,000</cell></row><row><cell>Pseudo-Label</cell><cell cols="5">49.98 ± 1.17 40.55 ± 1.70 30.91 ± 1.73 21.96 ± 0.42 16.21 ± 0.11</cell></row><row><cell>Π-Model</cell><cell cols="5">53.02 ± 2.05 41.82 ± 1.52 31.53 ± 0.98 23.07 ± 0.66 17.41 ± 0.37</cell></row><row><cell>Mean Teacher</cell><cell cols="5">47.32 ± 4.71 42.01 ± 5.86 17.32 ± 4.00 12.17 ± 0.22 10.36 ± 0.25</cell></row><row><cell>VAT</cell><cell cols="5">36.03 ± 2.82 26.11 ± 1.52 18.68 ± 0.40 14.40 ± 0.15 11.05 ± 0.31</cell></row><row><cell>MixMatch</cell><cell>11.08 ± 0.87</cell><cell>9.65 ± 0.94</cell><cell>7.75 ± 0.32</cell><cell>7.03 ± 0.15</cell><cell>6.24 ± 0.06</cell></row><row><cell>UDA (RandAugment)</cell><cell>5.43 ± 0.96</cell><cell>4.80 ± 0.09</cell><cell>4.75 ± 0.10</cell><cell>4.73 ± 0.14</cell><cell>4.32 ± 0.08</cell></row><row><cell>Methods / # Sup</cell><cell>250</cell><cell>500</cell><cell>1,000</cell><cell>2,000</cell><cell>4,000</cell></row><row><cell>Pseudo-Label</cell><cell cols="5">21.16 ± 0.88 14.35 ± 0.37 10.19 ± 0.41 7.54 ± 0.27 5.71 ± 0.07</cell></row><row><cell>Π-Model</cell><cell cols="2">17.65 ± 0.27 11.44 ± 0.39</cell><cell>8.60 ± 0.18</cell><cell cols="2">6.94 ± 0.27 5.57 ± 0.14</cell></row><row><cell>Mean Teacher</cell><cell>6.45 ± 2.43</cell><cell>3.82 ± 0.17</cell><cell>3.75 ± 0.10</cell><cell cols="2">3.51 ± 0.09 3.39 ± 0.11</cell></row><row><cell>VAT</cell><cell>8.41 ± 1.01</cell><cell>7.44 ± 0.79</cell><cell>5.98 ± 0.21</cell><cell cols="2">4.85 ± 0.23 4.20 ± 0.15</cell></row><row><cell>MixMatch</cell><cell>3.78 ± 0.26</cell><cell>3.64 ± 0.46</cell><cell>3.27 ± 0.31</cell><cell cols="2">3.04 ± 0.13 2.89 ± 0.06</cell></row><row><cell>UDA (RandAugment)</cell><cell>2.72 ± 0.40</cell><cell>2.27 ± 0.09</cell><cell>2.23 ± 0.07</cell><cell cols="2">2.20 ± 0.06 2.28 ± 0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Error rate (%) for SVHN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>also enforces input-level consistency by constructing a similarity graph between unlabeled examples. Data Augmentation. Also related to our work is the field of data augmentation research. Besides the conventional approaches and two data augmentation methods mentioned in Section 2.1, a recent approach MixUp(Zhang et al., 2017)  goes beyond data augmentation from a single data point and performs interpolation of data pairs to achieve augmentation. Recently, Hernández-García &amp; König (2018) have shown that data augmentation can be regarded as a kind of explicit regularization methods similar to Dropout.Diverse Back Translation. Diverse paraphrases generated by back-translation has been a key component in the significant performance improvements in our text classification experiments. We use random sampling instead of beam search for decoding similar to the work by<ref type="bibr" target="#b14">Edunov et al. (2018)</ref>.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code is available at https://github.com/google-research/uda.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We also note that while translation uses a labeled dataset, the translation task itself is quite distinctive from a text classification task and does not make use of any text classification label. In addition, back-translation is a general data augmentation method that can be applied to many tasks with the same model checkpoints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">One exception is that we do not pursue BERTFINETUNE on DBPedia as fine-tuning BERT on DBPedia does not yield further performance gain. This is probably due to the fact that DBPedia is based on Wikipedia while BERT is already trained on the whole Wikipedia corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">http://bit.ly/2kRWoof, https://ai.stanford.edu/ ˜amaas/data/sentiment/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://www.kaggle.com/yelp-dataset/yelp-dataset, http://jmcauley.ucsd. edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/tensorflow/tpu/tree/master/models/official/resnet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We want to thank Hieu Pham, Adams Wei Yu, Zhilin Yang and Ekin Dogus Cubuk for their tireless help to the authors on different stages of this project and thank Colin Raffel for pointing out the connections between our work and previous works. We also would like to thank Olga Wichrowska, Barret Zoph, Jiateng Xie, Guokun Lai, Yulun Du, Chen Dan, David Berthelot, Avital Oliver, Trieu  Trinh, Ran Zhao, Ola Spyra, Brandon Yang, Daiyi Peng, Andrew Dai, Samy Bengio, Jeff  Dean and the Google Brain team for insightful discussions and support to the work. Lastly, we thank anonymous reviewers for their valueable feedbacks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13736</idno>
		<title level="m">Unlabeled data improves adversarial robustness</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<editor>. et al.</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Franc ¸ois Chollet. Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2017</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08370</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Practical data augmentation with no separate search</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Good semisupervised learning that requires a bad gan</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning noise-invariant representations for robust speech recognition</title>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8896" to="8905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
				<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13021</idno>
		<title level="m">Robustness to adversarial perturbations in learning from incomplete data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Daniel S Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkeon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-Jin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Revisiting lstm networks for semi-supervised text classification via mixed objective function</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Invariant representation learning for robust deep networks</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Integration of Deep Learning Theories</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07816</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognitiontangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Are labels required for improving adversarial robustness?</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13725</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining for image embedding</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Trieu H Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Selfie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Switchout: an efficient data augmentation algorithm for neural machine translation</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07512</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semi-supervised qa with generative domain-adaptive nets</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02206</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Runtian</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00555</idno>
		<title level="m">Adversarially robust generalization just requires more unlabeled data</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
