<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilayer and Multimodal Fusion of Deep Neural Networks for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
							<email>xiaodongy@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
							<email>pmolchanov@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
						</author>
						<title level="a" type="main">Multilayer and Multimodal Fusion of Deep Neural Networks for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CCEC738ECABBB4E74703F84F2005EB17</idno>
					<idno type="DOI">10.1145/2964284.2964297</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Information systems → Multimedia and multimodal retrieval</term>
					<term>•Computing methodologies → Video summarization</term>
					<term>Video Classification</term>
					<term>Deep Neural Networks</term>
					<term>Boosting</term>
					<term>Fusion</term>
					<term>CNN</term>
					<term>RNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel framework to combine multiple layers and modalities of deep neural networks for video classification. We first propose a multilayer strategy to simultaneously capture a variety of levels of abstraction and invariance in a network, where the convolutional and fully connected layers are effectively represented by our proposed feature aggregation methods. We further introduce a multimodal scheme that includes four highly complementary modalities to extract diverse static and dynamic cues at multiple temporal scales. In particular, for modeling the long-term temporal information, we propose a new structure, FC-RNN, to effectively transform pre-trained fully connected layers into recurrent layers. A robust boosting model is then introduced to optimize the fusion of multiple layers and modalities in a unified way. In the extensive experiments, we achieve state-of-the-art results on two public benchmark datasets: UCF101 and HMDB51.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Content-based video classification is fundamental to intelligent video analytics including automatic categorizing, searching, indexing, segmentation, and retrieval of videos. It has been applied to a wide range of real-word applications, for instance, surveillance event detection <ref type="bibr" target="#b49">[49]</ref>, semantic indexing <ref type="bibr" target="#b1">[1]</ref>, gesture control <ref type="bibr" target="#b11">[11]</ref>, and so forth. It is a challenging task to recognize unconstrained videos because 1) an appropriate video representation can be task-dependent, e.g., Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</p><p>MM <ref type="bibr">'16, October 15-19, 2016</ref>, Amsterdam, The Netherlands. coarse ("swim" vs. "run") or fine-grained ("walk" vs. "run") categorizations; 2) there may be multiple streams of information that need to be taken into account, such as actions, objects, scenes, etc.; 3) there are large intra-class variations, which arise from diverse viewpoints, occlusions and backgrounds. As the core information of videos, visual cues provide the most significant information for video classification. Most traditional methods rely on the bag-of-visual-words (BOV) representation which consists of computing and aggregating visual features <ref type="bibr" target="#b13">[13]</ref>. A variety of local and global visual features have been proposed, for instance, GIST <ref type="bibr" target="#b31">[31]</ref> and SIFT <ref type="bibr" target="#b27">[27]</ref> can be used to capture static information in spatial frames, while STIP <ref type="bibr" target="#b23">[23]</ref> and improved dense trajectories (iDT) <ref type="bibr" target="#b44">[44]</ref> are widely employed to compute both appearance and motion cues in videos.</p><p>There is a growing trend to learn robust feature representations with deep neural networks for various tasks such as image classification <ref type="bibr" target="#b20">[20]</ref>, object detection <ref type="bibr" target="#b35">[35]</ref>, natural language processing <ref type="bibr" target="#b40">[40]</ref>, and speech recognition <ref type="bibr" target="#b6">[6]</ref>. As one of the most successful network architectures, the recent surge of convolutional neural networks (CNN) has given rise to a number of methods to employ CNN for video classification. Karparthy et al. <ref type="bibr" target="#b19">[19]</ref> made the first attempt to use a buffer of video frames as input to networks, however, the results were inferior to those of the best hand-engineered features <ref type="bibr" target="#b44">[44]</ref>. Tran et al. <ref type="bibr" target="#b42">[42]</ref> proposed C3D using 3D-CNN over short video clips to learn appearance and micro-motion features simultaneously. However, these methods focus on short or mid-term information as feature representations are learned in short-time windows. This is insufficient for video classification since complex events are better described by leveraging the temporal evolution of short-term contents. In order to capture long-term temporal clues in videos, recurrent neural networks (RNN) were applied to explicitly model videos as an ordered sequence of frames <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b29">29]</ref>.</p><p>CNN-based video classification algorithms typically make predictions using the softmax scores or, alternatively, they use the last fully connected layer as a feature representation <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b42">42]</ref>, because CNN hierarchically compute abstract and invariant representations of the inputs. However, leveraging information across multiple levels in a network has proven beneficial for several tasks such as natural scene recognition <ref type="bibr" target="#b48">[48]</ref>, object segmentation <ref type="bibr" target="#b26">[26]</ref> and optical flow computation <ref type="bibr" target="#b10">[10]</ref>. This is somewhat expected since convolutional layers retain spatial information as opposed to fully connected layers. For video classification, we argue that appropriate levels of abstraction and invariance in CNN for video representation are also task-and class-dependent. For example, We use four modalities to extract highly complementary information across multiple temporal scales. For each single modality, discriminative representations are computed for convolutional and fully connected layers. We employ an effective boosting model to fuse the multiple layers and modalities. Box colors are encoded according to different networks: 2D-CNN and 3D-CNN with and without RNN. We propose FC-RNN to model long-term temporal information rather than using the standard RNN structure.</p><p>distinguishing "soccer game" and "basketball game" requires high-level representations to model global scene statistics. However, classification of "playing guitar" and "playing violin" demands fine-scale features to capture subtle appearance and motion features. Therefore, leveraging the multilayer abstractions is expected to simplify video classification.</p><p>Although a significant progress in recent years has been achieved in the development of feature learning by deep neural networks <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b37">37]</ref>, it is clear that none of the features have the same discriminative capability over all classes. For example, videos of "wedding ceremony" are strongly associated with static scenes and objects, while "kissing" is more related to dynamic motions. It is therefore widely accepted to adaptively combine a set of complementary features rather than using a single feature for all classes. Simonyan et al. <ref type="bibr" target="#b36">[36]</ref> proposed the two-stream networks based on 2D-CNN to explicitly incorporate motion information from optical flow to complement the static per-frame information. Simple late fusion was adopted to combine the softmax scores of two networks by either averaging or with a linear classifier. This method has been widely utilized for video analysis <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b46">46]</ref> thanks to the two complementary modalities and outstanding performance. Nevertheless, the question of which robust modalities to exploit and how to effectively perform multimodal fusion still remains open for video classification.</p><p>In this paper, we propose a multilayer and multimodal fusion framework of deep neural networks for video classification. The multilayer strategy can simultaneously cap-ture a variety of levels of abstractions, thus is able to adapt from coarse-to fine-grained categorizations. Instead of using only two modalities as in the two-stream networks <ref type="bibr" target="#b36">[36]</ref>, we propose to use four complementary modalities in our multimodal scheme, i.e., 2D-CNN on a single spatial (color) frame and optical flow image as well as 3D-CNN on a short clip of spatial (color) frames and optical flow images. They not only effectively harness cues about static objects and dynamic motions in videos but also effectively exploit across temporal scales. As for the fusion of multiple layers and modalities, we adopt a powerful boosting model to learn their optimal combination. Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the overview of our proposed multilayer and multimodal fusion framework. Given an input video, the four modalities are used to extract complementary information at short and mid-term temporal scales. Instead of using the standard RNN structure, we propose FC-RNN to model the long-term temporal evolution across a whole video. FC-RNN takes advantage of pre-trained networks to transform the pre-trained fully-connected (fc) layers into recurrent layers. In the following, we use 2D-CNN-SF, 2D-CNN-OF, 3D-CNN-SF, 3D-CNN-OF to indicate 2D-CNN and 3D-CNN on spatial (color) frames and optical flow, respectively. For each individual network, an improved Fisher vector (iFV) is proposed to represent convolutional (conv) layers and an explicit feature map is used to represent the fc layers. We then employ a robust boosting model to learn the optimal combination of multiple layers and modalities. The main contributions of this paper are summarized as follows:</p><p>• We present a multilayer fusion strategy to capture multiple levels of abstraction and invariance in a single network. We propose to use the iFV and explicit feature maps to represent features of conv and fc layers.</p><p>• We introduce a multimodal fusion scheme to incorporate four highly complementary modalities to extract static and dynamic cues from multiple temporal scales.</p><p>In particular, we propose FC-RNN to preserve the generalization properties of pre-trained networks.</p><p>• We adopt an effective boosting model for video classification by fusing multiple layers and modalities in an optimal and unified way.</p><p>• In the extensive experiments, our method achieves superior results on the well-known UCF101 and HMDB51 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Videos have been studied by the multimedia community for decades. Over the years a variety of problems like multimedia event recounting, surveillance event detection, action search, and many more have been proposed. A large family of these studies is about video classification. Conventional video classification systems hinge on extraction of local features, which have been largely advanced in both detection and description. Local features can be densely sampled or selected by maximizing specific saliency functions. Laptev <ref type="bibr" target="#b23">[23]</ref> proposed STIP to detect sparse spacetime interest points by extending the 2D Harris corner detector into 3D. Wang et al. <ref type="bibr" target="#b44">[44]</ref> introduced the improved dense trajectories (iDT) to densely sample and track interest points from multiple spatial scales, where each tracked interest point generates a set of descriptors to represent shape and motion. Many successful video classification systems use iDT together with the motion boundary histogram (MBH) descriptor, which comprises the gradient of horizontal and vertical components of optical flow. It is widely recognized as the state-of-the-art feature for video analysis.</p><p>After local feature extraction, a number of coding techniques have been proposed for feature quantization, e.g., sparse coding <ref type="bibr" target="#b28">[28]</ref> and locality-constrained linear coding <ref type="bibr" target="#b45">[45]</ref>. Then average pooling and max pooling are normally used to aggregate statistics from local features. Several more advanced coding methods, e.g., Fisher vector (FV) <ref type="bibr" target="#b34">[34]</ref> and vector of locally aggregated descriptors (VLAD) <ref type="bibr" target="#b16">[16]</ref>, have emerged to reserve high order statistics of local features and achieve noticeably better performance. However, these methods obviously incur the loss of spatio-temporal order of local features. Extensions to the completely orderless aggregation methods include spatio-temporal pyramids <ref type="bibr" target="#b24">[24]</ref> and super sparse coding vectors <ref type="bibr" target="#b50">[50]</ref>. Graphical models, such as hidden Markov models (HMM) and conditional random fields (CRF), are also popular methods to explore the longterm temporal information in videos.</p><p>Many improvements of video classification are motivated by advances in the image domain. The breakthrough on image classification <ref type="bibr" target="#b20">[20]</ref> also rekindled the interest in deep neural networks for video classification. The pioneering work of Karpathy et al. <ref type="bibr" target="#b19">[19]</ref> trained 2D-CNN on various forms of stacked video frames from Sports-1M. However, these deep networks are quite inferior to the shallow model based on the best hand-engineered features <ref type="bibr" target="#b44">[44]</ref>. This is because complex motions and long-term temporal patterns are difficult to learn only through the simply stacked video frames. Simonyan et al. <ref type="bibr" target="#b36">[36]</ref> designed the two-stream networks with two 2D-CNNs on spatial and temporal streams. This method takes advantage of the large-scale ImageNet <ref type="bibr" target="#b20">[20]</ref> dataset for pre-training and significantly reduces the complexity to model dynamic motions through optical flow. Ji et al. <ref type="bibr" target="#b17">[17]</ref> employed a head tracker and human detector to segment human regions in videos. The segmented regions are stacked as video volumes and used as inputs for 3D-CNN to recognize human actions. Tran et al. <ref type="bibr" target="#b42">[42]</ref> applied 3D-CNN on full video frames to avoid pre-processing and jointly captures appearance and motion information. With these methods, similar results to the hand-engineered features <ref type="bibr" target="#b44">[44]</ref> have been reported. In contrast to the previous methods with only single or dual modalities, we propose to use four complementary modalities during multimodal fusion.</p><p>The aforementioned models only concentrate on motions during short period and lack considerations of long-term temporal clues that are vital for video classification. Several methods have been proposed to address this limitation. Ng et al. <ref type="bibr" target="#b29">[29]</ref> explored two schemes to handle full-length videos. They proposed various temporal feature pooling architectures and explored RNN with long short-term memory (LSTM) cells. The trajectory-pooled deep convolutional descriptor (TDD) was presented in <ref type="bibr" target="#b46">[46]</ref> to incorporate temporal nature by trajectory constrained sampling and pooling. TDD shares the advantages of both hand-engineered features and deep-learned representations. While the improved networks using RNN can model long-term temporal order, our proposed multimodal method provides multi-temporal scales with short, mid, and long-term time contexts.</p><p>Recent work has investigated reasoning across multiple hierarchical levels in a network, which was shown to be advantageous for several tasks. Hariharan et al. <ref type="bibr" target="#b14">[14]</ref> proposed hypercolumns for image segmentation and fine-grained localization. A hypercolumn at a given location is the vector containing all the values above that location at all layers of the CNN. DAG-CNN <ref type="bibr" target="#b48">[48]</ref> introduced a multi-scale architecture to learn scale-specific features for natural scene recognition. FlowNet <ref type="bibr" target="#b10">[10]</ref> preserved feature maps of both coarser and lower layers for optical flow estimation. We propose to extract feature representations from multiple layers to reason at multi-scale abstraction and invariance for video classification.</p><p>Combining multiple complementary feature representations is often effective to improve classification. Tamrakar et al. <ref type="bibr" target="#b41">[41]</ref> evaluated various early and late fusion strategies in the context of multimedia event detection. Zhang et al. <ref type="bibr" target="#b52">[52]</ref> computed non-linear kernels for each feature type and summed up the kernels for SVM training. Multiple kernel learning (MKL) <ref type="bibr" target="#b22">[22]</ref> is a popular approach to estimate feature combination weights. However, it was observed <ref type="bibr" target="#b12">[12]</ref> that simple averaging and geometric mean were highly competitive to MKL. Jiang et al. <ref type="bibr" target="#b18">[18]</ref> proposed to jointly compute a codebook of audio and visual features for video classification with the intention to model correlations between the two modalities. Ngiam et al. <ref type="bibr" target="#b30">[30]</ref> proposed a deep autoencoder to enforce cross modality learning between audio and video inputs. Our fusion method differs in combining robust boosting model with deep-learned representations from multiple layers and modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTILAYER REPRESENTATIONS</head><p>As a hierarchical feed-forward architecture, CNN progressively compute abstract and invariant representations of inputs. Recognition algorithms based on CNN often make predictions based on softmax scores or the last layer which is the summary of variables in the preceding layers. However, we argue that various abstractions such as pose, articulation, parts, objects, etc., learned in the intermediate layers can provide a complete description, from fine-scale to global scale, for video classification. Moreover, we propose a concept of convlet to utilize the spatial information reserved in conv layers to refine the final feature representation. In this section, we describe the detailed procedures to compute multilayer representations as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Improved Fisher Vector with the Convlet</head><p>Recent work on visualizing and understanding CNN reveals that conv layers demonstrate many intuitively desirable properties such as strong grouping within each feature map and exaggeration of discriminative parts of objects <ref type="bibr" target="#b51">[51]</ref>. Therefore, a set of appropriate levels of compositionality in conv layers are able to supply plenty of fine-scale information to the category-level semantics. Meanwhile, the features from the layers come for free because they are already extracted during the forward pass. Furthermore, compared to fc layers, conv layers contain the spatial information, which can be applied to adaptive pooling and feature refinement because the discriminative information for video classification is often unevenly distributed in spatial domain.</p><p>We start from defining the convlet which is used to measure the spatial discriminability of activations at a conv layer. Assume s l is the size (height and width) of a feature map and d l denotes the total number of feature maps. We represent a set of conv layers extracted from a video by C = {c t,l ; t = 1, . . . , T ; l = 1, . . . , Lc}, where T is the number of frames or short clips, Lc is the number of selected conv layers, and c t,l ∈ R s l ×s l ×d l indicates the l-th conv layer computed at the t-th timestamp. Since each convolutional kernel can be treated as a latent concept detector <ref type="bibr" target="#b47">[47]</ref>, we convert c t,l to s l × s l feature descriptors, each of which is with the responses of d l concept detectors. Thus a video can generate n l = s l × s l × T feature descriptors xi ∈ R d l at the l-th convolutional level, where i = 1, . . . , n l . Let R indicate the pre-defined spatial neighboring cells over a conv layer and Rj denote the j-th cell. We obtain the convlet corresponding to a spatial cell by</p><formula xml:id="formula_0">q j = G {xi}i∈R j , j = 1, . . . , |R|,<label>(1)</label></formula><p>where G is a general coding and pooling operator and we employ FV <ref type="bibr" target="#b34">[34]</ref> as G in our experiments. The convlet q j is a representation that aggregates xi in a local spatial region across the entire video, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. We then use each convlet q j to make video classification and the accuracy αj associated with Rj indicates how discriminative this local spatial cell is in a conv layer. We The heat map in Fig. <ref type="figure" target="#fig_3">3</ref> demonstrates the spatial weights learned by convlets of conv5 in VGG16 <ref type="bibr" target="#b37">[37]</ref> on the UCF101 dataset <ref type="bibr" target="#b38">[38]</ref>. The features close to boundary regions are much less discriminative than those in the middle, in particular for the left two corner regions. It is also interesting to observe that the hot regions are not exactly centered but a bit shifted towards the right. In addition, spatial weights of different conv layers in the same network often exhibit slightly different spatial distributions. Since the weight wi of xi represents how discriminative or important xi is for classification, we can take advantage of this property to improve a general feature aggregation method. We demonstrate the improvement to FV <ref type="bibr" target="#b34">[34]</ref> in this paper.</p><p>As assumed in FV, the feature descriptors xi have a Gaussian mixture model (GMM) distribution characterized by parameters {π k , µ k , σ k } with k = 1, . . . , K, where π k , µ k , and σ k are the prior mode probability, mean, and covariance (diagonal) of the k-th Gaussian component ϕ k . To better fit the diagonal covariance assumption, we apply PCA to decorrelate xi and reduce feature dimensions. Each feature xi is then encoded by the deviations with respect to the parameters of GMM. Let γ i,k be the soft assignment of xi to the k-th Gaussian component:</p><formula xml:id="formula_1">γ i,k = π k ϕ k (xi) K j=1 πjϕj (xi) .<label>(2)</label></formula><p>We obtain the improved Fisher vector (iFV) representation of a video at a convolutional layer by concatenating the following derivative vectors from K Gaussian components:</p><formula xml:id="formula_2">ρ k = 1 n l √ π k n l i=1 γ i,k wi xi -µ k σ k ,<label>(3)</label></formula><formula xml:id="formula_3">τ k = 1 n l √ 2π k n l i=1 γ i,k wi (xi -µ k ) 2 σ 2 k -1 ,<label>(4)</label></formula><p>where ρ k and τ k are the d l -dimensional derivatives with respect to µ k and σ k of the k-th Gaussian component. We apply the spatial discriminative factor wi to weight the relative displacements of xi to the mean and covariance in Eq. <ref type="bibr" target="#b3">(3)</ref><ref type="bibr" target="#b4">(4)</ref>.</p><p>In this way, more informative features gain higher contributions to the final representation, while background or noisy features are suppressed. We use iFV to compute the video representations of selected conv layers over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Pooling and Mapping</head><p>We represent a set of fc layers computed from a video by F = {f t,l ; t = 1, . . . , T ; l = 1, . . . , L f }, where f t,l ∈ R d l denotes the l-th fc layer computed at timestamp t. The fc vector is more sensitive to the category-level semantic information and usually has high dimensions (e.g., d l = 4096 in VGG16). Compared to c t,l , which can generate s l × s l features at each timestamp, f t,l is far more sparse as spatial information is lost. Considering these properties, we first apply temporal max pooling to aggregate f t,l across time to obtain f l , which is the initial representation of a video at the l-th fc level.</p><p>While the last fully-connected layer in a network performs linear classification, it is flexible to inject additional nonlinearity to f l by using non-linear kernels in SVM. However, non-linear SVM is generally much slower than linear one in terms of both learning and prediction. In particular, we are able to train linear SVM in time linear with the number of training samples. This favorably extends the applicability of linear SVM algorithms to large-scale data, which is usually the case for video classification. We thus employ the explicit feature map <ref type="bibr" target="#b43">[43]</ref> to approximate largescale non-linear SVM by the linear one. In explicit feature map, the initial representation f l is lifted to a Hilbert space with moderately higher feature dimensions through ψ : R d l → R d l (2z+1) such that the inner product in this space can reasonably well approximate a non-linear kernel κ, i.e., ψ(f l ), ψ(f l ) ≈ κ(f l , f l ). Therefore the final representation ψ(f l ) of a fc layer makes use of not only the discriminative power of non-linear kernels but also the efficient training and evaluation of the linear one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FC-RNN STRUCTURE</head><p>Most networks hinge on short or mid-term contents such as a single frame <ref type="bibr" target="#b36">[36]</ref> or a buffer of frames <ref type="bibr" target="#b42">[42]</ref>, where features are independently extracted for video classification. We believe that there are important connections between frames of the entire video and that the order of frames matters. To address this intuition, we propose a simple and effective structure, FC-RNN, to transform a network pretrained on separate frames or clips to deal with video as a whole sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initialization of Recurrent Layers</head><p>One of the straightforward ways to enable networks to work with video as a sequence is to introduce a stack of recurrent layers on top of the last fc layer. This method is common <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b29">29]</ref> and shows improvement in performance. The output of such a recurrent layer at timestamp t is computed as:</p><formula xml:id="formula_4">ht = H(W ih f t + W hh ht-1 + b h ), (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where H is an activation function, W ih is the input-tohidden matrix, f t is the input to this layer, W hh is the hidden-to-hidden matrix, ht-1 is a hidden state vector from previous timestamp, and b h is an optional bias. Both W ih and W hh are randomly initialized. We refer to this as the standard initialization.</p><p>One drawback of the standard initialization is that it requires to train an entire layer (or a stack of layers) from scratch even if a pre-trained network is used for feature extraction. This would result in reducing important generalization properties of a network that is fine-tuned on a relatively small dataset. In this paper, we propose to transform fc layers of a pre-trained CNN into recurrent layers. In this way, we preserve the structure of a pre-trained network as much as possible. Assume that a pre-trained fc layer at timestamp t has the structure:</p><formula xml:id="formula_6">f t = H(W ioy t + b f ),<label>(6)</label></formula><p>where W io is the pre-trained input-to-output matrix, y t is output of the previous layer and b f is bias. We suggest to transform it into a recurrent layer as:</p><formula xml:id="formula_7">f t = H(W ioy t + W hh f t-1 + b f ).<label>(7)</label></formula><p>This fc initialized recurrent structure is referred as FC-RNN. Fig. <ref type="figure" target="#fig_5">4</ref> illustrates the difference between our proposed FC-RNN and the standard RNN. Our method only introduces a single weight matrix that needs training from scratch, i.e., the hidden-to-hidden matrix W hh . Other weight matrices have been already pre-trained and can be just fine-tuned. We observe that this design is effective to reduce over-fitting and expedite convergence. LSTM is not used in our networks because 1) the complicated cell structure in LSTM is not well-adapted to our design; 2) the sequence of clips processed by 3D-CNN in a video is not long as each clip covers a number of non-overlapping frames; 3) LSTM has comparable results to standard RNN in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularization</head><p>We apply a number of regularization techniques in the training of FC-RNN. The recurrent connection is prone to learn the specific order of videos in the training set, therefore we randomly permute the order of training videos for each epoch. This operation slows down convergence but improves generalization. The regularization term which forces to learn weights with smaller 2-norm also helps generalization. With intention of preventing the gradients from exploding in recurrent layers, we employ soft gradient clipping in the following way. For each computed gradient g during stochastic gradient descent (SGD), we check if its 2-norm g is greater than a pre-defined threshold δ = 10. If that is the case, we rescale the gradient to g ← gδ/ g . We find that without gradient clipping the explosion of gradient values is a critical barrier to successfully training the networks. To further improve generalization, we train networks with drop-out on the outputs of recurrent layers. During training, we set the outputs of the recurrent layers to 0 with a probability of p = 0.5, and scale the activations of other neurons by a factor of 1/(1 -p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MULTIMODAL REPRESENTATIONS</head><p>Since the visual information in videos is a juxtaposition of not only scenes and objects but also atomic actions evolving over the whole video sequence, it is favorable to capture and combine both static appearances and dynamic motions.</p><p>To address this challenge we use a multimodal approach to model a variety of semantic clues in multi-temporal scales. Fig. <ref type="figure" target="#fig_1">1</ref> demonstrates our proposed four modalities, which provide mutually complementary information in short, mid, and long-term temporal contexts.</p><p>The two networks operating on spatial frames (single frame in 2D-CNN-SF and short clip of frames in 3D-CNN-SF) can capture objects and scenes that are strongly correlated to certain video categories, e.g., snow and mountains in Fig. <ref type="figure" target="#fig_1">1</ref> indicate skiing. 2D-CNN-SF is essentially an image classification network which can be built upon the recent advances in large-scale image recognition methods and datasets. 3D-CNN-SF selectively attends to both motion and appearance cues through spatio-temporal convolution and pooling operations. It encapsulates the mid-term temporal information as the network's input is a short video clip (e.g., 16 spatial frames). We utilize the proposed FC-RNN for 3D-CNN-SF to learn the long-term temporal order. The recurrent structure is not used for 2D-CNN-SF due to very limited improvement (0.1%). This is probably because the static information such as objects and scenes modeled by 2D-CNN-SF is not very correlated with the temporal evolution.</p><p>Since optical flow <ref type="bibr" target="#b2">[2]</ref> explicitly captures dynamic motions, the two networks running on optical flow images (single image in 2D-CNN-OF and short clip of images in 3D-CNN-OF) provide vital clues to recognize actions. Moreover, optical flow also conveys rough shape cues of moving objects, e.g., the skier and ski poles in Fig. <ref type="figure" target="#fig_1">1</ref>. Note, in contrast to the temporal stream <ref type="bibr" target="#b36">[36]</ref> used in most previous methods, which work on the stacked optical flow maps, we input a single colorized optical flow image to 2D-CNN-OF. As illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>, a colorized optical flow image contains 3 chan-nels with RGB values, while an optical flow map includes 2 channels with the raw values of horizontal and vertical displacements. The hue and saturation of an colorized optical flow image indicate flow's orientation and magnitude. This enables us to reduce over-fitting and training time by leveraging pre-trained models from large-scale image datasets. Since the input is a single colorized image, 2D-CNN-OF captures the fine-scale and short-term temporal information between a pair of adjacent frames. 3D-CNN-OF models the high order motion cues such as spatial and temporal derivatives of optical flow, which has been successfully applied to hand-engineered features <ref type="bibr" target="#b44">[44]</ref>. This modality also encapsulates the mid-term temporal clues. Similar to 3D-CNN-SF, FC-RNN is also employed to learn the long-term temporal order of 2D-CNN-OF and 3D-CNN-OF.</p><p>To obtain the final multimodal representation of a video, we use the aforementioned iFV as well as temporal max pooling and explicit feature map to compute the representations of selected conv and fc layers (respectively for each modality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FUSION BY BOOSTING</head><p>Given the above representations of multiple layers and modalities, in this section, we focus on how to effectively utilize correlations across different representations. We formulate the multilayer and multimodal fusion as a boosting task to maximize the classification accuracy.</p><p>We where ξi is a slack variable and ν is a regularization parameter to control the smoothness of the resulting function. This is essentially a linear programming problem and can be solved by the column generation approach <ref type="bibr" target="#b7">[7]</ref>. Similar to image classification <ref type="bibr" target="#b12">[12]</ref>, in the multiclass case with C categories we have two variations of the mixing coefficients. We call the first variant boost-u which jointly learns a uniform coefficient vector θ ∈ R M for all classes. The alternative one boost-c learns a coefficient vector for each class resulting in a coefficient matrix Θ ∈ R M ×C . So the final decision functions for the fusion of multiple layers and modalities with the two boosting variants are:</p><formula xml:id="formula_8">y(v) = arg max c=1,...,C M m=1 θm Km(v) T ac,m + bc,m ,<label>(9)</label></formula><formula xml:id="formula_9">y(v) = arg max c=1,...,C M m=1 Θ c m Km(v) T ac,m + bc,m .<label>(10)</label></formula><p>This boosting algorithm is a unified method for both multilayer and multimodal fusion. It can be used by multilayer fusion to combine the video representations rm from multiple layers in a single modality. If the set of representations is extracted over multiple modalities, it then performs as multimodal fusion. We observe that the joint fusion of multiple layers over all modalities is slightly better than the separate fusion of individual modality first then across all modalities. This is probably because the joint fusion allows different modalities to explore better correlations at different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTS</head><p>In this section, we extensively evaluate the proposed multilayer and multimodal fusion method on two public benchmark datasets for video classification: UCF101 <ref type="bibr" target="#b38">[38]</ref> and HMDB51 <ref type="bibr" target="#b21">[21]</ref>. In all experiments, we use LIBLINEAR <ref type="bibr">[9]</ref> as the linear SVM solver. Experimental results show that our algorithm achieves the state-of-the-art results on the two benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Datasets</head><p>The UCF101 <ref type="bibr" target="#b38">[38]</ref> dataset contains 101 action classes with large variations in scale, viewpoint, illumination, camera motion, and cluttered background. It consists of 13,320 videos in total. We follow the standard experimental setting as in <ref type="bibr" target="#b38">[38]</ref> and use three training and testing splits. In each split, 20% of training data is randomly selected as validation set for the boosting model selection. The first split of UCF101 (denoted as UCF101*) is also used to evaluate and understand the contribution of each individual component. We report the average accuracy over the three splits as the overall measurement.</p><p>The HMDB51 dataset <ref type="bibr" target="#b21">[21]</ref> is collected from a wide range of sources from digitized movies to online videos. It contains 51 categories and 6,766 videos in total. This dataset includes original videos and stabilized ones. Our evaluations are based on the original version. There are 70 videos for training and 30 videos for testing in each class. We use 40% of training data as validation set to perform model selection for boosting. We follow the evaluation protocol defined in <ref type="bibr" target="#b21">[21]</ref> and use three training and testing splits and report the mean accuracy over the three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Implementations</head><p>We implement the networks of four modalities in Theano with cuDNN4 on an NVIDIA DIGITS DevBox with four Titan X GPUs. 2D-CNN and 3D-CNN in the experiments are initialized with VGG16 <ref type="bibr" target="#b37">[37]</ref> pre-trained on ImageNet and C3D <ref type="bibr" target="#b42">[42]</ref> pre-trained on Sports-1M, respectively. Outputs of the last four layers of each network are used to represent videos. Special attention is paid to assembling mini-batches in order to deal with varying video length. We fill all frames of a video into a mini-batch and use another video if there is still space in the mini-batch. When the limit of a minibatch is reached and there are frames left, we use them in the next one. When there are no more frames to fill a minibatch, we fill it with zeros and these examples are not used in computation. We shuffle video instances after each epoch to prevent learning a specific sequence of examples. The last hidden state vector of each mini-batch is propagated to the next batch.</p><p>We apply data augmentations to increase the diversity of videos. For 2D-CNN we skip every second frame and operate on a single frame resized to 320 × 240 and cropped to 224 × 224. 3D-CNN works on a clip of 16 frames resized to 160×120 and cropped to 112×112. Training frames are generated by random cropping and flipping video frames, while for testing, only a central crop with no flipping is evaluated. Since the two datasets are of quite different sizes, we apply different learning rate schedules. For UCF101, we fine-tune 9 epochs with an initial learning rate of λ = 3 × 10 -4 and divide it by 10 after each 4 epochs. For HMDB51, we perform fine-tuning for 30 epochs with the same initial learning rate and divide it by 10 after every 10 epochs. All network parameters that do not have pre-trained weights are initialized with random samples drawn from a zero-mean normal distribution (σ = 0.01).We use the frame-wise negative loglikelihood of a mini-batch as the cost function, which is optimized using SGD with a momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Evaluation of Feature Aggregations</head><p>We first evaluate the performance of iFV to represent conv layers in different modalities. Compared to the traditional aggregation methods, iFV retains high-order statistics; in particular, it adaptively weights the features of a conv layer according to the associated spatial weights learned by the proposed convlet. We keep 300 out of 512 components in PCA. For computing the spatial discriminative weights, we find the sigmoid is more discriminative than softmax, e.g., iFV with sigmoid outperforms that with softmax by 0.6% for conv5 layer in 2D-CNN-SF. The sigmoid function is therefore used in the following experiments. We set K = 128 Gaussian components for both methods so the final feature dimensionality is 76.8K. We compare iFV with the conventional FV <ref type="bibr" target="#b34">[34]</ref> in Table <ref type="table" target="#tab_1">1</ref>, where iFV consistently outperforms FV for conv layers in all modalities with the improvements ranging from 0.6% to 2.5%. A larger improvement is observed for conv4 than conv5, probably because of the finer spatial information preserved in the lower layer. These improvements clearly show the advantages of utilizing the spatial discriminability learned by convlets to enhance the feature representation.</p><p>We employ temporal max pooling to aggregate fc layers, which are further extended with an explicit feature map to approximate non-linear kernels. This representation also benefits from the same efficiency of learning and prediction as linear SVM. We demonstrate the results of fc layers in 3D-CNN-SF with approximated non-linearities in Both fc6 and fc7 are transformed to recurrent layers by FC-RNN. We use the 2-norm and z = 3 in the explicit feature map, so the extended feature dimension is 28,672. The baseline method is the linear representation by temporal max pooling without feature mapping. We evaluate three additive non-linear kernels: χ 2 , Jensen-Shannon and intersection kernels, which are widely used in machine learning and computer vision. All non-linear representations outperform the linear one, especially the representation with intersection kernel achieves the best results. We thus use the intersection non-linearity approximation to represent fc layers in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Evaluation of FC-RNN</head><p>Our method extracts static and dynamic information at multiple temporal scales. 2D-CNN and 3D-CNN on spatial frames and optical flow images compute features from shortterm and mid-term temporal contexts. FC-RNN is then employed to model each video as an ordered sequence of frames or clips to capture the long-term temporal order. Since FC-RNN maintains the structure of a pre-trained network to the greatest extent, it is therefore effective at preserving important generalization properties of the network, when fine-tuned on a smaller target dataset. Moreover, FC-RNN achieves higher accuracy and is faster to converge compared to the standard RNN. We compare the training and testing performances of our proposed FC-RNN and the standard RNN in Fig. <ref type="figure" target="#fig_7">5</ref>. To avoid clutter, we only show this comparison for 3D-CNN modalities-a similar phenomena is observed on 2D-CNN-OF as well. FC-RNN is generally able to alleviate over-fitting and converge faster, e.g., FC-RNN outperforms standard RNN and LSTM by 3.0% and 2.9% on 3D-CNN-SF. In comparison to the networks without recurrent connections, FC-RNN significantly improves the modalities of 2D-CNN-OF, 3D-CNN-SF and 3D-CNN-OF by 3.3%, 3.2% and 5.1%, respectively. This demonstrates the benefits of FC-RNN in modeling the long-term temporal clues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Evaluation of Multilayer Fusion</head><p>Here we evaluate the multilayer fusion on combining various layers for individual modalities. Table <ref type="table" target="#tab_3">3</ref> shows the performance of each single layer across different modalities and the fusion results on the two datasets. Although the last layer in a network is the most sensitive to category-level semantics, it is not unusual for lower layers to have on par or superior results, e.g., conv5 of 2D-CNN-OF on UCF101 and conv5 of 2D-CNN-SF on HMDB51. So it is of great potential to exploit the intermediate abstractions such as parts, objects, poses, articulations and so on for video classification. It is also of interest to observe that most layers produce accuracies better than the baseline of softmax, i.e., the prediction outputs of a network. This again validates the merit of the proposed feature aggregation methods to represent conv and fc layers.</p><p>If we use the boosting algorithm to combine multiple layers, the fusion result significantly outperforms the baseline for all modalities, especially for 3D-CNN-OF with 7.2% and 7.9% gains on UCF101 and HMDB51. This demonstrates that various abstractions extracted in multiple layers are of rich complementarity. Although boost-c is more flexible to have class-specific mixing coefficients, its results are inferior to those of boost-u. This is because the model of boost-c tends to over-fit, since the C ×M parameters to fit in boost-c require more training data than the M parameters in boostu. We thus use boost-u in the following fusion experiments. 3D-CNN-SF is the best modality before fusion as it jointly models appearance and motion information. After multilayer fusion the other two modalities involving dynamic cues are enhanced to a similar performance level, which shows that the boosting method successfully maximizes the capability of a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Evaluation of Multimodal Fusion</head><p>We now demonstrate multimodal fusion, combining the proposed four modalities. Our networks are initialized by models pre-trained on large-scale image and video datasets so it is natural to fine-tune these networks for the two modalities of spatial frames. However, for the other two modalities involving optical flow, they are distant from the source if we UCF101 (%) HMDB51 (%) regard fine-tuning as a way of domain transformation. We introduce a simple but effective method to bridge the two domains-initialize optical flow networks by spatial frame models that have been fine-tuned on the target domain. As shown in Table <ref type="table" target="#tab_4">4</ref>, compared to the networks directly finetuned on the source model (i.e., not initialized by 3D-CNN-SF), our initialization remarkably improves the results. Table <ref type="table" target="#tab_5">5</ref> contains the accuracies for various combinations of the four modalities. Observe that fusing any pair of modalities improves the individual results. The best classification accuracy of 91.9% is obtained by the combination of all modalities. In the end, we achieve an accuracy of 91.6% on UCF101 and 61.8% on HMDB51 through combining the four modalites by boost-u. In comparison to the results in Table <ref type="table" target="#tab_3">3</ref>, the multimodal fusion produces much higher accuracy than any individual modality. This indicates the strong complementarity between the four modalities that capture diverse static and dynamic features at multiple temporal scales. In comparison to the baseline fusion methods, boostu improves the result by 2.3% over geometric mean <ref type="bibr" target="#b41">[41]</ref>, 4.3% over SVM-based fusion <ref type="bibr" target="#b36">[36]</ref>, and 7.9% over AdaBoost <ref type="bibr" target="#b5">[5]</ref> on UCF101*. This demonstrates that boost-u is more effective to exploit and fuse the complementary relationship of multiple modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D-CNN-SF 2D-CNN-OF 3D-CNN-SF 3D-CNN-OF 2D-CNN-SF 2D-CNN-OF 3D-CNN-SF 3D-CNN</head><p>We finally compare our results with the most recent stateof-the-art methods in Table <ref type="table" target="#tab_6">6</ref>. Our method produces the best accuracy on UCF101 with a clear margin over other competing algorithms. It is more challenging to fine-tune UCF101 (%) HMDB51 (%) STIP + BOVW <ref type="bibr" target="#b21">[21]</ref> 43.9 STIP + BOVW <ref type="bibr" target="#b21">[21]</ref> 23.0 DT + MVSV <ref type="bibr" target="#b4">[4]</ref> 83.5 DT + MVSV <ref type="bibr" target="#b4">[4]</ref> 55.9 iDT + HSV <ref type="bibr" target="#b33">[33]</ref> 87.9 iDT + HSV <ref type="bibr" target="#b33">[33]</ref> 61.1 C3D <ref type="bibr" target="#b42">[42]</ref> 85.2 iDT + FV <ref type="bibr" target="#b44">[44]</ref> 57.2 LRCN <ref type="bibr" target="#b8">[8]</ref> 82.9 Motionlets [3] 42.1 TDD <ref type="bibr" target="#b46">[46]</ref> 90.3 TDD <ref type="bibr" target="#b46">[46]</ref> 63.2 RNN-FV <ref type="bibr" target="#b25">[25]</ref> 88.0 RNN-FV <ref type="bibr" target="#b25">[25]</ref> 54.3 Two-Stream <ref type="bibr" target="#b36">[36]</ref> 88.0 Two-Stream <ref type="bibr" target="#b36">[36]</ref> 59.4 MultiSource CNN <ref type="bibr" target="#b32">[32]</ref> 89.1 MultiSource CNN <ref type="bibr" target="#b32">[32]</ref> 54.9 Composite LSTM <ref type="bibr" target="#b39">[39]</ref>  networks and train boost-u on HMDB51, where each training split is 2.6 times smaller than UCF101. Our method still achieves superior performance on HMDB51. Other competitive results <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b46">46]</ref> are based on the improved dense trajectories, which require quite a few hand-crafted processes such as dense point tracking, human detection, camera motion estimation, etc. As shown on UCF101, large training data is beneficial for training networks and boosting, so we are planing to explore techniques such as multi-task learning and temporal elastic deformation to increase the effective training size of HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>In this paper, we have presented a novel framework to fuse deep neural networks in multiple layers and modalities for video classification. A multilayer strategy is proposed to incorporate various levels of semantics in each single network. We employ effective feature aggregation methods, i.e., iFV and explicit feature maps to represent conv and fc layers. We further introduce a multimodal approach to capture diverse static and dynamic cues from four highly complementary modalities at multiple temporal scales. FC-RNN is then proposed to effectively model long-term temporal order by leveraging the generalization properties of pre-trained networks. A powerful boosting model is used for the optimal combination of multilayer and multimodal representations. We evaluate our approach extensively on two public benchmark datasets and achieve superior results compared to a number of recent methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-3603-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2964284.2964297</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the proposed multilayer and multimodal fusion framework for video classification.We use four modalities to extract highly complementary information across multiple temporal scales. For each single modality, discriminative representations are computed for convolutional and fully connected layers. We employ an effective boosting model to fuse the multiple layers and modalities. Box colors are encoded according to different networks: 2D-CNN and 3D-CNN with and without RNN. We propose FC-RNN to model long-term temporal information rather than using the standard RNN structure.</figDesc><graphic coords="2,53.80,53.80,502.12,272.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of multilayer representation and fusion. The proposed feature aggregation methods are used to represent fully connected and convolutional layers over time. The introduced boosting algorithm is applied to combine the representations from multiple layers.</figDesc><graphic coords="4,53.80,53.80,239.11,143.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning spatial discriminative weights of a convolutional layer by convlets. A spatial weight indicates how discriminative or important that local spatial region is in a convolutional layer.</figDesc><graphic coords="4,316.81,53.80,239.10,123.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>transform the classification accuracy αj to a spatial discriminative weight wj with softmax function wj = exp (αj)/ |R| k=1 exp (α k ) or sigmoid function wj = 1/ [1 + exp (α -αj)], where α is a parameter to control the relative weight. All features xi of spatial cell Rj have the same associated weight wj.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of standard RNN and FC-RNN. The variables in red correspond to the parameters that need to be trained from scratch.</figDesc><graphic coords="6,53.80,53.80,239.09,157.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>θm = 1 ,</head><label>1</label><figDesc>represent a training set by {(vi, yi)} N i=1 which contains N instance pairs of a video vi ∈ V and a class label yi ∈ {1, . . . , C}. Let {rm : V → R dm } M m=1 indicate M video representations extracted by the proposed feature aggregation methods from conv and fc layers of multiple modalities. We use a general kernel function κ to measure the similarity between instances by the m-th video representation: κm (v, v ) = κ (rm(v), rm(v )). So the kernel response of a given instance v ∈ V to the whole training samples is defined as Km(v) = [κm(v, v1), . . . , κm(v, vN )] T . We focus on the binary classification problem in the following derivation, which extends straightforwardly to multiple classes. Here the objective is to optimize a linear combination of the predictions using M representations: U (v) = M m=1 θmum(v), where θm is a mixing coefficient and um is a decision function. In this paper, we use SVM with the decision function um(v) = Km(v) T am + bm, but the weak learner um is not necessarily SVM. All parameters of the fusion model can be solved by training um based on each individual video representation and subsequently optimizing θm through: ) + ξi ≥ , i = 1, . . . , N M m=1 θm ≥ 0, m = 1, . . . , M,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of the proposed FC-RNN and the standard RNN in training and testing of 3D-CNN-SF and 3D-CNN-OF on UCF101*.</figDesc><graphic coords="8,316.81,53.80,239.10,171.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Modality</cell><cell>Layer</cell><cell>FV [34]</cell><cell>iFV</cell></row><row><cell>2D-CNN-SF</cell><cell>conv4 conv5</cell><cell>74.2% 79.6%</cell><cell>76.7% 80.6%</cell></row><row><cell>2D-CNN-OF</cell><cell>conv4 conv5</cell><cell>75.6% 81.9%</cell><cell>78.1% 82.6%</cell></row><row><cell>3D-CNN-SF</cell><cell>conv4 conv5</cell><cell>83.6% 83.3%</cell><cell>84.8% 84.6%</cell></row><row><cell>3D-CNN-OF</cell><cell>conv4 conv5</cell><cell>78.2% 78.1%</cell><cell>78.8% 78.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of FV and the proposed iFV to represent convolutional layers of different modalities on UCF101*.</figDesc><table><row><cell>Layer</cell><cell>Linear</cell><cell>χ 2</cell><cell cols="2">Jensen-Shannon Intersection</cell></row><row><cell>fc6</cell><cell>84.1%</cell><cell>84.8%</cell><cell>84.6%</cell><cell>84.9%</cell></row><row><cell>fc7</cell><cell>82.4%</cell><cell>82.9%</cell><cell>83.0%</cell><cell>83.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different non-linear approximations to represent fully connected layers in 3D-CNN-SF on UCF101*.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performances of individual layers over different modalities and multilayer fusion results.</figDesc><table><row><cell>-OF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the initialization methods for 3D-CNN-OF on UCF101*.</figDesc><table><row><cell>Modality</cell><cell>Accuracy</cell><cell>Combinations</cell></row><row><cell>2D-CNN-SF</cell><cell>83.2</cell><cell></cell></row><row><cell>2D-CNN-OF</cell><cell>84.8</cell><cell></cell></row><row><cell>3D-CNN-SF</cell><cell>85.9</cell><cell></cell></row><row><cell>3D-CNN-OF</cell><cell>81.4</cell><cell></cell></row><row><cell cols="2">Fusion Accuracy</cell><cell>90.3 90.8 87.1 90.4 91.2 91.3 91.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies (%) of different modalities and various combinations on UCF101*.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>84.3 Composite LSTM [39] 44.1 Comparison of the multimodal fusion to the state-of-the-art results.</figDesc><table><row><cell>Ours</cell><cell>91.6 Ours</cell><cell>61.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motionlets: mid-level 3D parts for human motion recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast feature selection and training for AdaBoost-based concept detection with large scale datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linear programming boosting via column generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FlowNet: learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multimodal probabilistic model for gesture based control of sound synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Francoise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bevilacqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual word ambiguity</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Short-term audio-visual atoms for generic video concept classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">RNN Fisher vectors for action recognition and image annotation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale invariant feature transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond short snippets: deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Combining multiple sources of knowledge in deep CNNs for action recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">UCF101: a dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluation of low-level features and their combinations for complex event detection in open source videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamrakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A robust and efficient video representation for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with DAG-CNNs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">AT&amp;T Research at TRECVID 2013: surveillance event detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zavesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahraray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition using super sparse coding vector with spatio-temporal awareness</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: a comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
