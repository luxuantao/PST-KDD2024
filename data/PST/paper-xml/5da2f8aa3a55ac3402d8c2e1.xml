<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Barrage of Random Transforms for Adversarially Robust Defense</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edward</forename><surname>Raff</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Sylvester</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Forsyth</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Mclean</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Booz</forename><forename type="middle">Allen</forename><surname>Hamilton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Physical Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Barrage of Random Transforms for Adversarially Robust Defense</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Defenses against adversarial examples, when using the ImageNet dataset, are historically easy to defeat. The common understanding is that a combination of simple image transformations and other various defenses are insufficient to provide the necessary protection when the obfuscated gradient is taken into account. In this paper, we explore the idea of stochastically combining a large number of individually weak defenses into a single barrage of randomized transformations to build a strong defense against adversarial attacks. We show that, even after accounting for obfuscated gradients, the Barrage of Random Transforms (BaRT) is a resilient defense against even the most difficult attacks, such as PGD. BaRT achieves up to a 24⇥ improvement in accuracy compared to previous work, and has even extended effectiveness out to a previously untested maximum adversarial perturbation of ✏ = 32.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Adversarial machine learning has been a research area for over a decade <ref type="bibr" target="#b0">[1]</ref>, but it has recently received increased focus and attention from the larger community. This is largely due to the success of modern deep learning techniques within the realm of computer vision tasks and the surprising ease with which such systems are fooled into giving incorrect decisions <ref type="bibr" target="#b1">[2]</ref>. In particular, there are concerns about the safety of self-driving cars, as they could be fooled into misreading stop signs as speed limits, and other possible nefarious actions <ref type="bibr" target="#b2">[3]</ref>.</p><p>Consider an adversary A whom, given some victim model f (•),w a n t st oa l t e re x = A(x) such that f (x) 6 = f (A(x)).M a n yw o r k sh a v ea t t e m p t e dt ofi n d at r a n s f o r mt(•) that can be applied to an image x to yield a new image x = t(x) such that f (x)=f (t(A(x))). If it were possible to find such a defensive transform t(•) it would allow us a simple and convenient way to circumvent the adversarial problem. This is particularly alluring for computer vision, since there exists a rich literature of computer vision transformations to pull from. Athalye, Carlini, and Wagner <ref type="bibr" target="#b3">[4]</ref> has shown that the many attempts to find such a defensive transformation t(•) that defeats adversarial attacks have all failed, due to a problem they term obfuscated gradients. More broadly, every defense we are aware of that has undergone thorough evaluation has failed to produce any level of protection for ImageNet <ref type="bibr" target="#b4">[5]</ref>, as exemplified in the Ro-bustML catalog where all ImageNet results are reduced to  0.1%a c c u r a c y . <ref type="foot" target="#foot_0">1</ref> In contrast, we present a new, state-of-the-art defense for ImageNet that pays some cost to accuracy when not under attack, but achieves a Top-5 accuracy of up to 57.1% when under attack. These attacks are carried out by the strongest adversary we could construct, which is significantly stronger than those used in similar work in key respects.</p><p>In our work, we instead look not for a single transformation t(•),b u tp r o p o s et ob u i l dac o l l e c t i o no fm a n y different transforms t 1,...,n from which we will randomly select a subset to apply to each image at both training and testing time. The individual transforms will be randomly parameterized as will the particular subset chosen and the order in which they are applied. By creating a barrage of random transformations, we show that such an ensemble defense can provide tangible benefits against attack, even after taking into account all of the methods by which obfuscated gradients can mislead us into using a broken defense <ref type="bibr" target="#b3">[4]</ref>.</p><p>Overall we provide the following contributions:</p><p>• A new, state-of-the-art defense on ImageNet, that fully accounts for the obfuscated gradients issue.</p><p>• Results that show ensembling weak defenses can create a strong defense, provided they are combined in a randomized fashion and the population of defenses is large. Prior work had conjectured that this was not the case <ref type="bibr" target="#b5">[6]</ref>. BaRT is inspired by and builds upon a number of prior works that have used singular transformations to try to defend against attacks. We will review work related to our approach in section 2 and detail both the BaRT strategy and it's constituent transformations in section 3, as well as the threat model of our adversary in section 4. While heuristic in nature, we find that after accounting for our strongest adversary we obtain stateof-the-art robustness against attack on the ImageNet dataset, which we show in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While work on adversarial attacks against machine learning models has existed for over a decade, recent work that showed their success against neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> has spawned increased motivation and attention to this problem. There were some who thought this concern was over stated, and that the number of variations in position, lighting, angle, and other factors that would occur in the real world would render adversarial attacks a non-issue for physical systems <ref type="bibr" target="#b8">[9]</ref>. However, it was later shown that these difficulties could be circumvented making it possible for adversarial examples to be constructed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Still, the intuition that adjustments in angle, position, or other kinds of visual transformations of some object could defeat an adversary by somehow filtering or removing the adversary's perturbations was strong and alluring. As such, many papers have been presented that attempt to defeat adversaries using some kind of image pre-processing before classification (e.g., <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>). As far as we are aware, these types of defenses have all been defeated in the white-box threat model, either by correctly incorporating the defense into the adversary's search procedure <ref type="bibr" target="#b1">[2]</ref>, or by properly accounting for obfuscated gradients <ref type="bibr" target="#b3">[4]</ref>. Obfuscated gradients occur when the defense has, intentionally or not, masked information about the gradient making it unreliable (or non-existent) for the adversary to use. These can occur in a number of ways, but all of which have proposed workarounds to obtain a suitable approximate gradient for the adversary to use <ref type="bibr" target="#b3">[4]</ref>. In this work, we use only techniques which have already been defeated to build our defense. This way we can leverage known solutions to the obfuscated gradient and thus fully account for the problem and ensure our adversary's attack has full knowledge of the defense.</p><p>Few approaches have b een able to scale up to Im-ageNet's size, and we find most works that have attempted to defend it against attack have been based on transformations or denoising. Prakash, Moran, Garber, et al. <ref type="bibr" target="#b14">[15]</ref> claimed 81% accuracy under attack and Liao, Liang, Dong, et al. <ref type="bibr" target="#b15">[16]</ref> 75%, but both were reduced to 0% under just ✏ =4when obfuscated gradients were accounted for <ref type="bibr" target="#b16">[17]</ref>. Xie, Zhang, Yuille, et al. <ref type="bibr" target="#b17">[18]</ref> claimed 86% accuracy and Guo, Rana, Cissé, et al. <ref type="bibr" target="#b12">[13]</ref> 75%, but these were later also reduced to 0% accuracy <ref type="bibr" target="#b3">[4]</ref>. Even different approaches with more modest claims were later shown to be deficient, such as Kannan, Kurakin, and Goodfellow <ref type="bibr" target="#b18">[19]</ref> who initially reported 27.9% accuracy but which was later demonstrated to be 0.1% <ref type="bibr" target="#b19">[20]</ref>.</p><p>Others before us have looked at building a multicomponent defense, but prior work has reached the conclusion that a combined defense is no stronger than any of its constituent members <ref type="bibr" target="#b5">[6]</ref>. In this paper we demonstrate that this is not necessarily true. Prior attempts at ensembling defenses have all combined their constituents in a fixed strategy, which has failed to be useful. In contrast, we demonstrate that a stochastic combination of weak defenses is effective.</p><p>An u m b e ro fr e c e n tw o r k sh a v el o o k e da td e v e l o ping provably secure training procedures for deep learning <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. We believe that in the long term this is the most encouraging and desirable path toward defending against adversarial attacks. However, these methods are not yet usable for large datasets. The most recent work in this area has been "scaling up" to cifar-10 <ref type="bibr" target="#b23">[24]</ref>, which is orders of magnitude smaller than ImageNet.</p><p>The state-of-the-art defense that has been repeatedly found to be effective is Adversarial Training, which involves augmenting the training data with adversarially crafted examples generated as the training progresses <ref type="bibr" target="#b7">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type="bibr" target="#b24">[25]</ref> used adversarial training on the cifar dataset, which still has the best empirical robustness to attack <ref type="bibr" target="#b23">[24]</ref> and has been repeatedly validated as effective and capable of fully defending against the best known adversaries under the whitebox threat model <ref type="bibr" target="#b3">[4]</ref>. Kurakin, Goodfellow, and Bengio <ref type="bibr" target="#b25">[26]</ref> attempted to scale adversarial training up to the ImageNet dataset, which they found especially difficult. As far as we are aware, their work provides the best uncontested defense against adversarial attack on ImageNet. Against an adversary operating in the L ∞ distance, they obtain Top-1 and Top-5 accuracy of 1.5% and 5.5% for Top-1 and Top-5 respectively for a max perturbation of ✏ = 16. We will show that our defense outperforms adversarial training across all ✏ 2 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, and even continues to provide a robust defense up to ✏ = 32. We are not aware of any prior work which has considered an L ∞ adversary given this wide of a range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Barrage of Random Transforms</head><p>Given the research that has been performed over the past year, it is clear that a single transformation of the input image is not sufficient to produce a reliable defense. We take the perspective that given an omnipotent adversary, randomness is one way to construct a decision process that the adversary can not trivially circumvent. The question then becomes: is there a way to randomly pre-process images before they are classified by a CNN, such that accuracy is not obliterated and the adversary is unable to effectively operate?</p><p>Since we are working on images, we can make use of aplethoraofpre-existingimagetransformationandpreprocessing steps that have been developed by the computer vision community over the past several decades. We leverage these to create 10 groups G 1,...,10 of transformations. Each group G j will have some number of transforms t(•) contained within that group. We used a total of n = 25 different transforms t 1,...,25 , and denote the set of all transforms T = S 25 i=1 t i ,a n d 8j, G j ⇢ T , with each group of transforms having no overlap (G j \ G k = ; for j 6 = k).</p><p>Each transform t i (•) will have some parameters p i that alter the behavior of the transform, and so by randomly selecting the values of p we can can have t i (x|p i ) produce many different outputs, introducing a stochastic component. This alone is not new, but we also have a collection of n different transforms to choose from. To further maximize the randomness, we select an ordering, or "permutation," ⇡ of k transforms to apply. The ordering ⇡ will change every time we attempt to use a model f (•), with the goal being that</p><formula xml:id="formula_0">f (x)=f (t π(1) (t π(2) (...(t π(k) (A(x)))))).</formula><p>The intuition is that by randomly selecting k out of n transforms, where each transform is itself randomized, and applying them in a random order, we create a defense that the adversary A can not easily defeat. We fo cused on this randomness on top of randomness because it provides a mechanism that the adversary can not easily deal with, even if they have perfect knowledge of all transformations t i and the parameters p i that alter their behavior. The space of possible actions is too large to find a single alteration e x = A(x) such that the attacker will successfully induce an error by the model for all permutations ⇡ and parameterizations p π(...) .</p><p>The transforms we use are listed below. There are five singleton-groups (a group that has only one transform member, |G i | =1). When a group has more than one constituent transform, we randomly select a transform from the group to act as the group's representative, selecting a new representative on every application. This is to prevent the choice of multiple transformations which all have very similar effects from being applied at the same time, thereby increasing the diversity of changes made to each input.</p><p>We emphasize that for every individual transform we evaluate in this work, we have independently tested the transform and achieved 100% evasion success against it using the attack methodology outlined in subsection 4.1. As such, we know that all of these defenses are insufficient in isolation. Thus it is their stochastic combination that makes them significantly stronger than any constituent member. This is counter to previous conclusions that ensembling defenses are not effective and only as strong as the strongest individual defense in the ensemble <ref type="bibr" target="#b5">[6]</ref>. The critical difference between our own and prior ensembling defense work is is the number of defenses (25 weak defenses, compared to  3 for most prior work), and the use of randomness to select subsets of defenses in random orderings.</p><p>We employ 25 transforms in total, and so only briefly describe the larger groups here. Further explanation, and Python code, are provided in the appendix.</p><p>Color Precision Reduction Reducing bit-resolution of color was originally proposed as a defense by Xu, Evans, and Qi <ref type="bibr" target="#b26">[27]</ref> and later reduced to 0% effectiveness <ref type="bibr" target="#b3">[4]</ref>. It works by simply reducing the number of bits used to represent the color space of an image, and was tested down to using just 1 bit of color. We incorporate this approach, and make the transform random in two ways. First, the number of colors will be reduced to a value selected from U <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">200]</ref>. Second, with 50% probability we choose between: 1) using the same number of colors for each channel, or 2) selecting a different random number of colors to be used by each channel.</p><p>JPEG Noise Using lossy JPEG compression to introduce artifacts was introduced by Kurakin, Goodfellow, and Bengio <ref type="bibr" target="#b9">[10]</ref>. Their work looked at how different values of the JPEG compression level (a range from 1 to 100) reduced the impact of adversarial attacks for different values of ✏  16.H o w e v e r ,i tw a ss u b s e q u e n t l y defeated, having 0% effectiveness <ref type="bibr" target="#b3">[4]</ref>. When using this approach, we randomize it by selecting the compression level from U [55, 95].</p><p>Swirl We introduce a simple defense which is to apply a weak swirl to the image, rotating the pixels around a randomly selected point in the image. The radius of intensity is randomly selected from U <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">200]</ref></p><formula xml:id="formula_1">,a n d strength from U [0.1, 2.0].</formula><p>Noise Injection In early work Tabacof and Valle <ref type="bibr" target="#b27">[28]</ref> looked at the impact of addition Gaussian noise on adversarial attacks. We extend this by randomly selecting from Gaussian, Poisson, Salt, Pepper, Salt &amp; Pepper, and Speckle noise to be inserted. With a 50% chance we will either: 1) apply the same noise to every channel, or 2) apply a randomly selected noise type to each channel independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFT Perturbation</head><p>We intro duce a defense built around perturbing the 2D FFT of each channel of the input image separately. In the frequency domain of the image, we scale all coefficients by a value sampled from U [0.98, 1.02] (used for all channels). Then for each channel, we randomly choose between 1) zeroing out random coefficients of the FFT, or 2) zeroing out the lowest frequency coefficients of the FFT. The proportion of coefficients that will be set to zero is a random value selected from U [0.0, 0.95].A f t e ra l t e r i n gt h ec o e ffi c i e n t s in the frequency domain we return a new, modified image in the spatial domain.</p><p>Zoom Group We consider two transforms that have the effect of zooming into the image. To prevent "over zooming" into the image, they are grouped and only one is selected from the group at each step. A simple zoom into a random portion of the image is done, similar to prior work <ref type="bibr" target="#b12">[13]</ref>, as well as a content-aware zoom based on seam carving <ref type="bibr" target="#b28">[29]</ref>.</p><p>Color Space Group We include four transforms that operate by altering the channels of the image by adding a random constant value, but provide larger impact by first converting the image from RGB to a different color space, and then converting back to RGB after modification. While a more difficult approach would be to allow every pixel in every color coordinate to receive ad i ff e r e n tv a l u e ,w ei n t e n t i o n a l l yc h o o s et h es i m p l e r constant value to aid our adversary. This approach is applied to the HSV, XYZ, LAB, and YUV color spaces as the four transform members of this group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrast Group</head><p>We consider three different typ es of histogram equalization. Because each one attempts to re-scale and redistribute the values of the histogram of an image to broaden the covered range, they do not make sense to apply in a sequential manner. We use a simple version of Histogram Equalization, an adaptive variant called clahe <ref type="bibr" target="#b29">[30]</ref>, and an approach known as "contrast-stretching."</p><p>Grey Scale Group We as humans are usually able to recognize most objects from grey scale imagery, and as such, include conversion to grey scale as one of our defense techniques. For this reason we perform greyscale transformation four different ways which can be applied selectively to different color channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising Group</head><p>The final group we consider is a number of classical denoising operations and transformations. We group them to avoid over-zealous application that can result in images which appear overly blurry and become difficult to interpret. This includes aG a u s s i a nb l u r ,m e d i a n ,m e a n ,a n dm e a n -b i l a t e r a l[ 3 1 ] filtering, Chambolle and wavelet <ref type="bibr" target="#b31">[32]</ref> denoising, and nonlocal mean denoisng. Prior works have used the median filter <ref type="bibr" target="#b11">[12]</ref>, wavelet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, and non-local mean <ref type="bibr" target="#b26">[27]</ref> as defenses, but all have since been defeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>Given the set of transformations outlined in section 3, we will use a ResNet50 model as our base architecture for experimentation. In particular, we will start with a pre-trained ResNet50 model, and perform an additional 100 epochs of training on ImageNet using Adam <ref type="bibr" target="#b32">[33]</ref>. For each dataset in the batch, we randomly pick k ⇠U[0, 5] transformations to apply to each image, so that the model is familiar with the transformations we apply at test time. Following Biggio, Fumera, and Roli <ref type="bibr" target="#b33">[34]</ref>, we will now fully state the threat model that we will operate in.</p><p>Once we have a trained model, our adversary will attack it in three ways: 1), reduce the Top-1 accuracy (any output besides the correct class is a success for the attacker); 2) reduce the Top-5 accuracy (any output is as u c c e s sf o rt h ea t t a c k e rp r o v i d e dt h ec o r r e c tc l a s si s ranked sixth or lower), and; 3) increase the targeted success rate. In the first two conditions the attacker can trick the model into any incorrect classification. In the final condition, the attacker has a specific, randomly selected class that it must induce the model into outputting. All of these attacks will be performed on the standard ImageNet validation set.</p><p>Our adversary's capability will include making modifications to any input feature of the test data under the L ∞ metric, for which the adversary will attempt to modify the input x to a new input x such that kx xk ∞ &lt;✏.</p><p>In our experiments, we will test a range of ✏ 2 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>We will operate in the white-box scenario, and assume that our adversary has full and complete knowledge of our training data, architecture, weights, and defensive transforms. To perform the attacks, we will use the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b7">[8]</ref> because it is a common baseline. More importantly, we will also use Projected Gradient Descent (PGD) <ref type="bibr" target="#b25">[26]</ref>, which is also targeted toward the L ∞ metric and is currently the strongest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type="bibr" target="#b24">[25]</ref>. We use the Fo olBox library for the implementation of these attacks <ref type="bibr" target="#b34">[35]</ref>.</p><p>To further ensure the attacker's strength, we follow recommendations from Demontis, Melis, Pintor, et al. <ref type="bibr" target="#b35">[36]</ref>, and attempt to optimize for the adversary in the L ∞ ball with maximum confidence,r a t h e rt h a nm i nimum distance. This includes making sure that the PGD attack runs through all optimization steps, even </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Making A Strong Adversary</head><p>To maximize the strength of our attacker, we must first resolve two issues. The first is that our transformation process is randomized, which means we can not take the gradient from a single instance of the attack, as the next realization of a transformed image will have ad i ff e r e n t l yp a r a m e t e r i z e dt r a n s f o r m . T or e m e d yt h i s situation, we use the Expectation over Transformation (EoT) <ref type="bibr" target="#b10">[11]</ref>. The idea of EoT is to perform the transformation multiple times, and take the average gradient over several runs. When we use iterative attacks like PGD, this means for every iteration of the attack we will take the average of several transforms at that step in the attack.</p><p>The second issue we have is that not all of our transformations are differentiable. The solution to this problem was proposed by Athalye, Carlini, and Wagner <ref type="bibr" target="#b3">[4]</ref>, and is called Backward Pass Differentiable Approximation (BPDA). The idea is simple: when a transform t(•) is not itself differentiable, use a neural network to learn a function f t (•) that approximates the transform. Since it is implemented with a neural network, f t (•) is differentiable, and so we can use rf t (•) to obtain a gradient that is useful for the adversary as an approximation to rt(•). This approach is effective, and using an a i v ei d e n t i t yf u n c t i o nf t (x)=x is often sufficient to defeat many attacks. Indeed, it is enough to defeat most of our transforms individually. However, we learn a small CNN to approximate this gradient to maximize the adversary's advantage.</p><p>We also recognize that while we have a repertoire of transforms that are selected from at random, each transform t itself is randomized as well. Denoting the set of parameters of a transform t as P (t),thetransformisde-terministic given a specific realization p ⇠ P (t) of these transforms. To learn our model f t (•), we will create an input that has 5+|P (t)| channels, and is the same size as the image to be learned. The first three channels will be the RGB channels of the original image. The next two channels will be the CoordConv values proposed by Liu, Lehman, Molino, et al. <ref type="bibr" target="#b36">[37]</ref>, so that our networks can deal with location specific transformations. We found CoordConv necessary in our BPDA model to effectively approximate the Swirl transform. The remaining |P (t)| channels will each have a constant value, which is the value of the realized parameters p.P l a c i n g the random values p each into their own distinct channel provides a mechanism for us to allow the network f t (•) to learn the fully specified deterministic mapping. Each CNN f t (•) has 6 convolutional layers, followed by batch-normalization and then a ReLU activation. For each layer we include a skip connection from the input, following the DenseNet approach. (See Figure <ref type="figure">1</ref>). We train the network as a denoising auto-encoder, where the target is the parameterized transform of the image (i.e., the loss is kf t (x, p) t(x|p)k 2  2 ), with 100 epochs of training for all 25 BPDA networks. Once f t (•) is trained, we perform BPDA by back-propagating through f t (•) to the first 3 channels that correspond to the original image RGB values.</p><p>Combining BPDA and EoT as we have described above, we can defeat any of our transforms individually 100% of the time for both targeted and un-targeted attacks. This confirms that we have implemented these approaches appropriately, and have maximized the strength of our adversary.</p><p>As part of our evaluation, we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type="bibr" target="#b24">[25]</ref>, which is the computational cost of a threat model. They argue that the strength of an adversary should be in some way computationally constrained, in the same manner that cryptographic problems are secure because we assume the adversary does not have the dramatic compute resources necessary to attack a given encryption scheme. Using 10 iterations of EoT combined with the iterative nature of PGD (40 optimization steps) means we must perform 400 gradient calculations per attack, combined with the time to compute the image transformations and back-propagate through the additional BPDA networks. This takes about 48 hours per experiment given a workstation with 10 CPU cores and a Titan X GPU. We will also consider results with 40 EoT iterations -the highest we have observed in the literature -to evaluate if an even stronger adversary would be significantly more successfully, but these experiments required 240 hours each on a DGX-1. In total, the results presented in this paper consumed approximately 320 days on our DGX-1. We include tests at both of these EoT scales to help confirm our attack is robust, and simply increasing the number of iterations of the adversary does not dramatically change results. We also feel we are approaching a limit of reasonable compute for an adversary to have, and would be the largest barrier to replication if we pushed to even more attack iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Medoid over Transformations</head><p>We take a moment to define a new typ e of attack to help ensure that we are not inadvertently engaging in accidental obfuscation of gradients. In particular, we note that the expectation over transformation approach uses the mean gradient over some transformation, shown more formally in Equation 1 where z EoT is the number of iterations of the EoT sampling and t (i) is a deterministic realization of the transform t (i.e., the pseudo random number generator has been seeded with the value i to provide a deterministic result).</p><formula xml:id="formula_2">rE t (i) ∼t f (t (i) (x)) ⇡ 1 z EoT zEoT X i=1 rf (t (i) (x))<label>(1)</label></formula><p>One possible source of gradient obfuscation might be that the mean of the distribution does not exist or is not well defined. This notion comes from the recognition that in our larger framework t in Equation 1 corresponds to our entire randomized pipeline of selecting k transforms from G 1,...,10 .O u rc o n c e r nb ya n a l o g yi st h a tw e could have a situation similar to the Cauchy distribution: The mean of the Cauchy distribution does not exist; every empirical mean is equally likely. However, one can successfully estimate the Cauchy distribution's position by instead using the median.</p><p>With this notion in mind, we include a new Medoid over Transformations (MoT) estimate, in which we use the medoid of the gradients of a sample of z MoT transformations as our gradient estimate to perform attacks with.</p><formula xml:id="formula_3">argmin i zMoT X j=1 krf (t (i) (x)) rf (t (j) (x))k 2 2 (2)</formula><p>If we are accidentally performing gradient obfuscation by instead pushing information from the mean to the medoid -similar to the behavior of a Cauchy distribution -we would expect to see an increase in performance with the MoT attack compared to the EoT. Our results will confirm that this is not the case, as the MoT attack performs worse than the EoT attack. However, we include the results and attack description here to build further confidence that we have attempted to make the strongest attack possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Few p eople have had their defense stand up to further testing due to a variety of issues related to obfuscated gradients and failing to fully account for all components of the defense when designing the whitebox adversary. Fewer still have been able to scale their defensive techniques up to the ImageNet dataset. Now that we have defined our methodology to make sure we have accounted for both obfuscated gradients and ensure our adversary has fully captured the defense in their attack, we will show how we obtain new state-ofthe-art results on the ImageNet dataset, as well as the associated costs in achieving such performance.</p><p>Kurakin, Goodfellow, and Bengio <ref type="bibr" target="#b25">[26]</ref> provide the strongest results on the full ImageNet dataset that we are aware of. They do this with adversarial training, which they noted had great difficulty scaling up to the ImageNet corpus. At a maximum perturbation of ✏ = 16,theyac hiev edonlyaT op-1accuracyof1.5%and a Top-5 accuracy of 5.5% when under attack by PGD.</p><p>For BaRT, we will by default assume ✏ = 16,t h e number of transformations k =5 ,a n dt h en u m b e ro f EoT runs will be 10. In our experiments we will investigate changing all of the values to observe their impact on our effectiveness against attack. We begin by showing the accuracy of our methods in Table <ref type="table" target="#tab_1">1</ref>. Here we can see the first immediate down side to BaRT, which is a significant reduction in accuracy if our model is not under attack. The off-setting benefit is the first significant improvement in accuracy when the model is under attack. At a cost of increased runtime, it is possible to create multiple inferences of an input by applying the transform t(•) multiple times, and then classifying each differently transformed version of the image. This creates an ensemble effect, and removes any loss in accuracy due to BaRT's application. Due to space, details on ensembling BaRT are left to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments</head><p>The immediate product of our work is that the BaRT strategy provides a 9.3-24 times improvement in accuracy on ImageNet compared to prior state of the art. We now further investigate the differing parameters of our defense. First we look at the larger range of ✏,t h e bound on the adversary's freedom to alter the input.</p><p>In Figure <ref type="figure">2</ref> we show accuracy under PGD attack as ✏ varies from 2 to 32. We note that ✏ = 16 is the largest we have observed in any prior work, and is considered ap o w e r f u la d v e r s a r y . W ea r et h efi r s tt ot e s t✏ = 32, and still show non-trivial robustness to attack.</p><p>In these results we see that BaRT dominates adversarial training across all values of ✏.W ea l s os e et h a t adversarial training degrades quickly as ✏ moves from just 2 to 4. In contrast BaRT Top-1 and Top-5 accuracy when attacked with ✏ = 32 is still better than the results with adversarial training and ✏ =4.F o r✏&gt;2, BaRT also shows Top-1 accuracy higher than adversarial training's Top-5 accuracy. These results also demonstrate that while increasing the number of EoT steps does increase the adversary's success rate, the difference is not large. Using 40 steps already requires a level of compute not reasonable for most institutions, and gives us confidence that other attempts to simply throw even more compute to the adversary will be nonviable. This is before we consider that it is relatively easy to write these transformations, and we could add even more transformations to the pipeline to further impede the adversary's compute requirements and reduce their success rate. While we do not have the resources to test exhaustively, we show in Appendix F that using even 520 PGD steps shows no significant change in the attacker's success rate.</p><p>Next we investigate the number of transforms applied. For these results, we remind the reader that BaRT was only trained with up to k =5transforms applied to the training data. In Figure <ref type="figure">3</ref> we plot the Top-1 and Top-5 accuracy of BaRT (under attack and on clean images) as a function of the number of transforms k selected at test time. Our initial expectation was that we would see the best performance (i.e., greatest accuracy under attack) when k = n/2,a st h i sw o u l dm a x i m i z et h e number of combinatorial paths n k .H o w e v e r ,t h i sw a s not the case.</p><p>Instead we see that every transformation t i (•) we apply produces some associated costs and benefits. The benefit is that increasing k ! n improves our performance when under attack. A slight dip occurs after Overall this validates that an ensemble of weak defenses can form a single strong defense, provided that the ensemble is applied in a random fashion. We also see that maximizing the combinatorial search space is not a dominating strategy, since we see maximal adversarial robustness at k = 10 instead of k =5 . This tells us that the amount of transformation applied to the image is also an important component of defeating the adversary, as this is maximized at k = 10. Cumulatively, we could argue that selecting the value of k to use in practice should be a function of the likelihood of being under attack. If a model is continuously under attack or needs maximal worst-case performance, one should choose k = 10 because the non-attacked accuracy is not meaningful when under attack.</p><p>We also explore the impact on targeted adversarial attacks in Figure <ref type="figure" target="#fig_1">4</ref>, where we look at the attacker's success rate as a function of k. Here we can see that when no transformations are present, PGD attack achieves 100% success rate against the model, but quickly degrades as transformations are added -reaching 0.0% success at k = 10 transformations. We note that additional runs may produce values near zero instead of at zero, but it suffices to show that the ability for the adversary to perform targeted attacks can be almost completely impeded by our BaRT defense. Lastly, in subsubsection 4.1.1 we considered the possibility that we might be engaging in obfuscated gradients by moving information to the medoid of the distribution. We developed a new Medoid over Transformation attack to test this hypothesis. The results are shown in Figure <ref type="figure" target="#fig_2">5</ref>. While MoT does produce adversarial examples, it has uniformly worse performance compared to using the mean gradient. As such we further conclude that we have not relied on obfuscated gradients, and that our defense is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced BaRT, a strategy for defending image classifiers against attack by randomly selecting af e wt r a n s f o r m sf r o mal a r g ep o o lo fs t o c h a s t i ct r a n sformations, and apply each in a random order before processing the image. This scales to datasets like Ima-geNet, and provides state-of-the-art results when under attack even after accounting for all known obfuscated gradients. While heuristic in nature, our results provide evidence that a strong defense can be made from many weaker ones, and indicates strategic applications of randomness may benefit future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 Figure 3 :</head><label>53</label><figDesc>Figure 3: Accuracy of model when varying the number of transforms used, both when not under attack and when being attacked by PGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attacker success rate against BaRT model when varying the number of transforms used, for both FGSM and PGD attacks with ✏ = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy of model under attack by EoT and MoT versions of PGD for varying adversarial distances ✏ and EoT steps=40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) of baseline prior work on adversarial training<ref type="bibr" target="#b25">[26]</ref> and BaRT. 'Clean Images' is the results of classifying non-attacked images without any transforms; 'Attacked' shows results when using PGD with ✏ = 16.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Clean Images</cell><cell>Attacked</cell></row><row><cell cols="2">Model</cell><cell></cell><cell cols="3">Top-1 Top-5 Top-1 Top-5</cell></row><row><cell cols="2">Inception v3</cell><cell></cell><cell>78</cell><cell>94</cell><cell>0.74 .4</cell></row><row><cell cols="3">Inception v3 w/Adv. Train</cell><cell>78</cell><cell>94</cell><cell>1.55 .5</cell></row><row><cell cols="2">ResNet50</cell><cell></cell><cell>76</cell><cell>93</cell><cell>0.00 .0</cell></row><row><cell cols="3">ResNet50-BaRT, k = 5</cell><cell>65</cell><cell>85</cell><cell>16</cell><cell>51</cell></row><row><cell cols="3">ResNet50-BaRT, k = 10</cell><cell>65</cell><cell>85</cell><cell>36</cell><cell>57</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 4</cell><cell>8</cell><cell>16</cell><cell>22</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell cols="3">Max Adversary Distance ✏</cell></row><row><cell></cell><cell cols="3">BaRT Top-1 EoT=10</cell><cell cols="2">Bart Top-5 EoT=10</cell></row><row><cell></cell><cell cols="3">BaRT Top-1 EoT=40</cell><cell cols="2">BaRT Top-5 EoT=40</cell></row><row><cell></cell><cell cols="2">Adv. Train Top-1</cell><cell></cell><cell cols="2">Adv. Train Top-5</cell></row><row><cell cols="6">Figure 2: Accuracy of model under attack by PGD for</cell></row><row><cell cols="6">varying adversarial distances ✏ with EoT steps={10,40}.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.robust-ml.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We would like to thank Battista Biggio for reviewing a draft of this work and providing insightful comments and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.07.023</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition,v o l</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>3 1 ,D e c</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3128572.3140444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, ser. AISec &apos;17</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, ser. AISec &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust Physical-World Attacks on Deep Learning Models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV),v o l .1</title>
		<imprint>
			<date type="published" when="0211">1 5 ,n o .3 ,p p .2 1 1 -2 5 2 ,2 0 1 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial Example Defenses: Ensembles of Weak Defenses Are Not Strong</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Offensive Technologies, ser. WOOT&apos;17</title>
				<meeting>the 11th USENIX Conference on Offensive Technologies, ser. WOOT&apos;17<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1021/ct2009208</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2 0 1 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2 0 1 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">No Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Negative Results in Computer Vision. CVPR 2017</title>
				<imprint>
			<date type="published" when="2017">2 0 1 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2 0 1 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Synthesizing Robust Adversarial Examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.615</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="5775" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Countering Adversarial Images Using Input Transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deflecting Adversarial Attacks with Pixel Deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00894</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mitigating Adversarial Effects Through Randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial Logit Pairing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="DOI">10.4103/0972-124X.94617</idno>
		<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating and Understanding the Robustness of Adversarial Logit Pairing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning,J .D ya n dA .K r a u s e ,E d s . ,s e r .P r oceedings of Machine Learning Research</title>
				<meeting>the 35th International Conference on Machine Learning,J .D ya n dA .K r a u s e ,E d s . ,s e r .P r oceedings of Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5283" to="5292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Certified Defenses Against Adversarial Examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vision</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Dual Approach to Scalable Verification of Deep Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2 0 1 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial Machine Learning at Scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2 0 1 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.14722/ndss.2018.23198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2018 Network and Distributed System Security Symposium</title>
				<meeting>2018 Network and Distributed System Security Symposium<address><addrLine>VA</addrLine></address></meeting>
		<imprint>
			<publisher>Internet Society</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>R e s t o n</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727230</idno>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seam Carving for Content-aware Image Resizing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="DOI">10.1145/1275808.1276390</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2007 Papers, ser. SIGGRAPH &apos;07</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrast Limited Adaptive Histogram Equalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Gems IV</title>
				<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press Professional, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="474" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast Median and Bilateral Filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="DOI">10.1145/1179352.1141918</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers, ser. SIGGRAPH &apos;06</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<idno type="DOI">10.1109/83.862633</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="0532">5 3 2 -1 5 4 6 ,2 0 0 0</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2 0 1 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2013.57</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1984">8 4 -9 9 6 ,2 0 1 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Foolbox: A Python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<imprint>
			<date type="published" when="2017">2 0 1 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pintor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2 0 1 8</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An Algorithm for Total Variation Minimization and Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:JMIV.0000011325.36760.1e</idno>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1920-01">v o l .2 0 ,n o .1 -2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast nonlocal filtering applied to electron cryomicroscopy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2008.4541250</idno>
	</analytic>
	<monogr>
		<title level="m">2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
				<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="1331" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning,v o l</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1920">8 1 -2 0 7 ,2 0 0 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The relative performance of ensemble methods with deep convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bibaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2800" to="2818" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
