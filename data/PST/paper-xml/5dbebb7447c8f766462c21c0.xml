<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-06">6 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiw@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
							<email>liujiayi7@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Software College</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
							<email>fdzhu@smu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="department">School of Information Systems</orgName>
								<address>
									<country>Singapore Management University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<email>panzhou@hust.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchong</forename><surname>Hu</surname></persName>
							<email>yuchonghu@hust.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-06">6 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357937</idno>
					<idno type="arXiv">arXiv:2106.03044v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dialogue generation</term>
					<term>emotional conversation</term>
					<term>emotional chatbot</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The consistency of a response to a given post at semantic-level and emotional-level is essential for a dialogue system to deliver human-like interactions. However, this challenge is not well addressed in the literature, since most of the approaches neglect the emotional information conveyed by a post while generating responses. This article addresses this problem by proposing a unified end-to-end neural architecture, which is capable of simultaneously encoding the semantics and the emotions in a post for generating more intelligent responses with appropriately expressed emotions. Extensive experiments on real-world data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of both content coherence and emotion appropriateness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Dialogue systems in practice are typically built for various purposes like emotional interaction, customer service or information acquisition, which can be roughly categorized into three classes, i.e., chitchat chatbots, task-oriented chatbots and domain-specific chatbots. For example, a task-specific chatbot can serve as a customer consultant, while a chitchat chatbot is commonly designed for convincingly simulating how a human would respond as a conversational partner. In fact, most recent work on response generation in chitchat domain can be summarized as follows, i.e., retrievalbased, matching-based, or statistical machine learning based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Recently, with the increasing popularity of deep learning, many research efforts have been dedicated to employing an encode-decoder architecture i.e., Sequence-tosequence (Seq2seq) models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>, to map a post to the corresponding response with little hand-crafted features or domain-specific knowledge for the conversation generation problem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>. Subsequently, several variants of Seq2seq models are also proposed to address different issues <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Despite the great progress made in neural dialogue generation, a general fact is that few work has been reported to automatically incorporate emotional factors for dialogue systems. In fact, several empirical studies have proven that chatbots with the ability of emotional communication with humans are essential for enhancing user satisfaction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. To this end, it is highly valuable and desirable to develop an emotion-aware chatbot that is capable of perceiving/expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type="bibr" target="#b45">[46]</ref> successfully build an emotional chat machine (ECM) that is capable of generating emotional responses according to a pre-defined emotion category, and several similar efforts are also made by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>, such as <ref type="bibr" target="#b47">[48]</ref> proposed by <ref type="bibr">Zhou et al.</ref> that utilizes emojis to control the emotional response generation process within conditional variational autoencoder (CAVE) framework.</p><p>Nevertheless, these mentioned approaches cannot work well owing to the following facts: (i) These approaches solely generate the emotional responses based on a pre-specified label (or emoji) as shown in Figure <ref type="figure" target="#fig_0">1</ref>, which is unrealistic in practice as the welldesigned dialogue systems need to wait for a manually selected emotion category for response generation; (ii) The generation process apparently divided into two parts would significantly reduce the smoothness and quality of generating responses; and (iii) As shown in Figure <ref type="figure" target="#fig_0">1</ref>, some emotionally-inappropriate responses (even conflicts) might apparently affect the interlocutor's satisfaction. Thereby, here we argue that fully exploiting the emotional information of the given post to supervise the learning process is definitely beneficial for automatically generating responses with the optimal emotion (rf. "EACM response" generated by our method shown in Figure <ref type="figure" target="#fig_0">1</ref>). Previous methods greatly contribute to emotion-aware conversation generation problem, however, they are insufficient and several issues emerge when trying to fully-address this problem. First, it is not easy to model human emotion from a given sentence due to semantic sparsity. Psychological studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> demonstrate that human emotion is quite complex and one sentence may contain multi-types of emotions with different intensities. For example, a hotel guest might write a comment like "The environment is not bad, however the location is too remote. " As such, solely using the post's emotion label is insufficient and we need to appropriately extract the emotional information of the input post for representation. Second, it is difficult for a model to decide the optimal response emotion for generation, and it is also not reasonable to directly map the post's emotion label to the response's emotion label, as the emotion selection process is determined not only by the post's emotion but also by its semantic meaning. Third, it is also problematic to design a unified model that can generate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type="bibr" target="#b45">[46]</ref>. Hence, the response generation problem faces a significant challenge: that is, how to effectively leverage the emotion and semantic of a given post to automatically learn the emotion interaction mode for emotion-aware response generation within a unified model.</p><p>In this paper, we propose a novel emotion-aware chat machine (i.e., EACM for short), which is capable of perceiving other interlocutor's feeling (i.e., post's emotion) and generating plausible response with the optimal emotion category (i.e., response's emotion). Specifically, EACM is based on a unified Seq2seq architecture with a self-attention enhanced emotion selector and an emotionbiased response generator, to simultaneously modeling the post's emotional and semantic information for automatically generating appropriate response. Experiments on the public datasets demonstrate the effectiveness of our proposed method, in terms of two different types of evaluation metrics, i.e., automatic metric and human evaluation, which are used to measure the diversity of words in the generated sentences (automatic metric, indirectly reflecting the diversity of expressed emotions, e.g., distinct-n), and whether the generated responses' emotions are appropriate according to human annotations (human evaluation). The main contributions of this research are summarized as follows:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>The current conversation generation approaches are mostly based on the basic Sequence-to-sequence (Seq2seq) framework, which has been proven <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref> that is able to effectively address the sequencemapping issues. Following the success of Seq2seq model, methods based on such framework have been applied to various domains, such as machine translation <ref type="bibr" target="#b1">[2]</ref> and image caption generation <ref type="bibr" target="#b0">[1]</ref>. Indeed, there exist some attempts on improving the performance of such encoder-decoder architecture for machine translation problem. Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref> utilize Bi-directional Long Short-Term Memory (Bi-LSTM) network with attention mechanism for longsentence generation, which is able to automatically search for relevant parts in the context. Luong et al. <ref type="bibr" target="#b19">[20]</ref> thoroughly evaluate the effectiveness of different types of attention mechanisms, i.e., global/local attention with different alignment functions. Furthermore, self-attention mechanism proposed by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref> is proved effective for machine translation, which can yield large gains in terms of BLEU <ref type="bibr" target="#b23">[24]</ref> as compared to the state-of-the-art methods. To cope with the increasing complexity in the decoding stage (caused by large-scale vocabularies), Jean et al. <ref type="bibr" target="#b11">[12]</ref> consider to use sampled softmax methods and thus achieve encouraging results. These works have improved the generation performance of the Seq2seq model and speeded up decoding process, which build a solid foundation for the future studies based on this architecture.</p><p>There also exist many efforts dedicated to research on how to apply Seq2seq model for conversation systems <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>, by regarding the conversation generation as a monolingual translation task, and later on several variants are proposed for a wide variety of domain-specific issues, such as hierarchical recurrent model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, topic-aware model <ref type="bibr" target="#b43">[44]</ref>. Besides, several persona-based <ref type="bibr" target="#b14">[15]</ref> models and identity-coherent models <ref type="bibr" target="#b28">[29]</ref> are proposed to endow the chatbots with personality for addressing the context-consistency problem. There have been numerous attempts to generate more diverse and informative responses, such as Maximum Mutual Information (MMI) based model <ref type="bibr" target="#b13">[14]</ref> and enhanced beam-search based model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. Several approaches are also proposed for specific tasks, such as Zhou et al. <ref type="bibr" target="#b46">[47]</ref> take account of static graph attention to incorporate commonsense knowledge for chatbots. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> propose different solutions for two classical conversation scenarios, i.e., chit-chat and domain-specific conversation.</p><p>In recent years, many researches propose that emotion factors are of great significance in terms of successfully building humanlike conversation generation models. Ghosh et al. <ref type="bibr" target="#b7">[8]</ref> propose affect language model to generate texture conditioned on the given affect categories with controllable affect strength. Hu et al. <ref type="bibr" target="#b9">[10]</ref> present a combined model of the Variational Auto-Encoder (VAE) and holistic attribute discriminators to generate sentences with certain types of sentiment and tense. However, these models are mainly built for emotional text generation task. Several proposals study the conversation generation problem with emotional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type="bibr" target="#b45">[46]</ref> develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.e., emotion embedding, internal memory and external memory) to generate responses according to the designated emotion category. Similarly, Zhou et al. <ref type="bibr" target="#b47">[48]</ref> propose a reinforcement learning approach within conditional variational auto-encoder framework to generate responses conditioned on the given emojis. In <ref type="bibr" target="#b10">[11]</ref>, Huang et al. propose three different models that are capable of injecting different emotion factors for response generation. Peng et al. <ref type="bibr" target="#b24">[25]</ref> utilize Latent Dirichlet allocation (LDA) models to extract topic information for emotional conversation generation. However, all of such models are based on a designated emotion category to generation emotional responses, which need human beings to decide an optimal response emotion category for generation. Besides, the emotion information of the given post is not explicitly modeled, and thus the generated responses are not good enough. As opposed, our proposed model is capable of effectively leverage the emotion and semantics of a given post to automatically learn the emotion interaction mode for emotion-aware response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL 3.1 Preliminary: Sequence-to-Sequence Attention Model</head><p>In the literature, sequence-to-sequence (Seq2seq) model is widely adopted for dialogue generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>. In order to promote the quality of the generated sentences, the Seq2seq-attention model <ref type="bibr" target="#b1">[2]</ref> is proposed for dynamically attending on the key information of the input post during decoding. In this paper, our approach is mainly based on Seq2seq-attention models for response generation and therefore we will first illustrate this basic model in principle. The Seq2seq-attention model is typically a deep RNN-based architecture with an encoder and a decoder. The encoder takes the given post sequence ğ’™ = {ğ‘¥ 1 , ğ‘¥ 2 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‡ } (ğ‘‡ is the length of the post) as inputs, and maps them into hidden representations</p><formula xml:id="formula_0">ğ’‰ = (ğ’‰ 1 , ğ’‰ 2 , â€¢ â€¢ â€¢ , ğ’‰ ğ‘‡ ).</formula><p>The decoder then decodes them to generate a possibly variably-sized word sequence, i.e.,</p><formula xml:id="formula_1">ğ’š = {ğ‘¦ 1 , ğ‘¦ 2 , â€¢ â€¢ â€¢ , ğ‘¦ ğ‘‡ â€² },</formula><p>where ğ‘‡ â€² is the length of the output sequence, and it may differ from ğ‘‡ .</p><p>In more detail, the context representation ğ’„ ğ‘¡ of the post sequence ğ’™ is computed by parameterizing the encoder hidden vector ğ’‰ with different attention scores <ref type="bibr" target="#b1">[2]</ref>, that is,</p><formula xml:id="formula_2">ğ’„ ğ‘¡ = ğ‘‡ âˆ‘ï¸ ğ‘—=1 ğœ¶ (ğ’” ğ‘¡ âˆ’1 , ğ’‰ ğ’‹ ) â€¢ ğ’‰ ğ’‹ ,<label>(1)</label></formula><p>where ğœ¶ (., .) is a coefficient estimated by each encoder token's relevance to the predicting ğ‘¦ ğ‘¡ . The decoder iteratively updates its state ğ’” ğ‘¡ using previously the generated word ğ’š ğ‘¡ âˆ’1 , namely,</p><formula xml:id="formula_3">ğ’” ğ‘¡ = ğ‘“ (ğ’” ğ‘¡ âˆ’1 , ğ’š ğ‘¡ âˆ’1 , ğ’„ ğ’• ), ğ‘¡ = 1, 2, â€¢ â€¢ â€¢ ,ğ‘‡ â€² ,<label>(2)</label></formula><p>where ğ‘“ is a non-linear transformation of RNN cells (e.g., LSTM <ref type="bibr" target="#b8">[9]</ref> or GRU <ref type="bibr" target="#b4">[5]</ref>).</p><p>Then, the probability of generating the ğ‘¡-th token ğ‘¦ ğ‘¡ conditioned on the input sequence ğ’™ and the previous predicted word sequence</p><formula xml:id="formula_4">ğ‘¦ 1:ğ‘¡ âˆ’1 is computed by Pr (ğ‘¦ ğ‘¡ |ğ‘¦ 1:ğ‘¡ âˆ’1 , ğ’™) = ğ‘”(ğ’š ğ‘¡ âˆ’1 , ğ’” ğ’• , ğ’„ ğ’• ),<label>(3)</label></formula><p>where ğ‘”(.) is a function (e.g., softmax) to produce valid probability distribution for sampling the next word of the output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition and Overview</head><p>In this paper, our task is to perceive the emotion involved in the input post and incorporate it into the generation process for automatically producing both semantically reasonable and emotionally appropriate response. Hence, our conversation generation problem is defined as, given an input post ğ’™ = {ğ‘¥ 1 , ğ‘¥ To address the problem, we propose an emotion-aware chat machine (EACM), which primarily consists of two subcomponents, the emotion selector and the response generator. More concretely, the emotion selector is in charge of the emotion selecting process, yielding an emotion distribution over E ğ‘ for the to-be-generated response based on the input post and its emotion:</p><formula xml:id="formula_5">ğ’† * ğ’“ â† arg max ğ’† ğ’“ âˆˆE ğ‘ Pr(ğ’† ğ’“ |ğ’™, ğ’† ğ’‘ ),<label>(4)</label></formula><p>where E ğ‘ is the vector space of the emotions. Subsequently, the generator generates a corresponding response to the given input based on the obtained emotion ğ’† * ğ’“ and the input post ğ’™, namely,</p><formula xml:id="formula_6">ğ’š * ğ’• â† arg max Pr(ğ’š ğ’• |ğ’š 1:ğ‘¡ âˆ’1 , ğ’™, ğ’† * ğ’“ ).<label>(5)</label></formula><p>Remark. Actually, previous approaches usually assume that the emotion of the generated response is derived from a unique emotion category <ref type="foot" target="#foot_0">1</ref> (denoted by the one-hot vectors in E ğ‘ ). However, human emotions are intuitively more delicate, and thus we argue that the emotion of each response may not be limited in a single emotion category. To this end, we assume that the emotion probability distribution is over the entire vector space E ğ‘ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-attention Enhanced Emotion Selector</head><p>3.3.1 Self-attention Based Encoding Network. Our information encoding network consists of two parts: an emotion encoder and a semantics encoder. We leverage these two encoders by explicitly extracting emotional information and semantic information, and then feed them into a fusion prediction network. Specifically, the emotion encoder is implemented using a GRU network to extract information from post sequence ğ’™ = (ğ‘¥ 1 , ğ‘¥ </p><formula xml:id="formula_7">ğ’‰ ğ‘¡ ğ‘’ = GRU(ğ’‰ ğ‘¡ âˆ’1 ğ‘’ , ğ‘¥ ğ‘¡ ),<label>(6)</label></formula><p>where ğ’‰ ğ‘¡ ğ‘’ denotes the hidden state of post's emotion information at time step ğ‘¡.  To enhance the representation power of the hidden states, we utilize self-attention mechanism <ref type="bibr" target="#b17">[18]</ref> to enable the encoders to attend to emotion-rich words in the post, and then obtain the emotional hidden state ğ’‰ ğ‘’ by calculating:</p><formula xml:id="formula_8">ğ’‰ ğ‘’ = ğ‘‡ âˆ‘ï¸ ğ‘–=1 ğ‘ ğ‘– ğ’‰ ğ‘– ğ‘’ ,<label>(7)</label></formula><p>where ğ‘ ğ‘– is the weight of hidden state ğ’‰ ğ’Š ğ’† , which is calculated by feeding ğ’‰ ğ’Š ğ’† into a multi-layer perceptron with a softmax layer to ensure that all the weights sum up to 1:</p><formula xml:id="formula_9">ğ‘ ğ‘– = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘½ ğ’‚ ğ‘¡ğ‘ğ‘›â„(ğ‘¾ ğ’‚ (ğ’‰ ğ‘– ğ‘’ ) âŠ¤ ).<label>(8)</label></formula><p>The network focuses on emotional information by imposing a cross entropy loss on the top of the emotion hidden state ğ’‰ ğ‘’ , that is, passing the emotion hidden state through a linear layer and a sigmoid layer to project it into an emotion distribution over E ğ‘ , and then calculating the cross entropy loss as follows,</p><formula xml:id="formula_10">Ãªğ‘ = ğœ (ğ‘¾ ğ’† ğ’‰ ğ‘’ + ğ‘),<label>(9)</label></formula><formula xml:id="formula_11">L ğ‘ = âˆ’ğ’† ğ‘ log( Ãªğ‘ ),<label>(10)</label></formula><p>where ğ’† ğ‘ is a multi-hot representation of post's emotion vector since it may contain various emotions and L ğ‘ is the loss function. However, simply mapping Ãªğ‘ to the response emotion category ğ’† ğ’“ is insufficient to model the emotion interaction process between partners, as we cannot choose emotion category only based on post's emotion category under some circumstances. In fact, some posts expressing negative feelings like sad are inappropriate to be replied with the same emotion, such as "It's a pity you can't come with us" or "I'm so sad that you broke my heart". Therefore, we not only consider the post's emotion information, but also take into account its semantic meaning by combing another GRU network (i.e., semantics encoder) to encode post's semantic information for generation. Similarly, we get a weighted summation of the hidden states represented as ğ’‰ ğ‘  .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Emotional and Semantic Word Embeddings.</head><p>In order to force the emotion selector to focus on different aspects of auxiliary information of the given post, we apply the emotion embedding for the emotional encoder and the semantic embedding for the semantic encoder, respectively. In particular, we make use of sentiment specific word embedding (SSWE) <ref type="bibr" target="#b36">[37]</ref> and word2vec embedding <ref type="bibr" target="#b21">[22]</ref> in our model. More concretely, SSWE encodes sentiment information in the continuous representation of words, mapping words with the same sentiment to the neighbor word vectors, which is used in the emotion encoder to promote the ability of perceiving emotional information in post's utterance. Simultaneously, word2vec is used to extract semantic information from the post, and the two embeddings work interactively to guarantee the efficacy of the encoding network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Fusion and Prediction Network.</head><p>To construct the response emotion category ğ‘’ ğ‘Ÿ , we consider to use a fusion network to balance the contributions derived from different types of information, and employ a prediction network to select the response emotion categories based on such mixed information. Then we concatenate the obtained ğ’‰ ğ‘  and ğ’‰ ğ‘’ and feed it into a sigmoid layer to yield a trade-off weight:</p><formula xml:id="formula_12">ğ‘¤ = ğœ ([ ğ’‰ ğ‘  ; ğ’‰ ğ‘’ ]),<label>(11)</label></formula><formula xml:id="formula_13">ğ’‰ â€² ğ‘’ = ğ‘¡ğ‘ğ‘›â„( ğ’‰ ğ‘’ ),<label>(12)</label></formula><formula xml:id="formula_14">ğ’‰ â€² ğ‘  = ğ‘¡ğ‘ğ‘›â„( ğ’‰ ğ‘  ).<label>(13)</label></formula><p>The final representation is a weighted sum of the semantic hidden state and the emotional hidden state:</p><formula xml:id="formula_15">ğ’‰ ğ‘’ğ‘  = ğ‘¤ âŠ— ğ’‰ â€² ğ‘  + (1 âˆ’ ğ‘¤) âŠ— ğ’‰ â€² ğ‘’ ,<label>(14)</label></formula><p>where âŠ— indicates element-wise multiplication. The final representation is fed into a prediction network to produce an emotion vector for generation, which is passed through MLP and then mapped into a probability distribution over the emotion categories:</p><formula xml:id="formula_16">Ãªğ‘Ÿ = ğœ (ğ‘¾ ğ’“ ğ’‰ ğ‘’ğ‘  + ğ‘),<label>(15)</label></formula><formula xml:id="formula_17">L ğ‘Ÿ = âˆ’ğ’† ğ‘Ÿ log( Ãªğ‘Ÿ ),<label>(16)</label></formula><p>where ğ’† ğ‘Ÿ is the multi-hot representation of the response emotion vector. Ãªğ‘Ÿ is the final response emotion vector generated through the proposed emotion selector, which is then passed to the generator for emotional response generation. Intuitively, the emotion selector can adaptively determine the appropriate emotion in the emotion selection process for emotional response generation, by taking into account both the post's semantic information and emotional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Emotion-Biased Response Generator</head><p>To construct the generator, we consider to use an emotion-enhanced seq2seq model that is capable of balancing the emotional part with the semantic part and generate intelligible responses. Thereby, we first generate the response emotion embedding ğ‘½ ğ‘’ by multiplying Ãªğ‘Ÿ with a randomly initialized matrix:</p><formula xml:id="formula_18">ğ‘½ ğ‘’ = ğ‘¾ ğ‘’ Ãªğ‘Ÿ ,<label>(17)</label></formula><p>where ğ‘¾ ğ‘’ is the emotion embedding matrix, which is the latent emotional factors, i.e., the high-level abstraction of emotion expressions by following Plutchik's assumptions <ref type="bibr" target="#b25">[26]</ref>. As mentioned, a one-hot emotion embedding is inappropriate and thus here we do not use a softmax on Ãªğ‘Ÿ to only pick an optimal emotion category for generation. As such, we call it as soft-emotion injection procedure, which is used to model the diversity of emotions. By following the work <ref type="bibr" target="#b39">[40]</ref>, we use a new encoder to encode ğ’™ for obtaining a sequence of hidden states ğ’‰ = (ğ’‰ 1 , ğ’‰ 2 , â€¢ â€¢ â€¢ , ğ’‰ ğ‘‡ ) through a RNN network, and then generate the context vector ğ’„ ğ‘¡ for decoding the current hidden state ğ’” ğ’• , via applying attention mechanism to re-assign an attended weight to each encoder hidden </p><formula xml:id="formula_19">ğ‘¢ ğ‘¡ ğ‘– = ğ’— âŠ¤ tanh(ğ‘¾ 1 ğ’‰ ğ‘– + ğ‘¾ 2 ğ’” ğ‘¡ ),<label>(18)</label></formula><formula xml:id="formula_20">ğ‘ ğ‘¡ ğ‘– = softmax(ğ‘¢ ğ‘¡ ğ‘– ),<label>(19)</label></formula><formula xml:id="formula_21">ğ’„ ğ‘¡ = ğ‘‡ âˆ‘ï¸ ğ‘–=1 ğ‘ ğ‘¡ ğ‘– ğ’‰ ğ‘– .<label>(20)</label></formula><p>At each time step ğ‘¡, the context vector encoded with attention mechanism enable our model to proactively search for salient information which is important for decoding over a long sentence. However, it neglects the emotion (ğ‘½ ğ‘’ ) derived from the response during generation, and thus we propose an emotion-biased attention mechanism to rewritten Eq.( <ref type="formula" target="#formula_19">18</ref>),</p><formula xml:id="formula_22">ğ‘¢ ğ‘¡ ğ‘– = ğ’— âŠ¤ tanh(ğ‘¾ 1 ğ’‰ ğ‘– + ğ‘¾ 2 ğ’” ğ‘¡ + ğ‘¾ 3 ğ‘½ ğ‘’ ).<label>(21)</label></formula><p>The context vector ğ’„ ğ‘¡ is concatenated with ğ’” ğ‘¡ and forms a new hidden state ğ’” â€² ğ‘¡ :</p><formula xml:id="formula_23">ğ’” â€² ğ‘¡ = ğ‘¾ 4 [ğ’” ğ‘¡ ; ğ’„ ğ‘¡ ],<label>(22)</label></formula><p>from which we make the prediction for each word; ğ’” ğ‘¡ is obtained by changing Eq. ( <ref type="formula" target="#formula_3">2</ref>) into:</p><formula xml:id="formula_24">ğ’” ğ‘¡ = GRU(ğ’” â€² ğ‘¡ âˆ’1 , [ğ’š ğ‘¡ âˆ’1 ; ğ‘½ ğ‘’ ]),<label>(23)</label></formula><p>which fulfills the task of injecting emotion information while generating responses. To be consistent with previous conversation generation approaches, here we consider to use cross entropy to be the loss function, which is defined by</p><formula xml:id="formula_25">L ğ‘ ğ‘’ğ‘2ğ‘ ğ‘’ğ‘ (ğœƒ ) = âˆ’ğ‘™ğ‘œğ‘”ğ‘ƒ (ğ’š|ğ’™) (24) = âˆ’ ğ‘‡ â€² âˆ‘ï¸ ğ‘¡ =1 ğ‘™ğ‘œğ‘”ğ‘ƒ (ğ‘¦ ğ‘¡ | ğ‘¦ 1 , ğ‘¦ 2 , â€¢ â€¢ â€¢ , ğ‘¦ ğ‘¡ âˆ’1 , ğ’„ ğ‘¡ , ğ‘½ ğ‘’ ),<label>(25)</label></formula><p>where ğœƒ denotes the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>The loss function of our model is a weighted summation of the semantic loss and sentiment loss:</p><formula xml:id="formula_26">L ğ¸ğ´ğ¶ğ‘€ (ğœƒ ) = ğ›¼ L ğ‘’ + (1 âˆ’ ğ›¼)L ğ‘ ğ‘’ğ‘2ğ‘ ğ‘’ğ‘ , (<label>26</label></formula><formula xml:id="formula_27">)</formula><p>where ğ›¼ is a balance factor, and L ğ‘’ denotes the emotional loss, namely, </p><formula xml:id="formula_28">L ğ‘’ = L ğ‘ + L ğ‘Ÿ . (<label>27</label></formula><formula xml:id="formula_29">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We conduct our experiments on a public dataset, i.e., Emotional Short-Text Conversation (ESTC) derived from STC <ref type="bibr" target="#b32">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type="bibr" target="#b45">[46]</ref> to train an emotion classifier for assigning emotional labels to the sentences in the dataset.</p><p>ESTC, which contains over four million real-world conversations obtained from Chinese micro-blogging, i.e., Weibo. The raw dataset contains 4, 433, 949 post-comment pairs, from which 1, 000 pairs are extracted for validation and another 1, 000 pairs are used for testing. Details of this dataset is illustrated in Table <ref type="table" target="#tab_4">1</ref>.</p><p>Preprocessing. As the raw dataset (STC) does not have emotion labels, so we train an emotion classifier based on BERT <ref type="bibr" target="#b5">[6]</ref> model over two different datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type="bibr" target="#b45">[46]</ref>, which contain 29, 417 manually annotated data in total, and the best performance(accuracy) of 0.7257 is achieved at the 19, 635 step. Specifically, each sentence is marked with two labels, namely, a primary label and a secondary one. We preprocess the labels over the mentioned emotion categories, i.e., (like, disgust, sad, angry, happy, other), note that here "other" indicates no any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type="bibr" target="#b45">[46]</ref> using solely one label for classification, we consider both of the emotion labels and thus regard it as a multi-label classification task. Under such circumstance, there are three cases appearing in the label sets, i.e., no emotion labeled with (other, other), one emotion labeled with (emo1, other), as well as two emotions labeled with (emo1, emo2), respectively.</p><p>To evaluate the emotion perception ability of different approaches over the emotion categories, we build an emotion-rich dialogue set for a fair empirical comparison. Specifically, we randomly chose 1, 000 pairs whose primary emotion label is among the (like, disgust, sad, angry or happy) categories, with 200 pairs for each emotion, respectively. In addition, we also present an in-depth analysis the emotion interaction pattern (EIP) over conversations, in which each EIP is defined as a (ğ’† ğ’‘ ,ğ’† ğ’“ ) pair for each conversation to reflect 2 http://tcci.ccf.org.cn/conference/2013/ 3 http://tcci.ccf.org.cn/conference/2014/ the transition pattern from the post's emotion to the response's emotion. Figure <ref type="figure" target="#fig_2">4</ref> shows a heatmap to depict the number of EIPs appearing in the dataset, and each row (or column) indicates the post's emotion ğ’† ğ’‘ (or the response's emotion ğ’† ğ’“ ). From the figure we can observe that: (1) The darker each grid is, the more pairs (ğ’† ğ’‘ ,ğ’† ğ’“ ) appearing in such grid; (2) The heat map is sparse since the EIPs of some post-response emotion pairs are overly rare to appear in our dataset.</p><p>Moreover, we also leverage "Weibo Sentiment Dataset" provided by Shujutang<ref type="foot" target="#foot_1">4</ref> to train the sentiment-specific word embeddings (SSWE), which consists of two million Weibo sentences with sentiment labels, and we remove some extra domain-specific punctuations like "@user" and "URLs".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>As reported in <ref type="bibr" target="#b18">[19]</ref>, BLEU might be improper to evaluate the conversation generation problem, as it correlates weakly with human judgements of the response quality, similar situations for ME-TEOR <ref type="bibr" target="#b2">[3]</ref> and ROUGE <ref type="bibr" target="#b16">[17]</ref>. Besides, there still exists a challenge of automatically evaluating the generation model from emotion perspective. As a result, in this paper we adopt distinct-1 and distinct-2 by follow the work <ref type="bibr" target="#b13">[14]</ref> to be as the metrics for evaluating the diversity of the generated responses, which measures the degree of diversity by computing the number of distinct uni-grams and bi-grams in the generated responses, and can indirectly reflect the degree of emotion diversity, as the generated sentence containing diverse emotions is more likely to have more abundant words in principle. In addition, we also carry out a manual evaluation for evaluate the performance of the generated responses at emotionallevel and semantic-level separately with human intuition, and then the response quality is calculated by combining such two results at different levels for integrally assessing different models.</p><p>Automatic Evaluation. As mentioned, we consider to use distinct-1 and distinct-2 <ref type="bibr" target="#b13">[14]</ref> to be as our automatic evaluation metric. Distinct-n is defined as the number of distinct n-grams in generated responses. The value is scaled by total number of generated tokens.</p><p>Human Evaluation. We randomly sampled 200 posts from the test set, and then aggregate the corresponding responses returned by each evaluated method, then three graduate students (whose research areas are not in text processing area) are invited for labeling. Each generated response is labeled from two different aspects, i.e., emotion and semantics. Specifically, from emotion perspective, each generated response is labeled with (score 0) if its emotion is apparently inappropriate (namely evident emotion collision,e.g., angry-happy) to the given post, and (score 1) otherwise. From semantic perspective, we evaluate the generated results using the scoring metrics as follows. Note that if conflicts happen, the third annotator determines the final result.</p><p>â€¢ 1: If the generated sentence can be obversely considered as a appropriate response to the input post; â€¢ 0: If the generated sentence is hard-to-perceive or has little relevance to the given post. To conduct an integral assessment of the models at both emotional-level and semantic-level, we measure the response quality by using the formula as follows,</p><formula xml:id="formula_30">ğ‘„ ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ = ğ‘† ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡ âˆ§ ğ‘† ğ‘ ğ‘’ğ‘šğ‘ğ‘›ğ‘¡ğ‘–ğ‘ğ‘  ,<label>(28)</label></formula><p>where ğ‘„ ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ reflects the response quality and ğ‘† ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡ , ğ‘† ğ‘ ğ‘’ğ‘šğ‘ğ‘›ğ‘¡ğ‘–ğ‘ğ‘  denote the sentiment score and semantic score, respectively, which is used to means the response quality of each case is equal to 1 if and only if both of its sentiment score and semantic score are scored as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare our model with the following baselines.</p><p>Seq2seq <ref type="bibr" target="#b35">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type="bibr" target="#b45">[46]</ref>, as mentioned, ECM model is improper to directly be as the baseline since it cannot automatically select an appropriate emotion label to the respond. Thereby, we manually designate a most frequent response emotion to ECM for fairness comparison. Specifically, we train a post emotion classifier to automatically detect post's emotion, and then choose the corresponding response emotion category using the most frequent response's emotion to the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopted in the same manner. This model encode the emotion category into an embedding vector, and then utilize it as an extra emotion input when decoding. Intuitively, the generated responses from ECM and Seq2seq-emb can be viewed as the indication of the performance of simply incorporating the EIPs for modeling the emotional interactions among the conversation pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>For all approaches, each encoder and decoder with 2-layers GRU cells containing 256 hidden units, and all of the parameters are not shared between such two different layers. The vocabulary size is set as 40, 000, and the OOV (out-of-vocabulary) words are replaced with a special token UNK. The size of word embeddings is 200, which are randomly initialized. The emotion embedding is a 6 Ã— 200-dimensional matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type="bibr" target="#b45">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch for optimization when training, and the batch size and the learning rate are set as 128 and 0.5, respectively. The greedy search algorithm is adopted for each approach to generate responses. Additionally, for speeding up the training process, we leverage the well-trained Seq2seq model to initialize other methods.</p><p>The parameters for our proposed method are empirically set as follows: SSWE is trained by following the parameter settings in <ref type="bibr" target="#b36">[37]</ref>. The length of hidden layer is set at 20, and we use AdaGrad <ref type="bibr" target="#b6">[7]</ref> to update the trainable parameters and the learning rate is set as 0.1. The size of emotion embedding and word embedding are both set at 200. In particular, the Word2vec embedding is used based on Tencent AI Lab Embedding 5 , which is pre-trained over 8 million high-quality Chinese words and phrases by using directional skip-gram method 5 https://ai.tencent.com/ailab/nlp/embedding.html <ref type="bibr" target="#b33">[34]</ref>. We use jieba<ref type="foot" target="#foot_2">6</ref> for word segmentation during the evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results and Discussion</head><p>In this section, we evaluate the effectiveness of generating emotional responses by our approach as comparison to the baseline methods.</p><p>Automatic Evaluation. From Table <ref type="table" target="#tab_5">2</ref>, we can observe that: (i) ECM performs worse than Seq2seq, the reason might be the emotion selection process is based on two-stage process, i.e., post emotion detection process and response emotion selection process, which would significantly reduce the diversity and quality of emotion response generation due to the errors of emotion classification and the transition pattern modeling procedure. In particular, the emotion category (other, other) is more likely to be chosen than other emotion categories. We will present an in-depth analysis in Section 4.6. and (ii) Our proposed EACM consistently outperforms all of the baselines in terms of distinct-1 and distinct-2. The results demonstrate that our emotion selection process is really effective in enhancing the ability of generating more diverse words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Distinct  <ref type="table">4</ref>: The percentage of the sentiment-semantic score given by human evaluation.</p><p>The percentage of the sentiment-semantics scores under the human evaluation is shown in Table <ref type="table">4</ref>. For ECM, the percentage of (0-0) degrades while the percentage of (1-0) increases as opposed to Seq2seq, which suggests that the effectivenss of EIP, i.e., the most frequent response emotions have low probability to result in emotional conflicts, In addition, the percentage of (1-1) degrades while the percentage of (1-0) increases, which reflects that directly using emotion classifier to model emotion interaction process is insufficient. In comparison, EACM reduces the percentage of generating responses with wrong emotion and correct semantics (i.e., 0-1) while increase the percentage of (1-1) correspondingly, which demonstrate that EACM is capable of successfully model the emotion interaction pattern among human conversations and meanwhile guarantee semantic correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>In this section, we present an in-depth analysis of emotion-aware response generation results of our proposed approach. We select 3 samples (with the input posts and their corresponding responses) generated by different methods are shown in Figure <ref type="figure">5</ref>, as can be seen:</p><p>Case 1. The results of EACM and ECM with (like, other) are similar and correct. However, responses given by ECM with other emotions are improper in semantics. Similar scenario for Seq2seq-emb with (Happy, other). However, the most frequent emotion to (Sad, other) is never be (Happy, other) over EIPs, and thus EIPs would fail to achieve the task.</p><p>Case 2. EACM can generate the relevant emotional response i.e., (Other, other) for the post with (Other, other). However, all of ECM with different emotions seems improper for response (especially for ECM with (Happy,other)), which reflects directly using a designated emotion for generation might be a unreasonable way for modeling the emotion interaction pattern. In addition, Seq2seq cannot detect the emotion and thus generate a irrelevant response. As most of conversations belong to (other, other), and thus the diversity of such emotion category is quite complicated and hard for training.</p><p>Case 3. The emotion in the case is (Angry, other), however the responses provided by ECM with (Angry, other) is obviously incorrect in semantics, which demonstrate that simply using post's emotion is inappropriate for response generation. As compared, EACM generates the correct emotional response with a emotionspecific word (i.e., "Holy crap"), which demonstrates EACM is good at controlling sentence emotion for generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an emotion-aware chat machine (EACM) to address the emotional response generation problem, which is composed of an emotion selector and a response generator. Specifically, a unified fusion-prediction network with self-attention mechanism is also developed to supervise the emotion selection process for generating a emotion-biased response. Extensive experiments conducted on a public dataset demonstrate the effectiveness of our proposed method as compared to baselines at both semantic-level and emotional-level, in terms of automatic evaluation and human evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sampled dialogue generated from ECM and EACM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Overview of EACM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of emotion interaction pattens (EIPs). Numerical values on each axis represent different emotion categories. The darker each grid is, the more pairs (ğ’† ğ’‘ ,ğ’† ğ’“ ) appear in such grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‡ } with its emotion ğ’† ğ’‘ , for a dialogue system, generate a corresponding response sequence ğ’š = {ğ‘¦ 1 , ğ‘¦ 2 , â€¢ â€¢ â€¢ , ğ‘¦ ğ‘‡ â€² } with proper emotion ğ’† ğ’“ .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‡ ), and map them into hidden representations ğ’‰ ğ’† = (ğ’‰ 1</figDesc><table><row><cell>ğ‘’ , ğ’‰ 2 ğ‘’ , â€¢ â€¢ â€¢ , ğ’‰ ğ‘‡ ğ‘’ ) using the</cell></row><row><cell>following formula:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Details of ESTC dataset.</figDesc><table><row><cell></cell><cell>Posts</cell><cell>219,162</cell></row><row><cell>Training #</cell><cell>No Emotion</cell><cell>1,586,065</cell></row><row><cell>Responses</cell><cell>Single Emotion</cell><cell>2,792,339</cell></row><row><cell></cell><cell>Dual Emotion</cell><cell>53,545</cell></row><row><cell>Validation #</cell><cell>1,000</cell><cell></cell></row><row><cell>Testing #</cell><cell>1,000</cell><cell></cell></row></table><note>state ğ’‰ ğ‘– .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation: distinct-1 and distinct-2.Human Evaluation. From Table3, we can observe that: Seq2seqemb provides the worst performance as expected, this is because the generation process apparently interrupted would significantly reduce the accuracy and quality of generating responses and thus generate some hard-to-perceive sentences. ECM achieves a relatively better result, as it is good at modeling the emotion dynamics when decoding (i.e., internal memory) and assigning different generation probabilities to emotion/generic words for explicitly modeling emotion expressions (i.e., external memory). Specifically, ECM results in a remarkable improvement over Seq2seq in terms of sentiment score, but it performs poorly when comparing semantic score, which demonstrates the effectiveness of emotion injection, however the explicit two-stage procedure might reduce the smoothness of generated responses with low semantic score. Our proposed model EACM consistently outperforms all baseline methods, and the improvements are statistically significant on all metrics. For example, EACM outperforms ECM by 16.9%, 1.72% and 25.81% in terms of semantic score, sentiment score and response quality, respectively. The reason might due to the fact that EACM is capable of simultaneously encoding the semantics and the emotions in a post for generating appropriately expressed emotional response within a unified end-to-end neural architecture, which are benefit for alleviating the hard emotion injection problem (as compared to soft emotion injection).</figDesc><table><row><cell></cell><cell></cell><cell>-1 Distinct-2</cell></row><row><cell>Seq2seq</cell><cell>0.0608</cell><cell>0.2104</cell></row><row><cell>Seq2seq-emb</cell><cell>0.0628</cell><cell>0.2370</cell></row><row><cell>ECM</cell><cell>0.0551</cell><cell>0.2022</cell></row><row><cell>EACM</cell><cell>0.0745</cell><cell>0.2749</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation: averaged semantic score, sentiment score and response quality.</figDesc><table><row><cell>Method(%)</cell><cell cols="4">1-1 1-0 0-1 0-0</cell></row><row><cell>Seq2seq</cell><cell cols="2">36 45.5</cell><cell>3</cell><cell>15.5</cell></row><row><cell cols="3">Seq2seq-emb 25 54.5</cell><cell>3</cell><cell>17.5</cell></row><row><cell>ECM</cell><cell>31</cell><cell cols="3">56 4.5 8.5</cell></row><row><cell>EACM</cell><cell cols="3">39 49.5 2.5</cell><cell>9</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Here we follow the work<ref type="bibr" target="#b45">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy, Like, Sad, Other}.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">http://www.datatang.com/data/45439</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">https://github.com/fxsjy/jieba</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant No.61602197, and in part by Equipment Pre-Research Fund for The 13th Five-year Plan under Grant No.41412050801.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Posts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2seq</head><p>Seq2seq-emb ECM EACM  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting user mental states in spoken dialogue systems</title>
		<author>
			<persName><forename type="first">Zoraida</forename><surname>Callejas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Griol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">RamÃ³n</forename><surname>LÃ³pez-CÃ³zar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJASP</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="257" to="269" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Laksana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<title level="m">Affect-lm: A neural language model for customizable affective text generation</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic Dialogue Generation with Expressed Emotions</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osmar</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amine</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">SÃ©bastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">On using very large target vocabulary for neural machine translation</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breakdown in human-machine interaction: the error is the clue</title>
		<author>
			<persName><forename type="first">Bilyana</forename><surname>Martinovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA tutorial and research workshop</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topicenhanced emotional conversation generation with attention mechanism</title>
		<author>
			<persName><forename type="first">Yehong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="429" to="437" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A general psychoevolutionary theory of emotion</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theories of emotion</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="3" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American scientist</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="344" to="350" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using human physiology to evaluate subtle expressivity of a virtual quizmaster in a mathematical game</title>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJHCS</title>
		<imprint>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Assigning personality/identity to a chatting machine for coherent conversation generation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A neural network approach to context-sensitive generation of conversational responses</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ramprasath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The elements of AIML style</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<publisher>Alice AI Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond FaÃ§ade: Pattern matching for natural language applications</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GamaSutra. com</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Topic Aware Neural Response Generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tailored Sequence to Sequence Models to Different Conversation Scenarios</title>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1479" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Commonsense Knowledge Aware Conversation Generation with Graph Attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<title level="m">Mojitalk: Generating emotional responses at scale</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
