<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quality Controlled Paraphrase Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elron</forename><surname>Bandel</surname></persName>
							<email>elron.bandel@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Shmueli-Scheuer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Shnayderman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<email>noams@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liat</forename><surname>Ein-Dor</surname></persName>
							<email>liate@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ibm</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quality Controlled Paraphrase Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Paraphrase generation has been widely used in various downstream tasks. Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence. Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases. Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree. However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability. Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions. Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases. We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline. The models, the code, and the data can be found in https://github.com/IBM/quality-c ontrolled-paraphrase-generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Paraphrase generation, namely rewriting a sentence using different words and/or syntax while preserving its meaning <ref type="bibr" target="#b3">(Bhagat and Hovy, 2013)</ref>, is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering <ref type="bibr" target="#b7">(Fader et al., 2014a;</ref><ref type="bibr" target="#b21">McCann et al., 2018)</ref>, summarization <ref type="bibr" target="#b30">(Rush et al., 2015)</ref>, data augmentation <ref type="bibr">(Yu et al., 2018)</ref> and adversarial learning <ref type="bibr" target="#b15">(Iyyer et al., 2018)</ref>. However, not all paraphrases are equally useful. For most real-world applications, paraphrases which are too similar to the original sentence are of limited value, while those with high linguistic diversity, i.e. with large syntactic/lexical differences between the paraphrase and the original sentence, are more beneficial to the robustness and accuracy of automatic text evaluation and classification, and can avoid the blandness caused by repetitive patterns <ref type="bibr" target="#b26">(Qian et al., 2019)</ref>. The quality of paraphrases is often evaluated using three dimensions, where high quality paraphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity <ref type="bibr" target="#b22">(McCarthy et al., 2009)</ref>.</p><p>Generating high quality paraphrases can be challenging (for both humans and automatic models) since it is increasingly difficult to preserve meaning with increasing linguistic diversity. Indeed, when examining the quality of paraphrases among paraphrase generation datasets, one can find a wide range of paraphrase qualities, where the area of high quality is often very sparse (see Figure <ref type="figure" target="#fig_0">1</ref>). This in turn results in scarcity of supervised data for high-quality paraphrase generation.</p><p>A recent approach aiming to produce high quality paraphrases is controlled paraphrase generation, which exposes control mechanisms that can be manipulated to produce diversity. While the controlled generation approaches have yielded impressive results, they require providing the model with very specific information regarding the target sentence, such as its parse tree <ref type="bibr" target="#b15">(Iyyer et al., 2018)</ref> or the list of keywords it needs to contain <ref type="bibr">(Zeng et al., 2019)</ref>. However, for most downstream applications, the important property of the paraphrase is its overall quality, rather than its specific syntactic or lexical form. The over-specificity of existing controlbased methods not only complicates their usage and limits their scalability, but also hinders their coverage. Thus, it would be desirable to develop a paraphrase generation model, which uses a simple mechanism for directly controlling paraphrase quality, while avoiding unnecessary complications associated with fine-grained controls.</p><p>In this paper we propose QCPG, a Quality Controlled Paraphrase Generation model, that given an input sentence and quality constraints, represented by a three dimensional vector of semantic similarity, and syntactic and lexical distances, produces a target sentence that conforms to the quality constraints.</p><p>Our constraints are much simpler than previously suggested ones, such as parse trees or keyword lists, and leave the model the freedom to choose how to attain the desired quality levels.</p><p>Enabling the direct control of the three quality dimensions, allows flexibility with respect to the specific requirements of the task at hand, and opens a range of generation possibilities: paraphrases of various flavors (e.g. syntactically vs. lexically diverse), quasi-paraphrases (with lower semantic similarity), and even non-paraphrases which may be useful for downstream tasks (e.g. hard negative examples of sentences that are linguistically similar but have different meanings <ref type="bibr" target="#b11">(Guo et al., 2018;</ref><ref type="bibr" target="#b29">Reimers and Gurevych, 2020)</ref>).</p><p>Our results show that the QCPG model indeed enables controlling paraphrase quality along the three quality dimensions.</p><p>Furthermore, even though the training data is of mixed quality, and exhibits scarcity in the high quality area (see Figure <ref type="figure" target="#fig_0">1</ref>), our model is able to learn high quality paraphrasing behavior, i.e. it increases the linguistic diversity of the generated paraphrases without decreasing the semantic simi-larity compared to the uncontrolled baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section we provide a general description of our approach. We first explain how the different quality dimensions are measured. We then describe the controlled paraphrase generation model, QCPG, and finally we suggest a method that given the task requirements, detects the input control values which maximize the quality of the generated paraphrases. Figure <ref type="figure" target="#fig_1">2</ref> summarizes our proposed solution for generating controlled paraphrases, which is detailed in the rest of the section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quantifying Paraphrase Quality</head><p>The most common dimensions for measuring paraphrase quality are the semantic, syntactic and lexical dimensions. Several previous works used also a fluency evaluation metric <ref type="bibr">(Siddique et al., 2020)</ref>. However, since our focus is on the supervised setting, we rely on the gold paraphrases as fluency guidance for the model <ref type="bibr">(Mc-Carthy et al., 2009)</ref>. Thus, given a sentence s and a paraphrase s ′ , we define the paraphrase quality as a three dimensional vector q(s, s ′ ) = (q sem (s, s ′ ), q syn (s, s ′ ), q lex (s, s ′ )), where q sem is a measure of semantic similarity, and q syn and q lex are measures of syntactic and lexical variation, respectively. For the syntactic score, inspired by <ref type="bibr" target="#b15">Iyyer et al. (2018)</ref> we choose q syn (s, s ′ ) to be the normalized tree edit distance <ref type="bibr">(Zhang and Shasha, 1989)</ref> between the third level constituency parsetrees of s and s ′ , after removing the tokens -to increase the decoupling from the lexical distance metric. We define the lexical score q lex (s, s ′ ) to be the normalized character-level minimal edit distance between the bag of words. This measure is independent of word order, and hence increases the decoupling from syntactic measures. Additionally, calculating the token distances on the character level enables to capture tokens that share the same stem/lemma. Character-level distance is also more robust to typos that may be found in noisy data. As for the semantic score, several strong metrics have been recently proposed for measuring semantic similarity between sentences. In order to select q sem (s, s ′ ), we studied the agreement between the candidate metrics and human judgments, using only development data, and found Bleurt <ref type="bibr" target="#b31">(Sellam et al., 2020)</ref> to have the highest correlation with human judgments (see Appendix A). Thus, we define q sem (s, s ′ ) to be the Bleurt score, normalized using the sigmoid function to ensure a uniform range of values, [0, 1], for all three quality dimensions. For ease of presentation all metrics are presented on a 0 − 100 scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The QCPG Model</head><p>The main component of our solution is a quality controlled paraphrase generation model (QCPG), which is an encoder-decoder model trained on the task of controlled paraphrase generation. Given an input sentence s and a control vector c = (c sem , c syn , c lex ), the goal of QCPG is to generate an output paraphrase QCP G(s, c) that conforms to c. We train QCPG using the training set pairs (s, t), by setting c to be q(s, t), and maximizing P (t|s, c = q(s, t)) over the training set via the autoregressive cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Control Values Selection</head><p>A major challenge in the research of controlled paraphrase generation, is selecting appropriate input control values that can be achieved by the model <ref type="bibr" target="#b10">(Goyal and Durrett, 2020)</ref>. Clearly, given a sentence, not all paraphrase qualities are achievable. Some sentences are more amenable to paraphrasing than others. For example, named entities and numbers are much harder to be replaced while keeping sentence meaning, and hence, the potential lexical diversity of paraphrases involving such terms is relatively limited. Forcing QCPG to conform to quality control values that are too high with respect to the input sentence, may lead to suboptimal quality of the resultant paraphrases. Thus, for a more effective use of QCPG, the control values should be determined with respect to the input sentence.</p><p>Below we describe the second part of our solution, namely a method that given a sentence, predicts the input control values, c(s), that optimize the expected quality of the paraphrases generated by QCPG. For simplicity we assume that the quality distribution p(q|s) of all paraphrases of sentence s, is approximately normally distributed around a sentence dependent mean q 0 (s), and that the variance is approximately sentenceindependent. We further assume that given an input sentence s, the difficulty to generate a paraphrase of a given quality, q, is dominated by p(q|s) rather than by the quality vector q itself. Following our assumptions, the level of difficulty can be expressed by the offset, o = (o sem , o syn , o lex ) of q from q 0 (s). Thus, the input control, c(s), for QCPG, is the sum of q 0 (s) and an offset o.</p><p>Our aim is to analyze the model results for varying levels of difficulty, namely under different offsets, o, from q 0 (s).</p><p>The Quality Predictor (QP): Since q 0 (s) is unknown, we introduce QP, a regressor whose output, termed the reference of s, r(s) = (r sem (s), r syn (s), r lex (s)), approximates q 0 (s).</p><p>During training, QP aims to predict q(s, t) given s, where (s, t) are the input-output pairs of the training data.</p><p>To summarize, we define sentence-aware quality control by decomposing the QCPG input control, c, into a sum of a sentence dependent reference point, r(s), and a sentence independent offset, o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Implementation Details 3.1 Datasets</head><p>To test the ability of our model to learn high quality behavior from mixed quality data we use weakly annotated datasets. These datasets are large but noisy, and contain only a relatively small amount of high quality paraphrases.</p><p>MSCOCO: This dataset consists of 123K images, where each image contains at most five human-labeled captions <ref type="bibr" target="#b18">(Lin et al., 2014)</ref>. Similar to previous works we consider different captions of the same image as paraphrases.</p><p>WikiAnswers (WikiAns for short): The WikiAnswers corpus contains clusters of questions tagged by wiki-answers.com users as similar. There are 30, 370, 994 clusters with 25 question in each on average. In total, the corpus contains over 70 million question pairs <ref type="bibr" target="#b8">(Fader et al., 2014b)</ref>.</p><p>ParaBank2.0: A dataset containing clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering <ref type="bibr" target="#b14">(Hu et al., 2019)</ref>. The dataset is composed of avarage of 5 paraphrases in every cluster and close to 100 million pairs in total.</p><p>To get comparable results across all datasets, we randomly sub-sampled ParaBank2.0 and WikiAns to the same size as MSCOCO, and split them to train, dev and test sets, of sizes 900K, 14K and 14K respectively. We carefully made sure that there are no pairs from the same cluster in different splits of the data. The full data splits will be published with our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs. Full details as well as train and dev results can be found in Appendix C.1.</p><p>QCPG: We use the pre-trained T5-base <ref type="bibr" target="#b27">(Raffel et al., 2020)</ref> as the encoder-decoder model. The control input vector to QCPG is quantized at every dimension into 20 equally spaced values ranging from 0 to 100. Each value is assigned to a special saved-token. The three tokens corresponding to the quantized values of the control vector c, are concatenated to the head of the input sentence, and together used as input to the model. r(s) and o are also quantized in a similar way.</p><p>QP: An Electra base model <ref type="bibr" target="#b4">(Clark et al., 2020)</ref> finetuned with MSE loss to predict the typical quality values (see Section 2.3).</p><p>Baseline Model (BL): A T5-base model finetuned on the training data.</p><p>For all the models, we adopt the experimental setup used in <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, i.e. we train the model with several learning rates and choose the one that achieves the highest dev set performance (see appendix C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Controlling the Quality Dimensions</head><p>The aim of the following analysis is to study the level of control achieved by QCPG. To this end, we measure the model response to changes in the input offsets. We compute the expected difference in paraphrase quality, as a result of applying an input offset o compared to zero offset as a reference. More formally, we define the 3-dimensional responsiveness vector of QCPG at an offset o, Specifically, in the following analysis we are interested in studying the model response to each of the dimensions separately, i.e. how changing the input offset along a given quality dimension dim -the controlled dimension -while keeping the two other dimensions constant, affects the responsiveness in each of the three dimensions. A good control mechanism would imply that increasing the input offset in one dimension will result in a monotonically increasing responsiveness in that dimension, with relatively small responsiveness in the other two dimensions.</p><formula xml:id="formula_0">R(o) as Q(o) − Q((0, 0, 0)), where Q(o)</formula><p>Figure <ref type="figure">3</ref> shows, for each of the three datasets, the responsiveness in the three quality dimensions, when changing the input offset along each of the three dimensions, while fixing the input offsets in the other two dimensions at 0. Examining the actual values of quality in the paraphrases of the dev sets, reveals that the standard deviation is different in each dimension. Hence, for clarity of presentation, we present the input offset values and the responsiveness in units of standard deviation as measured in the respective dimension and dev set.</p><p>For the range of offsets displayed in Figure <ref type="figure">3</ref>, the responsiveness in the controlled dimension increases monotonically with the input offsets across all datasets and dimensions. As expected, the responsiveness in the uncontrolled dimensions does not zeros due to the inherent coupling between the dimensions. For example, many changes that increase syntactic diversity, also increase lexical diversity (e.g. a move from passive to active voice). Still, our control mechanism is able to increase the responsiveness in the controlled dimension with relative low responsiveness in the uncontrolled dimensions. Specifically, focusing on the relation between semantic similarity and expression diversity, the figure shows that there is a minor decrease in semantic similarity in response to an increase in lexical and syntactic diversity. In the next section, we will show that this does not prevent our model from generating paraphrases that are not only more lexically and syntactically diverse, but also more semantically similar to the source sentences, compared to the paraphrases generated by the uncontrolled baseline.</p><p>Figure <ref type="figure">3</ref> focused on small to moderate input offsets, i.e. offsets up to 2 stds from the reference point. However, as we speculated before, with increasing offsets, i.e. the more the requested control value deviates from the typical value, it becomes increasingly difficult to generate a paraphrase that conforms to the requested control value. Figure <ref type="figure">4</ref> depicts the responsiveness in the syntactic and lexical dimensions for a larger range of offset values. For the semantic dimension, the typical values are too high to allow large positive offsets, which for most sentences result in exceeding the upper limit of the semantic score. Indeed, as can be seen in Figure <ref type="figure">4</ref>, when moving to high offset values, the responsiveness in the syntactic and lexical dimensions starts to decrease. This behavior is in line with our aforementioned hypothesis, and reflects the detrimental effect of feeding QCPG with input control values that are too far from the typical paraphrase qualities of the input sentence. The nonmonotonic behavior of the responsiveness implies that the input offsets should be selected carefully in order to optimize the quality of the resultant paraphrases. In Section 4.2 we suggest a method for identifying these optimal offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Selecting Optimal Input Control Values</head><p>In this section, we suggest a method that given task requirements, selects the input offsets that are expected to yield the desired quality of paraphrases. The idea is to compute the estimated expected quality, Q(o), for each input offset o, using the dev set as described in Section 4.1, and then search the 3D grid of input offsets to find the point for which Q(o) is best suited for the user's requirements. We envision this analysis as a preliminary step in which the user chooses the input control parameters that best achieve his desired paraphrasing operation point, and then uses the chosen values at inference -which is why we use the dev set.</p><p>We study the behavior of Q(o) as a function of the 3D grid of offset points in the relevant range, i.e every o where o sem , o syn and o lex in 0, 5, 10...50. The QCPG results are compared to two reference points, which are invariant to o and are marked on the colorbars with black squares: 'Dataset' is the semantic-similarity/linguistic-diversity average value over the corresponding dev set paraphrases, and 'Baseline' is the average semanticsimilarity/linguistic-diversity of the uncontrolled baseline over the corresponding dev set. Notice that the average diversity level achieved by the uncontrolled baseline is lower than that of the dev set mean, reflecting the difficulty of this model to generate diverse paraphrases. QCPG on the other hand, with suitable input offset values, is able to generate paraphrases which are on average higher than the baseline both in their linguistic diversity and in their semantic similarity, and in fact even higher in many cases than the values of the ground truth paraphrases in the dev-set.</p><p>In general, the estimates of the expected quality achieved by QCPG at different input offsets, enable a user to generate paraphrases at different operation points, by manipulating the input offset control o to meet her desired quality values. Con-   sider for example a typical use case, of aiming to maximize linguistic diversity under a constraint on semantic similarity. An example of such a case is an operation point, denoted by QCP G ⋆ , which aims to exemplify the advantage of QCPG over the baseline, by maximizing linguistic diversity under the constraint that the semantic similarity is at least 5 points higher than the baseline. The input offset values to obtain this operation point depend on the dataset, and can be found using heatmaps such as in Figure <ref type="figure">5</ref>. For WikiAns the input offset for the QCP G ⋆ operation point values are (50, 35, 5) (entry marked by the black square).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quality Evaluation on the Test Set</head><p>In the previous section we saw, using estimates based on the dev sets, that there are many operation points which generate paraphrases with higher quality than those achieved by the uncontrolled baseline. We now turn to evaluate one such operation point, namely QCP G ⋆ , using the source sentences of the test sets which were not used in the selection of the input offset values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation</head><p>We use four quality measures to evaluate different aspects of generated paraphrases. The three quality measures used in the control of QCPG (Section 2.1) and Self-BLEU <ref type="bibr">(Zhu et al., 2018)</ref> as adapted in <ref type="bibr" target="#b17">Li et al. (2019)</ref>; <ref type="bibr" target="#b19">Liu et al. (2020a)</ref>, which aims to measure the linguistic diversity in the generated paraphrases by penalizing copying from input sentences. As can be seen in Table <ref type="table">1</ref>, QCP G ⋆ outperforms the baseline in all metrics across all datasets, as predicted using the dev-set heatmaps. A clear advantage is obtained even for Self-BLEU, which was not part of the metrics used as input controls. Importantly, the quality of the paraphrases generated by our model is comparable to, or at times better than the quality of the paraphrases in the ground truth of the datasets. Examples of paraphrases generated by QCP G ⋆ compared to the ground truth paraphrases appear in Table <ref type="table">10</ref>. This is an important step towards the goal of obtaining paraphrases in the sparse area of high quality (recall the top right corner of Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Additionally, we examined QCPG from another perspective: the effect of the quality guidance on the model's ability to predict the ground truth paraphrases. Tables <ref type="table" target="#tab_7">5 and 6</ref> show the BLEU scores <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> obtained by QCPG and the uncontrolled baseline respectively. The results verify that the input quality vectors induced by the target sentences are effectively utilized by QCPG to achieve better prediction performance.</p><p>Human Evaluation While linguistic diversity can be automatically measured by reliable metrics such as Self-BLEU, measuring semantic similarity is more challenging. We therefore rely on automatic metrics for evaluating the lexical and syntactic diversity, but use human annotation for validating the semantic evaluation. To this end, we selected a sample of 50 source sentences from each test set, and generated one paraphrase using the uncontrolled baseline and one using QCP G ⋆ . The Table <ref type="table">1</ref>: Automatic evaluation of the QCPG model on the test set. The semantic similarity (q sem ), syntactic diversity (q syn ) and lexical diversity (q lex ), are measured using Bleurt, Tree edit distance, and character-level edit distance respectively, as described in Section 2. Self-BLUE is an external measure of linguistic diversity (see text for details). BL: uncontrolled baseline. Gold: the test set ground truth paraphrases. QCP G ⋆ is the QCPG model in the operation point defined in Section 4.2. Best performance amongst the compared models is highlighted in bold. Best results amongst the models and the gold labels are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Votes</head><p>Agreement annotators were shown the source sentence, along with the two generated paraphrases (randomly ordered), and were asked which of the two better preserves the semantic meaning of the source sentence (ties are also allowed). In total, 150 triplets were evaluated by 5 judges. Table <ref type="table" target="#tab_0">2</ref> demonstrates an advantage for QCP G ⋆ in all datasets, with a large margin in MSCOCO and WikiAns. This advantage is statistically significant (p − value &lt; 0.05) as obtained by applying the Wilcoxon signed-rank test to the difference between the number of annotators that voted for QCP G ⋆ and those voted for the baseline, across all datasets. Thus, the human evaluation is in line with the results of the automatic semantic similarity measure. We also verified, that the results of this sample, in terms of linguistic diversity, are very similar to those shown in Table <ref type="table">1</ref>.</p><formula xml:id="formula_1">QCP G ⋆ BL (</formula><p>For examples of paraphrases generated by QCP G ⋆ see Table <ref type="table">10</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases. These works can be divided into supervised and unsupervised approaches.</p><p>Supervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search <ref type="bibr">(Vijayakumar et al., 2018)</ref>. Other works generate multiple outputs by perturbing latent representations <ref type="bibr" target="#b12">(Gupta et al., 2018;</ref><ref type="bibr" target="#b25">Park et al., 2019)</ref>. or by using distinct generators <ref type="bibr" target="#b26">(Qian et al., 2019)</ref>. These methods achieve some diversity, but do not control generation in an interpretable manner.</p><p>The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically <ref type="bibr">(Zeng et al., 2019;</ref><ref type="bibr">Thompson and Post, 2020)</ref> or syntactically <ref type="bibr">(Chen et al., 2019;</ref><ref type="bibr" target="#b10">Goyal and Durrett, 2020)</ref> diverse paraphrases. One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase <ref type="bibr">(Chen et al., 2019;</ref><ref type="bibr" target="#b2">Bao et al., 2019;</ref><ref type="bibr" target="#b13">Hosking and Lapata, 2021</ref>). An alternative is to directly employ constituency tree as the syntax guidance <ref type="bibr" target="#b15">(Iyyer et al., 2018;</ref><ref type="bibr" target="#b16">Li and Choi, 2020)</ref>. <ref type="bibr" target="#b10">Goyal and Durrett (2020)</ref> promote syntactic diversity by conditioning over possible syntactic rearrangements of the input. Zeng et al. ( <ref type="formula">2019</ref>) use keywords as lexical guidance for the generation process. Here we introduce a simple model for jointly controlling the lexical, syntactic and semantic aspects of the generated paraphrases.</p><p>Unsupervised Approaches <ref type="bibr" target="#b23">Niu et al. (2020)</ref> rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens. <ref type="bibr" target="#b20">Liu et al. (2020b)</ref> optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point. <ref type="bibr" target="#b9">Garg et al. (2021)</ref> and Siddique et al. ( <ref type="formula">2020</ref>) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency. In this work, we employ similar metrics for guiding the generation of paraphrases within the supervised framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we propose a novel controlled paraphrase generation model, that leverages measures of paraphrase quality for encouraging the generation of paraphrases with desired quality. We demonstrate the high level of control achieved by the model, and suggest a method for coping with the challenging problem of finding suitable control values.</p><p>Aside from offering a simple and effective way for controlling models' output quality, the quality control paradigm enables a holistic view of the data, the training process and the final model analysis. Namely: (I) Examination of the training data through the lens of data quality enables to characterize the data at hand, its strengths and limitations. (II) A quality-aware training process can be viewed as multi-task learning, where each quality level is a separate task with its own accurate supervision, as opposed to the standard quality-agnostic approach, where low quality data is in fact used as a poor supervision for a model which aims at generating higher quality output. (III) Analyzing the model behavior under different quality controls, allows finer understanding of the different model behaviors and the trade-offs between their output qualities. Better understanding the expected output quality of neural NLG models, for different input quality controls, can increase the trust in their output.</p><p>Finally, our model analysis consistently shows that although the models generally follow the quality requirements, there is still room for improvement. A possible direction for future research is exploring methods, such as reinforcement learning, for further improving the ability of the model to satisfy the quality requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Selecting the semantic similarity measure</head><p>Recently, several strong metrics have been proposed for measuring semantic similarity between sentences <ref type="bibr" target="#b28">(Reimers and Gurevych, 2019;</ref><ref type="bibr">?;</ref><ref type="bibr" target="#b31">Sellam et al., 2020)</ref>. In order to select the semantic similarity metric for QCPG, we performed a small experiment over the three dev sets, with the aim of measuring the agreement of the candidate metrics with human judgments. To this end, we leveraged two properties that characterize weakly labeled datasets, the underlying clusters of sentences, and the high variability of semantic similarity. Given a dataset, we randomly selected 100 clusters, and picked three sentences at random from each cluster. For each triplet of sentences t = (t 1 , t 2 , t 3 ) we asked 5 human annotators to choose which of the two sentences, t 2 or t 3 , better preserves the semantic meaning of t 1 . In order to find the candidate similarity measure with the highest agreement with human judgments, we first computed, for each triplet, the difference between the number of annotators voted for t 2 and those voted for t 3 . We then computed for each candidate measure, the difference between the similarity of t 2 to t 1 and and of t 3 to t 1 . We then measured Kendall's Tau correlation <ref type="bibr" target="#b5">(Daniel, 1990)</ref> between the difference vector of the human judgments and that of the judgments of each of the candidate measures. Table <ref type="table" target="#tab_2">3</ref> shows the resultant correlations. The highest correlations are obtained for SBERT <ref type="bibr" target="#b28">(Reimers and Gurevych, 2019)</ref>, but since it was trained on WikiAns and MSCOCO, we could not use it in our study. We selected Bleurt due to its highest correlation with human judgments over the three datasets (among the methods that were not exposed to the considered datasets). We normalize Bleurt score using the sigmoid function to ensure a uniform range of values, [0, 1], for the three quality dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Correlation of semantic similarity measures with linguistic diversity</head><p>We study the coupling between the different semantic similarity measures and the linguistic diversity.</p><p>We assume that the level of coupling of a good similarity measure will resemble that of humans, and will be less sensitive to lexical and syntactic properties of the paraphrase.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Models Details and Training Results</head><p>The learning rates for the QCPG and the Baseline models were selected in the following way. For a given dataset, we finetuned the models with 4 learning rates (1e-3, 1e-4, 5e-3, 5e-4) (The training results of the baseline presented in Table <ref type="table" target="#tab_5">5</ref> and the results of QCPG presented in Table <ref type="table" target="#tab_7">6</ref>.). For the baseline we selected the one which yielded the best BLEU score <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> on the corresponding dev set The best learning rate for every dataset was chosen based on the Dev set BLEU score. For the QCPG we chose the model that best conforms to the control input as measured by the MSE between the input control vector and the output quality vector (see Table <ref type="table" target="#tab_10">9</ref>). The QP model is an Electra-Base model finetuned with 4 different learning rates (1.5e-4, 1e-4, 3e-5, 5e-5).</p><p>We choose the learning rate the yields the minimal MSE on the dev set (For full results see Table <ref type="table" target="#tab_9">8</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Full Heatmaps</head><p>The full heatmaps can be found in Figure <ref type="figure" target="#fig_7">6</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Density of paraphrases in WikiAnswers as a function of the semantic similarity and the linguistic diversity. The marked area, which contains high quality paraphrases, is very sparse (The measures used in the figure are described in Section 2.1) .</figDesc><graphic url="image-1.png" coords="1,337.61,242.79,175.01,129.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Solution Architecture. The input to the paraphrase generation model, QCPG, is composed of two elements: a sentence s, and a three-dimensional quality vector c = (c sem , c syn , c lex ), which controls the quality of the generated paraphrase. Selecting appropriate values of c is crucial for obtaining high-quality paraphrases. The quality predictor model, QP, helps select suitable input quality vectors, by predicting the typical quality, r(s), of the paraphrases of s. The control vector c is the sum of r(s), and an offset vector o, which indicates the extent to which the requested quality deviates from the typical value. Dev-set results can help the user in selecting suitable values of o, as shown in Figure 5</figDesc><graphic url="image-3.png" coords="3,71.99,62.79,453.55,130.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is the expected quality of the paraphrases generated by QCPG at an offset o. We estimate Q(o) by averaging q(QCP G(s, r(s) + o)) over the input sentences s of the dev set, and denote this estimate by Q(o) = ( Qsem (o), Qsyn (o), Qlex (o)), and the corresponding estimate of R(o) by R(o).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5 depicts Q(o) for WikiAns, on a slice of the full offset grid. The results for the full grid on all datasets are shown in Figure 6. The righthand-side map depicts the estimated linguistic diversity (the average of Qsyn (o) and Qlex (o)) and the left-hand-side depicts the semantic similarity, Qsem (o)). The maps are presented for o sem = 50, and for different values of o syn and o lex . As expected, the two measures are anti-correlated, where areas with increased semantic similarity are characterized by decreased linguistic diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>#$%&amp;'!())*+'!,$!'-+!.($'/(00+1!1,2+$*,($!3 ⎯⎯ Lexical ⎯⎯ Syntactic ⎯⎯ Semantic ⎯⎯ Fixed Dimension ----Controlled Dimension</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Controllability of QCPG. The responsiveness of QCPG to changes in the input quality vector. In each graph only one dimension of the input is changed (the control dimension), where the other two dimensions are fixed at zero offset. The control dimensions in the top middle and bottom rows are the lexical syntactic and semantic dimensions respectively. Each color represents a different quality dimension of the generated paraphrases. The responsiveness in the control dimension is plotted in a dashed line. Responsiveness and offsets are shown in standard deviation units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5: Estimated Quality at different offset values for WikiAns. Average of linguistic diversity (left) and semantic similarity (right) of the paraphrases generated for the dev-set sentences, as a function of o syn and o lex , for fixed o sem = 50. The average quality of the gold-label paraphrases, and the average values achieved by the uncontrolled baseline, are marked on the color bars. Red/blue shades correspond to above/below the dev-set mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heatmaps of linugstic diversity (left column) and semantic similarty (right column) as a function of input control offsets for the datasets.</figDesc><graphic url="image-30.png" coords="14,94.67,62.78,408.23,662.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation of semantic similarity. The numbers represent the proportion of annotators that voted for each method. QCP G ⋆ : the QCPG model in the operation point defined in Section 4.2. BL: Uncontrolled Baseline.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Tie) Cohen's Kappa</cell></row><row><cell>MSCOCO</cell><cell>.56</cell><cell>.36 (.08)</cell><cell>.38</cell></row><row><cell>WikiAns</cell><cell>.48</cell><cell>.36 (.16)</cell><cell>.47</cell></row><row><cell>ParaBank2</cell><cell>.30</cell><cell>.26 (.44)</cell><cell>.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Correlation of different semantic similarity models with human evaluations.</figDesc><table><row><cell></cell><cell cols="3">MSCOCO WikiAns ParaBank2</cell></row><row><cell>SBERT</cell><cell>.52</cell><cell>.43</cell><cell>.41</cell></row><row><cell>BERTSCORE</cell><cell>.38</cell><cell>.3</cell><cell>.31</cell></row><row><cell>BLEURT</cell><cell>.45</cell><cell>.4</cell><cell>.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>presents the Kendall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Correlation of different semantic similarity models with linguistic diversity.</figDesc><table><row><cell>Dataset</cell><cell>LR</cell><cell cols="3">Dev BLEU ↑ Dev Loss ↓ Train Loss</cell></row><row><cell></cell><cell>1e-3</cell><cell>10.19</cell><cell>2.10</cell><cell>1.52</cell></row><row><cell>MSCOCO</cell><cell>1e-4 5e-3</cell><cell>10.94 0.00</cell><cell>1.89 2.23</cell><cell>1.65 2.76</cell></row><row><cell></cell><cell>5e-4</cell><cell>10.53</cell><cell>2.07</cell><cell>1.51</cell></row><row><cell></cell><cell>1e-3</cell><cell>27.28</cell><cell>1.38</cell><cell>0.65</cell></row><row><cell>ParaBank2</cell><cell>1e-4 5e-3</cell><cell>30.22 0.00</cell><cell>1.15 3.45</cell><cell>0.69 3.88</cell></row><row><cell></cell><cell>5e-4</cell><cell>28.40</cell><cell>1.37</cell><cell>0.61</cell></row><row><cell></cell><cell>1e-3</cell><cell>13.09</cell><cell>2.24</cell><cell>1.46</cell></row><row><cell>WikiAns</cell><cell>1e-4 5e-3</cell><cell>15.22 0.00</cell><cell>1.95 3.62</cell><cell>1.59 4.03</cell></row><row><cell></cell><cell>5e-4</cell><cell>13.51</cell><cell>2.17</cell><cell>1.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Training and dev set loss of the finetuned T5 baseline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Training and dev set loss of the QCPG.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Diversity Lexical Syntactic Semantic</cell></row><row><cell>MSCOCO</cell><cell>25.4</cell><cell>23.0</cell><cell>27.8</cell><cell>50.0</cell></row><row><cell>ParaBank2</cell><cell>17.7</cell><cell>18.6</cell><cell>16.8</cell><cell>77.8</cell></row><row><cell>WikiAns</cell><cell>22.8</cell><cell>20.9</cell><cell>24.7</cell><cell>46.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Automatic evaluation of the chosen finetuned T5 baseline.</figDesc><table><row><cell>Dataset</cell><cell>LR</cell><cell cols="2">Dev MSE ↓ Train MSE</cell></row><row><cell></cell><cell>1.5e-4</cell><cell>0.0242</cell><cell>0.0240</cell></row><row><cell>MSCOCO</cell><cell>1e-4 3e-5</cell><cell>0.0242 0.0206</cell><cell>0.0240 0.0161</cell></row><row><cell></cell><cell>5e-5</cell><cell>0.0205</cell><cell>0.0164</cell></row><row><cell></cell><cell>1.5e-4</cell><cell>0.0260</cell><cell>0.0239</cell></row><row><cell>ParaBank2</cell><cell>1e-4 3e-5</cell><cell>0.0248 0.0169</cell><cell>0.0239 0.0124</cell></row><row><cell></cell><cell>5e-5</cell><cell>0.0170</cell><cell>0.0126</cell></row><row><cell></cell><cell>1.5e-4</cell><cell>0.0402</cell><cell>0.0374</cell></row><row><cell>WikiAns</cell><cell>1e-4 3e-5</cell><cell>0.0404 0.0317</cell><cell>0.0374 0.0200</cell></row><row><cell></cell><cell>5e-5</cell><cell>0.0445</cell><cell>0.0372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Training results of the QP models.</figDesc><table><row><cell>Dataset</cell><cell>LR</cell><cell>MSE ↓</cell></row><row><cell></cell><cell>1e-3</cell><cell>0.0124</cell></row><row><cell>MSCOCO</cell><cell>1e-4 5e-3</cell><cell>0.0119 0.2943</cell></row><row><cell></cell><cell>5e-4</cell><cell>0.0118</cell></row><row><cell></cell><cell>1e-3</cell><cell>0.0140</cell></row><row><cell>ParaBank2</cell><cell>1e-4</cell><cell>0.0129</cell></row><row><cell></cell><cell>5e-4</cell><cell>0.0125</cell></row><row><cell></cell><cell>1e-3</cell><cell>0.0166</cell></row><row><cell>WikiAns</cell><cell>1e-4 5e-3</cell><cell>0.0153 0.3091</cell></row><row><cell></cell><cell>5e-4</cell><cell>0.0155</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>MSE between the required control and the evaluations of the outputs of the QCPG models.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO Source</head><p>Ground-truth QCP G ⋆</p><p>A table filled with assorted prepared foods in a buffet fashion.</p><p>Fresh fruits, vegetables, and other foods are spread out on the table.</p><p>A table with food on it in a buffet line.</p><p>Ornately decorated assortment of vases displayed on shelf.</p><p>A display of pottery in a glass case</p><p>A decorated shelf with vases on display Group of people seated at a long What are the three meninges that cover the brain and spinal cord?</p><p>The three memebranous coverings that protect the brain and spinal cord?</p><p>What three meninges cover the brain and spinal cord?</p><p>We're having trouble with Roger. I've got issues on Roger.</p><p>We have a problem with Roger.</p><p>Everything on schedule. All on schedule.</p><p>All in the plan.</p><p>The internet no longer maked the distance matter: the world may indeed be our classroom.</p><p>Because of the Internet, distance doesn't matter anymore: the world may indeed be an our classroom.</p><p>The Internet doesn't matter: the world could be our school.</p><p>Article 2 deals with the scope of application of a directive extending cooperation between Member States to include taxes of whatever type.</p><p>Article 2 concerns an area which is covered by a Directive which broadens cooperation among Member States so as that it covers taxes of any kind.</p><p>Article 2 concerns the scope of the directive extending the cooperation between the Member States to include taxation of any kind.</p><p>You're free to move forward. You're free to move on. You can go on.</p><p>Table <ref type="table">10</ref>: Paraphrases generated by QCP G ⋆ compared to ground-truth paraphrases.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">MSCOCO WikiAns</title>
				<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">q sem ↑ q syn ↑ q lex ↑ Self-BLEU↓ q sem ↑ q syn ↑ q lex ↑ Self-BLEU↓ q sem ↑ q syn ↑ q lex ↑ Self-BLEU↓ Gold</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from disentangled syntactic and semantic spaces</title>
		<author>
			<persName><forename type="first">References</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6008" to="6019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Controllable paraphrase generation with a syntactic exemplar</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<editor>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</editor>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013. 2019</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="5972" to="5984" />
		</imprint>
	</monogr>
	<note>Squibs: What is a paraphrase? Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Applied nonparametric statistics pws</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014a</date>
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Open Question Answering Over Curated and Extracted Knowledge Bases</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised contextual paraphrase generation using lexical control and reinforcement learning</title>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Srinivasaraghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural syntactic preordering for controlled paraphrase generation</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective parallel corpus mining using bilingual sentence embeddings</title>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6317</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep generative framework for paraphrase generation</title>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prawaan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorising meaning and form for intent-preserving paraphrasing</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale, diverse, paraphrastic bitexts via sampling and clustering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Holzenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1875" to="1885" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering</title>
		<author>
			<persName><forename type="first">Changmao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.505</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5709" to="5714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decomposable neural paraphrase generation</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3403" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Zitnick</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised paraphrasing by simulated annealing</title>
		<author>
			<persName><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.28</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised paraphrasing by simulated annealing</title>
		<author>
			<persName><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.28</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The components of paraphrase evaluations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebekah</forename><forename type="middle">H</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guess</surname></persName>
		</author>
		<author>
			<persName><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="682" to="690" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised paraphrase generation via dynamic blocking</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Nitish Shirish Keskar, and Caiming Xiong</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Paraphrase diversification using counterfactual debiasing</title>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6883" to="6891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring diverse expressions for paraphrase generation</title>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1313</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
