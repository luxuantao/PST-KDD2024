<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Diversity and Source Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jérôme</forename><surname>Bobin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yassir</forename><surname>Moudden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jean-Luc</forename><surname>Starck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
						</author>
						<title level="a" type="main">Morphological Diversity and Source Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B877EA633063296A6520357B6BED0855</idno>
					<idno type="DOI">10.1109/LSP.2006.873141</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Blind source separation</term>
					<term>morphological component analysis (MCA)</term>
					<term>sparse representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This letter describes a new method for blind source separation, adapted to the case of sources having different morphologies. We show that such morphological diversity leads to a new and very efficient separation method, even in the presence of noise. The algorithm, coined multichannel morphological component analysis (MMCA), is an extension of the morphological component analysis (MCA) method. The latter takes advantage of the sparse representation of structured data in large overcomplete dictionaries to separate features in the data based on their morphology. MCA has been shown to be an efficient technique in such problems as separating an image into texture and piecewise smooth parts or for inpainting applications. The proposed extension, MMCA, extends the above for multichannel data, achieving a better source separation in those circumstances. Furthermore, the new algorithm can efficiently achieve good separation in a noisy context where standard independent component analysis methods fail. The efficiency of the proposed scheme is confirmed in numerical experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A COMMON assumption in signal or image processing is that measurements made typically using an array of sensors often consist of mixtures of contributions from various possibly independent underlying physical processes . The simplest mixture model is linear and instantaneous and takes the form , where and are random matrices of respective sizes and , and is an matrix. Multiplying by linearly mixes the sources into observed processes. Thus, the rows of , are the sources, and the rows of , are the mixture weights. An random matrix is included to account for instrumental noise or model imperfections. The problem is to invert the mixing process so as to separate the data back into its constitutive elementary building blocks.</p><p>In the blind approach (where both the mixing matrix and the sources are unknown), and assuming minimal prior knowledge on the mixing process, source separation is merely about devising quantitative measures of diversity or contrast. Classical independent component analysis (ICA) methods assume that the mixed sources are statistically independent; these techniques Manuscript received December 5, 2005; revised February 2, 2006. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. James E. Fowler.</p><p>J. Bobin and Y. Moudden are with the DAPNIA-SEDI-SAP, Service d'Astrophysique, CEA/Saclay, 91191 Gif sur Yvette, France (e-mail: jbobin@cea.fr; ymoudden@cea.fr).</p><p>J.-L. Starck is with the DAPNIA-SEDI-SAP, Service d'Astrophysique, CEA/ Saclay, 91191 Gif sur Yvette, France and also with Laboratoire APC, 75231 Paris Cedex 05, France (e-mail: jstarck@cea.fr).</p><p>M. Elad is with the Computer Science Department, Technion-Israel Institute of Technology, Haifa 32000, Israel (e-mail: elad@cs.technion.ac.il).</p><p>Digital Object Identifier 10.1109/LSP.2006.873141</p><p>(for example, JADE, FastICA, and Infomax) have proven to be successful in a wide range of applications (see <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> and references therein). Indeed, although statistical independence is a strong assumption, it is, in many cases, physically plausible. An especially important case is when the mixed sources are highly sparse, meaning that each source is rarely active and mostly (nearly) zero. The independence assumption in such a case implies that the probability for two sources to be significant simultaneously is extremely low, so that the sources may be treated as having nearly disjoint supports. This is exploited, for instance, in sparse component analysis <ref type="bibr" target="#b4">[5]</ref>. Indeed, it has been already shown in <ref type="bibr" target="#b5">[6]</ref> that first moving the data into a representation in which the sources are assumed to be sparse greatly enhances the quality of the separation. Possible representation dictionaries include the Fourier and related bases, wavelet bases, and more. Working with combinations of several bases or with very redundant dictionaries such as the undecimated wavelet frames or the more recent ridgelets and curvelets <ref type="bibr" target="#b6">[7]</ref> could lead to even more efficient representations. However, finding the smallest subset of elements (that linearly combine to reproduce a given signal or image) is a hard combinatorial problem. Nevertheless, several pursuit algorithms have been proposed that can help build very sparse decompositions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In fact, a number of recent results prove that these algorithms will recover the unique optimal decomposition, provided that this solution is sparse enough and the dictionary is sufficiently incoherent <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>In another context, the morphological component analysis (MCA) described in <ref type="bibr" target="#b11">[12]</ref> uses the idea of sparse representation for the separation of sources from a single mixture. MCA constructs a sparse representation of a signal or an image considering that it is a combination of features that are sparsely represented by different dictionaries. For instance, images commonly combine contours and textures: the former are well accounted for using curvelets, while the latter may be well represented using local cosine functions. In searching a sparse decomposition of a signal or image , it is assumed that is a sum of components, , where each can be described as with an over-complete dictionary and a sparse representation . It is further assumed that for any given component, the sparsest decomposition over the proper dictionary yields a highly sparse description, while its decomposition over the other dictionaries, , is highly non-sparse. Thus, the different can be seen as discriminating between the different components of the initial signal. Ideally, the are the solutions of subject to (1)</p><p>However, as the norm is nonconvex, optimizing the above criterion is combinatorial by nature. Substituting the -norm by an , as motivated by recent equivalence results <ref type="bibr" target="#b9">[10]</ref>, and relaxing the equality constraint, the MCA algorithm seeks a solution to with (2) A detailed description of MCA is given in <ref type="bibr" target="#b11">[12]</ref> along with results of experiments in contour/texture separation in images and inpainting. Note that there is no mixing matrix to be estimated in the MCA model, and the mixture weights are absorbed by the source signals .</p><p>The purpose of this contribution is to extend the MCA to the case of multichannel data, as described in the next section. In handling several mixtures together, the mixing matrix becomes an unknown as well, which adds some complexity to the overall problem. On the other hand, having more than one mixture is expected to help the separation, leading to better performance compared to regular MCA. Section III illustrates the performance of MMCA and demonstrates its superiority over both MCA and several ICA techniques. We should note that our method could also be considered as an extension of the algorithm described in <ref type="bibr" target="#b5">[6]</ref>, with two major differences: 1) while <ref type="bibr" target="#b5">[6]</ref> uses a single transform to sparsify the data, our technique assumes the use of different dictionaries for different sources, and 2) the numerical scheme that we lead to in the construction of the algorithm is entirely different. Interestingly, a similar philosophy has been employed by <ref type="bibr" target="#b12">[13]</ref> for audiophonic signals.</p><p>Their method assumes that an audio signal is mainly made of a "tonal" part (sparse in a discrete cosine dictionary), a transient part (well sparsified by a wavelet transform), and a residual. However, their decomposition algorithm is not based on an iterative scheme, which is a major difference with MMCA. Indeed, experiments show that such an iterative process is needed when the considered transforms are far from being incoherent (for instance, DCT and curvelet transform).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MULTICHANNEL MCA</head><p>We consider the mixing model ( <ref type="formula">1</ref>) and make the additional assumption that each source is well (i.e., sparsely) represented by a specific and different dictionary . Assigning a Laplacian prior with precision to the decomposition coefficients of the th source in dictionary is a practical way to implement this property. Here, denotes the array of the th source samples. Classically, we assume zero-mean Gaussian white noise. This leads to the following joint estimator of the source processes and the mixing matrix :</p><p>(</p><formula xml:id="formula_0">)<label>3</label></formula><p>where trace is the Frobenius norm. In the above formulation, we define , implying that the transform is applied in an analysis mode of operation, very much like in the MCA <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, this minimization problem suffers from a lack of scale invariance of the objective function: scaling the mixing matrix by , and an inverse scaling of the source matrix, , leaves the quadratic measure of fit unchanged while deeply altering the sparsity term. This problem can be alleviated by forcing the mixing matrix to have normalized columns , implying that each of the source signals is scaled by a scalar. Practically, this can be achieved by normalizing these columns at each iteration and propagating the scale factor to the corresponding source by . We propose solving (3) by breaking into rank-1 terms, , and updating one at a time. Define the th multichannel residual as corresponding to the part of the data unexplained by the other couples . Then, minimizing the objective function with respect to assuming is fixed as well as all and leads to Sign <ref type="bibr" target="#b3">(4)</ref> This is a closed-form solution, known as soft-thresholding, known to be exact for the case of unitary matrices . As becomes a redundant transform, we keep this interpretation as an approximate solution and update the source signal by soft-thresholding the coefficients of the decomposition of a coarse version with a scalar threshold (see <ref type="bibr" target="#b13">[14]</ref> for more details on the justification of this step). Then, considering a fixed , the update on follows from a simple least-squares linear regression. The MMCA algorithm is given at the top of the next page.</p><p>At each iteration, coarse (i.e., smooth) versions of the sources are computed. The mixing matrix is then estimated from sources that contain the most significant parts of the original sources. The overall optimization proceeds by alternately refining both the sources and the mixing matrix. The use of a progressive thresholding scheme with a set of thresholds decreasing slowly toward enforces a certain robustness to noise. Indeed, both alternate projections and iterative thresholding define a non-trivial path for the variables to estimate (sources and mixing matrix) during the optimization. This optimization scheme leads to a good estimation as underlined in <ref type="bibr" target="#b11">[12]</ref>. MMCA benefits from the potential of overcomplete dictionaries for sparse representation. In comparison with the algorithm in <ref type="bibr" target="#b5">[6]</ref>, which uses a single sparsifying transform and a quadratic programming technique, our method considers more than just one transform and a shrinkage-based optimization.</p><p>In the case where we have only one channel and the mixing matrix is known and equal to , then we can see that  MMCA is equivalent to MCA. The next section will illustrate the efficiency of the MMCA algorithm when the sources to be separated have different morphologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment 1: One-Dimensional Toy Example</head><p>We start by illustrating the performance of MMCA with the simple BSS experiment on one-dimensional data. The two source signals at the top left of Fig. <ref type="figure" target="#fig_0">1</ref> were linearly mixed to form the three synthetic observations shown at the top right. A Gaussian noise with was also added to the mixtures (note that each channel has a unit variance). The two sources are morphologically different: one consists of four bumps and the other is a plain sine-wave. separation was conducted using the above MMCA algorithm, using the Fourier and the trivial basis as the representing dictionaries. For the sake of comparison, a publicly available implementation of the JADE algorithm was also tested. As can be seen, MMCA is clearly able to efficiently separate the original source signals. Note that denoising is an intrinsic part of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 2: Blind Separation of Images</head><p>We now turn to use MMCA to separate efficiently two-dimensional data. In Fig. <ref type="figure" target="#fig_2">2</ref>, the two left pictures are the sources. The first source image is composed of three curves, which are well represented by a curvelet transform. We use the global discrete cosine transform (DCT) to represent the second source image. Although the resulting representation may not be extremely sparse, what is significant here is that contrastingly, the representation of the first component using the global DCT is not sparse. The mixtures are shown in the second image pair. A Gaussian noise has been added to these mixtures, using different noise variances for the different channels. Finally, the two images in the last column show the MMCA source estimates. Visually, the MMCA performs well.</p><p>We compare the MMCA with two standard source separation techniques: JADE and FastICA <ref type="bibr" target="#b0">[1]</ref>. As the original JADE algorithm has not been devised to take into account additive noise, we apply denoising on its outputs (using a standard wavelet denoising technique assuming that the noise variances are known). Note that we could denoise the data before separation; however, the nonlinear wavelet denoising erases the coherence between the channels, so an ICA-based method would fail to separate the sources from the denoised data. We also compare MMCA Fig. <ref type="figure" target="#fig_2">2</ref>. Experiment 2 (using curvelet and DCT): First column: Original sources of variance 1. Second column: Their mixtures (a Gaussian noise is added: = 0:4 and 0.6 for channels 1 and 2, respectively. The mixtures are such that x = 0:5s 0 0:5s and x = 0:3s + 0:7s ). Third column: Sources estimated by MMCA. Fig. <ref type="figure">3</ref>. Experiment 2: Correlation between the true source signals and the sources estimated by JADE (dotted line), denoised JADE (dashed line), Fas-tICA (5), denoised FastICA (+), the relative Newton method (dashdot), and MMCA (solid), as a function of the noise power .</p><p>with a more recent method based on sparse representations that is described in <ref type="bibr" target="#b14">[15]</ref>. We also estimate the mixing matrix using the relative Newton method after a 2-D-wavelet transform of the mixtures. The graphs in Fig. <ref type="figure">3</ref> show the correlation between the original sources and their estimates as the data noise variance increases. One can note that both JADE and FastICA have similar performance. As the data noise variance increases, MMCA clearly achieves better source estimation and shows clear robustness compared to non-denoised ICA-based methods and to the relative Newton method. We also observed that the relative Newton method <ref type="bibr" target="#b14">[15]</ref> seems rather unstable as the noise variance increases. MMCA provides a similar behavior compared to denoised versions of the classical ICA-based algorithms.</p><p>As the noise variance increases, the mixing matrices estimated using ICA-based methods are biased, and thus, these methods fail to correctly estimate the sources. Moreover, denoising after the separation process softens the separation error. Hence, the denoised versions of JADE and FastICA seem to perform as well as MMCA. As a consequence, a more efficient criterion is needed. A natural way of assessing the separation quality is to compare the estimated and original mixing matrices. Quantitative results are shown in Fig. <ref type="figure">4</ref>, where the mixing matrix estimation error is defined as (vector norm). is the true mixing matrix, is the estimated one, and is a matrix that restores the right scaling and permutation on the estimated matrix. If (i.e., is equal to up to scaling and permutation), then ; thus, measures a deviation from the true mixture. Contrasting with standard ICA methods, MMCA iteratively estimates the mixing matrix from coarse (i.e., smooth) versions of the sources and thus is not penalized by the presence of noise. As a consequence, MMCA Fig. <ref type="figure">4</ref>. Experiment 2: Mixing matrix error (defined via ) for JADE (dotted line), FastICA (5), the relative Newton method (dashdot), and MMCA (solid), as a function of the noise power . is clearly more robust to noise than standard ICA methods, even in the case of very noisy mixtures. Indeed, it can be noticed in Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref> that when the noise variance increases, standard ICA-based methods fail, whereas MMCA still performs well. MMCA also performs better than a sparsity-based algorithm described in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 3: MMCA Versus MCA</head><p>MCA <ref type="bibr" target="#b11">[12]</ref> has been devised to extract both texture and cartoon components from a single image. We describe here an experiment where we use MMCA for a similar purpose in order to compare the two methods. Note that MCA is applied when only one mixture is provided. Let us point out that the main difference between these methods is the estimation of the mixing matrix in MMCA, which is not needed in MCA. Fig. <ref type="figure" target="#fig_3">5</ref> features two original pictures: the first one is mainly a cartoon well sparsified by a curvelet transform; the other is a texture represented well by global 2-D-DCT. Two noisy mixtures are shown in the second column. We applied MCA to the sum of the two original sources and MMCA to a random number of mixtures (between two and ten channels). The last column of Fig. <ref type="figure" target="#fig_3">5</ref> features the two sources estimated by MMCA based on ten mixtures. Quantitatively, Fig. <ref type="figure" target="#fig_4">6</ref> shows the correlation between the original sources and those estimated using MMCA as the number of mixtures increases. Clearly, the amount of in- formation provided by the multichannel data improves source estimation, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>The MCA algorithm provides a powerful and fast signal decomposition method, based on sparse and redundant representations over separate dictionaries. The MMCA algorithm described in this letter extends MCA to the multichannel case. For blind source separation, this extension is shown to perform well, provided the original sources are morphologically different, meaning that the sources are sparsely represented in different bases. We also demonstrated that MMCA performs better than standard ICA-based source separation in a noisy context. We are currently working on improvements and generalizations of MMCA where each source can be modeled as a linear combination of morphologically different components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Experiment 1: Top left: Two initial source signals. Top right: Three noisy observed mixtures. Bottom left: Two source signals reconstructed using MMCA. Bottom right: Two source signals reconstructed with JADE.</figDesc><graphic coords="2,45.42,66.38,239.00,172.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Set number of iterations L max and thresholds 8k, k = Lmax 1 k =2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>While k &gt; k =2, For k = 1; . . . ; ns:• Renormalize a k , s k , and k • Update s k assuming all s k 6 =k and a k k .• Update a k assuming all s k and a k 6 =k are fixed:-a k = (1=ks k k 22 )X k s T k Lower the thresholds: k = k 0 k =2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Experiment 3 (using curvelet and DCT). First column: Original sources. They have been normalized to unit variance. Second column: Mixtures of the initial sources. A Gaussian noise of variance = 0:3 was added to each channel. Third column: Sources estimated by MMCA from ten mixtures.</figDesc><graphic coords="4,54.36,270.80,221.00,142.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Experiment 3: Correlation between the true sources and the MMCA estimates as the number of mixtures increases. Left: Cartoon component. Right: Texture component. Note that the results for one mixture correspond to MCA.</figDesc><graphic coords="4,302.64,66.86,251.00,91.00" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<ptr target="http://www.cis.hut.fi/projects/ica/book/,481+xxiipages" />
		<title level="m">Independent Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A blind source separation technique based on second order statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belouchrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Meraim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="434" to="444" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind separation of instantaneous mixtures of non stationary sources</title>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1837" to="1848" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-detector multi-component spectral matching and applications for CMB data analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Delabrouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patanchon</surname></persName>
		</author>
		<ptr target="http://arXiv.org/abs/astro-ph/0211504" />
	</analytic>
	<monogr>
		<title level="j">Monthly Notices R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse component analysis and blind source separation of underdetermined mixtures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="992" to="996" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blind source separation by sparse decomposition in a signal dictionary</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zibulevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural-Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="863" to="882" />
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The curvelet transform for image denoising</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="2002-06">Jun. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximal sparsity representation via l minimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="2197" to="2202" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse representations in unions of bases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3320" to="3325" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Redundant multiscale transforms and their application for morphological component analysis</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Imaging Electron Phys</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="287" to="348" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hybrid representations for audiophonic signal encoding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process. (Special Issue on Image and Video Coding Beyond Standards)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1595" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why simple shrinkage is still relevant for redundant representations?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind source separation with relative newton method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zibulevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICA2003</title>
		<meeting>ICA2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="897" to="902" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
