<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="laboratory">Yue Gao is with Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Biomedical Research Imaging Center</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Tat-Seng Chua is with School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D260879DFFEF027DA32A8F73BDA5D8F</idno>
					<idno type="DOI">10.1109/TMM.2014.2323014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2014.2323014, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploiting Web Images for Semantic Video Indexing via Robust Sample-specific Loss Yang Yang, Zheng-Jun Zha, Member, IEEE, Yue Gao, Senior Member, IEEE, Xiaofeng Zhu and Tat-Seng Chua</p><p>Abstract-Semantic video indexing, also known as video annotation or video concept detection in literatures, has been attracting significant attention in recent years. Due to deficiency of labeled training videos, most of the existing approaches can hardly achieve satisfactory performance. In this paper, we propose a novel semantic video indexing approach, which exploits the abundant user-tagged Web images to help learn robust semantic video indexing classifiers. The following two major challenges are well studied: (a) noisy Web images with imprecise and/or incomplete tags; and (b) domain difference between images and videos. Specifically, we first apply a nonparametric approach to estimate the probabilities of images being correctly tagged as confidence scores. We then develop a robust transfer video indexing (RTVI) model to learn reliable classifiers from a limited number of training videos together with the abundance of user-tagged images. The RTVI model is equipped with a novel sample-specific robust loss function, which employs the confidence score of a Web image as prior knowledge to suppress the influence and control the contribution of this image in the learning process. Meanwhile, the RTVI model discovers an optimal kernel space, in which the mismatch between images and videos is minimized for tackling the domain difference problem. Besides, we devise an iterative algorithm to effectively optimize the proposed RTVI model and a theoretical analysis on the convergence of the proposed algorithm is provided as well. Extensive experiments on various real-world multimedia collections demonstrate the effectiveness of the proposed robust semantic video indexing approach.</p><p>Index Terms-Semantic video indexing, transfer learning, robust</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years we have witnessed an explosive growth of video data driven by the wide availability of massive storage devices, video cameras, fast networks and media sharing sites (e.g., Youtube 1 ). There is a compelling need for effective and Fig. <ref type="figure">1</ref>. A Flickr "flower" image which is tagged with "football" inaccurately. efficient retrieval of video content. Despite the progress of multimedia content analysis, many commercial search engines, such as Google<ref type="foot" target="#foot_0">2</ref> and Microsoft Bing <ref type="foot" target="#foot_1">3</ref> , still rely heavily on the textual metadata (e.g., titles, descriptions and tags) associated with videos. However, the problem is that the textual metadata are usually noisy, incomplete, and inconsistent with video content.</p><p>Recent research moves beyond text-based search and utilizes a set of semantic concepts as intermediate semantic descriptors to facilitate video search <ref type="bibr" target="#b0">[1]</ref>. The task of predicting the presence of semantic concepts in video clips is known as semantic video indexing, video annotation, or video concept detection in the literatures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Most of the existing semantic video indexing approaches are developed based on machine learning techniques, where a large number of training samples are often required in order to achieve reasonable performance due to the complexity of video content. A major difficulty is that manual labelling of video data is extremely time-consuming and labor-extensive. For example, it has shown that labelling 1 hour of video with 100 concepts costs around 8 to 15 hours <ref type="bibr" target="#b4">[5]</ref>.</p><p>Recently, transfer learning techniques <ref type="bibr" target="#b5">[6]</ref> have been widely explored in order to alleviate the influence of deficiency of labeled training data. On the one hand, several transfer learning based video indexing approaches acquire auxiliary labeled training data from other video domains to assist in learning the classifiers in the target domain <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. However, most of these approaches focus on considering the same media types, i.e., videos, where the issue of scarcity of training samples still exists due to inevitable manual labelling efforts.</p><p>On the other hand, an abundance of user-tagged videos are available on the Web and easy to access. Web video data has the following characteristics: 1) lower quality, in order to achieve better accessability, Web videos are often compressed in smaller sizes and/or lower resolutions; 2) less reliable tags, compared to tagging a piece of photo, tagging a whole video may be much more labor-intensive and time-consuming due to the complexity of video content, which will lead users to be impatient and unmotivated to provide high-quality tags; and 3) coarse-grained tags, in most video sharing sites, user-provided tags are normally attached to the whole video rather than individual video shots and/or video frames. In the application of keyframe-level semantic video indexing, these unique properties may probably cause content information loss and make the task of collecting appropriate training samples quite difficult. Another potentially valuable resource is the abundance of user-tagged Web image data, which can also help to learn reliable semantic video indexing classifiers. In this case, one may ask: is it possible to exploit such "free" images to help improve the performance of semantic video indexing? We argue that it is non-trivial to involve these images in the learning process because of two challenges: (a) Noisy samples: many user-contributed tags or texts are imprecise <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and in Flickr, there are only around 50% of tags actually related to the images <ref type="bibr" target="#b10">[11]</ref>. In particular, around 85% of Web images are not accurately tagged <ref type="bibr" target="#b11">[12]</ref>. For instance, as illustrated in Fig. <ref type="figure">1</ref>, a "flower" image from Flickr is tagged with "football" erroneously. If we involve such noisy images in the process of learning a "football" classifier, the performance will be severely degraded; and (b) Domain difference: images and videos are usually of different qualities and/or come from different domains, which will easily lead to great difference between their distributions.</p><p>Inspired by the above observations and analysis, in this paper, we propose a novel semantic video indexing approach which extends our previous work <ref type="bibr" target="#b12">[13]</ref>. The proposed semantic video indexing approach learns robust video classifiers with a limited number of labeled video samples together with an abundance of user-tagged Web images. The proposed approach not only tackles the domain difference between images and videos but also exhibits a high resilience to the noise in the Web images. Specifically, we first apply a non-parametric approach to preliminarily alleviate the influence of image noise by estimating the relevance between the visual content of Web images and the associated tags. The relevance are further transformed to confidence scores, i.e., the probabilities of Web images being correctly tagged. Then, we filter out the images with low confidence scores to attain a set of relatively clean images. Furthermore, we devise a novel sample-specific robust loss function based on the ℓ p loss, which expands the applicable range and flexibility of the ℓ 1 loss function. The confidence score of a Web image is utilized as prior knowledge to control the contribution of this image in the learning process. Moreover, we propose a new robust transfer video indexing (RTVI) model for learning the classifiers. The RTVI model jointly handles the influence of image noise and the domain difference between images and videos. Compared to our previous work <ref type="bibr" target="#b12">[13]</ref> using the ℓ 1 loss, the proposed RTVI model employs the confidence scores of the Web images to generalize the ℓ p loss, which results in more adaptive robustness as well as more applicable range and flexibility for handling different levels of noise in Web images.</p><p>The reminder of this paper is organized as follows. In Section II, we briefly review the related works, including transfer learning in semantic multimedia indexing and tag refinement for noisy Web data. We elaborate the proposed robust semantic video indexing approach in Section III, including the non-parametric noise filtering approach and the robust transfer video indexing model. Section IV reports the experimental results and analysis on various real-world multimedia collections. The conclusion is given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review the application of transfer learning techniques related to semantic multimedia indexing, and discuss several of the existing tag refinement strategies for noisy Web data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transfer Learning in Semantic Multimedia Indexing</head><p>Transfer learning has been extensively applied in multimedia fields to address the deficiency of labeled training data. The common underlying principle of transfer learning is how to bridge a "transportation tunnel", through which the knowledge in source domain can be effectively transferred to target domain for learning tasks. In this part, we roughly divide the previous work into two categories: homogeneous transfer <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b6">[7]</ref> and heterogeneous transfer <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>In homogeneous transfer, the source and target data are of the same data types and/or the same representation. Several efforts have been made towards extending the traditional models, e.g., SVM, with domain adaption ability. Duan et al. <ref type="bibr" target="#b16">[17]</ref> proposed to add the maximum mean discrepancy criterion into SVM for minimizing the mismatch of the source and the target domains. In <ref type="bibr" target="#b7">[8]</ref>, Yang et al. adapted the SVM classifier learned from the source domain with a "delta function" learned from both the source and target data. Similarly, Duan et al. <ref type="bibr" target="#b6">[7]</ref> extended both of the previous works and proposed to learn the adaptive "delta function" as well as reduce the domain mismatch with the maximum mean discrepancy. Jiang et al. <ref type="bibr" target="#b13">[14]</ref> proposed to assign the source data with different weights by measuring the distances to the target domain. Then, the SVM classifier was trained with these weighted patterns. In <ref type="bibr" target="#b17">[18]</ref>, a simple yet effective strategy was used to augment the feature spaces of both the source and target domains for achieving the domain adaption. In <ref type="bibr" target="#b18">[19]</ref>, the domainshift problem was handled using graph diffusion technique. Different from the traditional way of treating data points as nodes, they constructed a semantic graph in which nodes were comprised of semantic concepts. Raina et al. <ref type="bibr" target="#b19">[20]</ref> proposed a unsupervised self-taught learning based on sparse coding to perform image classification task. It first uses dictionary learning to discover a set of basic vectors from unlabeled source images. Then sparse coding was applied to learn a new feature for target data based on these basic vectors.</p><p>With the explosive growth of heterogeneous data on the Web, the recent research focus has been gradually turned to heterogeneous transfer learning, i.e., exploiting heterogeneous data types for learning in the target domain, such as facilitating video indexing with images <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and using text documents to help improve image classification <ref type="bibr" target="#b20">[21]</ref>. Jiang et al. <ref type="bibr" target="#b14">[15]</ref> employed Flickr images with tags to calculate a new semantic metric to facilitate semantic video indexing. They ignored content information and only considered tag co-occurrence statistical information, and domain difference problem was not well handled. <ref type="bibr">Yang et al.</ref> [?] integrated semisupervised learning and transfer learning techniques to exploit manually-labeled images for video tagging. Although these methods studied the domain difference, none of them takes the noisy sample problem into account. In <ref type="bibr" target="#b15">[16]</ref>, the authors considered the noisy sample problem and used Web images to help video classification, but the problem of domain difference between images and videos was not well handled in their work. In <ref type="bibr" target="#b20">[21]</ref>, Zhu et al. proposed a heterogeneous transfer learning scheme, which leverages abundant Web text documents to help boost semantic image indexing. They projected text documents and images into a common latent semantic space, in which images can be represented better.</p><p>Compared to the existing homogeneous transfer learning approaches, our proposed approach can effectively transfer the abundant and reliable heterogeneous knowledge from image domain to video domain for handling the domain difference in a non-parametric way and learn the robust video classifiers in a joint way. On the other hand, while most of the existing heterogeneous transfer learning approaches make use of welllabeled data from heterogeneous domains, which still costs huge amount of labelling effort and may not be affordable, our proposed approach can directly exploit the abundant freely-available user-tagged Web images as well as effectively handle the influence of the noise for better learning the video classifiers. In addition, different from <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b6">[7]</ref> which also utilize maximum mean discrepancy to handle to the domain difference but lack the robustness to noisy data, our approach can adaptively suppress the potential noise via the robust sample-specific loss function which incorporate the confidence scores of the Web images as prior knowledge into the ℓ p loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tag Refinement for Web Multimedia Data</head><p>Tag refinement refers to the process of improving the quality of the unreliable tags in Web multimedia data, such as deleting inaccurate tags and/or providing missing tags. It is the crucial process for guaranteeing the quality of the training data as well as the performance of the subsequent model learning. In literature, most of the existing tag refinement approaches are closely related to the outlier detection problem <ref type="bibr" target="#b21">[22]</ref> and tag relevance estimation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>Zhu et al. <ref type="bibr" target="#b23">[24]</ref> predicted the relevance of an image to a target tag by utilizing the knowledge from WordNet corpus and the statistical information from Flickr. They considered only the images' textual information, which may be irrelevant to image content and degrade the estimation accuracy. Li et al. <ref type="bibr" target="#b22">[23]</ref> assumed that the tag relevance can be inferred from the visual neighbors' tagging behaviors. Based on this assumption, they proposed a learning-based tag relevance estimation approach which accumulates votes from visually similar neighbors of the images. Liu et al. <ref type="bibr" target="#b10">[11]</ref> applied a probabilistic density estimation approach to calculate the tag relevance of an image to a given tag based on the images associated with this tag. Then, the relevance scores are further refined using a random walk model over a tag similarity graph. In <ref type="bibr" target="#b24">[25]</ref>, a system for jointly selecting relevant positive samples and relevant negative samples is developed. A compressing ensemble approach for SVM is then used to accelerate the classification process for the tag relevance estimation. Xu et al. <ref type="bibr" target="#b25">[26]</ref> explored the tag relevance by extending latent dirichlet allocation (LDA) model with a regularizer which incorporates visual information. Li et al. <ref type="bibr" target="#b11">[12]</ref> proposed an incremental approach to iteratively identify the relevant training data from the noisy pool of the Web and update the object models with the newly added data. In <ref type="bibr" target="#b26">[27]</ref>, Zhu et al. simultaneously explored low-rank properties, tag consistency and content consistency to identify a refined tag matrix for Web images. In <ref type="bibr" target="#b27">[28]</ref>, Tang et al. proposed to build a robust graph and applied semi-supervised learning technique to learn a tag ranking model to achieve tag refinement.</p><p>Under the circumstance of the user-tagged Web images, it may become much more difficult to preliminarily obtain the information of the data distribution and derive the proper assumption. In our approach, we apply a non-parametric method, i.e., kernel density estimation, to calculate the tag relevance. Compared to most of the existing methods that either rely on parametric methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> or require proper assumption <ref type="bibr" target="#b22">[23]</ref>, our approach provides a more flexible and applicable solution to handle the uncertain characteristics of Web images. In addition, different from <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> which are mainly built based on visual content, our proposed noisy sample removal approach not only considers the visual information of the images, but also explores the images' associated textual information as contextual clue for more effectively identifying noisy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ROBUST SEMANTIC VIDEO INDEXING FRAMEWORK</head><p>In this section, we elaborate the proposed robust semantic video indexing framework. We first introduce the problem formulation and framework overview. Then, we depict the main components of the framework, including the non-parametric noise filtering approach, the robust transfer video indexing model and the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation and Framework Overview</head><p>Given a concept, let Z = {(z i , y i )}| N V i=1 denote a limited number of training samples, i.e., video keyframes. z i ∈ R d represents the visual feature vector of the i th video keyframe and y i ∈ {-1, 1} is the corresponding label. As aforementioned, it is difficult to learn a reliable classifier from the limited number of training videos, and the abundant usertagged Web images should be used as auxiliary training samples. Let X = {(x j , t j )}| NI j=1 denote a set of Web images, in which some images are tagged with the given concept while   the rest are not. x j ∈ R d represents the visual feature vector of the j th image and t j ∈ {-1, 1} indicates whether the image is tagged with the concept. The objective is to develop an effective semantic video indexing approach, which can learn a robust classifier from the limited training videos Z together with abundant user-tagged Web images X .</p><formula xml:id="formula_0">K K K m ª º ª º ª º « » « » « » « » « » « » « » « » « » ¬ ¼ ¬ ¼ ¬ ¼ ª º ª º ª º 1 0 5 1 0 1 1 0 9 1 0 5 1 0 1 1 0 . . . . . . . . . « » « » « » ª º ª º ª º ª º ª º ª º « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » « » 0 5 1 0 1 1 0 9 1 0 5 1 0 1 1 0 9 « » « » « » « » « » « » « » « » 0 5 1 0 1 1 0 9 1 0 5 1 0 1 1 0 9 0 5 1 0 1 1 0 9 0 5 1 0 1 1 0 9 « » « » « » « » « » « » « » « » « » ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) K K K ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ¬ ¼ ¬ ¼ « » « » « » « » 0 5 1 0 . . . . . . ¬ ¼ ¬ ¼ ¬ ¼ « » « » « » « » « » « » 0 5 1 0 1 1 0 9 1 0 5 1 0 1 1 0 9 0 5 1 0 1 1 0 9 0 5 1 0 1 1 0 9 . . . . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Transfer Video Indexing Model</head><p>Fig. <ref type="figure" target="#fig_2">2</ref> illustrates the flowchart of the proposed robust transfer semantic video indexing framework. Given a set of concepts, we first collect the abundant Flickr images tagged with each concept. Then, we exploit a non-parametric approach to estimate relevance between all the Web images and each concept. The relevance are further transformed to confidence scores which indicate the probabilities of the Web images being correctly tagged with each concept. We filter out those Web images with low confidence scores to obtain a set of relatively clean images. Finally, the cleansed Web images and a limited number of labeled video samples are together considered as training samples and fed into the proposed robust transfer video indexing model to learn the robust semantic video indexing classifiers. In the online prediction, the learned classifiers are used to predict whether a given test video should be assigned with the concepts or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-cleansing Web Images</head><p>As aforementioned, a certain number of Web images in X contain inaccurate, incomplete and/or inconsistent tags. In order to utilize X to help facilitate semantic video indexing, it is required to alleviate the influence of the noisy samples and pre-select a set of useful training samples from X . In this subsection, we apply a non-parametric noise filtering strategy which automatically filters out potential noise from X based on confidence scores approximating the probabilities of Web images being correctly tagged. Compared to most of the existing methods that rely on parametric methods and require proper assumption on data distribution, our approach provides a more flexible and applicable solution to handle the uncertain characteristics of Web images.</p><p>Given a user-tagged Web image (x, t), where t ∈ {-1, 1} indicates if x is associated with a particular concept c. Let η(x, t|c) be a random variable indicating the probability that t is a correct tag for x. In order to attain a reasonable estimation of η(x, t|c), we first calculate the relevance between x and c. Based on a commonly-used assumption that the label of an image can be estimated from its visually similar neighbors <ref type="bibr" target="#b22">[23]</ref>, we adopt the classic Kernel Density Estimation (KDE) to compute the relevance rel(x, c) <ref type="bibr" target="#b10">[11]</ref> as:</p><formula xml:id="formula_1">rel(x, c) = 1 |X c | ∑ xj ∈ Xc κ(x, x j ),<label>(1)</label></formula><p>where X c is the set of images that are tagged with c and | • | is the cardinality of a set. κ(•, •) is a kernel function that measures the similarity between two images. Note that rel(x, c) is the relevance of an image and a given concept rather than the probability of x being correctly tagged with c. In order to transform the relevance to confidence score, we should also consider x's original tag t. Therefore, we estimate the confidence score η(x, t|c) from relevance rel(x, c) by considering x's original tag t:</p><formula xml:id="formula_2">η(x, t|c) = { rel(x, c), if t = +1. 1 -rel(x, c), if t = -1.</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>For brevity, hereafter we consistently use η i in short for η(x i , t i |c). Before feeding Web image set X and the corresponding confidences {η i }| NI i=1 into the subsequent robust semantic video indexing model, we need to alleviate the influence of noisy samples in X . To this end, we filter out images with low confidence scores. Given a threshold τ , we eliminate images with confidence scores of less than τ from X . Thus we obtain a set of relatively clean images, denoted as X = {(x i , t i )}| ÑI i=1 , where ÑI is the number of images remaining in X and ÑI &lt; N I . The corresponding confidence scores are represented as η = {η i }| ÑI i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robust Transfer Video Indexing</head><p>In this part, we propose a novel semantic video indexing model, termed Robust Transfer Video Indexing (RTVI), which simultaneously exploits a limited number of training video samples together with the abundant relatively clean Web images for learning the video classifiers. The proposed RTVI model exploits multiple kernel learning (MKL) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> and maximum mean discrepancy (MMD) <ref type="bibr" target="#b30">[31]</ref> to find the "optimal" kernel space, in which we jointly achieve two major goals, i.e., handling the domain difference and suppressing the Web image noise, for learning the robust video classifiers. Moreover, by using the MKL strategy, the intrinsic correlation of the above two targets are naturally explored to reinforce each other.</p><p>Next, we elaborate the details of the RTVI model, including a) the mechanism which integrates images and videos to handle the domain difference problem and b) the robust approach for handling image noise. Then, we present an iterative algorithm for effectively optimizing the RTVI model, as well as a mathematical analysis on the convergence of the algorithm.</p><p>1) Integrating Image and Video: As aforementioned, the domain difference exists between the Web images in X and the videos in Z. In order to tackle such mismatch, we intend to minimize certain "distance" of images and videos. While most of the existing distance estimation criteria (e.g., Kullback-Leibler divergence) either are parametric or require intermediate density estimation <ref type="bibr" target="#b31">[32]</ref>, the maximum mean discrepancy (MMD) criterion <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> provides a non-parametric choice for measuring the distance between distributions of two data sets in a Reproducing Kernel Hilbert Space (RKHS) H. Therefore, the MMD criterion is more suitable and flexible for handling different types of data with unpredictable distributions. The MMD distance is defined as:</p><formula xml:id="formula_4">mmd( X , Z) = 1 ÑI ÑI ∑ i=1 ϕ(x i ) - 1 N V NV ∑ j=1 ϕ(z j ) H ,<label>(3)</label></formula><p>where ϕ : R d → H is a kernel mapping function and</p><formula xml:id="formula_5">∥ • ∥ H is the ℓ 2 -norm in H. By defining a column vector s = [1/ ÑI ,. . ., 1/ ÑI ÑI ,-1/N V ,. . . ,-1/N V NV</formula><p>] T , we can rewrite the above MMD criterion as:</p><formula xml:id="formula_6">mmd( X , Z) = Tr(KS) 1/2 , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where S = ss T and K =</p><formula xml:id="formula_8">[ K xx K xz K zx K zz ]</formula><p>is a compound kernel matrix with K xx and K zz corresponding to the kernel matrices over images and videos, respectively. K xz = K T zx is the kernel matrix between images and videos. Tr(•) is the trace of a matrix.</p><p>In order to handle the domain difference issue, we intend to discover an optimal kernel space in which the distribution difference of X and Z is minimal. Inspired by the success in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we use Multiple Kernel Learning <ref type="bibr" target="#b28">[29]</ref> to find the optimized linear combination of a set of base kernels, denoted as {K (k) }| m k=1 . The multiple kernel Maximum Mean Discrepancy (MK MMD) criterion is written as:</p><formula xml:id="formula_9">mkmmd( X , Z) = Tr(( m ∑ k=1 u k K (k) )S) 1/2 = (u T p) 1/2 , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where p = [Tr(K (1) S),Tr(K (2) S),. . . ,Tr(K (m) S)] T ; and u = [u 1 ,u 2 , . . ., u m ] T is the combination coefficients of multiple kernels. In our work, we consider u ∈ ∆, where ∆ = {u|u k ≥ 0, k = 1, 2, . . . , m, u T 1 = 1} is a simplex.</p><p>2) Learning Robust Video Indexing Classifier: In this part, we aim to learn the reliable video classifiers by further addressing the problem of image noise. In order to substantially exploring the intrinsic correlation between handling the domain difference and learning the classifiers, we also employ the MKL strategy. Specifically, we learn the following classifier f : R d → {-1, 1}:</p><formula xml:id="formula_11">f (z) = κ u (z, :)v + b, (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where</p><formula xml:id="formula_13">κ u (z, :) = [κ u (z, x 1 ), . . . , κ u (z, x ÑI ), κ u (z, z 1 ), . . . , κ u (z, z NV )] and κ u (•, •) = ∑ k u k ϕ T k (•)ϕ k (•), k = 1, 2, . . . , m. {ϕ k }| m</formula><p>k=1 are the kernel mapping functions corresponding the m base kernels. v ∈ R N ×1 is the classification parameter and b ∈ R is the bias. N = ÑI +N V .</p><p>Although to some extent we may cleanse certain noisy images in the preprocessing step, there are still potentially unreliable ones remained. In this case, in order to learn a reliable video indexing classifier, we expect that the classifier learning model should be robust to the noisy samples in X . Also, we also expect the loss function should be adaptively reflect the noise level in accordance with certain prior knowledge of the noise. Recall that the confidence scores can indicate the probabilities that the images in X are correctly tagged, and thus can be used as prior knowledge to weigh the contribution of these images. In our previous work <ref type="bibr" target="#b12">[13]</ref>, we propose the following loss function, which employs the idea of sparse coding <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> to achieve the robustness:</p><formula xml:id="formula_14">L( X , Z; η|f ) = ÑI ∑ i=1 η i |t i -f (x i )|+ NV ∑ j=1 |y j -f (z j )|,<label>(7)</label></formula><p>where the two terms penalize the errors of X and Z, respectively. It has been shown that the ℓ 1 loss is robust to data noise <ref type="bibr" target="#b36">[37]</ref>. In fact, by using the ℓ 1 loss instead of the squared loss, the errors from those noisy samples will not be emphasized. In order to enhance the generalized ability of the ℓ 1 loss, the ℓ p loss <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> has been proposed. The traditional methods using ℓ p loss usually fix p as the same empirical constant for all the samples (e.g., p = 1 or 2). In order to further suppress the influence of the potentially noise as well as expand the applicable range and flexible control to adapt to different levels of Web image noise, we propose a novel sample-specific robust loss function, which integrates the confidence score of a Web image as prior knowledge into the ℓ p loss for adaptively quantizing the contribution of more (less) reliable images with more (less) weights.</p><p>For conciseness, hereafter we unify the notations of images and videos and the sample-specific loss function is defined as:</p><formula xml:id="formula_15">L( X , Z; η|f ) = N ∑ i=1 η i |ỹ i -f (x i )| pi , (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where ỹ = [t 1 ,. . ., t ÑI , y 1 ,. . ., y NV ] T and x ÑI +j = z j , j = 1, . . . , N V . Since the video labels are manually provided by experts, we set their confidence scores to 1, i.e., η i = 1, i = ÑI + 1, . . . , N . Similar to η i , p i ∈ (0, 2] should also reflect the noisy levels of the i-th training sample. In this case, we may set p i = 2η i to make p i positively relevant to η i . This indicates that if the sample is more reliable then it should contribute more in the learning process. Note that when we set p i = 1 for all the training samples, we conclude that our previous work <ref type="bibr" target="#b12">[13]</ref> is a special case of the RTVI model. By integrating the sample-specific robust loss function and the multiple kernel Maximum Mean Discrepancy criterion, we have the following problem:</p><formula xml:id="formula_17">min v,b,u∈∆ α(u T pp T u)+ N ∑ i=1 η i |ỹ i -f (x i )| pi +λΩ(u, v), (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>where α and λ are the balanced parameters. Ω(u, v) is the regularization term. In this work, we choose the ridge regularization, i.e., Ω(u,</p><formula xml:id="formula_19">v) = v T ( ∑ k u k K (k) )v.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>The problem in Eq. ( <ref type="formula" target="#formula_17">9</ref>) is difficult to solve because it is non-convex w.r.t. all the variables jointly, and the non-smooth property of the weighted loss function makes it non-trivial to optimize the problem as a whole. To handle the above challenges, we present an iterative algorithm to solve the above problem effectively. Besides, we provide a mathematical analysis on the convergence of the proposed algorithm.</p><p>For brevity, we rewrite Eq. ( <ref type="formula" target="#formula_17">9</ref>) as below:</p><formula xml:id="formula_20">min v,b,u∈∆ α(u T pp T u) + N ∑ i=1 |R i | pi + λΩ(u, v), (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>where</p><formula xml:id="formula_22">R i = ηi (ỹ i -f (x i )) and ηi = η 1/pi i , i = 1, 2, .</formula><p>. . ,N . In order to solve the above problem, we first introduce an alternative optimization problem as below. Further, we show that by iteratively solving the alternative problem, we can equivalently optimize the original problem in Eq. <ref type="bibr" target="#b8">(9)</ref>.</p><formula xml:id="formula_23">min v,b,u∈∆ α(u T pp T u) + tr(R T DR) + λΩ(u, v), (<label>11</label></formula><formula xml:id="formula_24">)</formula><p>where</p><formula xml:id="formula_25">R = [R 1 , R 2 , . . . ,R N ] T = ỹ • η - ∑ k u k K(k) v + ηb; K(k) = K (k) • (η1 T ); tr(•)</formula><p>is the trace of a matrix; • denotes Hadamard product; and D ∈ R N ×N is a diagonal matrix with its i-th diagonal element D ii defined as</p><formula xml:id="formula_26">D ii = p i 2|R i | 2-pi . (<label>12</label></formula><formula xml:id="formula_27">)</formula><p>Note that D is actually dependant on v, b and u, which drives us to develop an algorithm for alternatingly updating v, b, u and D. Suppose D is fixed, then we show the alternating process of solving the problem in Eq. <ref type="bibr" target="#b10">(11)</ref>.</p><p>1) Updating v and b when fixing u: When u is fixed, we obtain the following sub-problem:</p><formula xml:id="formula_28">min v,b tr(R T DR)+λ(v T K u v), (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>where K u = ∑ k u k K (k) . By setting the derivative of Eq. ( <ref type="formula" target="#formula_28">13</ref>) w.r.t. b to 0, we arrive at:</p><formula xml:id="formula_30">b = (η T D(ỹ • η)-ηT D Ku v)/η T Dη, (<label>14</label></formula><formula xml:id="formula_31">)</formula><p>where Ku = ∑ k u k K(k) . Then, by substituting b into Eq. ( <ref type="formula" target="#formula_28">13</ref>) and setting the derivative w.r.t. v to zero, we have</p><formula xml:id="formula_32">v = ( KT u H T DH Ku + λK u ) -1 KT u H T DH(ỹ • η),<label>(15)</label></formula><p>where</p><formula xml:id="formula_33">H = I n -(η ηT D/η T Dη).</formula><p>2) Updating u when fixing v and b: With v and b fixed, the problem in Eq. ( <ref type="formula" target="#formula_23">11</ref>) can be simplified as the following QP problem, which can be efficiently solved by the existing optimization toolbox:</p><formula xml:id="formula_34">min u∈∆ u T Au + u T d. (<label>16</label></formula><formula xml:id="formula_35">)</formula><p>The calculation of A and d is listed in Appendix A.</p><p>Overall, the optimization process of the problem in Eq. ( <ref type="formula" target="#formula_20">10</ref>) is illustrated in Algorithm 1.</p><p>Algorithm 1 An alternating algorithm for solving the problem in Eq. ( <ref type="formula" target="#formula_17">9</ref>). Input: Image data set X , video data set Z and confidence scores η; Output: v, b, u;</p><p>1: Generate m basic kernels {K (1) ,K (2) ,. . .,K (m) } over both X and Z; Update the diagonal matrix D according to Eq. ( <ref type="formula" target="#formula_26">12</ref>); 5:</p><p>Compute v according to Eq. ( <ref type="formula" target="#formula_32">15</ref>); 6:</p><p>Compute b according to Eq. ( <ref type="formula" target="#formula_30">14</ref>); Compute u by solving the QP problem in Eq. ( <ref type="formula" target="#formula_34">16</ref>); 8: until convergence 9: return v, b, u;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Convergence Analysis</head><p>In this part, we present a mathematical analysis on the convergence of Algorithm 1. We prove that by alternatively solving the optimization problem as summarized in Eq. <ref type="bibr" target="#b10">(11)</ref> and updating the auxiliary variable D, we are able to achieve the goal of solving the problem in Eq. <ref type="bibr" target="#b9">(10)</ref>. To this end, we first present two lemmas.</p><p>Lemma 1: Let R i be the updated residual of the i-th sample, i.e., R i = ηi (ỹ i -f (x i )) in previous iteration and Ri be the residual of the i th sample w.r.t. v, b and u in the current iteration, then the following inequality holds:</p><formula xml:id="formula_36">| Ri | pi - p i | Ri | 2 2|R i | 2-pi ≤ |R i | pi - p i |R i | 2 2|R i | 2-pi .</formula><p>The proof of Lemma 1 is shown in Appendix B.</p><formula xml:id="formula_37">Lemma 2: Given R = [R 1 , R 2 , . . . , R N ] T</formula><p>, where R i is the residual of the i-th sample, we have the following conclusion:</p><formula xml:id="formula_38">N ∑ i=1 | Ri | pi - N ∑ i=1 p i | Ri | 2 2 |R i | 2-pi ≤ N ∑ i=1 |R i | pi - N ∑ i=1 p i |R i | pi 2 |R i | 2-pi .</formula><p>Proof: According to Lemma 1, we can easily obtain the conclusion of Lemma 2 by summing up the inequalities over all the N samples.</p><p>Thereom 1: At each iteration (line 4-7) of Algorithm 1, the value of the objective function in Eq. ( <ref type="formula" target="#formula_17">9</ref>) monotonically decreases.</p><p>Proof: For brevity, we denote Θ(v, u) = α(u T pp T u) + λ(v T K u v). Suppose {v, b, û} is the solution to the optimization problem in Eq. ( <ref type="formula" target="#formula_23">11</ref>) and R is the residual vector of all the samples, then we have</p><formula xml:id="formula_39">RT D R + Θ(v, û) ≤ R T DR + Θ(v, u) ⇒ N ∑ i=1 p i | Ri | 2 2|R i | 2-pi + Θ(v, û) ≤ N ∑ i=1 p i |R i | 2 2|R i | 2-pi + Θ(v, u) ⇒ N ∑ i=1 | Ri | pi -( N ∑ i=1 | Ri | pi - N ∑ i=1 p i | Ri | 2 2|R i | 2-pi ) + Θ(v, û) ≤ N ∑ i=1 |R i | pi -( N ∑ i=1 |R i | pi - N ∑ i=1 p i |R i | 2 2|R i | 2-pi ) + Θ(v, u).</formula><p>According to Lemma 2, we obtain</p><formula xml:id="formula_40">N ∑ i=1 | Ri | pi + Θ(v, û) ≤ N ∑ i=1 |R i | pi + Θ(v, u).</formula><p>Thus, we have proved that at each iteration, the value of the objective function in Eq. ( <ref type="formula" target="#formula_17">9</ref>) monotonically decreases.</p><p>It is clear that Theorem 1 guarantees that Algorithm 1 will converge to an optima.</p><p>IV. EXPERIMENTS In this section, we evaluate the proposed robust semantic video indexing approach on various real-world multimedia data sets as compared to the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Data Sets and Settings</head><p>In our experiments, the Web images are from the NUS-WIDE <ref type="bibr" target="#b40">[41]</ref> consisting of 269, 648 Flickr images associated with 5, 018 user-provided tags, which are quite noisy. We used two video data sets, i.e., Kodak consumer video collection <ref type="bibr" target="#b41">[42]</ref> and TRECVID10 video data set <ref type="bibr" target="#b42">[43]</ref>. While the Kodak data set contains 25 concepts and 5, 166 keyframes extracted from 1, 358 consumer videos, the TRECVID10 data set comprises 130 concepts and around 200 hours of training videos as well as 200 hours of test videos.</p><p>With the image and video data sets, we constructed two cross-media data settings, i.e., NUS Kodak and NUS TRECVID10. We selected the overlapped concepts between image and video data sets. This gives rise to 18 concepts 4 for NUS Kodak; and 17 concepts for NUS TRECVID10. 4 There are 21 common concepts between NUS-WIDE and Kodak data sets. Three of them are audio-oriented tags, i.e., "cheer", "music" and "singing", and were not used in the experiments.</p><p>For image and video samples, we chose to use those annotated with at least one of the selected concepts. Thus, this gives rise to 51, 150 images and 2, 303 video keyframes for NUS Kodak. For NUS TRECVID10, we obtained 27, 304 images and 116, 236 training video keyframes. For each evaluating concept, we randomly selected 1, 000 Flickr images from NUS-WIDE data set as experimental images. For Kodak data set, we randomly divided the video keyframes into two subsets: 50% as training samples and the remaining 50% for testing. We varied the ratio of training videos in Kodak in the range of {10%, 20%, 30%, 40%, 50%}. For TRECVID10 data set, we randomly sampled 1, 000 video keyframes from each individual concept as training samples. We varied the number of training videos in TRECVID10 in the range of {200, 400, 600, 800, 1000}. We summarize the details of data sets and settings in Table <ref type="table" target="#tab_0">I</ref>.</p><p>For visual representation, we first extracted 256-D Local Binary Patterns (LBP) <ref type="bibr" target="#b43">[44]</ref>. We also extracted Scale-invariant feature transform (SIFT) feature from both images and video keyframes. The SIFT features from all the training samples are clustered by k-means to generate the visual vocabulary, based on which we generated two types of 2, 048-D bag-ofvisual-words with different encoding methods, i.e. Localityconstrained linear (LLC) encoding with max-pooling and histogram encoding with sum-pooling <ref type="bibr" target="#b44">[45]</ref>.</p><p>For each type of the visual features, we constructed base kernel matrices based on the three types of kernel functions, i.e., Gaussian kernel (i.e., k(•, To calculate the prior knowledge of confidence scores η in Eq. ( <ref type="formula" target="#formula_3">2</ref>), we used Gaussian kernel function to compute three similarities based on three types of visual features, respectively. The overall similarity between two images was calculated as the mean value of the three similarities between them. The kernel bandwidth parameter was set as mean distance over all the sample pairs. The threshold τ for cleansing noisy images was test in the range of [0, 0.25, 0.5, 0, 75]. For the proposed RTVI approach as in Eq. ( <ref type="formula" target="#formula_17">9</ref>), we set p i = 2η i for each training image and p i = 2 for each training video.</p><formula xml:id="formula_41">•) = exp (-γd 2 (•, •))), Lapla- cian kernel (i.e., k(•, •) = exp (- √ γd(•, •))),</formula><p>We compared the proposed RTVI approach to three transfer learning approaches, including (a) Adaptive Multiple Kernel Learning (AMKL) <ref type="bibr" target="#b6">[7]</ref>, (b) Adaptive SVM (ASVM) <ref type="bibr" target="#b7">[8]</ref> and (c) Feature Replication <ref type="bibr" target="#b17">[18]</ref>. We also compared the RTVI approach to its two variants, denoted as RTVI-1 and RTVI-2. In RTVI-1, we set p to a fixed value for all the training samples in Eq. ( <ref type="formula" target="#formula_17">9</ref>), i.e., the loss function is</p><formula xml:id="formula_42">η i |ỹ i -f (x i )| p ,</formula><p>where p is set in the range of {0.5, 1.0, 1.5}. In RTVI-2, we set η i = 1 for all the training images in Eq. ( <ref type="formula" target="#formula_17">9</ref>), i.e., the loss function is</p><formula xml:id="formula_43">|ỹ i -f (x i )| pi .</formula><p>For fair comparison, all the tradeoff parameters in all the evaluated methods were tuned in the range of {10 -6 ,10 -4 ,10 -2 ,10 0 ,10 2 ,10 4 ,10 6 }. We adopted the Average Precision (AP) as the performance metric, and the mean "animal", "baby", "beach", "birthday", "boat", "crowd", "dancing", "graduation", "museum", "night", "parade", "park", "picnic", "playground", "ski", "sports", "sunset" and "wedding" NUS TRECVID 27,304 (1,000 per concept for training) 116,236 (200, 400, 600, 800, 1000 per concept for training)</p><p>"animal", "bus", "cheering", "cityscape", "classroom", "dancing", "doorway", "flowers", "hand", "mountain", "nighttime", "running", "singing", "swimming", "telephones", "vehicle" and "walking"</p><p>Average Precision (mAP) over all the concepts was used as the overall performance metric. Each experiment was repeated five times and the average results were reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>1) Comparison to State-of-the-Art: Figure <ref type="figure" target="#fig_7">3</ref> illustrates the comparison between the proposed RTVI approaches and the state-of-the-art approaches. In particular, Figure <ref type="figure" target="#fig_7">3</ref> From these results, we obtain the following observations and analysis:</p><p>• The proposed RTVI approaches consistently achieve the best performance amongst all the comparing approaches on different ratios of training videos, in both the NUS Kodak and NUS TRECVID10 settings. Compared to other methods, the proposed RTVI approach not only well handles the problem of domain difference between images and videos with multiple kernel learning and maximum mean discrepancy, but also fully explores the ℓ p loss and the confidence scores of the Web images as prior knowledge for constructing the novel samplespecific loss function as well as suppressing the influence of noisy Web images. • In most cases, the approaches that use multiple kernel learning (MKL), i.e., the RTVI approaches and AMKL, can achieve better performance than those do not use MKL (i.e., ASVM and FR). Confront with the mismatch between images and videos, the MKL strategy not only assists the maximum mean discrepancy (MMD) in handling the domain difference problem, but also implicitly helps to learn an optimized feature space, in which more reliable video classifiers can be learned at the same time. • Similar to the proposed RTVI approaches, AMKL also uses multiple kernel learning (MKL) and maximum mean discrepancy (MMD) to handle the domain difference problem. However, our approaches can perform better than AMKL in all the settings. This reveals that the proposed sample-specific loss function, which substantially takes advantage of the robust property of the ℓ p loss and the prior knowledge of the Web images, indeed embodies more resilience to the noise in the Web images than the hinge loss used in the AMKL method.</p><p>• The RTVI approach can always obtain superior performance as compared to its two variant RTVI-1 and RTVI-2, which both do not fully use the prior knowledge in the loss function. With the thorough exploitation of the confidence scores as prior knowledge, the proposed sample-specific loss function in Eq. ( <ref type="formula" target="#formula_15">8</ref>) can exhibit more robustness to the noise of the Web images and help to learn more reliable video classifiers.    In order to illustrate the effects of MMD, we set α = 0 in our proposed RTVI to explicitly ignore MMD. As we can see, the performance of our RTVI approach slightly degrades when the MMD component is not incorporated. Due to the domain difference, the objects of different domains with the same semantics may have different features and/or properties. By explicitly mapping such data into the same optimized feature space with MMD, we may obtain certain consistent (implicit) representations, which can exerts positive efforts on learning the reliable classifiers. 3) Effect of Multiple Kernel Learning: Figure <ref type="figure" target="#fig_10">5</ref> shows the comparison of using multiple kernel learning (MKL) and using the average kernel (i.e., uniform weights for all the base kernels) in our approach. In particular, Figure <ref type="figure" target="#fig_10">5</ref> We can see from the results in Figure <ref type="figure" target="#fig_10">5</ref>, the performance of using MKL in our approach always exceeds that of without using MKL. We may conclude that the exploitation of multiple kernel learning strategy is able to discover an optimal kernel space in which not only the mismatch between images and videos can be minimized but also more reliable and robust video classifiers can be learnt.</p><p>4) Web Images v.s. Web Videos: In this part, we compare the capabilities of Web images and Web videos as auxiliary training data for video classifier learning. We crawled 1, 682 YouTube 1 videos which are tagged with at least one evaluated concept in the video data sets. Since there is no keyframe-level tags for the Web videos, we treat each individual video as a training data point. We sampled frames from each video clip at the rate of one frame per second. Then we extracted the 256-D LBP feature and the two 2, 048-D bag-of-visual-word feature for each frame, and the average visual features over all the frames within a video are treated as the features of the video. All other experimental settings are the same as those of the Web images. Besides, we also combine the Web video data and Web image data together for further comparison.</p><p>We report the comparison results in Figure <ref type="figure">6</ref>. The mAP performance of Web images, Web videos and their combination on both the Kodak and TRECVID10 data sets are shown in Figure <ref type="figure">6</ref>.(a) and 6.(b), respectively. Figure <ref type="figure">6</ref>.(c) and 6.(d) provides the detailed AP of each individual concept on Kodak and TRECVID10, respectively. From the results, we can see that the Web images consistently achieve better performance than the Web videos and the combined source, which indicates that the quality of Web images are more reliable than Web videos in the semantic video indexing task. Based on these experimental results and the observation that Web videos contain more unreliable tags and have lower quality, for either video classification or image classification, Web image data would be the better choice than the Web video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Capability of Pre-cleansing Web Images:</head><p>In this subsection, we evaluate the capability of our approach in tag refinement by comparing to (1) user tagging (denoted as UT), i.e., the original tags provided by the users, (2) neighbor voting (NV) and (3) a tag refinement approach based on lowrank matrix recovery <ref type="bibr" target="#b26">[27]</ref> (denoted as LR). The comparison results are shown in Figure <ref type="figure">7</ref>. As can be seen, the quality of the user tags is relatively poor, which indicates the user tagging is indeed unreliable. NV, LR and our approach are able to significantly improve the tag quality compared to the original user tags. Besides, our approach achieves the comparable results to the LR, which is one of the state-ofthe-art tag refinement approaches, in both the NUS Kodak and NUS TRECVID10 settings.</p><p>6) Parameter Sensitivity: In this part, we empirically test the parameter sensitivity in our approach. First, we evaluate the effects of τ in the noise removal. We set τ in the range of {0.0, 0.25, 0.5, 0.75} to filter out different ratios of potential noisy images. For video, we use 50% per concept of NUS Kodak and 1, 000 per concept of NUS TRECVID for training. The results on both data sets are reported in Figure <ref type="figure" target="#fig_13">8</ref>. As we can see, when τ = 0.25, we can consistently achieve the best results. This phenomenon implies that if τ is too small (e.g., τ= 0), too much potential noise will be remained in the training image data, which may severely degrade the performance; as τ becomes larger from 0.25 to 0.75, the number of the training images keeps decreasing, which directly impairs the tagging performance.</p><p>Next, we analyze the effects of both the tradeoff parameters α and λ in our approach. The joint influence of α and λ on  both the NUS Kodak and NUS TRECVID10 data sets are reported in Figure <ref type="figure" target="#fig_14">9</ref>.(a) and Figure <ref type="figure" target="#fig_14">9</ref>.(b), respectively. We observe that for both the parameters α and λ, neither large nor small value helps our approach to obtain the best performance, which reveals that the MMD component, the robust samplespecific loss and the regularizer should be well balanced.</p><p>7) Computational Cost: We empirically evaluate the computational cost of both the training and the testing processes   in our proposed RTVI approach on the two data sets. We conducted all the experiments on a Dell PC with 3.1GHz CPU and 8GB memory. Table <ref type="table" target="#tab_3">II</ref> reports the computational cost on the NUS Kodak data set; and Table <ref type="table" target="#tab_3">III</ref> shows the computational cost on the NUS TRECVID10. We can see that with the increase of the training samples, both the training process and the testing process cost more time. For testing an individual video frame, our approach can finish the tagging within 1ms, which is efficient for large-scale indexing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8) Convergence Study:</head><p>As analyzed before, Algorithm 1 is able to converge to an optimal solution. In this part, we conduct empirical study on the convergence in NUS Kodak setting, as demonstrated in Figure <ref type="figure" target="#fig_16">10</ref>. Here, we fix the tradeoff parameters α = 1 and λ = 1. As we can see, our algorithm can quickly converge within a few iterations, which clearly shows its efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a novel robust semantic video indexing framework which leverages an abundance of usertagged Web images to enhance semantic video indexing. In order to overcome the challenges imposed by domain difference between images and videos and noisy samples in Web images, we first filtered out potentially noisy images by estimating confidence scores indicating the probabilities of images being accurately tagged. Images with low confidence scores were eliminated. We then proposed a robust transfer video indexing (RTVI) model, which finds an optimal kernel space in which domain difference between images and videos is minimal, as well as leverages a robust sample-specific loss function to control the contribution of each unreliable Web image. We developed an iterative algorithm for optimize the proposed model effectively. Both theoretical and empirical analysis on the convergence of the proposed algorithm was provided. Extensive experiments on three real-world multimedia data sets showed the superiority of the proposed semantic video indexing framework as compared to the state-of-theart approaches. In future, we intend to improve the efficiency and scalability of the current solution in order to handle big multimedia data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOF OF LEMMA 1</head><p>Proof: We first consider a function g(σ) = aσ 2 -2σ a + (2 -a), <ref type="bibr" target="#b16">(17)</ref> where a ∈ (0, 2]. We expect to show that when σ &gt; 0, g(σ) ≥ 0. The first and second order derivatives of the function in Eq. ( <ref type="formula">17</ref>) are g ′ (σ) = 2aσ -2aσ a-1 and g ′′ (σ) = 2a -2a(a -1)σ a-2 , respectively. We can see that σ = 1 is the only point that satisfies g ′ (σ) = 0. Also, when 0 &lt; σ &lt; 1, g ′ (σ) &lt; 0 and when σ &gt; 1, g ′ (σ) &gt; 0. This means that g(σ) is monotonically decreasing when 0 &lt; σ &lt; 1 and monotonically increasing when σ &gt; 1. Moreover, we have g ′′ (1) = 2a(2 -a) ≥ 0. Therefore, for ∀σ &gt; 0, g(σ) ≥ g(1) = 0. Then, by substituting σ = | Ri| |Ri| and a = p i into Eq. ( <ref type="formula">17</ref>), we may obtain the conclusion in Eq. (17) as follows.</p><formula xml:id="formula_44">p i | Ri | 2 |R i | 2 -2 | Ri | pi |R i | pi + (2 -p i ) ≥ 0, ⇔p i | Ri | 2 |R i | pi-2 -2| Ri | pi + (2 -p i )|R i | pi ≥ 0, ⇔2| Ri | pi -p i | Ri | 2 |R i | pi-2 ≤ (2 -p i )|R i | pi , ⇔| Ri | pi - p i | Ri | 2 2|R i | 2-pi ≤ |R i | pi - p i |R i | pi 2|R i | 2-pi .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed robust semantic video indexing framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 :</head><label>2</label><figDesc>Randomly initialize v,b and u; 3: repeat 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Inverse Square Distance kernel (i.e., k(•, •) = 1/(γd 2 (•, •) + 1)) and Inverse Distance kernel (i.e., k(•, •) = 1/( √ γd(•, •)+1)), with γ varying as 1 d {4 -4 , 4 -3 , . . . , 4 3 , 4 4 }, where d is the mean distance over all the sample pairs. This gives rise to 108 (3 × 4 × 9) base kernel matrices in total.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.(a) and 3.(b) report mAP performance w.r.t. different ratios of training videos on NUS Kodak and NUS TRECVID10, respectively; while Figure 3.(c) and 3.(d) show the detailed AP of each individual concept on NUS Kodak and NUS TRECVID10, respectively. For clarity, in Figure 3.(c) and 3.(d), we compare only the RTVI approach to the three comparing algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison between the RTVI and the state-of-the-art approaches on the NUS Kodak and NUS TRECVID10 settings: (a) mAP on NUS Kodak; (b) mAP on NUS TRECVID10; (c) Detailed AP of each individual concept on NUS Kodak; and (d) Detailed AP of each individual concept on NUS TRECVID10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Effects of maximum mean discrepancy in the RTVI approach on the NUS Kodak and NUS TRECVID10 settings: (a) mAP on NUS Kodak; (b) mAP on NUS TRECVID10; (c) Detailed AP of each individual concept on NUS Kodak; and (d) Detailed AP of each individual concept on NUS TRECVID10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 )</head><label>2</label><figDesc>Effect of Maximum Mean Discrepancy: In this part, we analyze the capability of employing maximum mean discrepancy (MMD) to address the problem of domain difference between image domain and video domain. In particular, Figure 4.(a) and 4.(b) report mAP performance w.r.t. different ratios of training videos on NUS Kodak and NUS TRECVID10, respectively; while Figure 4.(c) and 4.(d) show the detailed AP of each individual concept on NUS Kodak and NUS TRECVID10, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Effects of multiple kernel learning in the RTVI approach on the NUS Kodak and NUS TRECVID10 settings: (a) mAP on NUS Kodak; (b) mAP on NUS TRECVID10; (c) Detailed AP of each individual concept on NUS Kodak; and (d) Detailed AP of each individual concept on NUS TRECVID10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>.(a) and 5.(b) report mAP performance w.r.t. different ratios of training videos on NUS Kodak and NUS TRECVID10, respectively; while Figure 5.(c) and 5.(d) show the detailed AP of each individual concept on NUS Kodak and NUS TRECVID10, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Comparison of Web images and Web videos in the RTVI approach on the NUS Kodak and NUS TRECVID10 settings: (a) mAP on Kodak; (b) mAP on TRECVID10; (c) Detailed AP of each individual concept on Kodak; and (d) Detailed AP of each individual concept on TRECVID10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Effects of τ for cleaning the noisy Web images on the NUS Kodak and NUS TRECVID10 settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Parameter sensitivity of α and λ in the RTVI approach on (a) NUS Kodak and (b) NUS TRECVID10 settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Convergence study of the proposed algorithm on NUS Kodak.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>d</head><label></label><figDesc>APPENDIX A CALCULATION OF A AND d IN EQ.<ref type="bibr" target="#b15">(16)</ref> In Eq. (16), A is computed as   A = αpp T + M, M = [o (1) , . . . , o (m) ] T D[o (1) , . . . , o (m) ], o (k) = K(k) v, k = 1, 2, . . . , m. d is calculated as follow. = h -2g, g = [r T Do (1) , . . . , r T Do (m) ] T , r = ỹ • η -ηb, h = [λv T K (1) v, • • • , λv T K (m) v] T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILS</head><label>I</label><figDesc>OF DATA SETS AND EXPERIMENTAL SETTINGS FOR DIFFERENT CROSS-MEDIA SETTINGS.</figDesc><table><row><cell>Data Sets</cell><cell>Images</cell><cell>Videos</cell><cell>Concepts</cell></row><row><cell>NUS Kodak</cell><cell>51,150 (1,000 per concept for training)</cell><cell>2,303 (10%, 20%, 30%, 40%, 50% per concept for training)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPUTATIONAL</head><label>II</label><figDesc>COST OF THE RTVI ON NUS KODAK. THE TRAINING COST IS MEASURED IN SECONDS AND THE TESTING COST IS MEASURED IN MILLISECONDS PER SAMPLE.</figDesc><table><row><cell></cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell></row><row><cell>Testing Cost (ms)</cell><cell>0.30</cell><cell>0.46</cell><cell>0.62</cell><cell>0.78</cell><cell>0.94</cell></row><row><cell>Training Cost (s)</cell><cell>2.56</cell><cell>6.50</cell><cell>12.07</cell><cell>19.35</cell><cell>28.79</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">COMPUTATIONAL COST OF THE RTVI APPROACH ON NUS TRECVID10.</cell></row><row><cell cols="6">THE TRAINING COST IS MEASURED IN SECONDS AND THE TESTING COST</cell></row><row><cell cols="5">IS MEASURED IN MILLISECONDS PER SAMPLE.</cell><cell></cell></row><row><cell></cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell></row><row><cell>Testing Cost (ms)</cell><cell>0.29</cell><cell>0.42</cell><cell>0.54</cell><cell>0.67</cell><cell>0.79</cell></row><row><cell>Training Cost (s)</cell><cell>2.33</cell><cell>5.54</cell><cell>9.89</cell><cell>15.39</cell><cell>22.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www.bing.com/</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2014.2323014, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 1 This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. (Y. Gao is the corresponding author.Xiaofeng Zhu was partly supported by the Natural Science Foundation of China (NSFC) under grant 61263035.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Concept-based video retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="215" to="322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive video indexing with statistical active learning</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="27" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond distance measurement: Constructing neighborhood similarity for video annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="476" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel lasso for large-scale video concept detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="65" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mpeg-7 video automatic labeling system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="98" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1959" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Brand data gathering from live social media streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMR</title>
		<imprint>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tag ranking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimol: automatic online picture collection via incremental model learning</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust semantic video indexing by harvesting web images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-domain learning methods for high-level visual concept classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zavesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="161" to="164" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic context transfer across heterogeneous sources for domain adaptive video search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning heterogeneous data for hierarchical web video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain transfer svm for video concept detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1375" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic diffusion for large scale context-based video annotation</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1420" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heterogeneous transfer learning for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An introduction to outlier analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Outlier Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning social tag relevance by neighbor voting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sampling and ontologically pooling web images for visual concept learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1068" to="1078" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifying tag relevance with relevant positive and negative examples</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM. ACM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="485" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tag refinement by regularized lda</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM MM</publisher>
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image tag refinement towards low-rank, content-tag prior and error sparsity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Inferring semantic concepts from community-contributed images and noisy tags</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM MM</publisher>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple kernel learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2211" to="2268" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Insights from classifying visual concepts with multiple kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e38897</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="e57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective transfer tagging from image to video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tag localization with spatial correlations and joint group sparsity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local image tagging via graph regularized joint group sparsity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1358" to="1368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust nonnegative graph embedding: Towards noisy data, unreliable graphs, and noisy labels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2464" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint schatten p-norm andℓpnorm robust matrix completion for missing value recovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KAIS</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ℓp-norm multiple kernel learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="953" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Harnessing lab knowledge for real-world action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<editor>CIVR</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Kodak consumer video benchmark data set: concept definition and annotation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Akira Yanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High-Level Feature Detection from Video in TRECVid: a 5-Year Retrospective of Achievements</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Content Analysis</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="151" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Ken Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="76" to="77" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
