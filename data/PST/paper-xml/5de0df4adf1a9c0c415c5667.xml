<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient descent optimizes over-parameterized deep ReLU networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-23">23 October 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
							<email>yuancao@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
							<email>drzhou@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
							<idno type="ORCID">0000-0001-9830-793X</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gradient descent optimizes over-parameterized deep ReLU networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-23">23 October 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s10994-019-05839-6</idno>
					<note type="submission">Received: 4 May 2019 / Revised: 6 July 2019 / Accepted: 6 September 2019 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural networks</term>
					<term>Gradient descent</term>
					<term>Over-parameterization</term>
					<term>Random initialization</term>
					<term>Global convergence D. Zou, Y. Cao: Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of training deep fully connected neural networks with Rectified Linear Unit (ReLU) activation function and cross entropy loss function for binary classification using gradient descent. We show that with proper random weight initialization, gradient descent can find the global minima of the training loss for an over-parameterized deep ReLU network, under certain assumption on the training data. The key idea of our proof is that Gaussian random initialization followed by gradient descent produces a sequence of iterates that stay inside a small perturbation region centered at the initial weights, in which the training loss function of the deep ReLU networks enjoys nice local curvature properties that ensure the global convergence of gradient descent. At the core of our proof technique is ( <ref type="formula">1</ref>) a milder assumption on the training data; (2) a sharp analysis of the trajectory length for gradient descent; and (3) a finer characterization of the size of the perturbation region. Compared with the concurrent work (Allen-Zhu et al. in A convergence theory for deep learning via over-parameterization, 2018a; Du et al. in Gradient descent finds global minima of deep neural networks, 2018a) along this line, our result relies on milder over-parameterization condition on the neural network width, and enjoys faster global convergence rate of gradient descent for training deep neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have achieved great success in many applications like image processing <ref type="bibr" target="#b19">(Krizhevsky et al. 2012)</ref>, speech recognition <ref type="bibr" target="#b15">(Hinton et al. 2012</ref>) and Go games <ref type="bibr" target="#b26">(Silver et al. 2016)</ref>. However, the reason why deep networks work well in these fields remains a mystery for long time. Different lines of research try to understand the mechanism of deep neural networks from different aspects. For example, a series of work tries to understand how the expressive power of deep neural networks are related to their architecture, including the width of each layer and depth of the network <ref type="bibr" target="#b27">(Telgarsky 2015</ref><ref type="bibr" target="#b28">(Telgarsky , 2016;;</ref><ref type="bibr" target="#b25">Lu et al. 2017;</ref><ref type="bibr" target="#b23">Liang and Srikant 2016;</ref><ref type="bibr" target="#b31">Yarotsky 2017</ref><ref type="bibr" target="#b32">Yarotsky , 2018;;</ref><ref type="bibr" target="#b10">Hanin 2017;</ref><ref type="bibr" target="#b11">Hanin and Sellke 2017)</ref>. These work shows that multi-layer networks with wide layers can approximate arbitrary continuous function.</p><p>Very recently, there emerges a large body of work that study the global convergence of gradient descent (GD) for training neural networks <ref type="bibr" target="#b21">(Li and Liang 2018;</ref><ref type="bibr" target="#b8">Du et al. 2018b;</ref><ref type="bibr" target="#b0">Allen-Zhu et al. 2018a;</ref><ref type="bibr" target="#b7">Du et al. 2018a)</ref>. In particular, <ref type="bibr" target="#b21">Li and Liang (2018)</ref> showed that for a onehidden-layer network with ReLU activation function using over-parameterization and random initialization, GD and stochastic gradient descent (SGD) can find the global near-optimal solution in polynomial time. <ref type="bibr" target="#b8">Du et al. (2018b)</ref> showed that under the assumption that the ReLU Gram matrix is positive definite, randomly initialized GD converges to a globally optimal solution of a one-hidden-layer network with ReLU activation function and quadratic loss function. Beyond shallow neural network, <ref type="bibr" target="#b7">Du et al. (2018a)</ref> considered regression problem with square loss function, and proved that under certain assumptions on the initialization and training data, gradient descent is able to converge to the global optimal solution for training deep neural networks. However, <ref type="bibr" target="#b7">Du et al. (2018a)</ref> only investigated DNNs with smooth activation functions, which exclude the widely-used ReLU activation function. Moreover, the theoretical results in <ref type="bibr" target="#b7">Du et al. (2018a)</ref> heavily rely on the assumption that the smallest eigenvalue of certain deep compositional Gram matrix is bounded below from zero, which does not explicitly tell the dependency on the problem parameter such as the number of training examples n and the number of hidden layers L, and this assumption cannot be verified in practice. <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> studied the same problem under a different assumption on the training data, and proved that random initialization followed by gradient descent is able to converge to the global optimal solution for training deep neural networks. Besides, <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> studied the convergence rate of SGD for training deep ReLU network and discussed various extensions to classification problem and various loss functions. However, the assumption on the training data made in <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> is very stringent, because they require that any two training data points are separated by some constant, but in practice the data from the same class can be arbitrarily close (e.g., due to data augmentation in deep learning). Our work is independent and concurrent to <ref type="bibr" target="#b7">Du et al. (2018a)</ref>, <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref>. 1  In this paper, we study the optimization properties of gradient-based methods for deep ReLU neural networks, with more realistic assumption on the training data, milder overparameterization condition and faster convergence rate. In specific, we consider an L-hiddenlayer fully-connected neural network with ReLU activation function. Similar to the onehidden-layer case studied in <ref type="bibr" target="#b21">Li and Liang (2018)</ref> and <ref type="bibr" target="#b8">Du et al. (2018b)</ref>, we study binary classification problem and show that GD can achieve the global minima of the training loss for any L ≥ 1, with the aid of over-parameterization and random initialization. The highlevel idea of our proof technique is to show that Gaussian random initialization followed by gradient descent generates a sequence of iterates within a small perturbation region centering 1 The first versions of all these three papers were all posted on arXiv in November 2018.</p><p>around the initial weights. In addition, we will show that the empirical loss function of deep ReLU networks has very good local curvature properties inside the perturbation region, which guarantees the global convergence of gradient descent. Compared with the proof technique in <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref>, we provide a sharper analysis on the GD algorithm and prove that GD can be guaranteed to have sufficient descent in a larger perturbation region with a larger step size. This leads to a faster convergence rate and a milder condition on the over-paramterization. More specifically, our main contributions are summarized as follows:</p><p>-We establish the global convergence guarantee for training deep ReLU networks in terms of classification problems. Compared with <ref type="bibr" target="#b21">Li and Liang (2018)</ref>, <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> our assumption on training data is more reasonable and is often satisfied by real training data. Specifically, we only require that any two data points from different classes are separated by some constant, while <ref type="bibr" target="#b21">Li and Liang (2018)</ref> assumes that the data from different classes are sampled from small balls separated by a constant margin, and <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> requires that any two data points are well separated, even though they belong to the same class. -We show that with Gaussian random initialization on each layer, when the number of hidden nodes per layer is at least n 14 L 16 /φ<ref type="foot" target="#foot_2">4</ref> , GD can achieve zero training error within O n 5 L<ref type="foot" target="#foot_1">3</ref> /φ iterations, where φ is the data separation distance,<ref type="foot" target="#foot_0">2</ref> n is the number of training examples, and L is the number of hidden layers. This significantly improves the state-of-the-art results by <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref>, where the authors proved that GD can converge within O n 6 L 2 /φ 2 iterations if the number of hidden nodes per layer is at least (n 24 L 12 /φ 8 ). Compared with <ref type="bibr" target="#b7">Du et al. (2018a)</ref>, our result only has a polynomial dependency on the number of hidden layers, which is much better than their result that has an exponential dependency on the depth for fully connected deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional related work</head><p>Due to the huge amount of literature on deep learning theory, we are not able to include all papers in this big vein here. Instead, we review the following two additional lines of research, which are also related to our work. One-hidden-layer neural networks with ground truth parameters Recently a series of work <ref type="bibr" target="#b29">(Tian 2017;</ref><ref type="bibr" target="#b5">Brutzkus and Globerson 2017;</ref><ref type="bibr" target="#b22">Li and Yuan 2017;</ref><ref type="bibr" target="#b6">Du et al. 2017;</ref><ref type="bibr" target="#b33">Zhang et al. 2018</ref>) studied a specific class of shallow two-layer (one-hidden-layer) neural networks, whose training data are generated by a ground truth network called "teacher network". This series of work aim to provide recovery guarantee for gradient-based methods to learn the teacher networks based on either the population or empirical loss functions. More specifically, <ref type="bibr" target="#b29">Tian (2017)</ref> proved that for two-layer ReLU networks with only one hidden neuron, GD with arbitrary initialization on the population loss is able to recover the hidden teacher network. <ref type="bibr" target="#b5">Brutzkus and Globerson (2017)</ref> proved that GD can learn the true parameters of a two-layer network with a convolution filter. <ref type="bibr" target="#b22">Li and Yuan (2017)</ref> proved that SGD can recover the underlying parameters of a two-layer residual network in polynomial time. Moreover, <ref type="bibr" target="#b6">Du et al. (2017)</ref> proved that both GD and SGD can recover the teacher network of a two-layer CNN with ReLU activation function. <ref type="bibr" target="#b33">Zhang et al. (2018)</ref> showed that GD on the empirical loss function can recover the ground truth parameters of one-hidden-layer ReLU networks at a linear rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep linear networks</head><p>Beyond shallow one-hidden-layer neural networks, a series of recent work <ref type="bibr" target="#b12">(Hardt and Ma 2016;</ref><ref type="bibr" target="#b18">Kawaguchi 2016;</ref><ref type="bibr" target="#b4">Bartlett et al. 2018;</ref><ref type="bibr" target="#b9">Gunasekar et al. 2018;</ref><ref type="bibr">Arora et al. 2018a, b)</ref> focused on the optimization landscape of deep linear networks. More specifically, <ref type="bibr" target="#b12">Hardt and Ma (2016)</ref> showed that deep linear residual networks have no spurious local minima. <ref type="bibr" target="#b18">Kawaguchi (2016)</ref> proved that all local minima are global minima in deep linear networks. <ref type="bibr" target="#b3">Arora et al. (2018b)</ref> showed that depth can accelerate the optimization of deep linear networks. <ref type="bibr" target="#b4">Bartlett et al. (2018)</ref> proved that with identity initialization and proper regularizer, GD can converge to the least square solution on a residual linear network with quadratic loss function, while <ref type="bibr" target="#b2">Arora et al. (2018a)</ref> proved the same properties for general deep linear networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We use lower case, lower case bold face, and upper case bold face letters to denote scalars, vectors and matrices respectively. For a positive integer n, we denote [n] = {1, . . . , n}. For a vector x = (x 1 , . . . , x d ) , we denote by</p><formula xml:id="formula_0">x p = d i=1 |x i | p 1/ p the p norm of x, x ∞ = max i=1,...,d |x i | the ∞ norm</formula><p>of x, and x 0 = |{x i : x i = 0, i = 1, . . . , d}| the number of non-zero entries of x. We use Diag(x) to denote a square diagonal matrix with the elements of vector x on the main diagonal. For a matrix A = (A i j ) ∈ R m×n , we use A F to denote the Frobenius norm of A, A 2 to denote the spectral norm (maximum singular value), and A 0 to denote the number of nonzero entries. We denote by</p><formula xml:id="formula_1">S d−1 = {x ∈ R d : x 2 = 1} the unit sphere in R d .</formula><p>For two sequences {a n } and {b n }, we use a n = O(b n ) to denote that a n ≤ C 1 b n for some absolute constant C 1 &gt; 0, and use a n = (b n ) to denote that a n ≥ C 2 b n for some absolute constant C 2 &gt; 0. In addition, we also use O(•) and (•) to hide logarithmic terms in Big-O and Big-Omega notations. We also use the following matrix product notation. For indices l 1 , l 2 and a collection of matrices {A r } r ∈Z + , we denote</p><formula xml:id="formula_2">l 2 r =l 1 A r := A l 2 A l 2 −1 • • • A l 1 if l 1 ≤ l 2 I otherwise. (3.1) 3.2 Problem setup Let {(x 1 , y 1 ), . . . , (x n , y n )} ∈ (R d × {−1, 1}) n be a set of n training examples. Let m 0 = d.</formula><p>We consider L-hidden-layer neural networks as follows:</p><formula xml:id="formula_3">f W (x) = v σ (W L σ (W L−1 • • • σ (W 1 x) • • • )),</formula><p>where σ (x) = max{0, x} is the entry-wise ReLU activation function, W l = (w l,1 , . . . , w l,m l ) ∈ R m l−1 ×m l , l = 1, . . . , L are the weight matrices, and v ∈ {−1, +1} m L is the fixed output layer weight vector with half 1 and half −1 entries. Let W = {W l } l=1,...,L be the collection of matrices W 1 , . . . , W L , we consider solving the following empirical risk minimization problem:</p><formula xml:id="formula_4">L S (W) = 1 n n i=1 (y i y i ) = 1 n n i=1 y i v σ (W L σ (W L−1 • • • σ (W 1 x i ) • • • )) (3.2)</formula><p>where y i = f W (x i ) denotes the output of neural network and (x) = log(1 + exp(−x)) is the cross-entropy loss for binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization algorithms</head><p>In this paper, we consider training a deep neural network with Gaussian initialization followed by gradient descent.</p><p>Gaussian initialization We say that the weight matrices W 1 , . . . , W L are generated from Gaussian initialization if each column of W l is generated independently from the Gaussian distribution N (0, 2/m l I) for all l = 1, . . . , L. This initialization mechanism is called Heinitialization, which was proposed in <ref type="bibr" target="#b13">He et al. (2015)</ref>.</p><p>Gradient descent We consider solving the empirical risk minimization problem (3.2) with gradient descent with Gaussian initialization: let</p><formula xml:id="formula_5">W (0) 1 , . . . , W<label>(0)</label></formula><p>L be weight matrices generated from Gaussian initialization, we consider the following gradient descent update rule:</p><formula xml:id="formula_6">W (k) l = W (k−1) l − η∇ W l L S (W (k−1) ), l = 1, . . . , L,</formula><p>where ∇ W l L S (•) is the partial gradient of L S (•) with respect to the l-th layer parameters W l , and η &gt; 0 is the step size (a.k.a., learning rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Calculations for neural network functions</head><p>Here we briefly introduce some useful notations and provide some basic calculations regarding the neural network in our setting.</p><p>-Output after the l-th layer: Given an input x i , the output of the neural network after the l-th layer is</p><formula xml:id="formula_7">x l,i = σ (W l σ (W l−1 • • • σ (W 1 x i ) • • • )) = l r =1 r ,i W r x i , where 1,i = Diag 1{W 1 x i &gt; 0} , 3 and l,i = Diag[1{W l ( l−1 r =1 r ,i W r )x i &gt; 0}] for l = 2, . . . , L.</formula><p>-Output of the neural network: The output of the neural network with input x i is as follows:</p><formula xml:id="formula_8">f W (x i ) = v σ (W L σ (W L−1 • • • σ (W 1 x i ) • • • )) = v L r =l r ,i W r x l−1,i ,</formula><p>where we define x 0,i = x i and the last equality holds for any l ≥ 1. -Gradient of the neural network: The partial gradient of the training loss L S (W) with respect to W l is as follows:</p><formula xml:id="formula_9">∇ W l L S (W) = 1 n n i=1 (y i y i ) • y i • ∇ W l [ f W (x i )],</formula><p>where the gradient of the neural network function is defined as</p><formula xml:id="formula_10">∇ W l [ f W (x i )] = x l−1,i v L r =l+1 r ,i W r l,i .</formula><p>In the remaining of this paper, we define the gradient ∇ L S (W) as the collection of partial gradients with respect to all W l 's, i.e.,</p><formula xml:id="formula_11">∇ L S (W) = {∇ W 1 L S (W), ∇ W 2 L S (W), . . . , ∇ W L L S (W)}.</formula><p>We also define the Frobenius norm of ∇ L S (W) as</p><formula xml:id="formula_12">∇ W l L S (W) F = L l=1 ∇ W l L S (W) 2 F 1/2</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main theory</head><p>In this section, we show that with random Gaussian initialization, over-parameterization helps gradient descent converge to the global minimum, i.e., find a point in the parameter space with arbitrary small training loss. We start with assumptions on the training data,</p><formula xml:id="formula_13">Assumption 4.1 x i 2 = 1 and (x i ) d = μ for all i ∈ {1, . . . , n}, where μ ∈ (0, 1) is a constant.</formula><p>As is shown in the assumption above, the last entry of input x is considered to be a constant μ.</p><p>This assumption is natural because it can be seen as adding a bias term in the input layer, and learning both weight vector and bias is equivalent to adding an additional dummy variable ((x i ) d = μ) to all input vectors and learning the weight vector only. The same assumption has been made in <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref>. In addition, we emphasize that Assumption 4.1 is made in order to simplify the proof. Actually, rather than restricting the norm of all training examples to be 1, this assumption can be relaxed to be that x i 2 is lower and upper bounded by some constants.</p><formula xml:id="formula_14">Assumption 4.2 For all i, i ∈ {1, . . . , n}, if y i = y i , then x i − x i 2 ≥ φ for some φ &gt; 0.</formula><p>Assumption 4.2 basically requires that inputs with different labels in the training data are separated from each other by at least a constant. This assumption is often satisfied in practice.</p><p>In contrast, <ref type="bibr" target="#b0">Allen-Zhu et al. (2018a)</ref> assumes that every two different data points in the training data are separated by a constant, which is much stronger and cannot be satisfied since in classification it is allowed that the data with the same label can be arbitrarily close. Furthermore, Assumption 4.2 can be easily verified based on the training data. As a comparison, the assumption made in <ref type="bibr" target="#b7">Du et al. (2018a)</ref> assumes that certain deep compositional Gram matrix defined on the training data is strictly positive definite, which is not easy to verify, since the definition of their special Gram matrix is based on integration.</p><p>Then we have the following assumption on the structure of neural network.</p><formula xml:id="formula_15">Assumption 4.3 Define M = max{m 1 , . . . , m L }, m = min{m 1 , . . . , m L }. We assume that M ≤ 2m.</formula><p>Assumption 4.3 states that the number of nodes at all layers are of the same order. The constant 2 is not essential and can be replaced with an arbitrary constant greater than or equal to 1. Under Assumptions 4.1-4.3, we are able to establish the global convergence of gradient descent for training deep ReLU networks. Specifically, we provide the following theorem which characterizes the required numbers of hidden nodes and iterations such that the gradient descent can attain the global minimum of the training loss function.</p><p>Theorem 4.4 Suppose W (0)  1 , . . . , W</p><p>L are generated by Gaussian initialization. Then under Assumptions 4.1-4.3, if the step size η = O(M −1 L −3 ), the number of hidden nodes per layer satisfies</p><formula xml:id="formula_17">m = n 14 L 16 φ −4 + n 12 L 16 φ −4 −1</formula><p>and the maximum number of iteration satisfies</p><formula xml:id="formula_18">K = O n 5 L 3 /φ + n 3 L 3 −1 /φ ,</formula><p>then with high probability, the last iterate of gradient descent W (K ) satisfies L S (W (K ) ) ≤ .</p><p>Remark 4.1 Note that our bound on the required number of hidden nodes per layer, i.e., m, depends on the target accuracy . However, in practical classification tasks, we are more interested in finding some points with zero training error. In specific, the cross-entropy loss  L) • n<ref type="foot" target="#foot_4">4</ref> /λ 4 0 , where λ 0 is the smallest eigenvalue of the deep compositional Gram matrix defined in their paper. Compared with their result, our condition on m has significantly better dependency in L. In addition, for real training data, λ 0 can have high degree dependency on the reciprocal of the sample size n, which makes the dependency of their result on n much worse.</p><formula xml:id="formula_19">(x) = log(1 + exp(−x)) is strictly decreasing in x, thus (y i y i ) ≤ (0) = log(2) implies y i y i ≥ 0. If we set L S (W) ≤ (0)/n = log(2)/n, it holds that (y i y i ) ≤ nL S (W) ≤ (0) for all i ∈ [n],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proof of the main theory</head><p>In this section, we provide the proof of the main theory. In specific, we decompose the proof into three steps:</p><p>Step 1: We characterize a perturbation region at the initialization, and prove that the neural network attains good properties within such region.</p><p>Step 2: Based on the assumption that all iterates are staying inside the region B(W (0) , τ ), we establish the convergence results of gradient descent.</p><p>Step 3: We verify that with our choice of m, until convergence all iterates of gradient descent would not escape from the perturbation region B(W (0) , τ ), which justifies the derived convergence guarantee. Now we characterize the perturbation as follows. Given the initialization generated by Gaussian distribution W (0) := {W (0) l } l=1,...,L , we define by B(W (0) , τ ) = {W : W l − W (0) l 2 ≤ τ for all l ∈ [L]} the perturbation region centered at W (0) . Then we provide the following Lemmas that provides key results which are essential to establish the convergence guarantees for (stochastic) gradient descent. Next we are going to state the following key lemmas that characterize some essential properties of the neural network when its weight parameters satisfy W ∈ B(W (0) , τ ). Firstly, the following lemma provides the lower and upper bounds of the Frobenious norm of the partial gradient</p><formula xml:id="formula_20">∇ W l [L S (W)].</formula><p>Lemma 5.2 (Gradient lower and upper bound) Under Assumptions 4.1, 4.2, and 4.3</p><formula xml:id="formula_21">, if τ = O φ 3/2 n −3 L −2 and m = (n 2 φ −1 ), then for all W ∈ B(W (0) , τ ), with probability at least 1 − exp − O(mφ/n) , there exist positive constants C and C such that ∇ W L [L S ( W)] 2 F ≥ C mφ n 5 n i=1 (y i y i ) 2 , ∇ W l [L S ( W)] F ≤ − C L M 1/2 n n i=1 (y i y i ),</formula><p>for all l ∈ [L], where y i = f W (x i ).</p><p>Then we provide the following lemma that characterizes the training loss decreasing after one-step gradient descent.</p><formula xml:id="formula_22">Lemma 5.3 (Sufficient descent) Let W (0) 1 , . . . , W (0) L be generated via Gaussian random initialization. Let W (k) = {W (k) l } l=1,...,L be the k-th iterate in the gradient descent and τ = O(L −11 log −3/2 (M)). If W (k) , W (k+1) ∈ B(W (0) , τ ), then there exist constants C and C such that with probability at least 1 − exp − O(mφ/n) the following holds, L S (W (k+1) ) − L S (W (k) ) ≤ − η − C M L 3 η 2 ∇ L S (W (k) ) 2 F − C L 8/3 τ 1/3 M log(M) • η ∇ L S (W (k) ) F n n i=1 (y i y (k) i )</formula><p>The second term on the R.H.S. of the result in Lemma 5.3 is due to the non-smoothness of ReLU activation, which can be characterized by counting how many nodes would change their activation patterns during the training process. Clearly, in order to guarantee that the gradient descent can bring sufficient descent in each step, we require the radius τ to be sufficiently small. In the following, we are going to complete the proof of Theorem 4.4 based on Lemmas 5.1-5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 4.4</head><p>We first prove that GD is able to achieve training loss under the condition that all iterates are staying inside the perturbation region B(W (0) , τ ). Note that by Lemma 5.2, we know that there exists a constant c 0 such that</p><formula xml:id="formula_23">∇ L S (W (k) ) 2 F ≥ ∇ W L [L S (W (k) )] 2 F ≥ c 0 mφ n 5 n i=1 (y i y (k) i ) 2 .</formula><p>We set the radius τ and the step size η as follows,</p><formula xml:id="formula_24">τ = c 1/2 0 m 1/2 φ 1/2 4C L 8/3 n 3/2 M log(M) 3 = O(n −9/2 L −8 φ 3/2 ), η= 1 4C M L 3 = O(M −1 L −3 ).</formula><p>Then we have</p><formula xml:id="formula_25">L S (W (k+1) ) − L S (W (k) ) ≤ − 3η 4 ∇ L S (W (k) ) 2 F − c 0 ηm 1/2 φ 1/2 4n 5/2 ∇ L S (W (k) ) F • n i=1 (y i y (k) i ) ≤ − η 2 ∇ L S (W (k) ) 2 F ≤ −η c 0 mφ 2n 5 n i=1 (y i y (k) i ) 2 , (<label>5.1)</label></formula><p>where the first inequality is by Lemma 5.3 and the choices of η and τ , the second inequality follows from Lemma 5.2, and the last inequality is due to the gradient lower bound we derived above. Note that (x) = log(1 + exp(−x)), which satisfies − (x) = 1/(1 + exp(x)) ≥ min α 0 , α 1 (x) where α 0 = 1/2 and α 1 = 1/(2 log(2)). This implies that</p><formula xml:id="formula_26">− n i=1 (y i y (k) i ) ≥ min α 0 , n i=1 α 1 (y i y (k) i ) ≥ min α 0 , nα 1 L S (W (k) ) .</formula><p>Note that min{a, b} ≥ 1/(1/a+1/b), we have the following by plugging the above inequality into (5.1)</p><formula xml:id="formula_27">L S (W (k+1) ) − L S (W (k) ) ≤ −η min c 0 mφα 2 0 2n 5 , c 0 mφα 2 1 2n 3 L 2 S (W (k) ) ≤ −η 2n 5 c 0 mφα 2 0 + 2n 3 c 0 mφα 2 1 L 2 S (W (k) ) −1</formula><p>.</p><p>Rearranging terms gives</p><formula xml:id="formula_28">2n 5 c 0 mφα 2 0 L S (W (k+1) ) − L S (W (k) ) + 2n 3 L S (W (k+1) ) − L S (W (k) ) c 0 mφα 2 1 L 2 S (W (k) )</formula><p>≤ −η.</p><p>(5.2)</p><p>Applying the inequality (x − y)/y 2 ≥ y −1 − x −1 and taking telescope sum over k give</p><formula xml:id="formula_29">kη ≤ 2n 5 c 0 mφα 2 0 L S (W (0) ) − L S (W (k) ) + 2n 3 L −1 S (W (k) ) − L −1 S (W (0) ) c 0 mφα 2 1 123 ≤ 2n 5 c 0 mφα 2 0 L S (W (0) ) + 2n 3 L −1 S (W (k) ) − L −1 S (W (0) ) c 0 mφα 2 1 .</formula><p>(5.3)</p><p>Now we need to guarantee that after K gradient descent steps the loss function L S (W (K ) ) is smaller than the target accuracy . By Lemma 5.1, we know that the training loss L S (W (0) ) = O(1). Therefore, by (5.3) and our choice of η, the maximum iteration number K satisfies</p><formula xml:id="formula_30">K = O n 5 L 3 /φ + n 3 L 3 −1 /φ .</formula><p>(5.4)</p><p>Then we are going to verify the condition that all iterates stay inside the perturbation region B(W (0) , τ ). We prove this by induction. Clearly, W (0) ∈ B(W (0) , τ ). Then we are going to prove W (k+1) ∈ B(W (0) , τ ) under the induction hypothesis that W (t) ∈ B(W (0) , τ ) holds for all t ≤ k. According to (5.1), we have</p><formula xml:id="formula_31">L S (W (t+1) ) − L S (W (t) ) ≤ − η 2 ∇ L S (W (t) ) 2 F ,</formula><p>(5.5) for any t &lt; k. Therefore, by triangle inequality, we have</p><formula xml:id="formula_32">W (k) l − W (0) l 2 ≤ η k−1 t=0 ∇ W l [L S (W (t) )] 2 ≤ η k k−1 t=0 ∇ L S (W (t) ) 2 F ≤ 2kη k−1 t=0 L S (W (t) ) − L S (W (t+1) )</formula><p>≤ 2kηL S (W (0) ).</p><p>By Lemma 5.1, we know that L S (W (0) ) = O(1). Then applying our choices of η and K , we have</p><formula xml:id="formula_33">W (k) l − W (0) l 2 ≤ 2K ηL S (W (0) ) = O n 5/2 φ −1/2 m −1/2 + n 3/2 −1/2 φ −1/2 m −1/2 .</formula><p>In addition, by Lemma 5.2 and our choice of η, we have</p><formula xml:id="formula_34">η ∇ W l [L S (W (k) )] 2 ≤ − ηC L M 1/2 n n i=1 y i • f W (k) (x i ) ≤ O(L −2 M −1/2 ),</formula><p>where the second inequality follows from the choice of η and the fact that −1 ≤ (•) ≤ 0.</p><p>Then by triangle inequality, we have</p><formula xml:id="formula_35">W (k+1) l − W (0) l 2 ≤ η ∇ W l [L S (W (k) )] 2 + W (k) l − W (0) l 2 = O(n −9/2 L −8 φ 3/2 ),</formula><p>which is exactly in the same order of τ , where the last equality follows from the overparameterization assumption m = n 14 L 16 φ −4 + n 12 L 16 φ −4 −1 . This verifies that W (k+1) ∈ B(W (0) , τ ) and completes the induction for k. Thus we can complete the proof. We first demonstrate that over-parameterization indeed helps optimization. We run GD for training deep ReLU networks with different network widths and plot the training loss in Fig. <ref type="figure" target="#fig_1">1</ref>, where we apply cross-entropy loss on both MNIST and CIFAR10 datasets. In addition, the step sizes are set to be small enough and fixed for ReLU networks with different width. It can be observed that over-parameterization indeed speeds up the convergence of gradient descent, which is consistent with Lemmas 5.2 and 5.3, since the square of gradient norm scales with m, which further implies that wider network leads to larger function decrease if the step size is fixed. We also display the distance between the iterates of GD and the initialization in Fig. <ref type="figure">2</ref>. It shows that when the network becomes wider, GD is more likely to converge to a point closer to the initialization. This suggests that the iterates of GD for training an overparameterized deep ReLU network are harder to exceed the required perturbation region, thus can be guaranteed to converge to a global minimum. This corroborates our theory.</p><p>Finally, we monitor the activation pattern changes of all hidden neurons during the training process, and show the results in Fig. <ref type="figure">3</ref>, where we use cross-entropy loss on both MNIST and CIFAR10 datasets. Specifically, in each iteration, we compare the activation status of all hidden nodes regarding all inputs with those at the initialization, and compute the number of nodes whose activation status differs from that at the initialization. From Fig. <ref type="figure">3</ref> it is clear that the activation pattern difference ratio dramatically decreases as the neural network becomes wider, which brings less non-smoothness during the training process. This implies that wider ReLU network can better guarantee sufficient function decrease after one-step gradient descent, which is consistent with our theory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future work</head><p>In this paper, we studied training deep neural networks by gradient descent. We proved that gradient descent can achieve global minima of the training loss for over-parameterized deep ReLU networks with random initialization, with milder assumption on the training data. Compared with the state-of-the-art results, our theoretical guarantees are sharper in terms of both over-parameterization condition and convergence rate. Our result can also be extended to stochastic gradient descent (SGD) and other loss functions (e.g., square hinge loss and smoothed hinge loss). Such extensions can be found in the longer version of this paper <ref type="bibr" target="#b35">(Zou et al. 2018</ref>). In the future, we will further improve the over-parameterization condition such that it is closer to width of neural networks used in practice. Our proof technique can also be extended to other neural network architectures including convolutional neural networks (CNNs) <ref type="bibr" target="#b19">(Krizhevsky et al. 2012)</ref>, residual networks (ResNets) <ref type="bibr" target="#b14">(He et al. 2016</ref>) and recurrent neural networks (RNNs) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber 1997)</ref>, and give sharper over-parameterization conditions than existing results for CNNs, ResNets <ref type="bibr" target="#b7">(Du et al. 2018a;</ref><ref type="bibr" target="#b0">Allen-Zhu et al. 2018a</ref>) and RNNs <ref type="bibr" target="#b1">(Allen-Zhu et al. 2018b</ref>). Moreover, it is also interesting to explore how our optimization guarantees of over-parameterized neural networks can be integrated with existing universal approximation ability results such as <ref type="bibr" target="#b17">Hornik (1991)</ref>, <ref type="bibr" target="#b28">Telgarsky (2016)</ref>, <ref type="bibr" target="#b24">Lin and</ref><ref type="bibr">Jegelka (2018), Zhou (2019)</ref>.</p><formula xml:id="formula_36">1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 ≥ C a ∞ /n</formula><p>The following lemma characterizes the Lipschitz continuity of the gradients when the neural network parameters are staying inside the required perturbation region, which is essential to bound the norms of gradients.</p><p>Lemma A.3 <ref type="bibr">[Lemmas B.1 and B.2 in Zou et al. (2018)</ref>] Suppose that W 1 , . . . , W L are generated via Gaussian initialization. For τ &gt; 0, let W 1 , . . . , W L with W l − W l 2 ≤ τ , l = 1, . . . , L be the perturbed matrices. Let l,i , l = 1, . . . , L, i = 1, . . . , n be diagonal matrices satisfying l,i − l,i 0 ≤ s and |( l,i − l,i ) j j |, |( l,i ) j j | ≤ 1 for all l = 1, . . . , L, i = 1, . . . , n and j = 1, . . . , m l . If τ, s log(M)/m ≤ κ L −3/2 for some small enough absolute constant κ, then</p><formula xml:id="formula_37">l 2 r =l 1 r ,i W r 2 ≤ C √ L, v L r =l 1 r ,i W r 2 ≤ C √ M, v L r =l 1 r ,i W r u 2 ≤ C s log(M)</formula><p>for any 1 ≤ l 1 &lt; l 2 ≤ L and vector u with u 2 = 1 and u 0 ≤ s, where C, C and C are absolute constants.</p><p>We then provide the following lemma which characterizes the difference between activation patterns and outputs of all hidden layers generated by any two different neural networks.</p><formula xml:id="formula_38">Lemma A.4 [Lemma B.3 in Zou et al. (2018)] Suppose that W 1 , . . . , W L are generated via Gaussian initialization. Let W = { W 1 , . . . , W L }, W = { W 1 , . . . , W L } be two collections of weight matrices satisfying W l − W l 2 , W l − W l 2 ≤ τ , l = 1, . . . , L. Let l,i , l,i , l,i</formula><p>and x l,i , x l,i , x l,i be the binary matrices and hidden layer outputs at the l-th layer with parameter matrices W, W, W respectively. If τ ≤ κ L −11 (log(M)) −3/2 for some small enough absolute constant κ &gt; 0, then there exits constants C and C such that</p><formula xml:id="formula_39">x l,i − x l,i 2 ≤ C L • l r =1 W r − W r 2 , l,i − l,i 0 ≤ C L 4/3 τ 2/3 m l ,</formula><p>for all l = 1, . . . , L and i = 1, . . . , n.</p><p>Now we ready to prove Lemma 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 5.2</head><p>We first prove the gradient upper bound. For the training example (x i , y i ), let y i = f W (x i ), the gradient ∇ W l (y i y i ) can be written as follows,</p><formula xml:id="formula_40">∇ W l (y i y i ) = (y i y i )y i ∇ W l [ f W (x i )] = (y i y i )y i x l−1,i v L r =l+1 r ,i W l l,i .</formula><p>Note that by Lemma A.3, there exists an absolute constant C 0 such that</p><formula xml:id="formula_41">l 2 r =l 1 r ,i W r 2 ≤ C 0 √ L.</formula><p>Hence, we have the following upper bound on ∇ W l (y i y i ) F ,</p><formula xml:id="formula_42">∇ W l (y i y i ) F = ∇ W l (y i y i ) 2 ≤ − (y i y i ) l−1 r =1 r ,i W r x i 2 L r =l+1 r ,i W l 2 v 2 ≤ − (y i y i )C 2 0 L M 1/2 ,</formula><p>where the first equality holds due to the fact that</p><formula xml:id="formula_43">∇ W l (y i y i ) = (y i y i )y i x l−1,i v L r =l+1</formula><p>r ,i W l l,i is a rank-one matrix, and the last inequality follows from the fact that v 2 = m 1/2 L ≤ M 1/2 . Moreover, we have the following for ∇ W l [L S ( W)]:</p><formula xml:id="formula_44">∇ W l [L S ( W)] F = 1 n n i=1 ∇ W l (y i y i ) F ≤ 1 n n i=1 ∇ W l (y i y i ) F ≤ − C 2 0 L M 1/2 n n i=1 (y i y i ),</formula><p>which completes the proof of gradient upper bound. Now we are going to prove the gradient lower bound. Given initialization W (0) and any W ∈ B(W (0) , τ ), let y i = f W (x i ), we define</p><formula xml:id="formula_45">g j = 1 n n i=1 (y i y i )y i v j σ ( w (0) L, j , x L−1,i )x L−1,i ,</formula><p>where x L,i denotes the output of the last hidden layer with input x i at the initialization. Then since W (0) is generated via Gaussian random initialization, by Lemma A.2, we have the following holds for at least C 2 m L φ/n nodes,</p><formula xml:id="formula_46">g j 2 ≥ C 1 max i | (y i y i )|/n</formula><p>where C 1 , C 2 &gt; 0 are positive absolute constants. Moreover, we rewrite the gradient ∇ W L, j L S ( W) as follows:</p><formula xml:id="formula_47">∇ W L, j L S ( W) = 1 n n i=1 (y i y i )y i v j σ ( w L, j , x L−1,i ) x L−1,i ,</formula><p>where x l,i denotes the output of the l-th hidden layer with input x i and weight matrices W. Let b i, j = (y i y i )y i v j , we have</p><formula xml:id="formula_48">g j 2 − ∇ WL, j L S ( W) 2 ≤ 1 n n i=1 b i, j σ ( w L, j , x L−1,i ) x L−1,i − σ ( w (0) L, j , x L−1,i )x L−1,i 2 ≤ 1 n n i=1 b i, j σ ( w L, j , x L−1,i ) − σ ( w (0) L, j , x L−1,i x L−1,i + σ ( w L, j , x L−1,i )( x L−1,i − x L−1,i ) 2 .</formula><p>According to Lemma A.4, the number of nodes satisfying σ ( w L, j ,</p><formula xml:id="formula_49">x L−1,i ) − σ ( w (0) L, j , x L−1,i = 0 for at least one i is at most C 3 nL 4/3 τ 2/3 m L , where C 3 is an absolute constant.</formula><p>For the rest of the nodes in this layer, we have</p><formula xml:id="formula_50">g j 2 − ∇ W L, j L S ( W) 2 ≤ 1 n n i=1 b i, j σ ( w L, j , x L−1,i )( x L−1,i − x L−1,i ) 2 ≤ 1 n n i=1 C 4 L 2 τ |b i, j | ≤ C 4 L 2 τ max i | (y i y i )|,</formula><p>where C 4 is an absolute constant, the first inequality holds since these nodes satisfy</p><formula xml:id="formula_51">σ ( w L, j , x L−1,i ) − σ ( w<label>(0)</label></formula><p>L, j , x L−1,i ) = 0 for all i, the second inequality follows from Lemma A.4 and triangle inequality. Let</p><formula xml:id="formula_52">τ ≤ C 2 φ 2C 3 n 2 L 4/3 3/2 ∧ C 1 2nL 2 C 4 = O φ 3/2 n −3 L −2 .</formula><p>Note that we have at least</p><formula xml:id="formula_53">C 2 m L φ/n nodes satisfying g j 2 ≥ C 1 max i | (y i y i )|/n, thus there are at least C 2 m L φ/n − C 3 nL 4/3 τ 2/3 m L = C 2 m L φ/(2n) nodes satisfying ∇ W L, j L S ( W) 2 ≥ C 1 max i | (y i y i )|/n − C 4 L 2 τ max i | (y i y i )|/n ≥ C 1 max i | (y i y i )| 2n .</formula><p>Therefore,</p><formula xml:id="formula_54">∇ W L L S ( W) 2 F = m L j=1 ∇ W L, j L S ( W) 2 2 ≥ C 2 φm L 2n C 1 max i | (y i y (k) i )y i v j | 2n 2 ≥ C 2 C 2 1 φm L 8n 5 n i=1 (y i y (k) i ) 2 ,</formula><p>where the last inequality follows from the fact that (•) &lt; 0 and</p><formula xml:id="formula_55">|y i v j | = 1. Let C = C 2 C 2</formula><p>1 /8, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3 Proof of Lemma 5.3</head><p>Proof of Lemma 5.3 Note that (x) is 1/4-smooth, thus the following holds for any and x,</p><formula xml:id="formula_56">(x + ) ≤ (x) + (x) + 1 8 2 .</formula><p>Then we have the following upper bound on L S (W (k+1) ) − L S (W (k) ),</p><formula xml:id="formula_57">L S (W (k+1) ) − L S (W (k) ) = 1 n n i=1 y i y (k+1) i − y i y (k) i ≤ 1 n n i=1 (y i y (k) i ) (k) i + 1 8 ( (k) i ) 2 , (A.1)</formula><p>where</p><formula xml:id="formula_58">(k) i = y i y (k+1) i − y (k) i</formula><p>. Therefore, our next goal is to bound the quantity</p><formula xml:id="formula_59">(k) i . The upper bound of | (k)</formula><p>i | can be derived straightforwardly. By Lemma A.4, we know that there exists a constant C 1 such that</p><formula xml:id="formula_60">x (k+1) L,i − x (k) L,i 2 ≤ C 1 L • L l=1 W (k+1) l − W (k) l 2 = C 1 Lη L l=1 ∇ W l [L S (W (k) )] 2 ≤ C 1 L 1.5 η ∇ L S (W (k) ) F . (A.2)</formula><p>Therefore, it follows that</p><formula xml:id="formula_61">| (k) i | = |y i v (x (k+1) L,i − x (k) L,i )| ≤ v 2 x (k+1) L,i − x (k) L,i 2 ≤ C 1 L 1.5 M 1/2 ∇ L S (W (k) ) F ,</formula><p>where we use the fact that v 2 ≤ M 1/2 . In what follows we are going to prove the lower bound of</p><formula xml:id="formula_62">(k) i . Note that (k) i = y i v x (k+1) L,i − x (k)</formula><p>L,i , thus we mainly focus on bounding the term x</p><formula xml:id="formula_63">(k+1) L,i − x (k)</formula><p>L,i . For l = 1, . . . , L, we define the diagonal matrix</p><formula xml:id="formula_64">(k) l,i as (k) l,i j j = (k+1) l,i − (k) l,i j j • w (k) l, j x (k) l−1,i w (k+1) l, j x (k+1) l−1,i − w (k) l, j x (k) l−1,i .</formula><p>Given the above definition of (k) l,i , we have</p><formula xml:id="formula_65">x (k+1) L,i − x (k) L,i = (k) L,i + (k) L,i W (k+1) L x (k+1) L−1,i − W (k) L x (k) L−1,i = (k) L,i + (k) L,i W (k+1) L x (k+1) L−1,i − x (k) L−1,i + (k) L,i + (k) L,i W (k+1) L − W (k) L x (k) L−1,i = L l=1 L r =l+1 (k) r ,i + (k) r ,i W (k+1) r (k) l,i + (k) l,i W (k+1) l − W (k) l x (k) l−1,i .</formula><p>Then we define</p><formula xml:id="formula_66">D (k) l,i = L r =l+1 (k) r ,i W (k) r (k) l,i , D (k) l,i = L r =l+1 (k) r ,i + (k) r ,i W (k+1) r (k) l,i + (k) l,i .</formula><p>Then by triangle inequality, we have</p><formula xml:id="formula_67">v D (k) l,i − D (k) l,i 2 ≤ v D (k) l,i − D (0) l,i 2 + v D (0) l,i − D (k) l,i 2 . Note that, v D (k) l,i − D (0) l,i 2 ≤ L r =l v L t=r +1 (k) t,i W (k) t (k) t,i W (k) t − (0) t,i W (0) t L t=l+1 (0) t,i W (0) t 2 ≤ L r =l v L t=r +1 (k) t,i W (k) t (k) t,i − (0) t,i 2 W (0) t L t=l+1 (0) t,i W (0) t 2 + L r =l v L t=r +1 (k) t,i W (k) t (k) t,i 2 W (k) t − W (0) t 2 L t=l+1 (0) t,i W (0) t 2 .</formula><p>Then by Lemma A.3, and use the fact that</p><formula xml:id="formula_68">(k) t,i − (0) t,i 0 ≤ O L 4/3 τ 2/3 M , we have v D (k) l,i − D (0) l,i 2 ≤ C 2 L 13/6 τ 1/3 M log(M) + C 3 L 3/2 √ Mτ,</formula><p>where C 2 and C 3 are absolute constants and we use the fact that v 2 ≤ √ M. Then note that τ ≤ 1, the second term on the R.H.S. of the above inequality is dominated by the first one. Then we have</p><formula xml:id="formula_69">v D (k) l,i − D (0) l,i 2 ≤ C 5 L 13/6 τ 1/3 M log(M), (A.3)</formula><p>where C 5 is an absolute constant. This inequality also holds for v D</p><formula xml:id="formula_70">(k) l,i − D (0) l,i</formula><p>2 . Therefore, we have</p><formula xml:id="formula_71">(k) i = y i v x (k+1) L,i − x (k) L,i = y i v L l=1 D (k) l,i W (k+1) l − W (k) l x (k) l−1,i = −y i v L l=1 D (k) l,i − D (k) l,i ∇ W l [L S (W (k) )] x (k) l−1,i I (k) 1,i −y i v L l=1 D (k) l,i ∇ W l [L S (W (k) )] x (k) l−1,i I (k) 2,i</formula><p>. By (A.3) and triangle inequality, we know that</p><formula xml:id="formula_72">|I (k) 1,i | ≤ 2C 5 L 13/6 τ 1/3 M log(M)η • L l=1 ∇ W l [L S (W (k) )] 2 ≤ 2C 5 L 8/3 τ 1/3 M log(M)η • ∇ L S (W (k) ) F .</formula><p>Moreover, we have</p><formula xml:id="formula_73">1 n n i=1 (y i y (k) i )I (k) 2,i = − η n n i=1 (y i y (k) i )y i v L r =l+1 (k) r ,i W (k) r (k) l,i ∇ W l [L S (W (k) )] x (k) l−1,i = − η n 2 n i=1 (y i y (k) i )y i x (k) l−1,i v L r =l+1 (k) r ,i W (k) r (k) l,i 2 F = −η ∇ W l [L S (W (k) )] 2 F .</formula><p>Therefore, putting everything together, we have</p><formula xml:id="formula_74">L S (W (k+1) ) − L S (W (k) ) ≤ 1 n n i=1 (y i y (k) i ) (k) i + 1 8 ( (k) i ) 2 ≤ 1 n n i=1 (y i y (k) i ) I (k) 1,i + I (k) 2,i + C 3 M L 3 η 2 ∇ L S (W (k) ) 2 F ≤ − η − C 6 M L 3 η 2 ∇ L S (W (k) ) 2 F − C 7 L 8/3 τ 1/3 M log(M) • ∇ L S (W (k) ) F n n i=1 (y i y (k) i ),</formula><p>where C 6 and C 7 are absolute constants. Thus we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of lemmas in Appendix A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Lemma A.1</head><p>Proof of Lemma A.1 In order to prove the desired results, it suffices to prove the inequality</p><formula xml:id="formula_75">x l,i 2 2 − x l−1,i 2 2 ≤ C x l−1,i 2 2 • log(nL/δ) m l</formula><p>for all i = 1, . . . , n and l = 1, . . . , L, since this inequality implies that</p><formula xml:id="formula_76">x l,i 2 ≤ 1 + C log(nL/δ) m x l−1,i 2 ≤ • • • ≤ 1 + C log(nL/δ) m l/2 x i 2 ≤ 1 + C l log(nL/δ) m ,</formula><p>where C is an absolute constant, and the last inequality follows by the fact that (1 + x) l/2 ≤ 1 + lx for x ∈ (0, 1/(2L)), which is applicable here because of the assumption m ≥ C L 2 log(nL/δ) for some large enough constant C. Similarly, we can also proved that</p><formula xml:id="formula_77">x l,i 2 ≥ 1 − C l log(nL/δ) m</formula><p>for some absolute constant C . Combining the upper and lower bounds of x l,i 2 derived above gives the result of Lemma A.1. For any fixed i ∈ {1, . . . , n}, l ∈ {1, . . . , L} and j ∈ {1, . . . , m l }, condition on x l−1,i we have w l, j x l−1,i ∼ N (0, 2 x l−1,i 2 2 /m l ). Therefore,</p><formula xml:id="formula_78">E[σ 2 (w l, j x l−1,i )|x l−1,i ] = 1 2 E[(w l, j x l−1,i ) 2 |x l−1,i ] = 1 m l x l−1,i 2 2 . Since x l,i 2 2 = m l j=1 σ 2 (w l, j x l−1,i ) and condition on x l−1 , σ 2 (w l, j x l−1,i ) ψ 1 ≤ C 1 x l−1,i 2 2 /</formula><p>m l for some absolute constant C 1 , by Bernstein inequality (see Proposition 5.16 in Vershynin 2010), for any ξ ≥ 0 we have</p><formula xml:id="formula_79">P l,i W l x l−1,i 2 2 − x l−1,i 2 2 ≥ x l−1,i 2 2 ξ x l−1,i ≤ 2 exp(−C 2 m l min{ξ 2 , ξ}).</formula><p>Taking union bound over l and i gives</p><formula xml:id="formula_80">P x l,i 2 2 − x l−1,i 2 2 ≤ x l−1,i 2 2 ξ, i = 1, . . . , n, l = 1, . . . , L ≥ 1 − 2nL exp(−C 2 m l min{ξ 2 , ξ}).</formula><p>The inequality above further implies that if m l ≥ C 2 3 log(nL/δ), then with probability at least 1 − δ, we have</p><formula xml:id="formula_81">x l,i 2 2 − x l−1,i 2 2 ≤ C 3 x l−1,i 2 2 • log(nL/δ) m l</formula><p>for any i = 1, . . . , n and l = 1, . . . , L, where C 3 is an absolute constant. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Lemma A.2</head><p>In order to prove the gradient bounds, one key aspect is that the separation property for training data can be well preserved after passing through layers. The following lemma shows that the separation distance can be well preserved for all intermediate layers.</p><p>Lemma B.1 Under the same conditions in Lemma A.2, with probability at least 1 − δ,</p><formula xml:id="formula_82">x l,i − x l,i 2 ≥ φ/2</formula><p>for all i, i = 1, . . . , n with y i = y i , l = 1, . . . , L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.2</head><p>Let z 1 , . . . , z n ∈ S d−1 be n unit vectors and y 1 , . . . , y n ∈ {−1, 1} be the corresponding labels. Assume that for any i = j such that y i = y j , z i −z j 2 ≥ φ and z i z j ≥ μ 2 for some φ, μ &gt; 0. For any a = (a 1 , . . . ,</p><formula xml:id="formula_83">a n ) ∈ R n + , let h(w) = n i=1 a i y i σ ( w, z i )z i where w ∼ N (0, I) is a Gaussian random vector. If φ ≤ μ/2, then there exist absolute constants C, C &gt; 0 such that P h(w) 2 ≥ C a ∞ ≥ C φ/n.</formula><p>The following lemma is essential to show that deep ReLU network can provide significantly large gradient at the initialization.</p><p>Proof of Lemma A.2 For any given j ∈ {1, . . . , m L } and a with a ∞ = 1. By Lemma B.1, we know that for any i = j and y</p><formula xml:id="formula_84">i = y j , xL−1,i − xL−1, j 2 ≥ φ, where xL−1,i = x L−1,i / x L−1,i 2 and xL−1, j = x L−1, j / x L−1,i 2 .</formula><p>Then by Lemma B.2, we have</p><formula xml:id="formula_85">P 1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 ≥ C 1 n ≥ C 2 φ n , where C 1 , C 2 &gt; 0 are absolute constants. Let S n−1 ∞,+ = {a ∈ R n + : a ∞ = 1}, and N = N [S n−1 ∞,+ , C 1 /(4n)] be a C 1 /(4n)-net covering S n−1 ∞,+ in ∞ norm. Then we have |N | ≤ (4n/C 1 ) n .</formula><p>For j = 1, . . . , m L , define</p><formula xml:id="formula_86">Z j = 1 1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 ≥ C 1 n .</formula><p>Let p φ = C 2 φ/n. Then by Bernstein inequality and union bound, with probability at least 1</p><formula xml:id="formula_87">− exp[−C 3 m L p φ + n log(4n/C 1 )] ≥ 1 − exp(C 4 m L φ/n), we have 1 m L m L j=1 Z j ≥ p φ /2, (B.1)</formula><p>where C 3 , C 4 are absolute constants. For any a ∈ S n−1 ∞,+ , there exists a ∈ N such that a − a ∞ ≤ C 1 /(4n).</p><p>Therefore, we have </p><formula xml:id="formula_88">1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 − 1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 ≤ 1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i − 1 n n i=1 a i y i σ ( w L, j , x L−1,i )x L−1,i 2 ≤ 2 n n i=1 |a i − a i | ≤ C 1 2n . (<label>B</label></formula><formula xml:id="formula_89">a i y i σ ( w L, , x L−1,i )x L−1,i 2 ≥ C 1 2n .</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs of lemmas in Appendix B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Lemma B.1</head><p>The following lemma is necessary for proving Lemma B.1.</p><p>Lemma C.1 <ref type="bibr">[Lemma A.3 in Zou et al. (2018)</ref>] For θ &gt; 0, let Z 1 , Z 2 be two jointly Gaussian random variables with</p><formula xml:id="formula_90">E(Z 1 ) = E(Z 2 ) = 0, E(Z 2 1 ) = E(Z 2 2 ) = 1 and E(Z 1 Z 2 ) ≤ 1−θ 2 /2. If θ ≤ κ for some small enough absolute constant κ, then E[σ (Z 1 )σ (Z 2 )] ≤ 1 2 − 1 4 θ 2 + Cθ 3 ,</formula><p>where C is an absolute constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma B.1</head><p>We first consider any fixed l ≥ 1. Suppose that</p><formula xml:id="formula_91">x l−1,i − x l−1,i 2 ≥ [1 − (2L) −1 log(2)] l−1 φ.</formula><p>If we can show that under this condition, with high probability</p><formula xml:id="formula_92">x l,i − x l,i 2 ≥ [1 − (2L) −1 log(2)] l φ,</formula><p>then the result of the lemma follows by union bound and induction. Denote</p><formula xml:id="formula_93">φ l−1 = [1 − (2L) −1 log(2)] l−1 φ.</formula><p>Then by assumption we have σ (w l, j x )σ (w l, j x l−1,i )</p><formula xml:id="formula_94">x l−1,i − x l−1,i 2 2 ≥ φ 2 l−1 . Therefore x l−1,i x l−1,i ≤ 1 − φ 2 l−1 /2. It</formula><formula xml:id="formula_95">x l−1,i , x l−1,i ⎤ ⎦ ≤ 2 m x l−1,i 2 x l−1,i 2 • m • 1 2 − 1 4 φ 2 l−1 + Cφ 3 l−1 = x l−1,i 2 x l−1,i 2 • 1 − 1 2 φ 2 l−1 + 2Cφ 3 l−1 .</formula><p>Therefore, E x l,i − x l,i 2 2 x l−1,i , x l−1,i ≥ ( x l−1,i 2 − x l−1,i 2 ) 2 + x l−1,i 2 x l−1,i 2 (φ 2 l−1 − 4Cφ 3 l−1 ).</p><p>(C.1)</p><p>Condition on x l−1,i and x l−1,i , by Lemma 5.14 in <ref type="bibr" target="#b30">Vershynin (2010)</ref> we have</p><formula xml:id="formula_96">[σ (w l, j x l−1,i ) − σ (w l, j x l−1,i )] 2 ψ 1 ≤ 2 [σ (w l, j x l−1,i ) ψ 2 + σ (w l, j x l−1,i ) ψ 2 2 ≤ C 1 ( x l−1,i 2 + x l−1,i 2 ) 2 /m l ,</formula><p>where C 1 is an absolute constant. Therefore if m l ≥ C 2 2 log(4n 2 L/δ), by Bernstein inequality and union bound, with probability at least 1 − δ/(4n 2 L) we have</p><formula xml:id="formula_97">x l,i − x l,i 2 2 − E x l,i − x l,i 2 2 x l−1,i , x l−1,i ≤ C 2 ( x l−1,i 2 + x l−1,i 2 ) 2 • log(8n 2 L/δ) m l ,</formula><p>where C 2 is an absolute constant. Therefore with probability at least 1 − δ/(4n 2 L) we have</p><formula xml:id="formula_98">x l,i − x l,i 2 2 ≥ ( x l−1,i 2 − x l−1,i 2 ) 2 + x l−1,i 2 x l−1,i 2 (φ 2 l−1 − 4Cφ 3 l−1 ) − C 2 ( x l−1,i 2 + x l−1,i 2 ) 2 • log(8n 2 L/δ) m l .</formula><p>By union bound and Lemma A.1, if m r ≥ C 3 L 4 φ −4 l log(4n 2 L/δ), r = 1, . . . , l for some large enough absolute constant C 3 and φ ≤ κ L −1 for some small enough absolute constant κ, then with probability at least 1 − δ/(2n 2 L) we have</p><formula xml:id="formula_99">x l,i − x l,i 2 2 ≥ [1 − (4L) −1 log(2)]φ 2 l−1 ≥ [1 − (4L) −1 log(2)] 2 φ 2 l−1 .</formula><p>Moreover, by Lemma A.1, with probability at least 1 − δ/(2n 2 L) we have</p><formula xml:id="formula_100">x l,i − x l,i 2 − x l,i − x l,i 2 ≤ x l,i − x l,i 2 + x l,i − x l,i 2 = 1 − x l,i 2 + 1 − x l,i 2 ≤ (4L) −1 log(2) • φ 2 l−1 ,</formula><p>and therefore with probability at least 1 − δ/(n 2 L), we have</p><formula xml:id="formula_101">x l,i − x l,i 2 ≥ [1 − (2L) −1 log(2)]φ l−1 = [1 − (2L) −1 log(2)] l φ.</formula><p>Applying union bound and induction over l = 1, . . . , L completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Lemma B.2</head><p>Proof of Lemma B.2 Without loss of generality, assume that a 1 = a ∞ . Since z 1 2 = 1, we can construct an orthonormal matrix Q = [z 1 , Q ] ∈ R d×d . Let u = Q w ∼ N (0, I) be a standard Gaussian random vector. Then we have</p><formula xml:id="formula_102">w = Qu = u 1 z 1 + Q u ,</formula><p>where u := (u 2 , . . . , u d ) is independent of u 1 . We define the following two events based on a parameter γ ∈ (0, 1]:</p><formula xml:id="formula_103">E 1 (γ ) = |u 1 | ≤ γ , E 2 (γ ) = | Q u , z i | ≥ γ for all z i such that z i − z 1 2 ≥ φ .</formula><p>Let E(γ ) = E 1 (γ ) ∩ E 2 (γ ). We first give lower bound for P(E) = P(E 1 )P(E 2 ). Since u 1 is a standard Gaussian random variable, we have</p><formula xml:id="formula_104">P(E 1 ) = 1 √ 2π γ −γ exp − 1 2 x 2 dx ≥ 2 πe γ.</formula><p>Moreover, by definition, for any i = 1, . . . , n we have</p><formula xml:id="formula_105">Q u , z i ∼ N 0, 1 − (z 1 z i ) 2 .</formula><p>Let I = {i : z i − z 1 2 ≥ φ}. By the assumption that φ ≤ μ/2, for any i ∈ I, we have</p><formula xml:id="formula_106">−1 + φ 2 /2 ≤ −(1 − μ 2 ) + μ 2 ≤ z i , z 1 ≤ 1 − φ 2 /2,</formula><p>and if φ 2 ≤ 2, then 1 − (z 1 z i ) 2 ≥ φ 2 − φ 4 /4 ≥ φ 2 /2.</p><p>Therefore for any i ∈ I,</p><formula xml:id="formula_107">P[| Q u , z i | &lt; γ ] = 1 √ 2π [1−(z 1 z i ) 2 ] −1/2 γ −[1−(z 1 z i ) 2 ] −1/2 γ exp − 1 2 x 2 dx ≤ 2 π γ [1 − (z 1 z i ) 2 ] 1/2 ≤ 2 √ π γ φ −1 .</formula><p>By union bound over I, we have</p><formula xml:id="formula_108">P(E 2 ) = P[| Q u , z i | ≥ γ, i ∈ I] ≥ 1 − 2 √ π nγ φ −1 .</formula><p>Therefore we have</p><formula xml:id="formula_109">P(E) ≥ 2 πe γ • 1 − 2 √ π nγ φ −1 .</formula><p>Setting γ = √ π φ/(4n), we obtain P(E) ≥ φ/( √ 32en). Now let I = [n] \ (I ∪ {1}). Then conditioning on event E, we have h(w) = n i=1 a i y i σ ( w, z i )z i = a 1 y 1 σ (u 1 )z 1 + i∈I a i y i σ u 1 z 1 , z i + Q u , z i z i 123</p><p>Since a 1 = a ∞ and P(E) ≥ φ/( √ 32en), we have</p><formula xml:id="formula_110">P h(w) 2 ≥ C a ∞ ≥ C φ/n,</formula><p>where C and C are absolute constants. This completes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Lemma 5.1 (Bounded initial training loss) Under Assumptions 4.1 and 4.3, with probability at least 1 − δ, at the initialization the training loss satisfies L S (W (0) ) ≤ C log(n/δ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 The convergence of GD for training deep ReLU network with different network widths. a MNIST dataset. b CIFAR10 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 Fig. 3</head><label>23</label><figDesc>Fig. 2 Distance between the iterates of GD and the initialization. a MNIST dataset. b CIFAR10 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>follows by direct calculation thatE x l,i − x l,i 2 2 x l−1,i , x l−1,i = E x l,i 2 2 + x l,i 2 2 x l−1,i , x l−1,i − 2E x l,i x l,i x l−1,i , x l−1,i = ( x l−1,i 2 2 + x l−1,i 2 2 ) − 2E x l,i x l,i x l−1,i , x l−1,i .123 By Lemma C.1 and the assumption that φ l−1 ≤ φ ≤ κ, we haveE x l,i x l,i x l−1,i , x l−1,i l, j x l−1,i )σ (w l, j x l−1,i ) x l−1,i , x l−1,i ⎤ ⎦ = x l−1,i 2 x l−1,i 2 • E</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We will define the data separation distance, training sample size n and number of hidden layers L formally in Sects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Here we slightly abuse the notation and denote 1{a &gt; 0} = (1{a 1 &gt; 0}, . . . , 1{a m &gt; 0}) for a vector a ∈ R m .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">It is worth noting that in practice we usually have n L, thus our improvements in terms of the overparameterization condition and convergence rate are indeed significant.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation BIGDATA IIS-1855099, IIS-1903202  and IIS-1906169. QG is also partially supported by the Salesforce Deep Learning Research Grant. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA award. The views and conclusions contained in this paper are those of the authors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Proofs of lemmas in Sect. 5</p><p>In this section we provide the proof of all lemmas in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Lemma 5.1</head><p>We first provide the following lemma that bounds the output of all hidden layer. Lemma A.1 With Gaussian random initialization, for any δ ∈ (0, 1), if m ≥ C L 2 log(nL/δ) for some large enough constant C, then with probability at least 1 − δ, the following holds for all l ∈ [L],</p><p>where m = min{m 1 , . . . , m L }, and C is an absolute constant.</p><p>Proof of Lemma 5.1 Note that half of the entries of v are 1's and the other half of the entries are −1's. Therefore, without loss of generality, here we assume that</p><p>Clearly, we have E( y i ) = 0. Moreover, plugging in the value of v gives</p><p>Apparently, we have</p><p>for some absolute constant C 1 . Therefore by Hoeffding's inequality and Lemma A.1, with probability at least 1 − δ, it holds that</p><p>for all i = 1, . . . , n. Then substituting the above bound into the formula of loss function (y i y i ), we are able to complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 5.2</head><p>In order to prove Lemma 5.2, we require the following lemmas. We first establish the gradient lower bound at the initialization. Specifically, the following lemma gives a lower bound of gradient norm with respect to the weight matrix in the last hidden layer.</p><p>+ , there exist at least C m L φ/n nodes in {1, . . . , j, . . . , m L } that satisfy</p><p>where the last equality follows from the fact that conditioning on event E, for all i ∈ I, it holds that</p><p>We then consider two cases: u 1 &gt; 0 and u 1 &lt; 0, which occur equally likely conditioning on E. Therefore we have</p><p>max h(u</p><p>(1)</p><p>By the inequality max{ a 2 , b 2 } ≥ a − b 2 /2, we have</p><p>For any u</p><p>(1)</p><p>1 &gt; 0 and u</p><p>(2)</p><p>1 z 1 + Q u . We now proceed to give lower bound for h(w 1 ) − h(w 2 ) 2 . By (C.2), we have</p><p>where a i = a i σ u</p><p>(1)</p><p>Note that for all i ∈ I , we have y i = y 1 and z 1 , z i ≥ 1 − φ 2 /2 ≥ 0. Therefore, since u</p><p>(1)</p><p>1 , we have σ u</p><p>(1)</p><p>Therefore a i ≥ 0 for all i ∈ I and</p><p>We have shown that z i , z 1 ≥ 0 for all i ∈ I . Therefore we have</p><p>Since the inequality above holds for any u </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03962</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the convergence rate of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12065</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A convergence analysis of gradient descent for deep linear neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02281</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06509</idno>
		<title level="m">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient descent with identity initialization efficiently learns positive definite linear transformations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Globally optimal gradient descent for a convnet with gaussian inputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07966</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">When is a convolutional filter easy to learn?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06129</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03804</idno>
		<title level="m">Gradient descent finds global minima of deep neural networks</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<title level="m">Gradient descent provably optimizes over-parameterized neural networks</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00468</idno>
		<title level="m">Implicit bias of gradient descent on linear convolutional networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal function approximation by deep neural nets with bounded width and ReLU activations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02691</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Approximating continuous functions by ReLU nets of minimal width</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sellke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11278</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04231</idno>
		<title level="m">Identity matters in deep learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Ar</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">;</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Citeseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01204</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convergence analysis of two-layer neural networks with ReLU activation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09886</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04161</idno>
		<title level="m">Why deep neural networks for function approximation?</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Resnet with one-neuron hidden layers is a universal approximator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6172" to="6181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02540</idno>
		<title level="m">The expressive power of neural networks: A view from the width</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08101</idno>
		<title level="m">Representation benefits of deep feedforward networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04485</idno>
		<title level="m">Benefits of depth in neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00560</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.3027</idno>
		<title level="m">Introduction to the non-asymptotic analysis of random matrices</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Error bounds for approximations with deep ReLU networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optimal approximation of continuous functions by very deep ReLU networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03620</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning one-hidden-layer ReLU networks via gradient descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07808</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universality of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied and computational harmonic analysis</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08888</idno>
		<title level="m">Stochastic gradient descent optimizes over-parameterized deep relu networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Publisher&apos;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
