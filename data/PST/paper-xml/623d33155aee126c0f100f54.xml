<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting the Effects of Leakage on Dependency Parsing</title>
				<funder ref="#_RvqkA9F #_TWvNhYe #_3TKgdsy">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-24">24 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nathaniel</forename><surname>Krasner</surname></persName>
							<email>natekrasner@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miriam</forename><surname>Wanner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting the Effects of Leakage on Dependency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-24">24 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12815v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent work by</head> <ref type="bibr" target="#b10">S?gaard (2020)</ref> <p>showed that, treebank size aside, overlap between training and test graphs (termed leakage) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings. We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. 1 * Equal contribution. Work performed at GMU. 1 Code and data are available here: https://github.com/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic parsing has long been one of the core natural language processing (NLP) tasks, and the proliferation of the Universal Dependencies project (UD; de <ref type="bibr" target="#b1">Marneffe et al., 2021;</ref><ref type="bibr">Nivre et al., 2017)</ref> has allowed the development and comparison of monolingual and multilingual models under the same syntactic framework.</p><p>The performance of the dependency parsers, however, varies wildly across languages, with stateof-the-art performance ranging from labeled attachment scores below 20 (e.g. for Amharic, Erzya, Komi, or Yoruba) to more than 90 (e.g. for Spanish, Polish, Russian, or Greek). As the UD treebanks follow mostly similar annotation guidelines, comparisons of the parsing performance across languages are now possible, to an extent. 2  In an effort to explain these cross-lingual performance differences, researchers have proposed treebank size <ref type="bibr" target="#b11">(Vania et al., 2019)</ref>, linguistic variation <ref type="bibr" target="#b9">(Nivre et al., 2007)</ref>, test data sentence length miriamwanner/reu-nlp-project 2 Different treebank creation protocols followed across languages (whose effects are hard to isolate or measure) can be a significant source of variation. Nevertheless, some of the observed variation can be possibly explained by other factors. We direct the reader to footnote 2 of <ref type="bibr" target="#b10">(S?gaard, 2020)</ref>. or average gold dependency length <ref type="bibr" target="#b6">(McDonald and Nivre, 2011)</ref>, and domain differences between training and test data <ref type="bibr" target="#b3">(Foster et al., 2011)</ref>, as potential predictors. Recently, <ref type="bibr" target="#b10">S?gaard (2020)</ref> proposed that the proportion of isomorphic graph structures between the training and testing data (leakage) is a stronger predictor of the parsers' performance than any of the previously listed attributes other than training treebank size. <ref type="bibr" target="#b10">S?gaard (2020)</ref> concludes that "some languages seem easier to parse because their treebanks leak." This finding is potentially crucial for current parser evaluation on the existing treebanks, as well as for future treebank construction. It implies, for instance, that parsers are perhaps not as good as they seem, because they are tested on "leaky" test data. Perhaps one should also consider designing treebanks that do not leak between train and test, as such a test set would not have a bias toward more common phenomena.</p><p>In this work, we examine this finding more closely. We extend S?gaard's definition to include labeled leakage, and study it over multiple parsers in both monolingual and cross-lingual settings. We show that the finding does not hold up when tested against more modern parsers and more languages. We do identify, though, that leakage indeed predicts parser performance in zero-shot cross-lingual settings, and we dive deeper in this phenomenon with an extensive study focusing on Faroese and other Germanic languages. Last, we propose a modification of the leakage measure that both predicts and correlates with parser performance in such settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Leakage and How to Measure it</head><p>In this section we first define leakage based on graph isomorphisms and reproduce S?gaard's experiments. We then show that parsers make local decisions that allow them to generalize to unseen graphs, and explore additional measures of leakage, studying whether they help explain parser performance. Last, we argue that sub-trees are more meaningful units than label-free, tree-level representations.</p><p>Leakage Definition Leakage can be broadly defined as the portion of test trees that have isomorphic counterparts in the train set. While dependency trees are labeled, directed graphs with labels both on the nodes and on the edges, S?gaard (2020) performed a reduction by removing labels from both nodes and edges.</p><p>Given these reduced graphs, S?gaard (2020) finds the different isomorphisms that are present in the training and the test set, using the VF2 algorithm <ref type="bibr" target="#b0">(Cordella et al., 2001)</ref>. We note that the isomorphism may or may not rely on node or edge labels. In the experiments below, we perform an ablation between using completely unlabeled directed graphs, node-labeled (but not edge-labeled) directed graphs, and between using the full information of the graphs to compute isomorphisms, namely both node and edge labels.</p><p>Reproducing <ref type="bibr" target="#b10">(S?gaard, 2020)</ref> Examples of the reductions needed for computing leakage for two sentences are shown in Figure <ref type="figure" target="#fig_0">1</ref>. Now, assume that the first sentence is in the training set and the second is part of the test set. Measuring leakage without labels implies that the first dependency tree is somehow informative for producing the tree for the second sentence, which we believe is counterintuitive. Hence, our first hypothesis is that a more informed leakage calculation is going to explain more of the performance variance.</p><p>We reproduce the experiments of S?gaard (2020) comparing the three different reductions (denoted as "none" for unlabeled graphs, "edges" and "nodes+edges" for respectively labeled graphs). The experiment consists of correlating the factors ? assumed to influence syntactic dependency parser performance with the performance of the parser under study. We train a simple linear regression model<ref type="foot" target="#foot_0">3</ref> with treebank size and ? as input and parser performance as output. ? will correspond to our measure of treebank leakage. Mathematically, we have ?t s +??+? with t s treebank size and ?, ?, ? learned parameters. Following S?gaard, we will focus on explained variance and mean absolute error (MAE) from five-fold cross-validation to avoid overfitting. Unlike S?gaard, we will additionally report Spearman's ? correlation coefficients<ref type="foot" target="#foot_1">4</ref> between factor and performance, which will reveal whether indeed leakage leads to better parser performance. <ref type="foot" target="#foot_2">5</ref>The results on the same data as S?gaard (2020) (using the best reported parser performance from the CoNLL 2018 shared task) are presented in the top three rows of Table <ref type="table" target="#tab_0">1</ref>. We find that unlabeled graph leakage produces positive explained variance, in line with previous work. However, we have to reject our hypothesis, as a more informed leakage measurement fails to meaningfully explain the output variance, producing negative scores. In fact, the more information we use when computing the graph isomorphisms, the less the model can explain output variance! <ref type="foot" target="#foot_3">6</ref>To further solidify this finding, we repeat the above experiment, this time using UDify, the stateof-the-art multilingual parser of <ref type="bibr" target="#b5">Kondratyuk and Straka (2019)</ref>. <ref type="foot" target="#foot_4">7</ref> The result is shown in the bottom  ), and they present more negative evidence for our hypothesis: there is minimal explained variance in the unlabeled version, and still negative explained variance in the labeled leakage versions. Hence, we have to -for now-reject our hypothesis: using labeled graph isomorphisms to compute leakage does not explain more downstream parser performance variations, at least when using treelevel leakage measures; we revisit this hypothesis for sub-tree leakage below. Concurrently, we need to highlight the fact that for all cases we focused on this experiment, there was a negative (inverse) correlation between leakage and parser performance.</p><p>While <ref type="bibr" target="#b10">S?gaard (2020)</ref> was correct (for the languages/parsers they studied) to state that there is a correlation between leakage and parser performance, we believe they reached an incorrect conclusion. The metric they used (explained variance) does not reveal the direction of the correlation, just that there is a correlation. Because of this they came to the wrong conclusion that there was a positive correlation between leakage and parser performance. Our results (Table <ref type="table" target="#tab_0">1</ref>, left table) instead imply that as leakage increases, parser performance worsens! Clearly, something is wrong and we need to re-examine S?gaard's reasoning.</p><p>Sub-Trees are More Meaningful Units We turn our attention to the parsers whose performance we are trying to explain.</p><p>The three parsers that S?gaard uses and UDify are graph-based ones. This means that they do not necessarily score or produce whole trees. Graphbased parsers score pairs of words, and from these scores a minimum spanning tree is selected to produce the final dependency parse. As such, we argue that whole trees as a measure of leakage are not appropriate for graph-based parsers.</p><p>To drive this point forward, we perform synthetic experiments removing adjectival modifiers from nominal subject or object. In particular, we created training data in which the data either did not contain adjectival modifiers on subject/object nouns. We then tested the models on gold unmodified test data which contained such modifiers. By removing adjectival modifiers only from the subjects (similarly for objects) in the training data, we ensure two things: that test instances with adjectival modifiers on subjects are not leaky; as such, if whole-tree leakage is a proper indication for parser performance, then the parser should perform poorly in producing such constructions.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the results of our experiment over the German HDT treebank. <ref type="foot" target="#foot_5">8</ref> We found that the parsers trained on our counterfactual data, which have zero leakage for these test instances, still produce the local constructions that they have never observed during training. The parsers trained without training subject modifiers produced about half of the expected subject modifiers (similarly for object modifier experiments). Nevertheless, they were still able to generalize based on other similar constructions seen in training, correctly parsing a nonzero amount of unseen-in-training constructions.</p><p>This observation is not unexpected. In the experiment above, even removing all adjectival modifiers from nouns that are subjects (hence a subtree -and consequently a whole tree containing it-has never been observed), the parser still observes adjectivenoun modifying pairs elsewhere in the sentences and is able to generalize, producing a tree that has never been observed at training. The fact that the parsers make local output decisions, along with the proven corollary that they can easily produce unseen trees, guides us to search for a leakage measure focusing on sub-trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-Tree Based Leakage</head><p>We define a leakage measure where a dependency tree is first decomposed into a set of subtrees, and then each subtree reduces into the graphs defined above to compute isomorphisms. These subtrees are created for each node (word), connecting it to its parent and to all its children. See example in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>We repeat our experiments, this time using our proposed leakage measure, and present the results in the right-hand side of Table <ref type="table" target="#tab_0">1</ref>. As before, for S?gaard's and for the UDify combinations of models/languages the explained variance is negative. However, now more information (edge/node labels) leads to higher Spearman's ? coefficients, implying that indeed the more test subtrees we have observed in training, the better the parser performance.</p><p>In the unlabeled setting, every sub-tree created by the parser was found in the training data, which was true of most gold files as well. We interpret this observation to mean that unlabeled sub-trees are not meaningful units, a point further reinforced by the negative explained variance and correlations.</p><p>At the same time, our measure still fails to explain any of the observed performance variance. Thus, we have to reach a conclusion opposite of <ref type="bibr" target="#b10">S?gaard (2020)</ref>, that in a monolingual setting the performance of modern graph-based parsers is not particularly explained by train-test leakage, however we compute that leakage. Table <ref type="table">3</ref>: Leakage explains zero-shot parser performance. Sub-tree leakage also correlates with it.</p><p>3 Leakage Explains 0-Shot Performance</p><p>Modern dependency parsing models trained on many languages perform well on languages unseen (zero-shot setting) during training <ref type="bibr" target="#b7">(Muller et al., 2021;</ref><ref type="bibr">Glava? and Vuli?, 2021, inter alia)</ref>.</p><p>We focus again on UDify, since it performs well in zero-shot settings. This is generally attributed to two factors: the presence of related languages in the training set, and the multilingual capabilities of the underlying representation model (here, a multilingual BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>model).</head><p>Table <ref type="table">3</ref> reports results with S?gaard's (wholetree) and our sub-tree leakage measures under all three settings, focusing only on 35 zero-shot test languages.<ref type="foot" target="#foot_6">9</ref> Now leakage<ref type="foot" target="#foot_7">10</ref> can indeed explain downstream parser performance. Our proposed measure explains as much variance as the original whole-tree measure and also correlates with performance.</p><p>Analysis on Faroese Looking deeper into the zero-shot setting, we perform an experiment on a simplified bilingual zero-shot setting. We train parsers in five Germanic languages (German, Swedish, Danish, Norwegian, Icelandic) and test on Faroese in a zero-shot fashion. For each language, we train a model on:</p><p>(a) a 'leaky' sample of the portion of training treebank, so that all training data overlapped with (some) test data, (b) a 'non-leaky' sample of trees such that there was no train-test overlap, (c) a control random sample from the training treebank, and (d) a 'diverse' training sample including a single tree from each isomorphism equivalence class. All of the above models are size-controlled for each language, so that the training data sizes are exactly the same. Leakage here is measured with unlabeled full-tree leakage, for simplicity. We similarly split the test set, for each language, into 'leaky' and 'non-leaky' subsets (also reporting numbers for the whole test set).</p><p>For example, take German training and Faroese test sets. First all German instances leaking into Faroese are added to the "German leaky" train set and the corresponding leaked Faroese sentences are put into the "Faroese-leaky" test set. Then the remaining sentences from the German training set are added to the "German nonleaky" train set and the remaining sentences from the Faroese test set are the "Faroese nonleaky" test set. Last, we take a random sample of same size across all settings, so that training data size is not a confounding factor for our analysis.</p><p>See Figure <ref type="figure" target="#fig_2">3</ref> and extensive results in Appendix C. For all languages, models trained on leaky data perform worse than models trained on the same amount of non-leaky or random data. For most transfer languages, in fact, training solely on nonleaky data performs better than training on other subsets! In addition, the leaky part of the testing data is clearly easier to parse in general, while the non-leaky part is more challenging.</p><p>The models trained on perfectly 'diverse' treebanks generally perform just as good as those trained on non-leaky or randomly sampled data and often better on the non-leaky test set, which means they generalize better. This indicates a way to reach better cross-lingual performance without the need for large training data, as long as the training set is diverse enough.</p><p>The large performance difference between models trained on leaky and non-leaky trees reveals that something is different about the parts of the treebanks that leak. We measured the diversity of the leaky, non-leaky, and randomly selected trees, defined as the number of unique trees divided by the total number of trees. We found that leaky treebanks were far less diverse and therefore contain fewer unique structures than non-leaky or randomly sampled counterparts. Across all treebanks, leaky instances are also generally shorter (8.4 vs 21.6 avg length), shallower (2.2 vs 4.8 average tree depth), and with shorter avg dependency length (2 vs 3.3).</p><p>We argue that the reasoning should be reverse: short "easy" examples are more likely to leak; it is not leakage that makes them easy!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We re-evaluate claims that training-test leakage can explain parser performance, define a subtree-based leakage measure that better explains performance, and show that this claim only holds for zero-shot transfer settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Graph Reduction Examples</head><p>Shown in Table <ref type="table" target="#tab_3">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B English Counterfactual Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complete Faroese Results</head><p>Shown in Tables 5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Only labeled reductions produce different graphs for these fundamentally different sentences. Under unlabeled leakage, the two trees do leak. When taking labels into account the two trees belong to different isomorphisms and are not considered "leaky".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Decomposition of a tree (top) into a set of sub-trees (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Zero-shot results on Faroese. Training on non-leaky and diverse data is best. The leaky portion of the test set is far easier than the rest.</figDesc><graphic url="image-1.png" coords="5,93.55,70.87,408.19,229.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Only labeled reductions produce different graphs for these fundamentally different sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Tree-level leakage (left) does not correlate with and does not always explain parser performance. Labeled sub-tree level leakage (right) however is positively correlated with parser performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Tree-level leakage</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sub-tree-level leakage</cell></row><row><cell>Leakage</cell><cell cols="3">Regression Expl.</cell><cell></cell><cell>Spearman's</cell><cell>Leakage</cell><cell cols="2">Regression Expl.</cell><cell>Spearman's</cell></row><row><cell>Attributes</cell><cell></cell><cell cols="3">Score Variance MAE</cell><cell>?</cell><cell>Attributes</cell><cell cols="2">Score Variance MAE</cell><cell>?</cell></row><row><cell cols="4">System: CoNLL'18 (S?gaard)</cell><cell></cell><cell></cell><cell cols="3">System: CoNLL'18 (S?gaard)</cell></row><row><cell>None</cell><cell></cell><cell>0.162</cell><cell cols="2">0.143 7.257</cell><cell>-0.194</cell><cell>None</cell><cell>0.054</cell><cell>-0.137 8.248</cell><cell>-0.238</cell></row><row><cell>Edges</cell><cell></cell><cell>0.091</cell><cell cols="2">-0.121 8.592</cell><cell>-0.181</cell><cell>Edges</cell><cell>0.083</cell><cell>-1.202 9.444</cell><cell>0.390</cell></row><row><cell cols="2">Nodes+Edges</cell><cell>0.085</cell><cell cols="2">-0.179 8.830</cell><cell>-0.161</cell><cell>Nodes+Edges</cell><cell>0.089</cell><cell>-0.210 8.197</cell><cell>0.538</cell></row><row><cell cols="2">System: UDify</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">System: UDify</cell></row><row><cell>None</cell><cell></cell><cell>0.250</cell><cell cols="2">0.047 13.07</cell><cell>-0.360</cell><cell>None</cell><cell>0.123</cell><cell>-0.171 14.632 -0.082</cell></row><row><cell>Edges</cell><cell></cell><cell>0.146</cell><cell cols="3">-0.083 14.012 -0.080</cell><cell>Edges</cell><cell>0.174</cell><cell>-0.333 14.182 0.579</cell></row><row><cell cols="2">Nodes+Edges</cell><cell>0.134</cell><cell cols="3">-0.108 14.156 -0.026</cell><cell>Nodes+Edges</cell><cell>0.217</cell><cell>-0.149 13.715 0.654</cell></row><row><cell></cell><cell></cell><cell cols="4">Produced by model trained without</cell><cell></cell><cell></cell></row><row><cell cols="4">Construction Actual nsubj mods</cell><cell cols="2">obj mods</cell><cell></cell><cell></cell></row><row><cell>nsubj mods</cell><cell>3166</cell><cell>1698</cell><cell></cell><cell></cell><cell>3167</cell><cell></cell><cell></cell></row><row><cell>obj mods</cell><cell>3910</cell><cell>4505</cell><cell></cell><cell></cell><cell>1940</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of adjectival modifiers produced by counterfactual models. The parsers can produce constructions not seen during training.</figDesc><table /><note><p>three rows of Table 1 (left Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Shown in Table 4. Number of adjectival modifiers produced by counterfactual models compared to the actual number in the gold file. The parsers can produce constructions not seen during training.</figDesc><table><row><cell></cell><cell cols="3">Produced by model trained without</cell></row><row><cell cols="3">Construction Actual nsubj mods</cell><cell>obj mods</cell></row><row><cell cols="2">German HDT treebank:</cell><cell></cell><cell></cell></row><row><cell>nsubj mods</cell><cell>3166</cell><cell>1698</cell><cell>3167</cell></row><row><cell>obj mods</cell><cell>3910</cell><cell>4505</cell><cell>1940</cell></row><row><cell cols="2">English EWT treebank:</cell><cell></cell><cell></cell></row><row><cell>nsubj mods</cell><cell>132</cell><cell>122</cell><cell>130</cell></row><row><cell>obj mods</cell><cell>254</cell><cell>258</cell><cell>237</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Exactly as<ref type="bibr" target="#b10">S?gaard (2020)</ref> does, just on different data/settings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We do not expect the correlation, if any, to be linear, hence we prefer Spearman's measure to Pearson's.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Note that the explained variance is basically the correlation squared. As such, it cannot reveal whether the correlation is positive or negative. Negative explained variance means that the model is a poor fit for the data (worse than just predicting the average).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>  6 S?gaard (2020)  gives this possible explanation: "The result is perhaps not too surprising, since graph isomorphisms correlate with syntactic constructions, which in turn correlate with the occurrence of linguistic markers and tail linguistic phenomena</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>."7  The model is trained jointly on all UD treebanks (that have a training set), and hence in this experiment we compute leakage multilingually (i.e. we compute leakage between the complete training set and the test set of each treebank).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>Also see results on English in Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>These 35 languages are treebanks with size 0 in<ref type="bibr" target="#b5">Kondratyuk and Straka (2019)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>  10  In this multilingual setting, leakage is computed against a training set comprised of 75 concatenated treebanks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors express their gratitude to <rs type="person">Djam? Seddah</rs> for very helpful initial discussions, and to the reviewers for their constructive feedback. The first two authors were supported in part by the <rs type="funder">NSF</rs> <rs type="grantName">IIS-1757064 Undergraduate Research in Educational Data Mining grant</rs>. <rs type="person">Antonios Anastasopoulos</rs> is also generously supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">BCS-2109578</rs> and <rs type="grantNumber">IIS-2125466</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RvqkA9F">
					<orgName type="grant-name">IIS-1757064 Undergraduate Research in Educational Data Mining grant</orgName>
				</org>
				<org type="funding" xml:id="_TWvNhYe">
					<idno type="grant-number">BCS-2109578</idno>
				</org>
				<org type="funding" xml:id="_3TKgdsy">
					<idno type="grant-number">IIS-2125466</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An improved algorithm for matching large graphs</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Pietro Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal dependencies</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="308" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From news to comment: Resources and benchmarks for parsing the language of web 2.0</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?zlem</forename><surname>?etinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="893" to="901" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Climbing the tower of treebanks: Improving low-resource dependency parsing via hierarchical source selection</title>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.431</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4878" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">75 languages, 1 model: Parsing universal dependencies universally</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2779" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing and integrating dependency parsers</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00039</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="197" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="448" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?eljko</forename><surname>Agi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Liesbeth Augustinus. et al. 2017. Universal dependencies 2.1</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mc-Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some languages seem easier to parse because their treebanks leak</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2765" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
