<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual interpretability for deep learning: a survey *</title>
				<funder ref="#_SG2dmRa">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_QWgUSXx">
					<orgName type="full">DARPA XAI Award</orgName>
				</funder>
				<funder ref="#_aTaK3r3">
					<orgName type="full">NSF IIS</orgName>
				</funder>
				<funder ref="#_jMUvjK2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Quan-Shi</forename><surname>Zhang</surname></persName>
							<email>zhangqs@ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Frontiers of Information Technology &amp; Electronic Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual interpretability for deep learning: a survey *</title>
					</analytic>
					<monogr>
						<idno type="ISSN">2095-9184 (print)</idno>
					</monogr>
					<idno type="DOI">10.1631/FITEE.1700808</idno>
					<note type="submission">Received Dec. 2, 2017; Revision accepted Jan.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial intelligence</term>
					<term>Deep learning</term>
					<term>Interpretable model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr">(LeCun et al., 1998a;</ref><ref type="bibr" target="#b12">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b7">He et al., 2016;</ref><ref type="bibr" target="#b9">Huang et al., 2017)</ref> have achieved superior performance in many visual tasks, such as object classification and detection. However, the end-to-end learning strategy makes CNN representations a black box. Except for the final network output, it is difficult to understand the logic of CNN predictions hidden inside the network. In recent years, a growing number of researchers have realized that high model interpretability is of significant value in both theory and practice, and have developed models with interpretable knowledge representations.</p><p>1. Visualization of CNN representations in intermediate network layers. These methods either synthesize mainly the image that maximizes the score of a given unit in a pre-trained CNN, or invert feature maps of a conv-layer back to the input image. Please see Section 2 for detailed discussions.</p><p>2. Diagnosis of CNN representations. Related studies may either diagnose a CNN's feature space for different object categories or discover potential representation flaws in conv-layers. Please see Section 3 for details.</p><p>3. Disentanglement of 'the mixture of patterns' encoded in each filter of CNNs. These studies disentangle mainly complex representations in conv-layers and transform network representations into interpretable graphs. Please see Section 4 for details.</p><p>4. Building explainable models. We discuss interpretable CNNs <ref type="bibr">(Zhang et al., 2018d)</ref>, capsule networks <ref type="bibr" target="#b25">(Sabour et al., 2017)</ref>, interpretable R-CNNs <ref type="bibr" target="#b34">(Wu et al., 2017)</ref>, and InfoGAN <ref type="bibr" target="#b3">(Chen et al., 2016)</ref> in Section 5.</p><p>5. Semantic-level middle-to-end learning via human-computer interaction. A clear semantic disentanglement of CNN representations may further enable 'middle-to-end' learning of neural networks with a weak supervision. Section 7 introduces methods to learn new models via human-computer interactions <ref type="bibr">(Zhang et al., 2017b)</ref> and active questionanswering with a limited human supervision <ref type="bibr">(Zhang et al., 2017a)</ref>. Among all the above, the visualization of CNN representations is the most direct way to explore network representations. The network visualization also provides a technical foundation for many approaches to diagnosing CNN representations. The disentanglement of feature representations of a pre-trained CNN and the learning of explainable network representations present more challenges to the state-ofthe-art algorithms. Finally, explainable or disentangled network representations are also the starting point for weakly-supervised middle-to-end learning.</p><p>The clear semantics in high conv-layers can help people trust a network's prediction. As discussed in <ref type="bibr">Zhang et al. (2018a)</ref>, considering dataset and representation bias, a high accuracy on testing images still cannot ensure that a CNN will encode correct representations. For example, a CNN may use an unreliable context-eye features-to identify the 'lipstick' attribute of a face image. Therefore, people usually cannot fully trust a network unless a CNN can semantically or visually explain its logic, e.g., what patterns are used for prediction.</p><p>In addition, the middle-to-end learning or debugging of neural networks based on the explainable or disentangled network representations may significantly reduce the requirements for human annotation. Furthermore, based on semantic representations of networks, it is possible to merge multiple CNNs into a universal network (i.e., a network encoding generic knowledge representations for different tasks) at the semantic level in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visualization of convolutional neural network representations</head><p>Visualization of filters in a CNN is the most direct way to explore visual patterns hidden inside a neural unit. Different types of visualization methods have been developed for network visualization.</p><p>First, gradient-based methods <ref type="bibr" target="#b27">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b37">Zeiler and Fergus, 2014;</ref><ref type="bibr" target="#b19">Mahendran and Vedaldi, 2015;</ref><ref type="bibr" target="#b28">Springenberg et al., 2015)</ref> are the mainstream of network visualization. These methods compute mainly gradients of the score of a given CNN unit w.r.t. the input image. They use the gradients to estimate the image appearance that maximizes the unit score. <ref type="bibr" target="#b22">Olah et al. (2017)</ref> provided a toolbox of existing techniques to visualize patterns encoded in different conv-layers of a pre-trained CNN.</p><p>Second, the up-convolutional net <ref type="bibr" target="#b4">(Dosovitskiy and Brox, 2016)</ref> is another typical technique to visualize CNN representations. The up-convolutional net inverts CNN feature maps to images. We can regard the up-convolutional net as a tool that indirectly illustrates the image appearance corresponding to a feature map, although compared to gradientbased methods, the up-convolutional net cannot ensure mathematically that the visualization results exactly reflect actual representations in CNN. Similarly, <ref type="bibr" target="#b21">Nguyen et al. (2017)</ref> further introduced an additional prior, which controls the semantic meaning of the synthesized image, to the adversarial generative network. We can use CNN feature maps as the prior for visualization.</p><p>In addition, <ref type="bibr" target="#b45">Zhou et al. (2015)</ref> proposed a method to accurately compute the image-resolution receptive field of neural activations in a feature map. The actual receptive field of neural activation is smaller than the theoretical receptive field computed using the filter size. The accurate estimation of the receptive field helps people understand the representation of a filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diagnosis of convolutional neural network representations</head><p>Some methods have went beyond the visualization of CNNs and diagnosed CNN representations to obtain insight understanding of features encoded in a CNN. We roughly divide all relevant research into the following five directions:</p><p>1. Studies in the first direction analyze CNN features from a global view. <ref type="bibr" target="#b30">Szegedy et al. (2014)</ref> explored semantic meanings of each filter. <ref type="bibr" target="#b36">Yosinski et al. (2014)</ref> analyzed the transferability of filter representations in intermediate conv-layers. <ref type="bibr" target="#b0">Aubry and Russell (2015)</ref> and <ref type="bibr" target="#b18">Lu (2015)</ref> computed feature distributions of different categories/attributes in the feature space of a pre-trained CNN.</p><p>2. The second research direction extracts image regions that directly contribute to the network output for a label/attribute to explain CNN representations of the label/attribute. This is similar to the visualization of CNNs. <ref type="bibr" target="#b5">Fong and Vedaldi (2017)</ref> and <ref type="bibr" target="#b26">Selvaraju et al. (2017)</ref> proposed methods to propagate gradients of feature maps w.r.t. the final loss back to the image plane to estimate the image regions. The LIME model proposed by <ref type="bibr" target="#b24">Ribeiro et al. (2016)</ref> extracts image regions that are highly sensitive to the network output. <ref type="bibr" target="#b46">Zintgraf et al. (2017)</ref>, <ref type="bibr" target="#b10">Kindermans et al. (2017)</ref>, and <ref type="bibr" target="#b13">Kumar et al. (2017)</ref> invented methods to visualize areas in the input image that contribute the most to the decision-making process of CNN. <ref type="bibr" target="#b31">Wang et al. (2017)</ref> and <ref type="bibr" target="#b6">Goyal et al. (2016)</ref> tried to interpret the logic for visual questionanswering encoded in neural networks. These studies have listed important objects (or regions of interests) detected from the images and crucial words in questions as the explanation of output answers.</p><p>3. The estimation of vulnerable points in the feature space of a CNN is also a popular direction for diagnosing network representations. Approaches proposed by <ref type="bibr" target="#b29">Su et al. (2017)</ref>, <ref type="bibr" target="#b11">Koh and</ref><ref type="bibr" target="#b11">Liang (2017), and</ref><ref type="bibr" target="#b30">Szegedy et al. (2014)</ref> were developed to compute adversarial samples for a CNN; i.e., these studies aim to estimate the minimum noisy perturbation of the input image that can change the final prediction. In particular, influence functions proposed by <ref type="bibr" target="#b11">Koh and Liang (2017)</ref> can be used to compute adversarial samples. The influence function can also provide plausible ways to create training samples to attack the learning of CNNs, fix the training set, and further debug representations of a CNN.</p><p>4. The fourth research direction is to refine network representations based on the analysis of network feature spaces. Given a CNN pretrained for object classification, <ref type="bibr" target="#b14">Lakkaraju et al. (2017)</ref> proposed a method to discover knowledge blind spots (unknown patterns) of CNN in a weakly-supervised manner. This method groups all sample points in the entire feature space of a CNN into thousands of pseudo-categories. It assumes that a well-learned CNN would use the sub-space of each pseudo-category to exclusively represent a subset of a specific object class. In this way, this study randomly showed object samples within each sub-space, and used the sample purity in the sub-space to discover potential representation flaws hidden in a pretrained CNN. To distill representations of a teacher network to a student network for sentiment analysis, <ref type="bibr" target="#b8">Hu et al. (2016)</ref> proposed a method of using logic rules of natural languages (e.g., I-ORG cannot follow B-PER) to construct a distillation loss to supervise the knowledge distillation of neural networks, to obtain more meaningful network representations.</p><p>5. Finally, <ref type="bibr">Zhang et al. (2018a)</ref> presented a method to discover potential, biased representations of a CNN. Fig. <ref type="figure">1</ref> shows biased representations of a CNN trained to estimate face attributes. When an attribute usually co-appears with specific visual features in training images, CNN may use such coappearing features to represent the attribute. When the co-appearing features used are not semantically related to the target attribute, these features can be considered as biased representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wearing lipstick</head><p>Fig. <ref type="figure">1</ref> Biased representations in a convolutional neural network <ref type="bibr">(Zhang et al., 2018a)</ref> Considering potential dataset bias, a high accuracy on testing images cannot always ensure that a convolutional neural network (CNN) learns correct representations. CNN may use unreliable co-appearing contexts to make predictions. For example, people may modify mouth appearances of two faces manually by masking mouth regions or pasting another mouth; however, such modifications do not significantly change prediction scores for the 'lipstick' attribute. Fig. <ref type="figure">1</ref> shows the heat maps of inference patterns of the 'lipstick' attribute, where red/blue patterns are positive/negative with the attribute score. CNN mistakenly considers unrelated patterns as contexts to infer the lipstick. References to color refer to the online version of this figure</p><p>Given a pre-trained CNN (e.g., a CNN that was trained to estimate face attributes), <ref type="bibr">Zhang et al. (2018a)</ref> required people annotate some ground-truth relationships between attributes; e.g., the 'lipstick' attribute is positively related to the 'heavy-makeup' attribute, and is not related to the 'black hair' attribute. Then, the method mines inference patterns of each attribute output from conv-layers, and uses inference patterns to compute actual attribute relationships encoded in CNN. Conflicts between the ground-truth and the mined attribute relationships indicate biased representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Disentangling convolutional neural network representations into explanatory graphs and decision trees</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disentangling convolutional neural network representations into explanatory graphs</head><p>Compared with the visualization and diagnosis of network representations in Sections 2 and 3, disentangling CNN features into humaninterpretable graphical representations (namely 'explanatory graphs') provides a more thorough explanation of network representations. <ref type="bibr" target="#b38">Zhang et al. (2016</ref><ref type="bibr">Zhang et al. ( , 2018b) )</ref> proposed disentangling features in conv-layers of a pre-trained CNN and used a graphical model to represent the semantic hierarchy hidden inside a CNN.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, each filter in a high convlayer of a CNN usually represents a mixture of patterns. For example, the filter may be activated by both the head and tail parts of an object. Thus, to provide a global view of how visual knowledge is organized in a pre-trained CNN, <ref type="bibr" target="#b38">Zhang et al. (2016</ref><ref type="bibr">Zhang et al. ( , 2018b</ref>) aimed to answer the following three questions:</p><p>1. How many types of visual patterns are memorized by each convolutional filter of CNN (here, a visual pattern may describe a specific object part or a certain texture)?</p><p>2. Which patterns are co-activated to describe an object part?</p><p>3. What is the spatial relationship between two co-activated patterns?</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the explanatory graph explains the knowledge semantic hidden inside CNN. The explanatory graph disentangles the mixture of part patterns in each filter's feature map of a convlayer, and uses each graph node to represent a part:</p><p>1. The explanatory graph has multiple layers. Each graph layer corresponds to a specific conv-layer of a CNN. 2. Each node in the explanatory graph consistently represents the same object part through different images. We can use the node to localize the corresponding part on the input image. To some extent, the node is robust to shape deformation and pose variations.</p><p>3. Each edge encodes the co-activation and spatial relationships between two nodes in adjacent layers.</p><p>4. We can regard an explanatory graph as a compression of feature maps of conv-layers. A CNN has multiple conv-layers. Each conv-layer may have hundreds of filters, and each filter may produce a feature map with hundreds of neural units. We can use tens of thousands of nodes in the explanatory graph to represent information contained in all tens of millions of neural units in these feature maps, i.e., by which part patterns the feature maps are activated, and where the part patterns are localized in input images.</p><p>5. Just like a dictionary, each input image can trigger only a small subset of part patterns (nodes) in the explanatory graph. Each node describes a common part pattern with a high transferability, which is shared by hundreds or thousands of training images.</p><p>Fig. <ref type="figure">4</ref> lists top-ranked image patches corresponding to different nodes in the explanatory graph. Fig. <ref type="figure">5</ref> visualizes the spatial distribution of object parts inferred by the top 50% nodes in the L th layer of the explanatory graph with the highest inference scores. Fig. <ref type="figure">6</ref> shows object parts inferred by a single node. There are many potential applications based on the explanatory graph. For example, we can regard the explanatory graph as a visual dictionary of a category and transfer graph nodes to other applications, such as multi-shot part localization.</p><p>Given a few bounding boxes of an object part, <ref type="bibr">Zhang et al. (2018b)</ref> proposed a method of retrieving hundreds of nodes that are related to part annotations from the explanatory graph, and then using the retrieved nodes to localize object parts in previously unseen images. Because each node in the explanatory graph encodes a part pattern shared by numerous training images, the retrieved nodes describe a general appearance of the target part without being over-fitted to the limited annotations of part bounding boxes. Given three annotations for each object part, the explanatory-graph-based method exhibits superior performance of part localization and decreases by about 1/3 localization errors w.r.t. the second best baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Disentangling convolutional neural network representations into decision trees</head><p>Zhang et al. ( <ref type="formula">2018c</ref>) further proposed a decision tree to encode decision modes in fully connected layers. The decision tree is not designed for classification. Instead, it is used to quantitatively explain the logic for each CNN prediction; i.e., given an input image, we use CNN to make a prediction. The decision tree tells people which filters in a conv-layer are used for the prediction and how much they contribute to the prediction.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">7</ref>, the method mines potential decision modes memorized in fully connected layers. The decision tree organizes these potential decision modes in a coarse-to-fine manner. Furthermore, this study uses the method proposed by Zhang et al. 5 Learning neural networks with interpretable/disentangled representations Almost all methods mentioned in Sections 2-4 focus on the understanding of a pre-trained network. In this section, we review studies of learning disentangled representations of neural networks, where representations in middle layers are no longer a black box but have clear semantic meanings. Compared with the understanding of pre-trained networks, learning networks with disentangled representations present more challenges. Up to now, only a few studies have been published in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interpretable convolutional neural networks</head><p>As shown in Fig. <ref type="figure">8</ref>, <ref type="bibr">Zhang et al. (2018d)</ref> developed a method to modify an ordinary CNN to obtain disentangled representations in high conv-layers by adding a loss to each filter in the conv-layers. The loss is used to regularize the feature map towards the representation of a specific object part.</p><p>Note that people do not need to annotate any object parts or textures to supervise the learning of interpretable CNNs. Instead, the loss automatically assigns an object part to each filter during the endto-end learning process. As shown in Fig. <ref type="figure" target="#fig_6">9</ref>, this method designs some templates. is a matrix with the same size of feature map. T ?i describes the ideal distribution of activations for the feature map when the target part triggers mainly the i th unit in the feature map.</p><p>Given the joint probability of fitting a feature map to a template, the loss of a filter is formulated as the mutual information between the feature map and the templates. This loss encourages a low entropy of inter-category activations; i.e., each filter in the conv-layer is assigned to a certain category. If the input image belongs to the target category, then the loss expects the filter's feature map to match a template well; otherwise, the filter needs to remain inactivated. In addition, the loss encourages a low entropy of spatial distributions of neural activations; i.e., when the input image belongs to the target category, the feature map is supposed to exclusively fit a single template. In other words, the filter needs to activate a single location on the feature map. <ref type="bibr">Zhang et al. (2018d)</ref> assumed that if a filter repetitively activates various feature-map regions, then this filter is more likely to describe low-level textures (e.g., colors and edges) instead of high-level parts. For example, the left eye and the right eye may be represented by different filters, because contexts of the two eyes are symmetric, but not the same.</p><p>Fig. <ref type="figure" target="#fig_7">10</ref> shows feature maps produced with different filters of an interpretable CNN. Each filter consistently represents the same object part through various images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interpretable region-based convolutional neural networks</head><p>Wu et al. ( <ref type="formula">2017</ref>) proposed the learning of qualitatively interpretable models for object detection based on the region-based convolutional neural network (R-CNN) to unfold latent configurations of object parts automatically during the object-detection process. This method is learned without using any part annotations for supervision. Wu et al. ( <ref type="formula">2017</ref>) used a top-down hierarchical and compositional grammar, namely an 'And-Or graph (AOG)', to model latent configurations of object parts. This method uses an AOG-based parsing operator to substitute for the RoI-Pooling operator used in R-CNN. The AOG-based parsing harnesses explainable compositional structures of objects and maintains the discrimination power of an R-CNN. This idea is related to the disentanglement of the local, bottom-up, and top-down information components for prediction <ref type="bibr" target="#b33">(Wu et al., 2007;</ref><ref type="bibr" target="#b35">Yang et al., 2009;</ref><ref type="bibr" target="#b32">Wu and Zhu, 2011)</ref>.</p><p>During the detection process, a bounding box is interpreted as the best parse tree derived from AOG on the fly. During the learning process, a foldingunfolding method is used to train AOG and R-CNN in an end-to-end manner.</p><p>Fig. <ref type="figure">11</ref> illustrates an example of object detection proposed by <ref type="bibr">Zhang et al. (2018d)</ref>. This method detects object bounding boxes. It also determines the latent parse tree and part configurations of objects as the qualitatively extractive rationale in detection. <ref type="bibr">et al. (2017)</ref> designed novel neural units, namely 'capsules', to substitute for traditional neural units to construct a capsule network. Each capsule outputs an activity vector instead of a scalar. The length of the activity vector represents the activation strength of the capsule, and the orientation of the activity vector encodes instantiation parameters. In addition to predicted bounding boxes, the method outputs the latent parse tree and part configurations as the qualitatively extractive rationale in detection. The parse trees are inferred on the fly in the space of latent structures, which follow a top-down compositional grammar of an And-Or graph (AOG)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Capsule networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sabour</head><p>Active capsules in the lower layer send messages to capsules in the adjacent higher layer. This method uses an iterative routing-by-agreement mechanism to assign higher weights with the low-layer capsules whose outputs better fit the instantiation parameters of the high-layer capsule.</p><p>Experiments showed that when people train capsule networks using the MNIST dataset <ref type="bibr">(LeCun et al., 1998b)</ref>, a capsule encoded a specific semantic concept. Different dimensions of the activity vector of a capsule controlled different features, including (1) scale and thickness, (2) localized part, (3) stroke thickness, (4) localized skew, and ( <ref type="formula">5</ref>) width and translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Information maximizing generative adversarial nets</head><p>The information maximizing generative adversarial net <ref type="bibr" target="#b3">(Chen et al., 2016)</ref>, namely 'InfoGAN', is an extension of the generative adversarial network. InfoGAN maximizes the mutual information between certain dimensions of the latent representation and the image observation. InfoGAN separates input variables of the generator into two types, i.e., incompressible noise z and latent code c. This study aims to learn latent code c to encode certain semantic concepts in an unsupervised manner.</p><p>InfoGAN was trained using the MNIST dataset <ref type="bibr">(LeCun et al., 1998b)</ref>, the CelebA dataset <ref type="bibr" target="#b17">(Liu et al., 2015)</ref>, the SVHN dataset <ref type="bibr" target="#b20">(Netzer et al., 2011)</ref>, the 3D face dataset <ref type="bibr" target="#b23">(Paysan et al., 2009)</ref>, and the 3D chair dataset <ref type="bibr" target="#b1">(Aubry et al., 2014)</ref>. Experiments have shown that the latent code successfully encodes the digit type, rotation, and width of digits in the MNIST dataset, the lighting condition and plate context in the SVHN dataset, the azimuth, existence of glasses, hairstyle, emotion in the CelebA dataset, and width and 3D rotation in the 3D face and chair datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation metrics for network interpretability</head><p>Evaluation metrics for model interpretability are crucial for the development of explainable models. This is because unlike traditional well-defined visual applications (e.g., object detection and segmentation), network interpretability is more difficult to define and evaluate. The evaluation metric of network interpretability can help people define the concept of network interpretability and guide the development of learning interpretable network representations. Up to now, only a few studies have discussed the evaluation of network interpretability. Proposing a promising evaluation metric is still a big challenge to state-of-the-art algorithms. In this section, we simply introduce two latest evaluation metrics for the interpretability of CNN filters, i.e., the filter interpretability proposed by <ref type="bibr" target="#b2">Bau et al. (2017)</ref> and the location instability proposed by <ref type="bibr">Zhang et al. (2018b)</ref>. <ref type="formula">2017</ref>) defined six types of semantics for CNN filters, i.e., 'objects', 'parts', 'scenes', 'textures', 'materials', and 'colors'. The evaluation of filter interpretability requires people annotate these six types of semantics on testing images at the pixel level. The evaluation metric measures the fitness between the image-resolution receptive field of a filter's neural activations (The method propagates the receptive field of each activated unit in a filter's feature map back to the image plane as the image-resolution receptive field of a filter) and the pixel-level semantic annotations on the image. For example, if the receptive field of a filter's neural activations usually overlaps highly with ground-truth image regions of a specific semantic concept through different images, then we can consider that the filter represents this semantic concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Filter interpretability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bau et al. (</head><p>For each filter f , this method computes its feature maps X = {x = f (I)|I ? I} on different testing images. Then, the distribution of activation scores in all positions of all feature maps is computed. <ref type="bibr" target="#b2">Bau et al. (2017)</ref> set an activation threshold T f such that p(x ij &gt; T f ) = 0.005, to select top activations from all spatial locations [i, j]'s of all feature maps x ? X as valid map regions corresponding to f 's semantics. Then, the method scales up low-resolution valid map regions to the image resolution, thereby obtaining the receptive field of valid activations on each image. We use S I f to denote the receptive field of f 's valid activations w.r.t. image I.</p><p>The compatibility between a filter f and a specific semantic concept is reported as an intersectionover-union (IoU) score</p><formula xml:id="formula_0">IoU I f,k = S I f ? S I k S I f ? S I k</formula><p>, where S I k denotes the ground-truth mask of the k th semantic concept on image I.</p><p>Given an image I, filter f is associated with the k th concept if IoU I f,k &gt; 0.04. The probability of the k th concept being associated with filter f is given as P f,k = mean I:with k th concept 1(IoU I f,k &gt; 0.04). Thus, we can use P f,k to evaluate the filter interpretability of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Location instability</head><p>Another evaluation metric is location instability. This metric was proposed by <ref type="bibr">Zhang et al. (2018b)</ref> to evaluate the fitness between a CNN filter and the representation of an object part. Given an input image I, CNN computes a feature map x ? R N ?N of filter f . We can regard unit x i,j (1 ? i, j ? N ) with the highest activation as the location inference of f , where N ? N is the size of the feature map. We use p to denote the image position that corresponds to the inferred feature map location (i, j), i.e., the center of unit x i,j 's receptive field when we backward propagated the receptive field to the image plane. The evaluation assumes that if f consistently represents the same object part (the object part may not have an explicit name according to people's cognition) through different objects, then distances between the image position p and some object landmarks should not change much among different objects. For example, if filter f represents the shoulder, then the distance between the shoulder and the head should remain stable through different objects.</p><p>Therefore, people can compute the deviation of the distance between the inferred position p and a specific ground-truth landmark among different images. The average deviation w.r.t. various landmarks can be used to evaluate the location instability of f . As shown in Fig. <ref type="figure" target="#fig_2">12</ref> Based on studies discussed in Sections 4 and 5, people may either disentangle representations of a pre-trained CNN or learn a new network with interpretable, disentangled representations. Such interpretable/disentangled network representations can further enable middle-to-end model learning at the semantic level without strong supervision. We briefly review two typical studies <ref type="bibr">(Zhang et al., 2017a,b)</ref> of middle-to-end learning as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Active question-answering for learning And-Or graphs</head><p>Based on the semantic And-Or representation proposed by <ref type="bibr" target="#b38">Zhang et al. (2016)</ref>, <ref type="bibr">Zhang et al. (2017a)</ref> developed a method to use active question-answering to semanticize neural patterns in conv-layers of a pre-trained CNN and built a model for hierarchical object understanding.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">13</ref>, CNN is pre-trained for object classification. The method aims to extract a four-layer interpretable AOG to explain the semantic hierarchy hidden in a CNN. The AOG encodes fourlayer semantics, ranging across the 'semantic part' (OR node), 'part templates' (AND nodes), 'latent patterns' (OR nodes), and 'neural units' (terminal nodes) on feature maps. In AOG, AND nodes represent compositional regions of a part, and OR nodes encode a list of alternative template/deformation candidates for a local part. The top part node (OR node) uses its children to represent some template candidates for the part. Each part template in the second layer (AND node) uses children latent patterns to represent its constituent regions. Each latent pattern in the third layer (OR node) naturally corresponds to a certain range of units within the feature map of a filter. The latent pattern selects a unit within this range to account for its geometric deformation. To learn an AOG, Zhang et al. (2017a) allowed the computer to actively identify and ask about objects, whose neural patterns cannot be explained by the current AOG. As shown in Fig. <ref type="figure">14</ref>, in each step of the active question-answering, the current AOG is used to localize object parts among all the unannotated images. The method actively selects objects that cannot well fit AOG, namely 'unexplained objects'. The method predicts the potential gain of asking about each unexplained object, and thus determines the best sequence of questions (e.g., asking about template types and bounding boxes of unexplained object parts). In this way, the method uses the answers to either refine an existing part template or mine latent patterns for new object-part templates, to grow AOG branches. Specifically, the method uses part annotations on a few (e.g., three) object images for supervision. Given a bounding-box annotation of a part, the proposed method first uses the method proposed by <ref type="bibr" target="#b38">Zhang et al. (2016)</ref> to mine latent patterns, which are related to the annotated part, from conv-layers of CNN. An AOG is used to organize all mined patterns as the representation of the target part. The method visualizes the mined latent patterns and asks people to remove latent patterns unrelated to the target part interactively. In this way, people can simply prune incorrect latent patterns from AOG branches to refine AOG. End-to-end learning of interpretable neural networks, whose intermediate layers encode comprehensible patterns, is also a prospective trend. Interpretable CNNs have been developed, where each filter in high conv-layers represents a specific object part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terminals</head><p>Furthermore, based on interpretable representations of CNN patterns, semantic-level middle-toend learning was proposed to speed up the learning process. Compared with traditional end-to-end learning, middle-to-end learning allows human interactions to guide the learning process and can be applied with a few annotations for supervision.</p><p>In the future, we believe that the middle-to-end learning will continuously be a fundamental research direction. In addition, based on the semantic hierarchy of an interpretable network, debugging CNN representations at the semantic level will create new visual applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig.2Feature maps of a filter obtained using different input images(Zhang et al., 2018b)   To visualize the feature map, the method propagates receptive fields of activated units in the feature map back to the image plane. In each sub-feature, the filter is activated by various part patterns in an image. This makes it difficult to understand the semantic meaning of a filter. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Explanatory graph (Zhang et al., 2018b) An explanatory graph represents the knowledge hierarchy hidden in conv-layers of a CNN. Each filter in a pre-trained CNN may be activated by different object parts. Zhang et al. (2018b) disentangles part patterns from each filter in an unsupervised manner, thereby clarifying the knowledge representation. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 Fig. 5 Fig. 6</head><label>456</label><figDesc>Fig.4Image patches corresponding to different nodes in the explanatory graph(Zhang et al., 2018b)    References to color refer to the online version of this figureL=1 L=2 L=3 L=4 L=1 L=2 L=3 L=4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Fig.7Decision tree that explains a convolutional neural network (CNN) prediction at the semantic level(Zhang et al., 2018c)    A CNN is learned for object classification with disentangled representations in the top conv-layer, where each filter represents a specific object part. The decision tree encodes various decision modes hidden inside fully connected layers of CNN in a coarse-to-fine manner. Given an input image, the decision tree infers a parse tree (red lines) to quantitatively analyze rationales for the CNN prediction, i.e., which object parts (or filters) are used for prediction and how much an object part (or filter) contributes to the prediction. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Fig. 8 Structures of an ordinary conv-layer and an interpretable conv-layer (Zhang et al., 2018d) Green and red lines indicate the forward and backward propagations, respectively. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10</head><label>10</label><figDesc>Fig.10Visualization of interpretable filters in the top conv-layer(Zhang et al., 2018d)    We used<ref type="bibr" target="#b45">Zhou et al. (2015)</ref> to estimate the image-resolution receptive field of activations in a feature map to visualize a filter's semantics. An interpretable CNN usually encodes head patterns of animals in its top conv-layer for classification. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig.12Notation for the computation of a filter's location instability(Zhang et al., 2018b)    References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 13 And-Or graph (AOG) grown on a pre-trained convolutional neural network (CNN) as a semantic branch (Zhang et al., 2017a) AOG associates specific CNN units with certain image regions. Red lines indicate the parse graph. References to color refer to the online version of this figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15</head><label>15</label><figDesc>Fig. 14 Illustration of the question-answering (QA) process (Zhang et al., 2017a): (a) method of sorting and selecting unexplained objects; (b) questions for each target object In (a), ?KL indicates the predicted information gain of the And-Or graph (AOG) model obtained from asking about different objects, and the horizontal axis indicates different objects sorted w.r.t. the predicted information gain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig.16Visualization of patterns for the head part before and after human interactions(Zhang et al.,  2017b)    </figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* Project supported by the <rs type="funder">ONR</rs> <rs type="projectName">MURI</rs> project (No. <rs type="grantNumber">N00014-16-1-2007</rs>), the <rs type="funder">DARPA XAI Award</rs> (No. <rs type="grantNumber">N66001-17-2-4029</rs>), and <rs type="funder">NSF IIS</rs> (No. <rs type="grantNumber">1423305</rs>) ORCID: Quan-shi ZHANG, http://orcid.org/<rs type="grantNumber">0000-0002-6108-2738</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SG2dmRa">
					<idno type="grant-number">N00014-16-1-2007</idno>
					<orgName type="project" subtype="full">MURI</orgName>
				</org>
				<org type="funding" xml:id="_QWgUSXx">
					<idno type="grant-number">N66001-17-2-4029</idno>
				</org>
				<org type="funding" xml:id="_aTaK3r3">
					<idno type="grant-number">1423305</idno>
				</org>
				<org type="funding" xml:id="_jMUvjK2">
					<idno type="grant-number">0000-0002-6108-2738</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding deep features with computer-generated imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.329</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.329" />
	</analytic>
	<monogr>
		<title level="m">IEEE Int Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2875" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.354</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.354" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2172" to="2180" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4829" to="4837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.371</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.371" />
	</analytic>
	<monogr>
		<title level="m">IEEE Int Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3429" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards transparent AI systems: interpreting visual question answering models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.08974" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1603.06318" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning how to explain neural networks: patternnet and patternattribution</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.05598" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 34 th Int Conf on Machine Learning</title>
		<meeting>34 th Int Conf on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining the unexplained: a class-enhanced attentive response (clear) approach to understanding deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.215</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2017.215" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1686" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying unknown unknowns in the open world: representations and policies for guided exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 31 st AAAI Conf on Artificial Intelligence</title>
		<meeting>31 st AAAI Conf on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2124" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
		<ptr target="https://doi.org/10.1109/5.726791" />
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<title level="m">The MNIST Database of Handwritten Digits</title>
		<imprint>
			<date type="published" when="1998-06">1998. June, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.425" />
	</analytic>
	<monogr>
		<title level="m">IEEE Int Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised learning on neural network outputs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.00990" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299155</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299155" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: conditional iterative generation of images in latent space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.374</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.374" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3510" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00007</idno>
		<ptr target="https://doi.org/10.23915/distill.00007" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A 3D face model for pose and illumination invariant face recognition. 6 th IEEE Int Conf on Advanced Video and Signal Based Surveillance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/AVSS.2009.58</idno>
		<ptr target="https://doi.org/10.1109/AVSS.2009.58" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939778" />
	</analytic>
	<monogr>
		<title level="m">Proc 22 nd ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining</title>
		<meeting>22 nd ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3859" to="3869" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.74</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.74" />
	</analytic>
	<monogr>
		<title level="m">IEEE Int Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6034" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Striving for simplicity: the all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inte Conf on Learning Representations</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kouichi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.08864" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6199" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The VQA-machine: learning how to use existing vision algorithms to answer new questions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.416</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.416" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A numerical study of the bottomup and top-down inference processes in And-Or graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="252" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compositional boosting for computing hierarchical image structures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383034</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2007.383034" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.05226" />
	</analytic>
	<monogr>
		<title level="j">Interpretable R-CNN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating information contributions of bottom-up and top-down processes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459386</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2009.5459386" />
	</analytic>
	<monogr>
		<title level="m">IEEE 12 th Int Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1042" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? NIPS</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10590-1_53" />
	</analytic>
	<monogr>
		<title level="m">European Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Growing interpretable part graphs on convnets via multi-shot learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 30 th AAAI Conf on Artificial Intelligence</title>
		<meeting>30 th AAAI Conf on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mining object parts from CNNs via active question-answering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.414</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.414" />
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="346" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interactively transferring CNN patterns for part localization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.01783" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Examining CNN representations with respect to dataset bias</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 32 nd AAAI Conf on Artificial Intelligence</title>
		<meeting>32 nd AAAI Conf on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpreting CNN knowledge via an explanatory graph</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 32 nd AAAI Conf on Artificial Intelligence</title>
		<meeting>32 nd AAAI Conf on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2124" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.00121" />
		<title level="m">Interpreting CNNs via decision trees</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene CNNs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6856" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Visualizing deep neural network decisions: prediction difference analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsct</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.04595" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
