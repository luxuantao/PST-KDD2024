<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-15">15 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yury</forename><surname>Malkov</surname></persName>
							<email>ymalkov@twitter.com</email>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omar</forename><surname>Florez</surname></persName>
							<email>oflorez@twitter.com</email>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serim</forename><surname>Park</surname></persName>
							<email>serimp@twitter.com</email>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
							<email>brimcwilliams@twitter.com</email>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
							<email>aelkishky@twitter.com</email>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-15">15 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.07562v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Construct a Twitter Heterogeneous Information Network (TwHIN) Embed Entities from TwHIN Engagement Data Fave</term>
					<term>Reply</term>
					<term>Retweet Tweet Embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present TwHIN-BERT, a multilingual language model trained on in-domain data from the popular social network Twitter. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision, but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on a variety of multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pretrained language models. We will freely opensource TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The proliferation of pre-trained language models (PLMs) <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b10">Conneau et al., 2020)</ref> based on the Transformer architecture <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> has pushed the state of the art across many tasks in natural language processing (NLP). As an application of transfer learning, these models are typically trained on massive text corpora and, when fine-tuned on downstream tasks, have demonstrated state-of-the-art performance.</p><p>Despite the success of PLMs in general-domain NLP, fewer attempts have been made in language model pre-training for user-generated text on social media. In this work, we pre-train a language model for Twitter -a prominent social media platform where users post short messages called Tweets. Tweets contain informal diction, abbreviations, emojis, and topical tokens such as hashtags. As a result, PLMs designed for general text corpora may struggle to understand Tweet semantics accurately. Existing works <ref type="bibr" target="#b26">(Nguyen et al., 2020;</ref><ref type="bibr" target="#b1">Barbieri et al., 2021)</ref> on Twitter LM pre-training do not address these challenges and simply replicate general domain pre-training on Twitter corpora.</p><p>A distinctive feature of Twitter social media is the user interactions through Tweet engagements. As seen in Figure <ref type="figure" target="#fig_0">1</ref>, when a user visits Twitter, in addition to posting Tweets, they can perform a variety of social actions such as "Favoriting", "Replying" and "Retweeting" Tweets. The wealth of such engagement information is invaluable to Tweet content understanding. For example, the post "bottom of the ninth, two outs, and down by one!!" would be connected to baseball topics by its co-engaged Tweets, such as "three strikes and you're out!!!". Without the social contexts, a conventional text-only PLM objective would struggle to build this connection. As an additional benefit, a socially-enriched language model will also vastly benefit common applications on social me-dia, such as social recommendations <ref type="bibr">(Ying et al., 2018)</ref> and information diffusion prediction <ref type="bibr" target="#b8">(Cheng et al., 2014;</ref><ref type="bibr" target="#b31">Sankar et al., 2020)</ref>.</p><p>We introduce TwHIN-BERT-a multilingual language model for Twitter pre-trained with social engagements. The key idea of our method is to leverage socially similar Tweets for pre-training. Building on this idea, TwHIN-BERT has the following features. (1) We construct a Twitter Heterogeneous Information Network(TwHIN) <ref type="bibr" target="#b15">(El-Kishky et al., 2022)</ref> to unify the multi-typed user engagement logs. Then, we run scalable embedding and approximate nearest neighbor search to sift through hundreds of billions of engagement records and mine socially similar Tweet pairs. (2) In conjunction with masked language modeling, we introduce a contrastive social objective that enforces the model to tell if a pair of Tweets are socially similar or not. Our model is trained on 7 billion Tweets from over 100 distinct languages, of which 1 billion have social engagement logs.</p><p>We evaluate the TwHIN-BERT model on both social recommendation and semantic understanding downstream evaluation tasks. To comprehensively evaluate on many languages, we curate two large-scale datasets, a social engagement prediction dataset focused on social aspects and a hashtag prediction dataset focused on language aspects. In addition to these two curated datasets, we also evaluate on established benchmark datasets to draw direct comparisons to other available pre-trained language models. TwHIN-BERT achieves state-ofthe-art performance in our evaluations with a major advantage in the social tasks.</p><p>In summary, our contributions are as follows: ? We build the first ever socially-enriched pretrained language model for noisy user-generated text on Twitter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TwHIN-BERT</head><p>In this section, we outline how we construct training examples for our social objectives and subsequently train TwHIN-BERT with social and text pre-training objectives. As seen in Figure <ref type="figure" target="#fig_1">2</ref>, we first construct and embed a user-Tweet engagement network. The resultant Tweet embeddings are then used to mine pairs of socially similar Tweets. These Tweet pairs and others are then used to pre-train TwHIN-BERT, which can then be fine-tuned for various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mining Socially Similar Tweets</head><p>With abundant social engagement logs, we (informally) define socially similar Tweets as Tweets that are co-engaged by a similar set of users. The challenge lies in how to implement this social similarity by (1) fusing heterogeneous engagement types, such as "Favorite", "Reply" , "Retweet", and (2) efficiently mining billions of similar Tweet pairs. To address these challenges, TwHIN-BERT first constructs a Twitter Heterogeneous Information Network (TwHIN) from the engagement logs, then runs a scalable heterogeneous network embedding method to capture co-engagement and map Tweets and users into a vector space. With this, social similarity translates to embedding space similarity. Subsequently, we mine similar Tweet pairs via ANN search on the Tweet embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Constructing TwHIN</head><p>We define and construct TwHIN as follows:</p><p>Definition 1 (TwHIN) Our Twitter Heterogeneous Information Network is a directed bipartite graph G = (U, T, E, ?), where U is the set of user nodes, T is the set of Tweet nodes, E = U ? T is the set of engagement edges. ? : E ? R is an edge type mapping function. Each edge e ? E belongs to a type of engagement in R.</p><p>Our curated TwHIN (Figure <ref type="figure" target="#fig_2">3</ref>) consists of approximately 200 million distinct users, 1 billion Tweets, and over 100 billion edges. We posit that our TwHIN encodes not only user preferences but also Tweet social appeal. We perform scalable network embedding to derive a social similarity metric from TwHIN. The network embedding fuses the heterogeneous engagements into a unified vector space that's easy to operate on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Embedding TwHIN Nodes</head><p>While our approach is agnostic to the exact methodology used to embed TwHIN, we follow the approach outlined in <ref type="bibr" target="#b15">(El-Kishky et al., 2022;</ref><ref type="bibr">El-Kishky et al., 2022b)</ref>. We perform training with the TransE embedding <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref> objective to co-embed users and Tweets using the  PyTorch-Biggraph <ref type="bibr" target="#b18">(Lerer et al., 2019)</ref> framework for scalability. Following previous approaches, we train for 10 epochs and perform negative sampling both uniformly and proportional to entity prevalence in TwHIN <ref type="bibr" target="#b4">(Bordes et al., 2013;</ref><ref type="bibr" target="#b18">Lerer et al., 2019)</ref>. Optimization is performed via Adagrad. Upon learning dense representations of nodes in TwHIN, we utilize the learned Tweet representations to mine socially similar Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Mining Similar Tweet Pairs</head><p>Given the learned TwHIN Tweet embeddings, we seek to identify pairs of Tweets with similar social appeal -that is, Tweets that appeal to (i.e., are likely to be engaged with) similar users. We will use these socially-similar Tweet pairs as selfsupervision when training TwHIN-BERT. To identify these pairs, we perform an approximate nearest neighbor (ANN) search in the TwHIN embedding space. To efficiently perform the search over 1B+ Tweets, we use the optimized FAISS<ref type="foot" target="#foot_0">2</ref> toolkit <ref type="bibr" target="#b17">(Johnson et al., 2019)</ref> to create a compact index of Tweets keyed by their engagement-based TwHIN embeddings. As each Tweet embedding is 256-dimensional, storing billion-scale Tweet embeddings would require more than one TB of memory. To reduce the size of the index such that it can fit on a 16 A100 GPU node with each GPU possessing 40GB of memory, we apply product quantization <ref type="bibr" target="#b16">(Jegou et al., 2010)</ref> to discretize and reduce embeddings size. The resultant index corresponds to OPQ64,IVF65536,PQ64 in the FAISS index factory terminology.</p><p>After creating the FAISS index and populating it with TwHIN Tweet embeddings, we search the index using Tweet embedding queries to find pairs of similar Tweets (t i , t j ) such that t i and t j are close in the embedding space as defined by their cosine distance. To ensure high recall, we query the FAISS index with 2000 probes. Finally, we select the k closet Tweets defined by the cosine distance between the query Tweet and retrieved Tweets' embeddings. These Tweet pairs are used in our socialobjective when pre-training TwHIN-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-training Objectives</head><p>Given the mined socially similar Tweets, we describe our language model training process. To train TwHIN-BERT, we first run the Tweets through the language model and then train the model with a joint contrastive social loss and masked language model loss.</p><p>Tweet Encoding with LM. We use a Transformer language model to encode each Tweet. Similar to BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, given the tokenized text w t = [w 1 , w 2 , ..., w n ] of a Tweet t, we add special tokens to mark the start and end of the Tweet: ?t = [CLS]w t <ref type="bibr">[SEP]</ref>. As the Tweets are usually shorter than the maximum sequence length of a language model, we group multiple Tweets and feed them together into the language model when possible. We then apply CLS-pooling, which takes the [CLS] token embedding of each Tweet. These Tweet embeddings are passed through an MLP projection head for the social loss computation.</p><formula xml:id="formula_0">[e t 1 , e t 2 , ...] = Pool LM([ ?t 1 , ?t 2 , ...]) (1) z t = MLP(e t )<label>(2)</label></formula><p>Contrastive Social Loss. We use a contrastive loss to let our model learn whether two Tweets are socially similar or not. For each batch of B socially similar Tweet pairs {(t i , t j )} B , we compute the NT-Xent loss <ref type="bibr" target="#b7">(Chen et al., 2020)</ref> with in-batch negatives:</p><formula xml:id="formula_1">L social (i, j) = -log exp(sim(z i , z j ))/? N B (i) exp(sim(z i , z k )/? ) (3) The negatives N B (i) of Tweet t i are the (2B -1)</formula><p>other Tweets in the batch that are not paired with t i . We use cosine similarity for function sim(?, ?). ? is the loss temperature.</p><p>Our overall pre-training objective is a combination of the contrastive social loss and the masked language model loss <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>:</p><formula xml:id="formula_2">L = L social + ?L MLM (4)</formula><p>? is a hyperparameter that balances the social and language loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-training Setup</head><p>Model Architecture. We use the same Transformer architecture as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> for our language model. We adopt the XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> tokenizer, which offers good capacity and coverage in all languages. The model has a vocabulary size of 250K. The max sequence length is set to 128 tokens. The detailed model setup can be found in Appendix B. Note that although we have chosen this specific architecture, our social objective can be used in conjunction with a wide range of language model architectures.</p><p>Pre-training Data. We collect 7 billion Tweets in 100 languages from Jan. 2020 to Jun. 2022. Additionally, we collect 100 billion user-Tweet social engagement data covering 1 billion of our Tweets. We re-sample the data based on language frequency raised to the power of 0.7 to mitigate under-representation of low-resource languages.</p><p>Training Procedure. Our training has two stages.</p><p>In the first stage, we train the model from scratch using the 6 billion Tweets without user engagement. The model is trained for 500K steps on 16 Nvidia A100 GPUs (a2-megagpu-16g) with a total batch size of 6K. In the second stage, the model is trained for another 500K steps on the 1 billion Tweets with the joint MLM and social loss. We use mixed precision during training. Overall pre-training takes approximately five days for the base model and two weeks for the large model. We refer readers to Appendix B for the detailed hyperparameter setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we discuss baseline model specifications, evaluation setup, and results from two families of downstream evaluation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluated Methods</head><p>We evaluate TwHIN-BERT against the following baselines. mBERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> are two popular general domain multilingual language models trained with gigantic datasets. <ref type="bibr">BERTweet (Nguyen et al., 2020)</ref> is the previous state-of-the-art English Tweet language model. XLM-T <ref type="bibr" target="#b1">(Barbieri et al., 2021</ref>) is a multilingual Twitter language model based on XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>.</p><p>We include three variations of our model trained on the same corpus: base and large sizes, and an ablated base-MLM trained with only an MLM objective. All baselines are base variants (with between 135M to 278M parameters depending on the size of the tokenizer). Our large model has around 550M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Social Engagement Prediction</head><p>Our first benchmark task is social engagement prediction. This task aims to evaluate how well the pretrained language models capture social aspects of user-generated text. In our task, we predict whether users modeled via a user embedding vector will perform a certain social engagement on a given Tweet.</p><p>We use different pre-trained language models to generate representations for Tweets, and then feed these representations into a simple prediction model alongside the corresponding user representation. The model is trained to predict whether a user will engage with a specific Tweet. Dataset. To curate our Tweet-Engagement dataset, we select the 50 popular languages on Twitter and sample 10,000 (or all if the total number is less than 10,000) Tweets of each language from a fixed time period. All Tweets are available via the Twitter public API. We then collect the user-Tweet engagement records associated with these Tweets. There are, on average, 29K engagement records per language. We ensure that there is no overlap between the evaluation and pre-training datasets.</p><p>Each engagement record consists of a pre-trained 256-dimensional user embedding <ref type="bibr" target="#b15">(El-Kishky et al., 2022)</ref> and a Tweet ID that indicates the user has engaged with the given Tweet. To ensure privacy, each user embedding appears only once, however each tweet may be engaged by multiple users. We split the Tweets into train, development, and test sets with a 0.8/0.1/0.1 ratio, and then collect the respective engagement records for each subset. Prediction Model. Given a pre-trained language model, we use it to generate an embedding for each Tweet t given its content w t : e t = Pool LM(w t )</p><p>We apply the following pooling strategies to calculate the Tweet embedding from the language model. First, we take [CLS] token embedding as the first part of overall embedding. Then, we take the average token embedding of non-special tokens as the second part. The two parts are concatenated to form the Combined embedding of a Tweet.</p><p>With LM-derived Tweet embeddings, pretrained user embeddings, and the user-Tweet engagement records, we build an engagement prediction model ? = (W t , W u ). Given a user u and a Tweet t, the model projects the user embedding e u and the Tweet embedding e t into the same space, and then calculates the probability of engagement:</p><formula xml:id="formula_3">h u = W T u e u , h t = W T t e t P (t | u) = ? h T u h t</formula><p>We optimize a negative sampling loss on the training engagement records R. For each engage-ment pair (u, t) ? R, the loss is defined as:</p><formula xml:id="formula_4">log ? h T u h t + E t ?Pn(R) log ? -h T u h t where P n (R) is a negative sampling distribution.</formula><p>We use the frequency of each Tweet in R raised to the power of 3/4 for this distribution.</p><p>Our prediction model closely resembles classical link prediction models such as <ref type="bibr">(Tang et al., 2015b)</ref>. We keep the model simple, making sure it will not overpower the language model embeddings. Evaluation Setup and Metrics. We conduct hyperparameter search on the English development dataset and use these hyperparameters for the other languages. The prediction model projects user and Tweet embedding to 128 dimensions. We set batch size to 512, learning rate to 1e-3. The best model on validation set is selected for test set evaluation.</p><p>In the test set, we pair each user with 1,000 Tweets: one Tweet they have engaged with and the rest are randomly sampled negatives. The model ranks the Tweets by the predicted probability of engagement, and we evaluate with HITS@10. We report median results from 6 runs with different initialization.</p><p>Results. We show results for high, mid, and lowresource languages (determined by language frequency on Twitter) in Table <ref type="table" target="#tab_1">1</ref>. Language abbreviations are ISO language codes<ref type="foot" target="#foot_1">3</ref> . We also show the average results from all 50 languages in the evaluation dataset, and leave the details in Appendix D. Our TwHIN-BERT model demonstrates significant improvement over the baselines on the social engagement task. Comparing our model to the ablation without the social loss, we can see the contrastive social pre-training provides significant lift over just MLM pre-training for social engagement prediction. An analysis on all 50 evaluation languages shows the large model to perform better than the base model on average, with more wins than losses. Additionally, we also observe that our method yields the most improvement when using the Combined [CLS] token and average non-special token embedding. We believe the [CLS] token embedding from our model captures social aspects of the Tweet, while averaging the other token embeddings captures the semantic aspects of the Tweet. Naturally, utilizing both aspects is essential to better model a Tweet's appeal and a user's inclination to engage with a Tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tweet Classification</head><p>Our second collection of downstream tasks is Tweet classification. In these tasks, we take as input the Tweet text and predict discrete labels corresponding to the label space for each task.</p><p>Datasets. We curate a multilingual Tweet hashtag prediction dataset (available via Twitter public API) to comprehensively cover the popular languages on Twitter. In addition, we evaluate on five external benchmark datasets for tasks such as sentiment classification, emoji prediction, and topic classification in selected languages. We show the dataset statistics in Table <ref type="table" target="#tab_2">2</ref>.</p><p>? Tweet Hashtag Prediction dataset is a multilingual hashtag prediction dataset we collected from Tweets. It contains Tweets of 50 popular languages on Twitter. For each language, 500 most popular hashtags were selected and 100k Tweets that has those hashtags were sampled. We made sure each Tweet will only contain one of the 500 candidate hashtags. Similar to work proposed in <ref type="bibr" target="#b25">Mireshghallah et al. (2022)</ref>, the task is to predict the hashtag used in the Tweet. ? Sentiment Analysis. The English dataset Se-mEval2017 task 4A <ref type="bibr" target="#b30">(Rosenthal et al., 2019)</ref>, Arabic dataset ASAD <ref type="bibr" target="#b0">(Alharbi et al., 2020)</ref>, code mixed Hindi/Spanish+English datasets Se-mEval2020 task 9 <ref type="bibr" target="#b27">(Patwa et al., 2020)</ref> are threepoint sentiment analysis tasks with labels of "positive", "negative", "neutral". ? Emoji Prediction. SemEval2018 task 2 ( <ref type="bibr">Barbieri et al., 2018)</ref> is an emoji prediction dataset in both English and Spanish. The objective is to predict the most likely used emoji in a Tweet. ? Topic Classification. COVID-JA <ref type="bibr" target="#b34">(Suzuki, 2019</ref>) is a Japanese Tweets classification dataset.</p><p>The objective is to classify each Tweet into one of the six pre-defined topics around COVID-19.</p><p>Setup and Evaluation Metrics. We use the standard language model fine-tuning method as described in <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and apply a linear prediction layer on top of the pooled output of the last transformer layer. Each model is fine-tuned for up to 30 epochs, and we evaluate the best model from the training epochs on the test set based on development set performance. The fine-tuning hyperparameter setup can be found in Appendix B. We report the median results from 3 fine-tuning runs with different random seeds. Results are the evaluation metrics recommended for each benchmark dataset or challenge (Appendix C). For hashtag prediction datasets, we report macro-F1 scores.</p><p>Multilingual Hashtag Prediction. In Table <ref type="table" target="#tab_3">3</ref>, we show macro F1 scores on selected languages from our multilingual hashtag prediction dataset. We also report the average performance of all 50 languages in the dataset, and leave detailed results in Appendix E. We can see that TwHIN-BERT significantly outperforms the baseline methods at the same base size. Our large model is slightly better than or on par with the base model, with a better overall performance. On the English dataset, our model outperforms the BERTweet monolingual language model trained exclusively on English Tweets and with a dedicated English tokenizer. Comparing our model to the ablation with no social loss, the two models demonstrate similar performance with our model being slightly better. These results show that while our model has a major advantage on social tasks, it retains high performance on semantic understanding applications.</p><p>External Classification Benchmarks. As shown in Table <ref type="table" target="#tab_4">4</ref>, TwHIN-BERT matches or outperforms the multilingual baselines on the established classification benchmarks. BERTweet fares better than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Varying Downstream Supervision</head><p>In this set of experiments, we study how TwHIN-BERT performs when the amount of downstream supervision changes. We fine-tune our model and baseline models on the hashtag prediction dataset (Section 3.3). We select English and Japanese as they are the most popular languages on Twitter. We change the number of training examples given to the models during fine-tuning. It is varied from 2 to 32 labeled training examples per class. We follow the same protocols as Section 3.3 and report macro F1 scores on the test set.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the results. TwHIN-BERT holds significant performance gain across different amount of downstream supervision. Note that when supervision is scarce, e.g., two labeled training examples per class given, our model has a even larger relative performance improvement over the baselines. The results indicate that our model may empower weakly supervised applications on Tweet natural language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature-based Classification</head><p>In addition to language model fine-tuning experiments, we evaluate TwHIN-BERT's performance as a feature extractor. We use the hashtag prediction datasets (Section 3.3) and select three popular languages with different scripts. We use our model and the baseline models to embed each Tweet into a feature vector and train a Logistic Regression classifier with the fixed feature vectors as input. TwHIN-BERT-base 51.16 64.12 37.20 TwHIN-BERT-large 54.12 64.03 38.78</p><p>Table <ref type="table" target="#tab_5">5</ref> shows TwHIN-BERT outperforming the baselines with a wide margin on all languages. This not only shows TwHIN-BERT has learned superior Tweet representations, but also showcases its potential in other feature-based downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Pre-trained Language Models: Since their introduction <ref type="bibr" target="#b28">(Peters et al., 2018;</ref><ref type="bibr" target="#b11">Devlin et al., 2019)</ref>, pre-trained language models have enjoyed tremendous success in all aspects of natural language processing. Follow up research has advanced PLMs by scaling them with respect to number of parameters and training data <ref type="bibr" target="#b24">(Micheli and Fleuret, 2021;</ref><ref type="bibr" target="#b29">Raffel et al., 2020;</ref><ref type="bibr" target="#b32">Shoeybi et al., 2019)</ref>, and by improving the training objectives <ref type="bibr" target="#b41">(Yang et al., 2019;</ref><ref type="bibr" target="#b9">Clark et al., 2020;</ref><ref type="bibr" target="#b23">Meng et al., 2021)</ref>. Despite these innovations in scaling and pre-training objectives, the vast majority of the work has focused on text-only training objectives applied to general domain corpora. In this paper, we deviate from most previous works by explore PLM training using solely Twitter in-domain data and training our model based on both text-based and social-based objectives.</p><p>Tweet Language Models: While a majority of PLMs are trained on general domain corpora, a few language models have been proposed specifically for Twitter and other social media platforms. <ref type="bibr">BERTweet (Nguyen et al., 2020)</ref> mirrors BERT training on 850 million English Tweets. TimeLMs <ref type="bibr" target="#b22">(Loureiro et al., 2022)</ref> trains a set of RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019</ref>) models for English Tweets on different time ranges. XLM-T ( <ref type="bibr" target="#b1">Barbieri et al., 2021)</ref> continues the pre-training process from an XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> checkpoint on 198 million multilingual Tweets. These methods mostly replicate existing general domain PLM methods and simply substitute the training data with Tweets. However, our approach utilizes additional social engagement signals to enhance the pre-trained Tweet representations.</p><p>Enriching PLMs with Additional Information: Several existing works introduce additional information for language model pre-training. ERNIE <ref type="bibr">(Zhang et al., 2019)</ref> and K-BERT <ref type="bibr" target="#b19">(Liu et al., 2020)</ref> inject entities and their relations from knowledge graphs to augment the pre-training corpus. OAG-BERT <ref type="bibr" target="#b20">(Liu et al., 2022)</ref> appends metadata of a document to its raw text, and designs objectives to jointly predict text and metadata. These works focus on bringing additional metadata and knowledge by injecting training instances, while our work leverage the rich social engagements embedded in the social media platform for text relevance. Recent work <ref type="bibr">(Yasunaga et al., 2022)</ref> has utilized document hyperlinks for LM pre-training, but does so with a simple three way classification objective.</p><p>Network Embedding: Network embedding has emerged as a valuable tool for transferring information from relational data to other tasks <ref type="bibr">(El-Kishky et al., 2022a)</ref>. With the introduction of heterogeneous information networks <ref type="bibr" target="#b33">(Sun and Han, 2013)</ref> as a formalism to model rich multi-typed, multirelational networks, many heterogeneous network embedding approaches were developed <ref type="bibr" target="#b5">(Chang et al., 2015;</ref><ref type="bibr">Tang et al., 2015a;</ref><ref type="bibr" target="#b40">Xu et al., 2017;</ref><ref type="bibr" target="#b6">Chen and Sun, 2017;</ref><ref type="bibr" target="#b12">Dong et al., 2017)</ref>. However, many of these techniques are difficult to scale to very large networks. In this work, we apply knowledge graph embeddings <ref type="bibr" target="#b39">(Wang et al., 2017;</ref><ref type="bibr" target="#b4">Bordes et al., 2013;</ref><ref type="bibr" target="#b37">Trouillon et al., 2016)</ref>, which have been shown to be both highly scalable and flexible enough to model multiple node and edge types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we introduce TwHIN-BERT, a multilingual language model trained on a large Tweet corpus. Unlike previous BERT-style language models, TwHIN-BERT is trained using two objectives: (1) a standard MLM pre-training objective and (2) a contrasting social objective. We perform a variety of downstream tasks using TwHIN-BERT on Tweet data. Our experiments demonstrate that TwHIN-BERT outperforms previously released language models on both semantic and social engagement prediction tasks. We release TwHIN-BERT 45 to the academic community to further research in social media NLP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Distribution of Languages in Training Dataset</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the distribution of languages in our pre-training dataset. Some languages with different variations (e.g., Hindi and Hindi Romanized) are represented with the same ISO language code. We run fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> language identification model lid.176.bin 6 to detect languages. We deem a language "high-resource" if we have more than 10 8 Tweets during pre-training after frequency-based re-sampling (Section 2.3); "midresource" if we have more than 10 7 and less than 10 8 Tweets; "low-resource" if we have less than 10 7 Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters for Pre-training and Fine-Tuning</head><p>Table <ref type="table" target="#tab_9">6</ref> shows the pre-training hyperparameters.</p><p>The model architecture and hyperparameters not shown in the table are the same as RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref>.</p><p>Table <ref type="table" target="#tab_10">7</ref> shows the hyperparameters for classification fine-tuning. We do hyperparameter selection on the development datasets and share the same set 6 https://fasttext.cc/docs/en/language-i dentification.html of hyperparameters for the base models, as we find them to perform well with this setting. The weight decay for base models is set to zero. A different set of hyperparameters were necessary for the large model because it behaves differently from the base models in terms of convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation Metrics for External Classification Benchmarks</head><p>The recommended evaluation metrics that we report in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Engagement Prediction Results on Additional Languages</head><p>Table <ref type="table" target="#tab_11">8</ref> shows the engagement prediction results on all available evaluation languages. Some languages have more examples than other languages due to data availability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hashtag Prediction Results on Additional Languages</head><p>Table <ref type="table" target="#tab_12">9</ref> shows the hashtag prediction results on all available evaluation languages. A small number of languages have less examples than shown in Table <ref type="table" target="#tab_2">2</ref> due to data availability. The Russian language is not evaluated as the XLM-T baseline fails on some Russian characters in our dataset.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) This mock-up shows a short-text Tweet and social engagements such as Faves, Retweets, Replies, Follows that create a social context to Tweets and signify Tweet appeal to engaging users. (b) Coengagement is a strong indicator of Tweet similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We outline the end-to-end TwHIN-BERT process. This three-step process involves (1) mining socially similar Tweet pairs by embedding a Twitter Heterogeneous Information Network (2) training TwHIN-BERT using a joint social and MLM objective and finally (3) fine-tuning TwHIN-BERT on downstream tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Twitter Heterogeneous Information Network (TwHIN) capturing social engagements between users and Tweets.</figDesc><graphic url="image-18.png" coords="3,81.78,260.79,196.43,92.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Macro-F1 score on English and Japanese hashtag prediction datasets w.r.t. number of labeled training examples per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The number of Tweets in the pre-training dataset for each language. Languages are marked by ISO language codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Engagement prediction HITS@10 on high, mid, low-resource, and average of all languages.</figDesc><table><row><cell></cell><cell cols="3">High-Resource</cell><cell cols="3">Mid-Resource</cell><cell cols="3">Low-Resource</cell><cell>All</cell></row><row><cell>Method</cell><cell>en</cell><cell>ja</cell><cell>ar</cell><cell>el</cell><cell>ur</cell><cell>nl</cell><cell>no</cell><cell>da</cell><cell>ps</cell><cell>Avg.</cell></row><row><cell>mBERT</cell><cell cols="10">.0633 .0227 .0532 .0496 .0437 .0616 .0731 .1060 .0522 .0732</cell></row><row><cell>XLM-R</cell><cell cols="10">.0850 .0947 .0546 .0628 .0315 .0650 .1661 .1150 .0727 .0849</cell></row><row><cell>XLM-T</cell><cell cols="10">.1181 .1079 .1403 .0562 .0352 .0762 .1156 .1167 .0662 .1043</cell></row><row><cell>TwHIN-BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Base-MLM</cell><cell cols="10">.1400 .1413 .1640 .0801 .0547 .0965 .1502 .1334 .0600 .1161</cell></row><row><cell>-Base</cell><cell cols="10">.1552 .2065 .2206 .0944 .0627 .1346 .1920 .1470 .0799 .1436</cell></row><row><cell>-Large</cell><cell cols="10">.1585 .2325 .1989 .1065 .0667 .1248 .2118 .1475 .0817 .1497</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Text classification dataset statistics. * Statistics for Hashtag shows the numbers for each language.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Lang. Label Train Dev Test</cell></row><row><cell>SE2017</cell><cell>en</cell><cell>3</cell><cell>45,389 2,000 11,906</cell></row><row><cell cols="2">SE2018-en en</cell><cell cols="2">20 45,000 5,000 50,000</cell></row><row><cell>SE2018-es</cell><cell>es</cell><cell cols="2">19 96,142 2,726 9,969</cell></row><row><cell>ASAD</cell><cell>ar</cell><cell cols="2">3 137,432 15,153 16,842</cell></row><row><cell cols="2">COVID-JA ja</cell><cell cols="2">6 147,806 16,394 16,394</cell></row><row><cell cols="3">SE2020-hi hi+en 3</cell><cell>14,000 3,000 3,000</cell></row><row><cell cols="3">SE2020-es es+en 3</cell><cell>10,800 1,200 3,000</cell></row><row><cell>Hashtag</cell><cell cols="3">multi 500  *  16,000  *  2,000  *  2,000  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Multilingual hashtag prediction Macro-F1 on high, mid, low resource, and average of all languages.</figDesc><table><row><cell></cell><cell cols="3">High-Resource</cell><cell cols="3">Mid-Resource</cell><cell cols="3">Low-Resource</cell><cell>All</cell></row><row><cell>Method</cell><cell>en</cell><cell>ja</cell><cell>ar</cell><cell>el</cell><cell>ur</cell><cell>nl</cell><cell>no</cell><cell>da</cell><cell>ps</cell><cell>Avg.</cell></row><row><cell>BERTweet</cell><cell>59.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>mBERT</cell><cell cols="10">54.56 68.43 38.48 44.00 36.44 39.75 46.09 59.54 29.41 50.05</cell></row><row><cell>XLM-R</cell><cell cols="10">53.90 69.07 37.85 43.94 37.56 40.85 48.94 60.35 34.92 50.86</cell></row><row><cell>XLM-T</cell><cell cols="10">55.08 70.55 42.27 44.15 39.22 41.01 49.22 59.97 33.27 51.74</cell></row><row><cell>TwHIN-BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Base-MLM</cell><cell cols="10">58.38 72.66 43.08 46.89 41.53 42.36 49.60 61.00 35.37 53.66</cell></row><row><cell>-Base</cell><cell cols="10">59.31 73.03 44.24 47.59 42.81 42.69 51.11 60.33 36.21 54.62</cell></row><row><cell>-Large</cell><cell cols="10">60.07 72.91 45.41 47.43 43.39 44.80 51.34 61.56 38.24 55.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>External classification benchmark results.</figDesc><table><row><cell></cell><cell>SE2017</cell><cell cols="2">SE2018</cell><cell cols="2">ASAD COVID-JA</cell><cell cols="2">SE2020</cell><cell>Avg.</cell></row><row><cell>Method</cell><cell>en</cell><cell>en</cell><cell>es</cell><cell>ar</cell><cell>ja</cell><cell cols="2">hi+en es+en</cell><cell></cell></row><row><cell>BERTweet</cell><cell>72.97</cell><cell>33.27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>mBERT</cell><cell>66.17</cell><cell cols="3">27.73 19.19 69.08</cell><cell>80.57</cell><cell cols="3">66.55 45.31 53.51</cell></row><row><cell>XLM-R</cell><cell>71.15</cell><cell cols="3">30.94 21.05 79.09</cell><cell>81.67</cell><cell cols="3">69.59 48.97 57.49</cell></row><row><cell>XLM-T</cell><cell>72.01</cell><cell cols="3">31.97 21.49 80.70</cell><cell>81.48</cell><cell cols="3">70.94 51.06 58.52</cell></row><row><cell>TwHIN-BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Base-MLM</cell><cell>72.10</cell><cell cols="3">32.44 21.79 80.48</cell><cell>82.12</cell><cell cols="3">72.42 51.67 59.00</cell></row><row><cell>-Base</cell><cell>72.30</cell><cell cols="3">32.41 22.23 80.73</cell><cell>82.37</cell><cell cols="3">71.30 54.32 59.38</cell></row><row><cell>-Large</cell><cell>73.10</cell><cell cols="3">33.31 22.80 81.19</cell><cell>82.50</cell><cell cols="3">73.08 54.47 60.06</cell></row><row><cell cols="4">our base model with its dedicated large English to-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">kenizer and monolingual training. Our large model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">outperforms all the baselines. Similar to hashtag</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">prediction, TwHIN-BERT performs on par with or</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">slightly better than the MLM-only ablation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Feature-based classification on hashtag prediction datasets (Macro-F1).</figDesc><table><row><cell>Method</cell><cell>en</cell><cell>ja</cell><cell>ar</cell></row><row><cell>XLM-R</cell><cell cols="3">30.88 41.14 21.55</cell></row><row><cell>XLM-T</cell><cell cols="3">41.66 51.56 32.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754-5764.    </figDesc><table><row><cell>Michihiro Yasunaga, Jure Leskovec, and Percy Liang.</cell></row><row><cell>2022. LinkBERT: Pretraining language models with</cell></row><row><cell>document links. In Proceedings of the 60th Annual</cell></row><row><cell>Meeting of the Association for Computational Lin-</cell></row><row><cell>guistics (Volume 1: Long Papers), pages 8003-8016,</cell></row><row><cell>Dublin, Ireland. Association for Computational Lin-</cell></row><row><cell>guistics.</cell></row><row><cell>Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombat-</cell></row><row><cell>chai, William L. Hamilton, and Jure Leskovec. 2018.</cell></row><row><cell>Graph convolutional neural networks for web-scale</cell></row><row><cell>recommender systems. In Proceedings of the 24th</cell></row><row><cell>ACM SIGKDD International Conference on Knowl-</cell></row><row><cell>edge Discovery &amp; Data Mining, KDD 2018, London,</cell></row><row><cell>UK, August 19-23, 2018, pages 974-983. ACM.</cell></row><row><cell>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,</cell></row><row><cell>Maosong Sun, and Qun Liu. 2019. ERNIE: en-</cell></row><row><cell>hanced language representation with informative en-</cell></row><row><cell>tities. In Proceedings of the 57th Conference of</cell></row><row><cell>the Association for Computational Linguistics, ACL</cell></row><row><cell>2019, Florence, Italy, July 28-August 2, 2019, Vol-</cell></row><row><cell>ume 1: Long Papers, pages 1441-1451. Association</cell></row><row><cell>for Computational Linguistics.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>are as follows. Average recall for</cell></row><row><cell>ASAD, SemEval 2017 datasets; Macro-F1 for Se-</cell></row><row><cell>mEval 2018 English and Spanish datasets; Accu-</cell></row><row><cell>racy for COVID-JA, SemEval 2020 datasets.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for pre-training TwHIN-BERT.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">TwHIN-BERT-base TwHIN-BERT-large</cell></row><row><cell>Max sequence length</cell><cell>128</cell><cell>128</cell></row><row><cell>Precision</cell><cell>BF16</cell><cell>BF16</cell></row><row><cell>Stage 1: MLM</cell><cell></cell><cell></cell></row><row><cell>Total batch size</cell><cell>6K</cell><cell>8K</cell></row><row><cell>Gradient accumulation steps</cell><cell>1</cell><cell>4</cell></row><row><cell>Peak learning rate</cell><cell>2e-4</cell><cell>2e-4</cell></row><row><cell>Warmup steps</cell><cell>30K</cell><cell>30K</cell></row><row><cell>Total steps</cell><cell>500K</cell><cell>500K</cell></row><row><cell>Stage 2: MLM + Social</cell><cell></cell><cell></cell></row><row><cell>Total batch size</cell><cell>6K</cell><cell>6K</cell></row><row><cell>Gradient checkpointing</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Peak learning rate</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>Warmup steps</cell><cell>30K</cell><cell>30K</cell></row><row><cell>Total steps</cell><cell>500K</cell><cell>500K</cell></row><row><cell>Contrastive projection head</cell><cell>[768, 768]</cell><cell>[1024, 512]</cell></row><row><cell>Contrastive loss temperature</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Loss balancing ?</cell><cell>0.05</cell><cell>0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for fine-tuning TwHIN-BERT and the baselines for classification.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="6">Hashtag SE2017 SE2018 ASAD COVID-JA SE2020</cell></row><row><cell>Base models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate</cell><cell>4e-5</cell><cell>4e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>2e-5</cell><cell>2e-5</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>TwHIN-BERT-large</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate</cell><cell>2e-5</cell><cell>2e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>0</cell><cell>5e-4</cell><cell>5e-4</cell><cell>0</cell><cell>5e-4</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Social engagement prediction results (HITS@10) on all evaluation Languages.</figDesc><table><row><cell>TwHIN-BERT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Hashtag prediction results (Macro-F1) on all evaluation languages.</figDesc><table><row><cell>TwHIN-BERT</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/facebookresearch/fa iss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://www.iso.org/iso-639-language-c odes.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://huggingface.co/Twitter/twhin-ber t-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://huggingface.co/Twitter/twhin-ber t-large</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Asad: A twitter-based benchmark arabic sentiment analysis dataset</title>
		<author>
			<persName><forename type="first">Basma</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hind</forename><surname>Alamro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Manal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuhair</forename><surname>Alshehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manal</forename><surname>Khayyat</surname></persName>
		</author>
		<author>
			<persName><surname>Kalkatawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Inji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2011.00578</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xlm-t: A multilingual language model toolkit for twitter</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.12250</idno>
	</analytic>
	<monogr>
		<title level="j">Francesco Barbieri</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Jose Camacho-Collados</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval 2018 task 2: Multilingual emoji prediction</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ronzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viviana</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-1003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Translating embeddings for modeling multirelational data. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Task-guided and pathaugmented heterogeneous network embedding for author identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can cascades be predicted?</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lada</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Alex</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2566486.2567997</idno>
	</analytic>
	<monogr>
		<title level="m">23rd International World Wide Web Conference, WWW &apos;14</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-04-07">2014. April 7-11, 2014</date>
			<biblScope unit="page" from="925" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">2022a. Graph-based representation learning for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="4784" to="4785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2022b. knnembed: Locally smoothed embedding mixtures for multi-interest candidate retrieval</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Portman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06205</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Twhin: Embedding the twitter heterogeneous information network for personalized recommendation</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serim</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chetan</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baekjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Portman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sof?a</forename><surname>Samaniego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539080</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14">2022. August 14 -18, 2022</date>
			<biblScope unit="page" from="2842" to="2850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12287</idno>
		<title level="m">Pytorch-biggraph: A largescale graph embedding system</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Oag-bert: Towards a unified backbone language model for academic knowledge services</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingnan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Timelms: Diachronic language models from twitter</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-demo.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 -System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 -System Demonstrations<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">COCO-LM: correcting and contrasting text sequences for language model pretraining</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021-12-06">2021. December 6-14, 2021</date>
			<biblScope unit="page" from="23102" to="23114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are few-shot butlers</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.734</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="9312" to="9318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Vogler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Florez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.05706</idno>
		<title level="m">Non-parametric temporal adaptation for social media topic classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2020 task 9: Overview of sentiment analysis of code-mixed tweets</title>
		<author>
			<persName><forename type="first">Parth</forename><surname>Patwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pykl</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Gamb?ck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.semeval-1.100</idno>
		<idno>De- cember 12-13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation, SemEval@COLING 2020</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation, SemEval@COLING 2020<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="774" to="790" />
		</imprint>
	</monogr>
	<note>International Committee for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno>CoRR, abs/1912.00741</idno>
		<title level="m">Semeval-2017 task 4: Sentiment analysis in twitter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inf-vae: A variational autoencoder framework to integrate homophily and influence in diffusion prediction</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371811</idno>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-02-03">2020. February 3-7, 2020</date>
			<biblScope unit="page" from="510" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: a structural analysis approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Filtering method for twitter streaming data using human-in-the-loop machine learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="DOI">10.2197/ipsjjip.27.404</idno>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="404" to="410" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LINE: large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2736277.2741093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW 2015</title>
		<meeting>the 24th International Conference on World Wide Web, WWW 2015<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-05-18">2015. May 18-22, 2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><surname>Bouchard</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding of embedding (eoe) joint embedding for coupled heterogeneous networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="741" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In Advances in Neural</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
