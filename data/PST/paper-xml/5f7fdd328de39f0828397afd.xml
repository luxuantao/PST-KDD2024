<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Graph Neural Networks via Bidirectional Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-29">29 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
							<email>chennnming@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
							<email>zhewei@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
							<email>bolin.ding@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliang.li@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<email>yuan-ye@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2020)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Graph Neural Networks via Bidirectional Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-29">29 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.15421v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNN) is an emerging field for learning on non-Euclidean data. Recently, there has been increased interest in designing GNN that scales to large graphs. Most existing methods use "graph sampling" or "layer-wise sampling" techniques to reduce training time. However, these methods still suffer from degrading performance and scalability problems when applying to graphs with billions of edges. This paper presents GBP, a scalable GNN that utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes. Theoretical analysis shows that GBP is the first method that achieves sub-linear time complexity for both the precomputation and the training phases. An extensive empirical study demonstrates that GBP achieves state-of-theart performance with significantly less training/testing time. Most notably, GBP can deliver superior performance on a graph with over 60 million nodes and 1.8 billion edges in less than half an hour on a single machine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the field of Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, biology <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, recommendation systems <ref type="bibr" target="#b35">[36]</ref>, and computer vision <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">[15]</ref> adopts a message-passing approach and gathers information from the neighbors of each node from the previous layer to form the new representation. The vanilla GCN uses a full-batch training process and stores each node's representation in the GPU memory, which leads to limited scalability. On the other hand, training GCN with mini-batches is difficult, as the neighborhood size could grow exponentially with the number of layers. These techniques can be broadly divided into three categories: 1) Layer-wise sampling methods: GraphSAGE <ref type="bibr" target="#b10">[11]</ref> proposes a neighbor-sampling method to sample a fixed number of neighbors for each node. VRGCN <ref type="bibr" target="#b5">[6]</ref> leverages historical activations to restrict the number of sampled nodes and reduce the variance of sampling. FastGCN <ref type="bibr" target="#b4">[5]</ref> samples nodes of each layer independently based on each node's degree and keeps a constant sample size in all layers to achieve scales linearly. LADIES <ref type="bibr" target="#b39">[40]</ref> further proposes a layer-dependent sampler to constrain neighbor dependencies, which guarantees the connectivity of the sampled adjacency matrix. 2) Graph Sampling methods: Cluster-GCN <ref type="bibr" target="#b7">[8]</ref> builds a complete GCN from clusters in each mini-batch. GraphSAINT <ref type="bibr" target="#b36">[37]</ref> proposes several light-weight graph samplers and introduces a normalization technique to eliminate biases of mini-batch estimation. 3) Linear Models: SGC <ref type="bibr" target="#b29">[30]</ref> computes the product of the feature matrix and the k-th power of the normalized adjacency matrix during the preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type="bibr" target="#b3">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm <ref type="bibr" target="#b1">[2]</ref> to accelerate computation.</p><p>While the above methods significantly speed up the training time of GNNs, they still suffer from three major drawbacks. First of all, the time complexity is linear to m, the number of edges in the graph. In theory, this complexity is undesirable for scalable GNNs. Secondly, as we shall see in Section 4, the existing scalable GNNs, such as GraphSAINT, LADIES, and SGC, fail to achieve satisfying results in the semi-supervised learning tasks. Finally and most importantly, none of the existing methods can offer reliable performance on billion-scale graphs.</p><p>Our contributions. In this paper, we first carefully analyze the theoretical complexity of existing scalable GNNs and explain why they cannot scale to graphs with billions of edges. Then, we present GBP (Graph neural network via Bidirectional Propagation), a scalable Graph Neural Network with sub-linear time complexity in theory and superior performance in practice. GBP performs propagation from both the feature vector and the training/testing nodes, yielding an unbiased estimator for each representation. Each propagation is executed in a localized fashion, leading to sub-linear time complexity. After the bidirectional propagation, each node's representation is fixed and can be trivially trained with mini-batches. The empirical study demonstrates that GBP consistently improves the performance and scalability across a wide variety of datasets on both semi-supervised and fully-supervised tasks. Finally, we present the first empirical study on a graph with over 1.8 billion edges. The result shows that GBP achieves superior results in less than 2,000 seconds on a moderate machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theoretical analysis of existing methods</head><p>In this section, we review some of the recent scalable GNNs and analyze their theoretical time complexity. We consider an undirected graph G=(V, E), where V and E represent the set of vertices and edges, respectively. For ease of presentation, we assume that G is a self-looped graph <ref type="bibr" target="#b14">[15]</ref>, with a self-loop attached to each node in V . Let n = |V | and m = |E| denote the number of vertices and edges in G, respectively. Each node is associated with an F -dimensional feature vector and we use X ∈ R n×F to denote the feature matrix. We use A and D to represent the adjacency matrix and the diagonal degree matrix of G, respectively. For each node u ∈ V , N (u) is the set of neighbors of u, and d(u) = |N (u)| is the degree of u. We use d = m n to denote the average degree of G. Following <ref type="bibr" target="#b14">[15]</ref>, we define the normalized adjacency matrix of G as Ã = D −1/2 AD −1/2 . The ( + 1)-th layer H ( +1) of the vanilla GCN is defined as</p><formula xml:id="formula_0">H ( +1) = σ ÃH ( ) W ( ) ,<label>(1)</label></formula><p>where W ( ) is the learnable weight matrix and σ(•) is the activation function. The training and inference time complexity of a GCN with L layers can be bounded by O LmF + LnF 2 , where O (LmF ) is the total cost of the sparse-dense matrix multiplication ÃH ( ) , and O(LnF 2 ) is the total cost of the feature transformation by applying W ( ) . At first glance, O(LnF 2 ) seems to be the dominating term, as the average degree d on scale-free networks is usually much smaller than the feature dimension F and hence LnF 2 &gt; LndF = LmF . However, in reality, feature transformation can be performed with significantly less cost due to better parallelism of dense-dense matrix multiplications. Consequently, O (LmF ) is the dominating complexity term of GCN and performing full neighbor propagation ÃH ( ) is the main bottleneck for achieving scalability.</p><p>In order to speed up GCN training, a few recent methods use various techniques to approximate the full neighbor propagation ÃH ( ) and enable mini-batch training. We divide these methods into three categories and summarize their time complexity in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Layer-wise sampling methods sample a subset of the neighbors at each layer to reduce the neighborhood size. GraphSAGE <ref type="bibr" target="#b10">[11]</ref> samples s n neighbors for each node and only aggregates the embeddings from the sampled nodes. With a batch-size b, the cost of feature propagation is bounded by O(bs L n F ), and thus the total per-epoch cost of GraphSAGE is O(ns L n F + ns L−1 n F 2 ). This complexity grows exponentially with the number of layers L and is not scalable on large graphs. Another work <ref type="bibr" target="#b5">[6]</ref> based on node-wise sampling further reduces sampling variance to achieve a better convergence rate. However, it suffers from worse time and space complexity. FastGCN <ref type="bibr" target="#b4">[5]</ref> and LADIES <ref type="bibr" target="#b39">[40]</ref> restrict the same sample size across all layers to limit the exponential expansion. If we use s l to denote the number of nodes sampled per layer, the per-batch feature propagation time is bounded by O(Ls 2 l F ). Since n s l batches are needed in an epoch, it follows that the per-epoch forward propagation time is bounded by O(Lns l F + LnF 2 ). Mini-batch training significantly accelerates the training process of the layer-wise sampling method. However, the training time complexity is still linear to m as the number of samples s l is usually much larger than the average degree d. Furthermore, it has been observed in <ref type="bibr" target="#b7">[8]</ref> that the overlapping nodes in different batches will lead to high computational redundancy, especially in fully-supervised learning.</p><p>Graph sampling methods sample a sub-graph at the beginning of each batch and perform forward propagation on the same subgraph across all layers. Cluster-GCN <ref type="bibr" target="#b7">[8]</ref> uses graph clustering techniques <ref type="bibr" target="#b13">[14]</ref> to partition the original graph into several sub-graphs, and samples one sub-graph to perform feature propagation in each mini-batch. In the worst case, the number of clusters in the graph is 1, and Cluster-GCN essentially becomes vanilla GCN in terms of time complexity. Graph-SAINT <ref type="bibr" target="#b36">[37]</ref> samples a certain amount of nodes and uses the induced sub-graph to perform feature propagation in each mini-batch. Let b denote the number of sampled node per-batch, and n b denote the number of batches. Given a sampled node u, the probability that a neighbor of u is also sampled is b/n. Therefore, the expected number of edges in the sub-graph is bounded by O(b 2 d/n). Summing over n b batches follows that the per-epoch feature propagation time of GraphSAINT is bounded by O(LbdF ), which is sub-linear to the number of edges in the graph. However, GraphSaint requires a full forward propagation in the inference phase, leading to the O(LmF + LnF 2 ) time complexity.</p><p>Linear model removes the non-linearity between each layer in the forward propagation, which allows precomputation of the final feature propagation matrix and result in an optimal training time complexity of O(nF 2 ). SGC <ref type="bibr" target="#b29">[30]</ref> repeatedly perform multiplication of normalized adjacency matrix Ã and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type="bibr" target="#b3">[4]</ref> calculates approximate the Personalized PageRank (PPR) matrix ∞ =0 α(1 − α) Ã by forward push algorithm <ref type="bibr" target="#b1">[2]</ref> and then applies the PPR matrix to the feature matrix X to derive the propagation matrix. Let ε denote the error threshold of the forward push algorithm, the precomputation cost is bounded by O( m ε ). A major drawback of PPRGo is that it takes O( n ε ) space to store the PPR matrix, rendering it infeasible on billion-scale graphs. </p><formula xml:id="formula_1">- O LmF + LnF 2 O LmF + LnF 2 GraphSAGE - O ns L n F + ns L−1 n F 2 O ns L n F + ns L−1 n F 2 FastGCN - O Lns l F + LnF 2 O Lns l F + LnF 2 LADIES - O Lns l F + LnF 2 O Lns l F + LnF 2 SGC O (LmF ) O nF 2 O nF 2 PPRGo O m ε O nKF + LnF 2 O nKF + LnF 2 Cluster-GCN O (m) O LmF + LnF 2 O LmF + LnF 2 GraphSAINT - O LbdF + LnF 2 O LmF + LnF 2 GBP (This paper) O LnF + L √ m lg n ε F O LnF 2 O LnF 2</formula><p>Other related work. Another line of research devotes to attention model <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>, where the adjacency matrix of each layer is replaced by a learnable attention matrix. GIN <ref type="bibr" target="#b30">[31]</ref> studies the expressiveness of GNNs, and shows that GNNs are not better than the Weisfeiler-Lehman test in distinguishing graph structures. GDC <ref type="bibr" target="#b16">[17]</ref> proposes to replace the graph convolutional matrix Ã with a graph diffusion matrix, such as the Heat Kernel PageRank or the Personalized PageRank matrix. Mixhop <ref type="bibr" target="#b0">[1]</ref> mixes higher-order information to learn a wider class of representations. JKNet <ref type="bibr" target="#b31">[32]</ref> explores the relationship between node influence and random walk in GNNs. DropEdge <ref type="bibr" target="#b23">[24]</ref> and PairNorm <ref type="bibr" target="#b37">[38]</ref> focus on over-smoothing problem and improve performance on GCNs by increasing the number of layers. These works focus on the effectiveness of GNNs; Thus, they are orthogonal to this paper's contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bidirectional Propagation Method</head><p>Generalized PageRank. To achieve high scalability, we borrow the idea of decoupling prediction and propagation from SGC <ref type="bibr" target="#b29">[30]</ref> and APPNP <ref type="bibr" target="#b15">[16]</ref>. In particular, we precompute the feature propagation with the following Generalized PageRank matrix <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_2">P = L =0 w T ( ) = L =0 w D r−1 AD −r • X,<label>(2)</label></formula><p>where r ∈ [0, 1] is the convolution coefficient, w 's are the weights of different order convolution matrices that satisfy ∞ =0 w ≤ 1, and T ( ) = D r−1 AD −r • X denotes the -th step propagation matrix. After the feature propagation matrix P is derived, we can apply a multi-layer neural network with mini-batch training to make the prediction. For example, for multi-class classification tasks, a two-layer GBP model makes prediction with Y = Sof tM ax (σ (PW 1 ) W 2 ) where W 1 and W 2 are the learnable weight matrices, and σ is the activation function.</p><p>We note that equation ( <ref type="formula" target="#formula_2">2</ref>) can be easily generalized to various existing models. By setting r = 0.5, 0 and 1, the convolution matrix D r−1 AD −r represents the symmetric normalization adjacency matrix <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>, the transition probability matrix AD −1 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>, and the reverse transition probability matrix D −1 A <ref type="bibr" target="#b31">[32]</ref>, respectively. We can also manipulate the weights w to simulate various diffusion processes as in <ref type="bibr" target="#b16">[17]</ref>. However, we will mainly focus on two setups in this paper: 1) w = α(1 − α) for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>; 2) w = 0 for = 0, . . . , L − 1 and w L = 1, in which case P degenerates to the L-th transition probability matrix in SGC <ref type="bibr" target="#b29">[30]</ref>.</p><formula xml:id="formula_3">D −1/2 AD −1/2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Bidirectional Propagation Algorithm</head><p>Input: Graph G, level L, training set V t , weight coefficients w , convolutional coefficient r, threshold r max , number of walks per node n r , feature matrix X n×F Output: Embedding matrix P n×F S ( ) ← Sparse (0 n×n ) for = 0, . . . , L; for each node s ∈ V t do Generate n r random walks from s, each of length L; if The j-th random walk visits node u at the -th step, = 0, . . . , L, j = 1, . . . , n r then t) ; return Embedding matrix Pn×F ;</p><formula xml:id="formula_4">S ( ) (s, u) += 1 nr ; Q ( ) , R ( ) ← Sparse (0 n×F ) for = 1, . . . , L; Q (0) ← 0 n×F and R (0) ← ColumnN ormalized (D −r X); for from 0 to L − 1 do for each u ∈ V and k ∈ {0, . . . , F − 1} with R ( ) (u, k) &gt; r max do for each v ∈ N (u) do R ( +1) (v, k) += R ( ) (u,k) d(v) ; Q ( ) (u, k) ← R ( ) (u, k) and R ( ) (u, k) ← 0; Q (L) ← R (L) ; P ← L =0 w • D r • Q ( ) + t=0 S ( −t) R (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Bidirectional Propagation Algorithm</head><p>To reduce the time complexity, we propose approximating the Generalized PageRank matrix P with a localized bidirectional propagation algorithm from both the training/testing nodes and the feature vectors. Similar techniques have been used for computing probabilities in Markov Process <ref type="bibr" target="#b2">[3]</ref>. Algorithm 1 illustrates the pseudo-code of the Bidirectional Propagation algorithm. The algorithm proceeds in three phases: Monte-Carlo Propagation (lines 1-5), Reverse Push Propagation (lines 6-13), and the combining phase (line 14).</p><p>Monte Carlo Propagation from the training/testing nodes. We start with a simple Monte-Carlo propagation (Lines 1-5 in Algorithm 1) from the training/testing nodes to estimate the transition probabilities. Given a graph G and a training/testing node set V t , we generate a number n r of random walks from each node s ∈ V t , and record S ( ) (s, u), the fraction of random walks that visit node u at the -th steps. Note that S ( ) ∈ R n×n is a sparse matrix with at most |V t | • n r non-zero entries. Since each random walk is independently sampled, we have that S ( ) is an unbiased estimator for the -th transition probability matrix D −1 A . We also note that D r−1 AD −r = D r D −1 A D −r , which means we can use D r S ( ) D −r X as an unbiased estimator for the -th propagation matrix T ( ) = D r−1 AD −r X. Consequently, L =0 w D r S ( ) D −r X serves as an unbiased estimator for the Generalized PageRank Matrix P. However, this estimation requires a large number of random walks from each training node and thus is infeasible for fully-supervised training on large graphs.</p><p>Reverse Push Propagation from the feature vectors. To reduce the variance of the Monte-Carlo estimator, we employ a deterministic Reverse Push Propagation (lines 6-13 in Algorithm 1) from the feature vectors. Given a feature matrix X, the algorithm outputs two sparse matrices for each level = 0, . . . , L: the reserve matrix Q ( ) that represents the probability mass to stay at level , and the residue matrix R ( ) that represents the probability mass to be distributed beyond level . We begin by setting the initial residue R (0) as the degree normalized feature matrix D −r X. We also perform column-normalization on R (0) such that each dimension of R (0) has the same L 1 -norm. Starting from level = 0, if the absolute value of the residue entry R ( ) (u, k) exceeds a threshold r max , we increase the residue of each neighbor v at level + 1 by R ( ) (u,k)</p><formula xml:id="formula_5">d(v)</formula><p>and transfer the residue of u to its reserve Q ( ) (u, k). Note that by maintaining a list of residue entries R ( ) (u, k) that exceed the threshold r max , the above push operation can be done without going through every entry in R ( ) . Finally, we transfer the non-zero residue R (L) (u, k) of each node u to its reserve at level L.</p><p>We will show that when Reverse Push Propagation terminates, the reserve matrix Q ( ) satisfies</p><formula xml:id="formula_6">T ( ) (s, k) − d(s) r • ( + 1) • r max ≤ D r Q ( ) (s, k) ≤ T ( ) (s, k)<label>(3)</label></formula><p>for each training/testing node s ∈ V t and feature dimension k. Recall that T ( ) = D r−1 AD −r X is the -th propagation matrix. This property implies that we can use</p><formula xml:id="formula_7">L =0 w D r Q ( )</formula><p>to approximate the Generalized PageRank matrix P = L =0 w T ( ) . However, there are two drawbacks to this approximation. First, this estimator is biased, which could potentially hurt the performance of the prediction. Secondly, the Reverse Push Propagation does not take advantage of the semi-supervised learning setting where the number of training nodes may be significantly less than the total number of nodes n.</p><p>Combining Monte-Carlo and Reverse Push Propagation. Finally, we combine the results from the Monte-Carlo and Reverse Push Propagation to form a more accurate unbiased estimator. In particular, we use the following equation to derive an approximation of the -th propagation matrix T ( ) = D r−1 AD −r X:</p><formula xml:id="formula_8">T( ) = D r • Q ( ) + t=0 S ( −t) R (t) .<label>(4)</label></formula><p>As we shall prove in Section 3.2, T( ) is an unbiased estimator for T ( ) = D r−1 AD −r X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consequently, we can use</head><formula xml:id="formula_9">P = L =0 w T( ) = L =0 w D r • Q ( ) + t=0 S ( −t) R (t)</formula><p>as an unbiased estimator for the Generalized PageRank matrix P defined in equation (2). To see why equation ( <ref type="formula" target="#formula_8">4</ref>) is a better approximation than the naive estimator D r S ( ) D −r X, note that each entry in the residue matrix R (t) is bounded by a small real number r max , which means the variance of the Monte-Carlo estimator is reduced by a factor of r max . It is also worth mentioning that the two matrices S ( −t) and R (t) are sparse, so the time complexity of the matrix multiplication only depends on their numbers of non-zero entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>We now analyze the time complexity and the approximation quality of the Bidirectional Propagation algorithm. Due to the space limit, we defer all proofs in this section to the appendix. Recall that |V t | is the number of training/testing nodes, n r is the number of random walks per node , and r max is the push threshold. We assume D −r X is column-normalized, as described in Algorithm 1. We first present a Lemma that bounds the time complexity of the Bidirectional Propagation algorithm.</p><p>Lemma 1 The time complexity of Algorithm 1 is bounded by O L|V t |n r F + LdF rmax .</p><p>Intuitively, the L|V t |n r F term represents the time for the Monte-Carlo propagation, and the LdF rmax term is the time for the Reverse Push propagation. Next, we will show how to set the number of random walks n r and the push threshold r max to obtain a satisfying approximation quality. In particular, the following technical Lemma states that the Reverse Push Propagation maintains an invariant during the push process.</p><p>Lemma 2 For any residue and reserve matrices Q ( ) , R ( ) , = 0, . . . , L, we have</p><formula xml:id="formula_10">T ( ) = D r−1 AD −r X = D r • Q ( ) + t=0 D −1 A −t R (t) .<label>(5)</label></formula><p>We note that the only difference between equations ( <ref type="formula" target="#formula_8">4</ref>) and ( <ref type="formula" target="#formula_10">5</ref>) is that we replace D −1 A with S ( ) in equation ( <ref type="formula" target="#formula_8">4</ref>). Recall that S ( ) is an unbiased estimator for the -th transition probability matrix D −1 A . Therefore, Lemma 2 ensures that T( ) is also an unbiased estimator for T ( ) . Consequently, P = L =0 w T( ) is an unbiased estimator for the propagation matrix P. Finally, to minimize the overall time complexity of Algorithm 1 in Lemma 1, the general principle is to balance the costs of the Monte-Carlo and the Reverse Push propagations. In particular, we have the following Theorem. </p><formula xml:id="formula_11">O L|V t |F + L √ |Vt|d log n ε F .</formula><p>For fully-supervised learning, we have |V t | = n and thus the time complexity of GBP becomes</p><formula xml:id="formula_12">O LnF + L √ m log n ε F .</formula><p>In practice, we can also make a trade-off between efficiency and accuracy by manipulating the push threshold r max and the number of walks n r .</p><p>Parallelism of GBP. The Bidirectional Propagation algorithm is embarrassingly parallelizable: We can generate the random walks on multiple nodes and perform Reverse Push on multiple features in parallel. After we obtain the Generalized PageRank matrix P, it is trivially to construct mini-batches for training the neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed <ref type="bibr" target="#b24">[25]</ref>, a Protein-Protein interaction network PPI <ref type="bibr" target="#b10">[11]</ref>, a customer interaction network Yelp <ref type="bibr" target="#b36">[37]</ref>, a co-purchasing networks Amazon <ref type="bibr" target="#b7">[8]</ref> and a large social network Friendster <ref type="bibr" target="#b33">[34]</ref>. Table <ref type="table" target="#tab_1">2</ref> summarizes the statistics of the datasets. We first evaluate GBP's performance for transductive semi-supervised learning on the three popular citation networks (Cora, Citeseer, and Pubmed). Then we compare GBP with scalable GNN methods three medium to large graphs PPI, Yelp, Amazon in terms of inductive learning ability. Finally, we present the first empirical study of transductive semi-supervised on billion-scale network Friendster.</p><p>Baselines and detailed setup. We adopt three state-of-the-art GNN methods GCN <ref type="bibr" target="#b14">[15]</ref>, GAT <ref type="bibr" target="#b28">[29]</ref>, GDC <ref type="bibr" target="#b16">[17]</ref> and APPNP <ref type="bibr" target="#b15">[16]</ref> as the baselines for evaluation on small graphs. We also use one state-ofthe-art scalable GNN from each of the three categories: LADIES (layer sampling) <ref type="bibr" target="#b39">[40]</ref>, GraphSAINT (graph sampling) <ref type="bibr" target="#b36">[37]</ref>, SGC and PPRGo (linear model) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initial residual connection <ref type="bibr" target="#b11">[12]</ref> across the hidden layers to facilitate training. For simplicity, we use the Personalized PageRank weights (w = α(1 − α) for some hyperparameter α ∈ (0, 1)). As we shall see, this weight sequence generally achieves satisfying results on graphs with real-world features. On the Friendster dataset, where the features are random noises, we use both Personalized PageRank and transition probability (w L = 1, w 0 =, . . . , = w L−1 = 0) for GBP. We set L = 4 across all datasets. Table <ref type="table">3</ref> summaries other hyper-parameters of GBP on different datasets. We use Adam optimizer to train our model, with a maximum of 1000 epochs and a learning rate of 0.01. For a fair comparison, we use the officially released code of each baseline (see the supplementary materials for URL and commit numbers) and perform a grid search to tune hyperparameters for models. All the experiments are conducted on a machine with an NVIDIA TITAN V GPU (12GB memory), Intel Xeon CPU (2.20 GHz), and 256GB of RAM.</p><p>Table <ref type="table">3</ref>: Hyper-parameters of GBP. r max is the Reverse Push Threshold, w is the number of random walks from the training nodes, w is the weight sequence, r is the Laplacian parameter in the convolutional matrix D r−1 AD −r . Transductive learning on small graphs. Table <ref type="table" target="#tab_2">4</ref> shows the results for the semi-supervised transductive node classification task on the three small standard graphs Cora, Citeseer, and Pubmed. Following <ref type="bibr" target="#b14">[15]</ref>, we apply the standard fixed training/validation/testing split with 20 nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. For each method, we set the number of hidden layers to 2 and take the mean accuracy with the standard deviation after ten runs. We observe that GBP outperforms APPNP (and consequently all other baselines) across all datasets. For the scalable GNNs, SGC is outperformed by the vanilla GCN due to the simplification <ref type="bibr" target="#b29">[30]</ref>. On the other hand, the results of LADIES and GraphSAINT are also not at par with the non-scalable GNNs 83.9 ± 0.7 72.9 ± 0.5 80.6 ± 0.4 such as GAT or APPNP, which suggests that the sampling technique alone might not be sufficient to achieve satisfying performance on small graphs. Inductive learning on medium to large graphs. Table <ref type="table" target="#tab_3">5</ref> reports the F1-score and running time (precomputation + training) of each method with various depths on three large datasets PPI, Yelp, and Amazon. For each dataset, we set the hidden dimension to be the same across all methods: 2048(PPI), 2048(Yelp), and 1024(Amazon). We use "fixed-partition" splits for each dataset, following <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b7">8]</ref> (see the supplementary materials for further details). The critical observation is that GBP can achieve comparable performance as GraphSAINT does, with 5-10x less running time. This demonstrates the superiority of GBP's sub-linear time complexity. For PPRGo, it has a longer running time than other methods because of its expensive feature propagation per epoch. On the other hand, SGC and LADIES are also able to run faster than GraphSAINT; However, these two models' accuracy is not comparable to that of GraphSAINT and GBP.</p><note type="other">Data</note><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the convergence rate of each method, in which the time for data loading, preprocessing, validation set evaluation, and model saving are excluded. We observe that the convergence rate of GBP and SGC is much faster than that of LADIES and GraphSAINT, which is a benefit from decoupling the feature propagation and the neural networks.</p><p>Transductive semi-supervised learning on billion-scale graph Friendster. Finally, we perform the first empirical evaluation of scalable GNNs on a billion-scale graph Friendster. We extracted the top-500 ground-truth communities from <ref type="bibr" target="#b33">[34]</ref> and use the community ids as the labels of each node. Note that one node may belong to multiple communities, in which case we pick the largest community as its label. The goal is to perform multi-class classification with only the graph structural information. This setting has been adapted in various works on community detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>. For each node, we generate a sparse random feature by randomly set one entry to be 1 in an d-dimensional all-zero vector. Note that even with a random feature matrix, GNNs are still able to extract structural information to perform the prediction <ref type="bibr" target="#b32">[33]</ref>. Among the labeled nodes, we use 50,000 nodes (100 from each class) for training, 15,982 for validation, and 25,000 for testing. Table <ref type="table" target="#tab_4">6</ref> report the running time and F1-score of each method with feature dimension F = 10, 40, 70 and 100. We omit GraphSAINT and LADIES as they run out of the 256 GB memory even with the dimension d set to 10. We first observe that both GBP and SGC can capture the structural information with random features, while PPRGo and GBP(PPR) fail to converge. This is because Personalized PageRank emphasizes each node's original feature (with w 0 being the maximum weight among w 0 , . . . , w L ) and, yet, the original features of Friendster are random noises. We also point out that PPRGo starts to converge and achieves an F1-score of 0.15 in 4500 seconds when the feature dimension is increased to 10000. On the other hand, we observe that GBP can achieve a significantly higher F1-score with less running time. Notably, on this 500-class classification task, GBP is able to achieve an F1-score of 0.79 with less than half an hour. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents GBP, a scalable GNN based on localized bidirectional propagation. Theoretically, GBP is the first method that achieves sub-linear time complexity for precomputation, training, and inference. The bidirectional propagation process computes a Generalized PageRank matrix that can express various existing graph convolutions. Extensive experiments on real-world graphs show that GBP obtains significant improvement over the state-of-the-art methods in terms of efficiency and performance. Furthermore, GBP is the first method that can scale to billion-edge networks on a single machine. For future work, an interesting direction is to extend GBP to heterogeneous networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The proposed GBP algorithm addresses the challenge of scaling GNNs on large graphs. We consider this algorithm a general technical and theoretical contribution, without any foreseeable specific impacts. For applications in bioinformatics, computer vision, and natural language processing, applying the GBP algorithm may improve the scalability of existing GNN models. We leave the exploration of other potential impacts to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 1</head><p>To show that Algorithm 1 achieves the desired accuracy, recall that equation ( <ref type="formula" target="#formula_8">4</ref>) is an unbiased estimator for the -th propagation matrix T ( ) . We also observe each entry in residue matrix R ( ) derived by the reserve push propagation is bounded by r max , and we multiply D r to the estimator Q ( ) + −1 t=0 S ( −t) R (t) , it follows the random variable of each random walk from node s ∈ V t is bounded by d(s) r • r max . By Chernoff Bound (Lemma 3), we have ) , the time complexity of Algorithm 1 can be express as</p><formula xml:id="formula_13">O L|V t |F + L|V t | r max log n ε 2 F + L d r max F .</formula><p>We observe that the above complexity is minimized when L|V t | rmax log n ε 2 F = L d rmax F , which implies that</p><formula xml:id="formula_14">r max = ε 2 d |V t | log n = ε d |V t | log n .</formula><p>Therefore, the number of random walks per node n r can be expressed as</p><formula xml:id="formula_15">n r = log n ε 2 • ε d |V t | log n = 1 ε d log n |V t | .</formula><p>Finally, the total time complexity of Algorithm 1 is bounded</p><formula xml:id="formula_16">O L|V t |F + L|V t | r max log n ε 2 F + L d r max F = O L|V t |F + L |V t |d log n ε F ,</formula><p>and the Theorem follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Comparison of inference time</head><p>Figure <ref type="figure" target="#fig_4">2</ref> shows the inference time of each method. We observe that in terms of the inference time, the three linear models, SGC, PPRGo and GBP, have a significant advantage over the two sampling-based models, LADIES and GraphSAINT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Additional details in experimental setup</head><p>Table <ref type="table" target="#tab_5">7</ref> summarizes URLs and commit numbers of baseline codes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPI</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1</head><label>1</label><figDesc>By setting n r = O 1 ε d log n |Vt| and r max = O ε d |Vt| log n , Algorithm 1 produces an estimator P of the propagation matrix P, such that for any s ∈ V t and k = 0, . . . , F − 1, the probability that P(s, k) − P(s, k) ≤ d(s) r ε is at least 1 − 1 n . The time complexity is bounded by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Convergence curves of 4-layer models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 2 , 2 3</head><label>22</label><figDesc>Pr T( ) (s, k) − T ( ) (s, k) ≥ d(s) r ε ≤ exp − n r • d(s) r • ε 2Where µ = T ( ) (s, k). By setting n r = O rmax log n ε we havePr T( ) (s, k) − T ( ) (s, k) ≥ d(s) r ε ≤ exp − log nBy Lemma 1, the time complexity of the Monte-Carlo Propagation is O(L|V t |n r F ) , and the time complexity of the Reserve Push Propagation is O(L d rmax F ). By setting n r = O( rmax log n ε 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inference time of 6-layers models on the entire test graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of time complexity for GNN training and inference.</figDesc><table><row><cell>Method</cell><cell>Precomputation</cell><cell>Training</cell><cell>Inference</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="3">Features Classes Label rate</cell></row><row><cell>Cora</cell><cell>multi-class</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>multi-class</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>0.036</cell></row><row><cell>Pubmed</cell><cell>multi-class</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>0.003</cell></row><row><cell>PPI</cell><cell>multi-label</cell><cell>56,944</cell><cell>818,716</cell><cell>50</cell><cell>121</cell><cell>0.79</cell></row><row><cell>Yelp</cell><cell>multi-label</cell><cell>716,847</cell><cell>6,977,410</cell><cell>300</cell><cell>100</cell><cell>0.75</cell></row><row><cell>Amazon</cell><cell>multi-class</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>100</cell><cell>47</cell><cell>0.70</cell></row><row><cell cols="5">Friendster multi-class 65,608,366 1,806,067,135 100 (random)</cell><cell>500</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results on Cora, Citeseer and Pubmed.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell cols="3">81.5 ± 0.6 71.3 ± 0.4 79.1 ± 0.4</cell></row><row><cell>GAT</cell><cell cols="3">83.3 ± 0.8 71.9 ± 0.7 78.0 ± 0.4</cell></row><row><cell>GDC</cell><cell cols="3">83.3 ± 0.2 72.2 ± 0.3 78.6 ± 0.4</cell></row><row><cell>APPNP</cell><cell cols="3">83.3 ± 0.3 71.4 ± 0.6 80.1 ± 0.2</cell></row><row><cell>SGC</cell><cell cols="3">81.0 ± 0.1 71.8 ± 0.1 79.0 ± 0.1</cell></row><row><cell>LADIES</cell><cell cols="3">79.6 ± 0.5 68.6 ± 0.3 77.0 ± 0.5</cell></row><row><cell>PPRGo</cell><cell cols="3">82.4 ± 0.2 71.3 ± 0.3 80.0 ± 0.4</cell></row><row><cell cols="4">GraphSAINT 81.3 ± 0.4 70.5 ± 0.4 78.2 ± 0.8</cell></row><row><cell>GBP</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results of inductive learning with scalable GNNs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">4-layer</cell><cell cols="2">6-layer</cell><cell>8-layer</cell></row><row><cell></cell><cell></cell><cell>F1-score</cell><cell cols="2">Time (s) F1-score</cell><cell cols="2">Time (s) F1-score</cell><cell>Time (s)</cell></row><row><cell></cell><cell>SGC</cell><cell>65.7 ± 0.01</cell><cell></cell><cell>76 62.4 ± 0.01</cell><cell></cell><cell>173 57.8 ± 0.01</cell><cell>295</cell></row><row><cell></cell><cell>LADIES</cell><cell>57.9 ± 0.30</cell><cell></cell><cell>187 59.4 ± 0.25</cell><cell></cell><cell>206 57.4 ± 0.24</cell><cell>315</cell></row><row><cell>PPI</cell><cell>PPRGo</cell><cell>61.5 ± 0.13</cell><cell></cell><cell>866 61.1 ± 0.02</cell><cell></cell><cell>1976 55.1 ± 0.19</cell><cell>1080</cell></row><row><cell></cell><cell cols="2">GraphSAINT 99.2 ± 0.05</cell><cell></cell><cell>1291 99.4 ± 0.03</cell><cell></cell><cell>1961 99.3 ± 0.01</cell><cell>2615</cell></row><row><cell></cell><cell>GBP</cell><cell>99.3 ± 0.02</cell><cell></cell><cell>117 99.3 ± 0.03</cell><cell></cell><cell>167 99.3 ± 0.01</cell><cell>220</cell></row><row><cell></cell><cell>SGC</cell><cell>41.5 ± 0.21</cell><cell></cell><cell>43 36.8 ± 0.33</cell><cell></cell><cell>70 34.8 ± 0.52</cell><cell>92</cell></row><row><cell></cell><cell>LADIES</cell><cell>27.3 ± 0.56</cell><cell></cell><cell>34 28.5 ± 0.97</cell><cell></cell><cell>39 30.0 ± 0.32</cell><cell>51</cell></row><row><cell>Yelp</cell><cell>PPRGo</cell><cell>64.0 ± 0.16</cell><cell></cell><cell>550 63.7 ± 0.71</cell><cell></cell><cell>1215 63.4 ± 0.49</cell><cell>1665</cell></row><row><cell></cell><cell cols="2">GraphSAINT 64.7 ± 0.08</cell><cell></cell><cell>712 62.0 ± 0.10</cell><cell></cell><cell>996 59.1 ± 0.35</cell><cell>1298</cell></row><row><cell></cell><cell>GBP</cell><cell>65.4 ± 0.03</cell><cell></cell><cell>19 65.5 ± 0.03</cell><cell></cell><cell>30 65.4 ± 0.05</cell><cell>37</cell></row><row><cell></cell><cell>SGC</cell><cell>90.4 ± 0.01</cell><cell></cell><cell>233 89.9 ± 0.03</cell><cell></cell><cell>284 89.7 ± 0.03</cell><cell>342</cell></row><row><cell></cell><cell>LADIES</cell><cell>85.4 ± 0.14</cell><cell></cell><cell>734 85.2 ± 0.20</cell><cell></cell><cell>784 84.6 ± 0.09</cell><cell>1421</cell></row><row><cell>Amazon</cell><cell>PPRGo</cell><cell>83.3 ± 0.51</cell><cell></cell><cell>2775 83.3 ± 0.09</cell><cell></cell><cell>5206 81.6 ± 0.22</cell><cell>9300</cell></row><row><cell></cell><cell cols="2">GraphSAINT 91.5 ± 0.01</cell><cell></cell><cell>957 91.3 ± 0.05</cell><cell></cell><cell>1228 91.4 ± 0.05</cell><cell>2618</cell></row><row><cell></cell><cell>GBP</cell><cell>91.5 ± 0.01</cell><cell></cell><cell>225 91.5 ± 0.01</cell><cell></cell><cell>243 91.6 ± 0.01</cell><cell>300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Results for semi-supervised learning on Friendster.</figDesc><table><row><cell>Dimension</cell><cell>10</cell><cell></cell><cell>40</cell><cell></cell><cell>70</cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>F1-score / Time</cell><cell>F1-score</cell><cell>Time</cell><cell>F1-score</cell><cell>Time</cell><cell>F1-score</cell><cell>Time</cell><cell>F1-score</cell><cell>Time</cell></row><row><cell>SGC</cell><cell cols="8">2.0 ± 0.27 1130 12.9 ± 0.01 2930 27.1 ± 0.01 4549 40.2 ± 0.01 6379</cell></row><row><cell>PPRGo</cell><cell>1.6 ± 0.01</cell><cell>-</cell><cell>1.6 ± 0.01</cell><cell>-</cell><cell>1.6 ± 0.01</cell><cell>-</cell><cell>1.6 ± 0.01</cell><cell>-</cell></row><row><cell>GBP(PPR)</cell><cell>1.6 ± 0.01</cell><cell>-</cell><cell>1.6 ± 0.01</cell><cell>-</cell><cell>7.3 ± 0.20</cell><cell>-</cell><cell>7.3 ± 0.12</cell><cell>-</cell></row><row><cell>GBP</cell><cell>7.5 ± 0.10</cell><cell cols="2">757 26.6 ± 0.04</cell><cell cols="5">863 50.3 ± 0.44 1392 79.7 ± 0.32 1849</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>URLs of baseline codes.</figDesc><table><row><cell>Methods</cell><cell>URL</cell><cell>Commit</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Ji-Rong Wen was supported by National Natural Science Foundation of China (NSFC) No.61832017, and by Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098. Zhewei Wei was supported by NSFC No. 61972401 and No. 61932001, by the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China under Grant 18XNLG21, and by Alibaba Group through Alibaba Innovative Research Program. Ye Yuan was supported by NSFC No. 61932004 and No. 61622202, and by FRFCU No. N181605012. Xiaoyong Du was supported by NSFC No. U1711261.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN</head><p>https://github.com/rusty1s/pytorch_geometric 5692a8 GAT https://github.com/rusty1s/pytorch_geometric 5692a8 APPNP https://github.com/rusty1s/pytorch_geometric 5692a8 GDC https://github.com/klicperajo/gdc 14333f SGC https://github.com/Tiiiger/SGC 6c450f LADIES https://github.com/acbull/LADIES c7f987 PPRGo https://github.com/TUM-DAML/pprgo_pytorch d9f991 GraphSAINT https://github.com/GraphSAINT/GraphSAINT cd31c3</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>We need the following Chernoff Bound for bounded i.i.d. random variables.</p><p>Lemma 3 (Chernoff Bound <ref type="bibr" target="#b8">[9]</ref>) Consider a set {x i } (i ∈ [1, n r ]) of i.i.d. random variables with mean µ and x i ∈ [0, r], we have</p><p>A. On the other hand, in the Reverse Push Propagation phase of Algorithm 1, we push the residue R ( ) (u, k) of node u to its neighbors whenever R ( ) (u, k) &gt; r max , k = 0, . . . , F − 1. For random features, the average cost for this push operation is d, the average degree of the graph. We also observe that for a given level and a given feature dimension k, there are at most 1/r max nodes with residues larger than r max . Consequently, the cost of Reverse Push for a given level and a given feature dimension k is d rmax . Summing up = 0, . . . , L − 1 and k = 0, . . . , F − 1, and the Lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 2</head><p>Let RHS denote the right hand side of equation ( <ref type="formula">5</ref>); We prove the Lemma by induction. Recall that in Algorithm 1, we initialize Q (t) = 0 and R (t) = 0 for t = 0, . . . , , and R (0) = D −r X . Consequently, we have</p><p>which is true by definition. Assuming Equation ( <ref type="formula">5</ref>) holds at some stage, we will show that the invariant still holds after a push operation on node u. More specifically, let I uk ∈ R n×F denote the matrix with entry at (u, k) setting to 1 and the rest setting to zero. Consider a push operation on u ∈ V and k ∈ 0, . . . , F − 1 with |R (t) (u, k)| &gt; r max . We have two cases:</p><p>• I vk for each v ∈ N (u). Consequently, we have</p><p>For the second last equation, we use the fact that v∈N (u)</p><p>( Therefore, the induciton holds, and the Lemma follows.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Abu-El-Haija</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Haru-Tyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast bidirectional probability estimation in markov models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lofgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1423" to="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey: Concentration inequalities and martingale inequalities: A survey</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Math</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="127" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep-aligned convolutional neural network for skeleton-based action recognition and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Montagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Engineering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weissenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heat kernel based community detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kloster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1386" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical comparison of algorithms for network community detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing generalized pagerank methods for seed-expansion community detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11705" to="11716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph star net for generalized multi-task learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR abs/1906</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">12330</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Deepinf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="2110" to="2119" />
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gamenet: Graph augmented memory networks for recommending medication combination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging domain context for question answering over knowledge graph</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Engineering</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ć</forename><surname>Veli Čkovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00216</idno>
		<title level="m">Heterogeneous network representation learning: Survey, benchmark, evaluation, and beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient estimation of heat kernel pagerank for local clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1339" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="974" to="983" />
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11247" to="11256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
