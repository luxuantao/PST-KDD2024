<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Question-Answer Pairs from Online Forums</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
							<email>gcong@inf.ed.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yueheng</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Chin-Yew Lin ‡ Young-In Song</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">§Department of Computer Science</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">†Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">‡Microsoft Research Asia</orgName>
								<orgName type="laboratory">NLP Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Question-Answer Pairs from Online Forums</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5EBC7ECF018A872594954A34E6A2BE4C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval models; I.2.7 [Artificial Intelligence]: Natural Language Processing Algorithms</term>
					<term>Experimentation question answering</term>
					<term>graph based ranking</term>
					<term>labeled sequential patterns</term>
					<term>classification</term>
					<term>information extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online forums contain a huge amount of valuable user generated content. In this paper we address the problem of extracting question-answer pairs from forums. Question-answer pairs extracted from forums can be used to help Question Answering services (e.g. Yahoo! Answers) among other applications. We propose a sequential patterns based classification method to detect questions in a forum thread, and a graph based propagation method to detect answers for questions in the same thread. Experimental results show that our techniques are very promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>An online forum is a web application for holding discussions and posting user generated content in a specific domain, such as sports, recreation, techniques, travel etc. Forums contain a huge amount of valuable user generated content on a variety of topics, and it is highly desirable if the human knowledge contained in user generated content in forums can be extracted and reused.</p><p>In this paper we focus on mining knowledge in the form of question-answer (QA) pairs from forums. Many forums contain question-answer knowledge. We investigated 40 forums and found that 90% of them contain question-answer knowledge. Mining question-answer pairs from forums has the following applications. First, question-answer pairs are essential to many QA services, including instant answers provided by search engines, QA search systems, and community-based Question Answering (CQA) services. For example, CQA services, such as Yahoo! Answers, Baidu and Naver 1 , have recently become very popular. Forum has a longer history than CQA and contains much larger user-generated content. For example, there were about 700,000 questions in the travel category of Yahoo! Answers as of January 2008. We extracted about 3,000,000 travel related questions from six online travel forums. One would expect that a CQA service with large QA data will attract more users to the service, and the question-answer knowledge embedded in forums could greatly enrich the knowledge base of CQA. Second, question-answer pairs seem to be a natural way to improve forum management (including querying and archiving). Questions are usually the focus of forum discussions and a natural means of resolving issues <ref type="bibr" target="#b18">[18]</ref>. Access to forum content could be improved by querying question-answer pairs extracted from forums, which highlight the questions asked and the answers given. Third, question-answer knowledge mined from forums can be used to augment the knowledge base of chatbot <ref type="bibr">[7]</ref>.</p><p>Although it is highly valuable and desirable to extract question answer pairs embedded in forums that are largely unstructured, to our surprise none of previous work addresses this problem. Each forum thread usually contains an initiating post and a couple of reply posts. The initiating post usually contains several questions and reply posts may contain answers to the questions in the initiating post or new questions. The asynchronous nature of forum discussion makes it common for multiple participants to pursue multiple questions in parallel.</p><p>In this paper, for each forum thread we find questions and their respective answers in the thread. We propose a new method to detect question-answer pairs in forums, which consists of two components, question detection and answer detection.</p><p>Question detection. The objective is to detect all the questions within a forum thread. The problem at first glance seems to be easy. Unfortunately, it turns out to be non-trivial on the basis of our analysis on 1,000 questions from forums. Questions in forums are often stated in an informal way and questions are stated in various formats. We found that simple rule based methods, such as questionmark and 5W1H question words, are not adequate for forum data. For example, 30% questions do not end with question marks while 9% sentences ending with question marks are not questions in a forum corpus. We develop a classification-based technique to detect questions using sequential patterns automatically extracted from both questions and non-question sentences in forums as features.</p><p>Answer detection. Given the questions detected in a forum thread, we aim to find their answer passages within the same forum thread. Answer detection is difficult due to the following reasons: First, multiple questions and answers may be discussed in parallel and are often interweaved together, while the reply relationship between posts is usually unavailable; Second, one post may contain answers to multiple questions and one question may have multiple replies.</p><p>One straightforward approach to finding answer is to cast answer-finding as a traditional document retrieval problem by considering each candidate answer as an isolated document and the question as a query. We then can employ ranking methods, such as cosine similarity, query likelihood language model and KLdivergence language model. However, these methods do not consider the relationship of candidate answers and forum-specific features, such as the distance of a candidate answer from a question.</p><p>To model the relationship between candidate answers and make use of forum-specific features, in this paper we develop a new graph-based approach for answer detection. We model the relationship between answers to form a graph using a combination of three factors, the probability assigned by language model of generating one candidate answer from the other candidate answer, the distance of candidate answer from question, and the authority of authors of candidate answer in forums. For each candidate answer, we can compute an initial score of being a true answer using a ranking method. To use the graph to compute a final propagated score, we consider two methods. The first one integrates the initial score after propagation, while the second one integrates the initial score in the process of propagation.</p><p>In this paper, we propose and address the problem of extracting question-answer pairs from forums. We make the following main contributions:</p><p>First, we develop a classification-based method for question detection by using sequential pattern features automatically extracted from both questions and non-questions in forums.</p><p>Second, we propose an unsupervised graph-based approach for ranking candidate answers. Better still, the graph-based approach can be integrated with classification method when training data is available. For example, the results of the graph-based approach can be used as features for supervised method.</p><p>Finally, we conduct extensive experiments on several data. The size of our data is much larger than those used in previous work on forums. The main experimental results include 1) our method outperforms rule-based methods for question detection; and 2) the unsupervised graph-based method outperforms other methods including the classification methods <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed technique for question detection and Section 4 presents the proposed techniques for answer detection. We evaluate our techniques in Section 5. Section 6 concludes this paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>To our knowledge, none of previous work finds question-answer pairs from forum data. There is some research on knowledge acquisition from forums or discussion boards, e.g. <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b23">23]</ref>. Close to our work is extracting input-reply pairs for chatbot knowledge in <ref type="bibr" target="#b23">[23]</ref>. The input-reply pair is quite different from question-answer pair in this paper: First, thread titles are treated as input while we detect one or multiple questions from a thread; Second reply post as a whole is considered as a reply while one reply post may contain answers to multiple questions. Moreover, a classification model using lexical features and structured features is employed to distinguish reply posts from non-reply posts <ref type="bibr" target="#b23">[23]</ref>. In contrast, our graph-based method is unsupervised. Feng et al. <ref type="bibr" target="#b5">[5]</ref> implemented a discussion-bot to automatically answer students' queries by matching the reply posts from an annotated corpus of archived threaded discussions with students' query using cosine similarity. The problem there is quite different from ours. The work in <ref type="bibr" target="#b23">[23]</ref> summarizes internet relay chat by clustering a chat log into several sub-topics, and a classification method is used to identify responding messages of the first message in each sub-topic. The classification method used in <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> can be adapted to rank candidate answers for answer finding, but experimental results show that our unsupervised techniques achieve better performance than them.</p><p>Another related work is extracting question-answer pairs for email summarization <ref type="bibr" target="#b18">[18]</ref> that detects interrogative questions using a classification method similar to <ref type="bibr" target="#b21">[21]</ref>, and build a classifier to find answers using lexical features and email-specific features. In our recent work <ref type="bibr" target="#b4">[4]</ref>, conditional random field models are trained to extract the contexts and answers of questions from forums and it does not consider question extraction. In contrast, we propose unsupervised techniques for answer detection, and a supervised method using automatically extracted sequence features for question detection. From our experience, it is easy to label questions, but it is much harder and thus expensive to label answers to a question.</p><p>Extensive research has been done in TREC or AQUAINT-style question-answering, e.g. <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b3">3]</ref>. They mainly focus on constructing short answers for a relatively limited types of question, such as factoid questions, from a large document collection. This makes it feasible to classify the answer type and target when a question is parsed. In contrast, our work focuses on the task of locating answers within a forum thread. Typical questions extracted in forums are more complex, and it is difficult to represent and identify answer types for questions of forums, which has long been recognized to be tough <ref type="bibr" target="#b14">[14]</ref>. The good news is that we can take advantage of forum-specific features, such as the distance of candidate answer from question, to extract answers from forums.</p><p>Research on FAQ retrieval <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b17">17]</ref> and CQA retrieval <ref type="bibr" target="#b8">[8]</ref> has focused on finding similar questions (together with answers) for a user-given question by leveraging answer information. Query expansion <ref type="bibr" target="#b1">[1]</ref> and machine translation model <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17]</ref> are employed to bridge the lexical chasm between question and answer. The above work is complementary to our work, and could be employed to enhance our methods. In this paper we learn the lexical gap between question and answer using query expansion method in <ref type="bibr" target="#b1">[1]</ref>. We also notice the work <ref type="bibr" target="#b9">[9]</ref> that retrieves question-answer pairs from FAQ pages by leveraging indentation (e.g. table) and line prefix (e.g. Q:, A:). The task there is easier than ours.</p><p>The graph-based methods, such as PageRank <ref type="bibr" target="#b2">[2]</ref>, have shown to be effective in the Web search, where explicit links between web documents are present. There is also some work building graphs induced by implicit relationships between documents <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b11">11]</ref>. A graphical model <ref type="bibr" target="#b11">[11]</ref> is employed to estimate the joint probability of all answers for answer processing in QA. A graph built using language models is leveraged for document retrieval in <ref type="bibr" target="#b12">[12]</ref>. Somewhat more closely related to our work is the LexRank <ref type="bibr" target="#b15">[15]</ref> for re-ranking candidate answers. LexRank method uses cosine similarity between candidate answers to build a undirected graph while we build weighted directed graph. In addition, we adopt a different propagation strategy to compute ranking scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ALGORITHMS FOR QUESTION DETECTION</head><p>For question detection in forums, rules, such as question mark and 5W1H words, are not adequate. With question mark as an example, we find that 30% questions do not end with question marks while 9% sentences ending with question marks are not questions in a corpus. This is because 1) questions can be expressed by imperative sentences, e.g. "I am wondering where I can buy cheap and good clothing in beijing."; 2) question marks are often omitted in forums; 3) short informal expressions, such as "really?", should not be regarded as questions. To complement the inadequacy of simple rules, in this paper we extract labeled sequential patterns from both questions and non-questions to characterize them, and then use the discovered patterns as features to build classifiers for question detection. Labeled sequential patterns are used to identify comparative sentences <ref type="bibr" target="#b10">[10]</ref> and erroneous sentences <ref type="bibr" target="#b19">[19]</ref>.</p><p>We next first explain labeled sequential patterns (LSPs) and present how to use them for question detection. Consider a question, "i want to buy an office software and wonder which software company is best." "wonder which...is" would be a good pattern to characterize the question.</p><p>A labeled sequential pattern (LSP), p, is an implication in the form of LHS → c, where LHS is a sequence and c is a class label. Let I be a set of items and L be a set of class labels. Let D be a sequence database in which each tuple is composed of a list of items in I and a class label in L. We say that a sequence</p><formula xml:id="formula_0">s 1 =&lt; a 1 , ..., a m &gt; is contained in a sequence s 2 =&lt; b 1 , ..., b n &gt; if 1) there exist integers i 1 , ...i m such that 1 ≤ i 1 &lt; i 2 &lt; ... &lt; i m ≤ n</formula><p>and aj = bi j for all j ∈ 1, ..., m, and 2) the distance between the two adjacent items b i j and b i j+1 in s 2 needs to be less than a threshold λ (we used 5). Similarly, we say that a LSP p 1 is contained by p 2 if the sequence p 1 .LHS is contained by p 2 .LHS and p 1 .c = p 2 .c. Note that it is not required that s1 appears continuously in s2.</p><p>The support of p, denoted by sup(p), is the percentage of tuples in database D that contain the LSP p. The probability of the LSP p being true is referred to as "the confidence of p ", denoted by conf(p), and is computed as</p><formula xml:id="formula_1">sup(p)</formula><p>sup(p.LHS) . The support is to measure the generality of the pattern p and minimum confidence is a statement of predictive ability of p. For example, consider a sequence database containing three tuples t1</p><formula xml:id="formula_2">= (&lt; a, d, e, f &gt;, Q), t2 = (&lt; a, f, e, f &gt;, Q) and t3 = (&lt; d, a, f &gt;, N Q). One ex- ample LSP p 1 = &lt; a, e, f &gt;→ Q, which is contained in tuples t 1 and t 2 .</formula><p>Its support is 66.7% and its confidence is 100%. As another example, LSP p 2 = &lt; a, f &gt;→ Q with support 66.7% and confidence 66.7%. p1 is a better indication of class Q than p2.</p><p>To mine LSPs, we need to pre-process each sentence by applying Part-Of-Speech (POS) tagger MXPOST Toolkit<ref type="foot" target="#foot_0">2</ref> to tag each sentence while keeping keywords including 5W1H, modal words, "wonder", "any" etc. For example, the sentence "where can you find a job" is converted into "where can PRP VB DT NN", where "PRP", "VB", "DT" and "NN" are POS tags. Each processed sentence becomes a database tuple. Note that the keywords are usually good indications of questions while POS tags can reduce the sparseness of words. The combination of POS tags and keywords allows us to capture representative features for question sentences by mining LSPs. Some example LSPs include "&lt;anyone, VB, how&gt; → Q", and "&lt;what, do, PRP, VB&gt; → Q". Note that the confidences of the discovered LSPs are not necessary 100%, their lengths are flexible and they can be composed of contiguous or distant words/tags.</p><p>Given a collection of processed data, we will mine LSPs by imposing both minimum support threshold and minimum confidence threshold. The minimum support threshold is to ensure that the discovered patterns are general while the minimum confidence threshold ensures that all discovered LSPs are discriminating and are capable of predicting question or non-question sentences. In our experiments, we empirically set minimum support at 0.5% and minimum confidence at 85%<ref type="foot" target="#foot_1">3</ref> . Existing frequent sequential pattern mining algorithms (e.g. <ref type="bibr" target="#b16">[16]</ref>) do not consider minimum confidence constraint, and we adapt them to mining LSPs with constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHMS FOR ANSWER DETECTION</head><p>In this section, we present our techniques to find answers in forums for extracted questions. The input is a forum thread with the questions annotated; the output is a list of ranked candidate answers for each question. We observed that paragraphs are usually good answer segments in forums. For example, given a question "Can anyone tell me where to go at night in Orlando?", its answer "You would be better off outside the city. look into International drive or Lake Buena Vista. for nightlife try Westside in the Disney Village. have a look at MARRIOTTVILLAGE.COM. located in LBV" is a paragraph. We also observed that the answers to a question usually appear in the posts after the post containing the question. Hence for each question we assume its set of candidate answers to be the paragraphs in the following posts of the question. We would point out that the proposed techniques are equally applicable to other kinds of segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary</head><p>We will briefly introduce three IR methods to rank candidate answers for a given forum question, and then discuss how to adapt the classification method <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> to rank answers.</p><p>Cosine Similarity. Given a question q and a candidate answer a, their cosine similarity weighted by inverse document frequency (idf) can be computed as follows:</p><formula xml:id="formula_3">COS(q, a) = w∈q,a f (w, q) × f (w, a)(idf w ) 2 w∈q (f (w, q)idf w ) 2 × w∈a (f (w, a)idf w ) 2</formula><p>(1) where f (w, X) is the frequency of word w in X, idf w is inverse document frequency (idf) (each document corresponds to a post in the thread of question q).</p><p>Query likelihood language model. The probability of generating a question q from language models of candidate answers can be used to rank candidate answers. Given a question q and a candidate answer a, the ranking function for the Query likelihood language model using Dirichlet smoothing is as follows:</p><formula xml:id="formula_4">QL(q|a) = w∈q P (w|a)<label>(2)</label></formula><formula xml:id="formula_5">P (w|a) = |a| |a| + λ × f (w, a) |a| + λ |a| + λ × f (w, C) |C| (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where f (w, X) denotes the frequency of word x in X, and C is the background collection used to smooth language model. KL-divergence language model. It has been shown that a model comparison approach outperforms query-likelihood model for information retrieval <ref type="bibr" target="#b13">[13]</ref>. We construct unigram question language model Mq for question q and unigram answer language model Ma for answer candidate answer a. We then compute KLdivergence between the answer language M a and question language model M q below. KL(M a ||M q ) = w p(w|M a )log(p(w|M a )/p(w|M q )) (4) Classification based re-ranking. Recall that as discussed in related work, classification method <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> is employed to extract knowledge from forums, though not question-answer pairs. Classifiers are built to extract input-response pairs <ref type="bibr">[7]</ref> using content features (e.g. the number overlapping words between input and reply post ) and structural features (e.g. is the reply posted by the thread starter). The other work <ref type="bibr" target="#b23">[23]</ref> uses slightly different features. We can treat each question and candidate answer pair as an instance, compute features for the pair, and train a classifier. The value returned by a classifier, called as classification scores, can be used to rank the candidate answers of a question. Note that classification based re-ranking method needs training data which are usually expensive to get.</p><p>The methods presented above do not make use of any inter candidate answer information, while the candidate answers for a questions are not independent in forums. We next present an unsupervised graph-based method that considers the inter-relationships of candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph based propagation method</head><p>The graph-based propagation method has been successful in Web search where links are usually obvious. However, it remains largely unexplored to apply graph-based propagation for answer finding especially in forum data. Intuitively, if a candidate answer is related to (e.g. similar to) an authoritative candidate answer with high score, the candidate answer, which may not have a high score, is also likely to be an answer. In this section, we first present how to build graphs for candidate answers, and then how to compute ranking scores of candidate answers using the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Building Graphs</head><p>Given a question q, and the set A q of its candidate answers, we build a weighted directed graph denoted as (V, E) with weight function w : E → R, where V is the set of vertices and E is the set of directed edges and w(u → v) is the weight associated with edge u → v. Each candidate answer in A q will correspond to a vertice in V . The problem is how to generate the edge set E.</p><p>Given two candidate answers a o and a g , we use KL-divergence language model KL(ao|ag) (resp. KL(ag|ao)) to determine whether there will be an edge a o → a g (resp. a g → a o ). The use of KL divergence language model can be motivated by the following example: consider two candidate answers for a question q: can tell me some about hotel. a1: world hotel is good but i prefer century hotel and a2: world hotel has a very good restaurant. Knowing that a 2 is answer would provide evidence that a 1 is also somewhat important and could be answer, but not vice versa. This is because a 1 concerns both world hotel and century hotel while a 2 concerns only world hotel. KL-divergence language model allows us to capture the asymmetry in how the authority is propagated. The cosine similarity used in <ref type="bibr" target="#b15">[15]</ref> to construct a graph cannot capture the asymmetry.</p><p>We next introduce the definitions of generator and offspring that will frame edge generation. Definition 1: Given two candidate answers ao and ag, if 1/(1 + KL(ao|ag)) is larger than a given threshold θ, an edge will be formed from a o to a g . We say that a g is a generator of a o and a o is an offspring of a g . 2</p><p>According to the definition, we can determine whether to generate an edge from a o to a g , and similarly we can determine the presence of an edge from a o to a g by comparing KL(a g |a o ) and θ. The parameter θ in the definition is determined empirically and we found in our experiments that our methods are not sensitive to the parameter. We allow self-loop, i.e., each candidate answer can be its own generator. The self-loop edge will allow that one candidate answer is its own generator and offspring. This will also function as a smoothing factor in computing weight and authority. Note that one candidate answer can be a generator of multiple candidate answers and that it is possible for one candidate answer to have no generator. In the extreme case, there is no edges in the graph and thus graph propagation is turned off.</p><p>After we have both vertices and edges, the remaining problem is to compute weight for each edge. One straightforward way is to use the KL-divergence score. To achieve better performance, we will consider two more factors in computing weight.</p><p>First, we observe that the replying posts far away from the question post usually are less likely to contain answers for the questions in the post in forums. Hence, when building the digraph for a question, we consider the distance between a candidate answer and the question, denoted by d(q, a).</p><p>Second, we observe that in forums the posts from authors with high authority are more likely to contain answers. Some forums may provide the authority level of authors while many forums do not have the information. We estimate the authority of an author in terms of the number of his replying posts and the number of threads initiated by him:</p><formula xml:id="formula_7">author(i) = (#reply i ) 2 /#start i max j∈I ((#reply j ) 2 /#start j )</formula><p>, where I is the set of all authors in a forum.</p><p>Given two candidate answers ao and ag, the weight for edge ao → ag is computed by a linear interpolation of the three factors, namely the similarity computed from KL-divergence KL(a o |a g ), the distance of a g from q, and the authority of the author of a g . w(ao → ag) = 1 1 + KL(P (ao)|P (ag))</p><p>+ λ1 1 d(ag, q)</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+λ2author(ag)</head><p>We employ the normalization method in PageRank algorithm <ref type="bibr" target="#b2">[2]</ref> to normalize weight. Intuitively, given a "damping factor" λ<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b2">[2]</ref> (was set at 0.01), a candidate answer ao and a set of its generators G a o in the set of candidate answers A, we normalized the weight w(a o → a g ) among all generators g of a o , g ∈ G ao .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nw(ao</head><formula xml:id="formula_8">→ ag) = λ 1 |Aq| + (1 -λ) w(a o → a g ) g∈Ga o w(ao → g) . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>If a candidate answer has multiple generators, the importance of the weight of the generators will be normalized across its generators. We next illustrate the normalization with an example. Consider the graph built from the candidate answers of a question given in Figure <ref type="figure" target="#fig_0">1</ref>. The candidate answer ao 1 has three generators, ag 1 , a g 2 and itself. The weight of edge a o 1 → a g 1 will be normalized from three weights w(a o 1 → a g 1 ), w(a o 1 → a g 2 ) and w(a o 1 → a o 1 ). Note that a candidate answer can be a generator of itself and would function as a smoothing factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Computing Propagated Scores</head><p>We develop two approaches to integrating the propagated authority with the initial ranking scores that are computed using any approach described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Propagation without initial score:</head><p>For each candidate answer a ∈ C a , the methods introduced in Section 4.1 can be employed to compute its initial ranking score. Moreover, we also compute its authority value, which can be understood as the "prior" of the candidate answer to be used to adjust the initial ranking score. The product of the authority value and the initial ranking score between candidate answer a and question q will be returned as the final ranking score for a. P r(q|a) := authority(a) × score(q, a)</p><p>where score(q, a) is the initial ranking score, and authority(a) implies the significance of answer a in the answer graph.</p><p>We next describe how to compute the authority score for a candidate answer a. Inspired by the work in <ref type="bibr" target="#b12">[12]</ref> that computes the authority of documents in information retrieval, we can compute authority for a candidate answer a by the weighted in-degree for each candidate answer a ∈ C a in the given graph, i.e. the initial authority of ag,</p><formula xml:id="formula_11">authority(a g ) = a o ∈C a nw(a o → a g ).<label>(8)</label></formula><p>As observed in <ref type="bibr" target="#b12">[12]</ref>, if the authority of offspring a o (generated by a g ) of a g is low, the authority of a g would not be high. Intuitively, if all answers generated by a specific answer are not central, it will not be central. Note that the reverse may not be true: even if the generator of ag is important, it is not necessary that its offspring a o is important. The motivation can be modeled by defining the authority of a g recursively as follows:</p><formula xml:id="formula_12">authority(ag) = ao∈Ca nw(ao → ag) × authority(ao) (9)</formula><p>The authority propagation will converge. The edge weights after normalization in Equation 6 correspond to transition probabilities for a Markov chain that is aperiodic and irreducible, and converges to the stationary distribution regardless of where it begins. The stationary distribution of a Markov chain can be computed by a simple iterative algorithm called power method which converged very quickly in our experiments. Propagation with initial score: Unlike the first approach, this approach incorporates the initial score between candidate answer and question into propagation. This propagation mechanism is inspired by the work <ref type="bibr" target="#b15">[15]</ref>. Given a question q and its set Cq of candidate answer, the ranking score of a candidate answer a, a ∈ Cq will be computed recursively as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P r(q|a) = λ P r(q|a)</head><p>t∈C q P r(q|t)</p><formula xml:id="formula_13">+(1-λ) v∈Cq nw(v → a)×P r(q|v) (10)</formula><p>where the parameter λ is a trade-off between the score of a and the scores of a's offsprings in the equation, and is determined empirically. For higher value of λ, we give more importance to the score of the candidate answers itself compared to the score of its offsprings. The weight nw is computed in Equation <ref type="formula" target="#formula_8">6</ref>.</p><p>The propagation will converge and the stationary distribution of a Markov chain can be computed by an iterative power method algorithm. The denominators t∈C q P r(q|t) are used for normalization and the second term in the equation is also normalized so that the weights of all edge leading out of any candidate answer will sum up to 1. Therefore, they can be treated as transition probabilities. With probability (1-λ), a transition is made to the nodes that are generators of the current node. Every transition is weighted according to the similarity distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Integration with other methods</head><p>One benefit of graph-based method is that it is complementary with supervised methods for knowledge extraction, e.g. <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> and techniques for question answering, e.g. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b17">17]</ref>. This section will discuss them respectively. First, the graph-based model can be integrated with classification model when training data is available. Second, we learn lexical matchings between questions and answers to enhance the IR methods for answer ranking, and thus graphbased methods. The integration of graph-based model and classification model. Graph-based method and classification method can be integrated in two ways when training data is available. First, for each candidate answer and question pair, the results returned by graph-based methods can be added as features for classification method to determine if the candidate answer is an answer of the question. The returned classification score for each candidate answer will be used to rank all the candidate answers of a question. In doing so, the classification model can make use of the relationship between candidate answers. Second, the classification score returned by a classifier is often (or can be transformed into) the probability for a candidate answer being a true answer and can be used as initial score for propagation of graph-based model. Bridging the lexical chasm between questions and answers for graph-based model. Question and answer may use different words. For example, why → because. This is studied in previous work e.g. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17]</ref>. The benefit from enhancing question with answer words can also be compared with that from topic models in TREC question answering <ref type="bibr" target="#b22">[22]</ref>. However, it is difficult to build a variety of topic models for different questions as mentioned in <ref type="bibr" target="#b22">[22]</ref>. We learn the mapping by computing the mutual information between question terms and answer terms in a training set of QA pairs <ref type="foot" target="#foot_3">5</ref> . We then make use of the answer terms by adding the top-k terms with the highest mutual information to expand question as in <ref type="bibr" target="#b1">[1]</ref>. Interested readers can refer to <ref type="bibr" target="#b1">[1]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we will evaluate the techniques for question detection and answer detection.</p><p>Data. We selected three forums of different scales to obtain source data. 1) We obtained 1,212,153 threads from TripAdvisor forum <ref type="foot" target="#foot_4">6</ref> ; 2) We obtained 86,772 threads from LonelyPlanet forum <ref type="foot" target="#foot_5">7</ref> ; 3) We obtained 25,298 threads from BootsnAll Network <ref type="foot" target="#foot_6">8</ref> .</p><p>From the source data, we generated two datasets for question identification. From the TripAdvisor data, we randomly sampled 650 threads. Each thread in our corpus contains at least two posts and on average each thread consists of 4.46 posts. Two annotators were asked to tag questions and their answers in each thread. The kappa statistic for identifying questions is 0.96. The kappa statistic for linking answers and questions given a question is 0.69, which is lower than that for questions. The reason would be that questions are easier to annotate while it is more difficult to link answers with questions. We then generated two datasets by taking the union of the two annotated data, denoted as Q-TUnion, and the intersection, denoted as Q-TInter. In Q-TUnion a sentence was labeled as a question if it was marked as a question by either annotator; In Q-TInter a sentence was labeled as a question if both annotators marked it as a question. From the source data, we generated five datasets for answer detection. First, two datasets are generated from the 650 annotated threads by taking the union and intersection of the two annotated data, denoted as A-TUnion and A-TInter, respectively. An answer candidate was labeled as an answer if either annotator marked it as an answer for A-TUnion, and if both annotators marked it for A-TInter. Here we used questions in Q-TInter. Second, we randomly sampled 100 threads from TripAdvisor, LonelyPlanet and BootsnAll, respectively. Thus we get another three datasets, denoted as A-Trip2, A-Lonely and A-Boots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on question detection</head><p>The experiment is to evaluate the performance of question detection method against simple rules and the method in <ref type="bibr" target="#b18">[18]</ref>, denoted as SM. We randomly selected two subsets (containing 1221 sentences) from Q-TUnion and Q-TInter, respectively, as test date, and randomly selected another 4003 sentences from the remaining sentences in Q-TUnion and Q-TInter, respectively, to mine LSPs. Table <ref type="table" target="#tab_0">1</ref> gives the results of Precision, Recall and F 1 -score. The experimental results are obtained through 10-fold cross-validation for SM and our method. The rule 5W-1H words is that a sentence is a question if it begins with 5W-1H words; The rule Question Mark is that a sentence is a question if it ends with question mark. Although Question Mark achieves good precision, its recall is low. Our method outperforms the simple rule approaches and also SM, and the improvements are statistically significant (p-value &lt; 0.001). The main reason for the improvement could be that the discovered labeled sequential patterns are able to characterize questions. Note that both SM and our approach employ Ripper to build classifiers. For example, in one experiment on Q-TUnion, we mined 2,316 patterns for questions, which consist of the combination of question mark, keywords (e.g. 5W1H words) and POS tags (e.g. 1,074 patterns contain question mark); we also mined 2,789 patterns for non-questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on answer identification</head><p>In this subsection, we evaluate the performance of graph-based answer detection method and compare it with other methods. We also study the performance of integrating graph-based method and classification method, and the effectiveness of question-answer lexical mapping.</p><p>Metrics. We evaluated the performance of our approaches for answer finding using three metrics -Mean Reciprocal Rank (MRR) <ref type="bibr" target="#b20">[20]</ref>, Mean Average Precision (MAP) and Precision@1(P@1). MRR is the mean of the reciprocal ranks of the first correct answers over a set of questions. This measure gives us an idea of how far down we must look in the ranked list in order to find a correct answer. MAP is the mean of the average of precisions computed after truncating the list after each of the correct answers in turn over a set of questions. MRR considers the first correct answer while MAP considers all correct answers. P@1 is the fraction of the top-1 candidate answers retrieved that are correct. In the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Abbrev. Nearest Answer/Random Guess NA LexRank <ref type="bibr" target="#b15">[15]</ref> Lex Classification <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">23]</ref>  context of extracting question-answer pairs, we are usually more interested in the top-1 returned answer and thus the P@1 measure would be ideal. However, some types of questions, such as asking for advice, often have more than 1 correct answer and it would be useful to find alternative answers. Hence, we report results using all the three metrics. Methods. Table <ref type="table" target="#tab_1">2</ref> lists the methods evaluated and their abbreviations. The better of the Nearest Answer and Random Guess was reported as a baseline. The LexRank algorithm <ref type="bibr" target="#b15">[15]</ref> was used for answer finding. Although LexRank assumed sentences as answer segments in <ref type="bibr" target="#b15">[15]</ref>, it is applicable to paragraphs used in our experiments. The classification methods used in <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">23]</ref> were adapted for re-ranking candidate answers (Section 4.1) and the better one was reported. Graph+Cosine similarity(G+CS) (resp. G+QL and G+KL) represents the graph-based model using cosine similarity (resp. Query Likelihood and KL divergence) as the initial ranking score. Graph(Classification) represents to use results of the classification based re-ranking <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">23]</ref> as the initial score and Classification(Graph) represents to use the results of graph-based models as features for classification based re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Results on fully annotated dataset</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the P@1(together with the number of correct top-1 answers), MRR scores and MAP scores on A-TUnion data containing 1,535 questions from 600 threads <ref type="foot" target="#foot_7">9</ref> . Each question has 10.5 candidate answers on average. As shown in Table <ref type="table" target="#tab_2">3</ref>, graphbased methods significantly outperform their respective counterparts in terms of all the three measures as expected. For example on A-TUnion data G+KL performs 15.1% (resp. 15.7%) better than KL on all questions (resp. questions with answers) in terms of P@1 and the improvements are statistical significant (p-value &lt; 0.001). The main reason for the improvements is that G+KL takes advantage of the relationship of candidate answers and some forum-specific features. The reason for reporting the results on the set of questions with answers is that 284 questions do not have answers and setting thresholds for the methods in Therefore, the results reported on questions with answers would be more informative to compare the performance of these methods. We will further discuss detecting questions without answers. Note that the parameters of graph-based method were determined on a development set with 50 threads (to be explained later).</p><p>We also observed that G+KL outperforms G+QL and G+CS and they all outperform the baseline method NA. The improvements are statistically significant on all three metrics (p-value &lt; 0.001). The nearest answer method performed better than random guess method and we only report the former. It is interesting to observe that G+KL outperforms the supervised methods adapted from <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> (statistically significant, p-value &lt; 0.001). The classification results are reported on the average of 10-fold cross-validation on 5 runs (20-fold cross-validation returned similar results). The reason for the superiority of G+KL is that it leverages the relationship between candidate answers while the supervised model <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">7]</ref> does not. G+KL also significantly outperforms Algorithm Lex. Improved results on subsets. As we have seen, our approaches work well on questions with answers. However, the overall performance deteriorated due to the questions without answers. Hence, the performance should be improved if we can successfully detect the questions without answers. Unfortunately, it is a tough problem, and setting thresholds did not work as mentioned earlier. We observed that most of first questions of each thread have answers. Of 486 first questions, only 21 of them do not have answers for A-TUnion data and 45 for A-TInter data. The results on the subset of A-TUnion are given in Table <ref type="table" target="#tab_4">4</ref>. Due to space limitation, we do not give results on subset of A-TInter. The table shows that the performance on the subset is much better than that on all the questions, although the subset contains only one third of all question-answer pairs in forums. In real QA services, correct answers would be desirable for users' satisfaction.</p><p>In addition, the classification methods <ref type="bibr">[7]</ref> would tell if a candidate answer is a real answer to a question, and thus we can determine if a question has answers by checking each pair of question and answer candidate. However the result was very poor. Instead, we built a classifier by treating each question and all its candidate answers as an instance. In addition to similarity features between question and its candidate answers, we extracted question-specific features, such as location of questions in a thread. The classifier returned 689 questions of which 49 do not have answers. We did error analysis and found neither similarity features nor questionspecific features are very effective. For example, only 17.8% of the questions without answers do not have common words with candidate answers while 15.6% of questions with answers do not, either. This is still an open problem to be investigated in the future. Graph-based propagation methods. The objective of this experiment is to evaluate the different options in graph-base propagation methods. The options include:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All questions Question with answer P@1(#) MRR MAP P@1 MRR MAP • Different ranking methods including CS, QL and KL</p><p>• Different methods of computing weight. We would like to know the usefulness of distance and authority in computing weight. Hence, we compare using KL-divergence alone, denoted as G K , and using all the three factors as in Equation <ref type="formula">5</ref>(by default and denoted as G A ).</p><p>Note that in graph-based method, propagation without initial score method and all the three factors in Equation 5 are used by default. For example, G+KL represents G A,1 +KL. By combining the different options, we got several methods shown in Table <ref type="table" target="#tab_5">5</ref>. For example GK,2+KL represents to use the propagation method, propagation with initial score and use KL to compute weight. The performance of using Equation <ref type="formula">5</ref>, G A , always outperforms using KL divergence alone G K . This demonstrates the usefulness of forumspecific features used in Equation <ref type="formula">5</ref>. The ranking method KL always performs better than other two methods CS and QL. We also found that propagation without initial score G 1 often outperforms the other G 2 , but not always.</p><p>There are three parameters in the graph-based model. They are determined on a development set of 157 questions from 50 threads by considering P@1 in G+KL. We found that our method is reasonably robust to these parameters. For the threshold θ in Definition 1, when we varied it from 0.1 to 0.35 on development set, the results remained the same and dropped a little if we used value larger than 0.35. We set it at 0.2 in our experiment. For the two parameters λ1 and λ2 in Equation <ref type="formula">5</ref>, we set λ1= 0.8 and λ2 = 0.05 based on the results on the development set. Performance did not change much when we varied λ 1 from 0.5 to 1 and λ 2 from 0.05 to 0.1. We set λ = 0.2 in Equation <ref type="formula">10</ref>; the performance nearly did not change when we varied it from 0.1 to 0.3. The integration of classification based re-ranking method and graph-based method. The experiment is to study the two ways of integration in Section 4.3. Table <ref type="table" target="#tab_6">6</ref> gives the results on A-TUnion. By comparing the results of G(Cla) with those of Cla in Tables 3, we found that graph-based method greatly improves the classification method Cla by using the result of Cla as the initial score of graph-based method. By comparing Cla(G) with Cla in Tables 3, we also found that using the results of graph-based methods as features can greatly improve method Cla. The reason for the improvement is that the integration can consider the relationship between candidate answers, while Cla alone does not consider the relationship between candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All questions</head><p>Question with answers P@1(#) MRR MAP P@1 MRR MAP A-T G(Cla) 0.652(1,000) 0.707 0.676 0.799 0.868 0.830 Union Cla(G) 0.673(1,033) 0.725 0.691 0.826 0.890 0.848 The effectiveness of lexical mapping. The experiment is to evaluate the effect of lexical mapping between question and answer discussed in Section 4.3. The results are beyond our expectations: the learned lexical mapping did not help for all the three ranking methods (CS, QL and KL). Due to space limitation, the detailed results are ignored. After our analysis, we found that the lexical mapping is not always effective for forum data. For example, lexical mapping how much → number would be useful in TREC QA to locate answers. In our corpus, 31.2% correct answers for how much questions do not contain a number. One example of answer to how much questions is "you can find it from the Website." On the other hand, many answer candidates containing number are not real answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results on more data</head><p>We applied our question detection method and answer detection method G+KL to the three forums that we crawled. The number of extracted question-answer pairs and its subset (the first questionanswer pairs in each thread) is given in Table <ref type="table" target="#tab_7">7</ref>. We evaluated three methods on the three datasets. An annotator was asked to check the top-1 return results of the three methods. (To control workload, we did not request to check other returned answers, and thus we only report P@1 results.) The results are reported in Table <ref type="table" target="#tab_8">8</ref>. The number of all questions in each data is given below the name of data, and the number of questions in subsets in each data is 100. We observed the same trends for the three methods on the three data: both KL and G+KL outperform the baseline method NA and G+KL outperforms KL (statistically significant, p-value &lt; 0.01). We also found that that the result on the subset is better than that on all questions as we observed on fully annotated data.</p><p>As a summary, our techniques are able to effectively extract question-answer pairs: 1) our question detection method outperformed rule-based methods and the method <ref type="bibr" target="#b18">[18]</ref>; 2) our graphbased method outperformed a baseline, three IR methods and classification-based re-ranking; 3) the integration of graph-based model with classification method improved the classification-based re-ranking; 4) the lexical mapping did not help in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we described a new approach to extracting question-answer pairs from online forums. The extracted questionanswer pairs could be used to enrich the knowledge base of community based QA service, instant answer service and Chatbot. Experimental results on real data show that our approach is effective. In the future, we will investigate the following problems: 1) to detect questions without answers; 2) to revisit more TREC QA techniques to see if they can help answer detection in forums 10 ; 3) to model the relationship of questions in the same thread to improve answer detection, considering that each thread contain about 10 In our corpus there are 10% factoid questions well studied in TREC QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Method P@1(#) on all Qs P@1(#) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of graph built from candidate answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of Question Detection</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">Prec(%) Rec(%) F1(%)</cell></row><row><cell cols="2">Q-TUnion 5W-1H words Question Mark SM [18] Our</cell><cell>69.0 96.8 81.9 96.5</cell><cell>14.8 78.4 87.8 98.5</cell><cell>24.4 86.6 84.6 97.5</cell></row><row><cell>Q-TInter</cell><cell>5W-1H words Question Mark SM [18] Our</cell><cell>69.0 98.7 92.7 97.8</cell><cell>15.3 77.6 86.8 97.0</cell><cell>25.0 86.9 89.7 97.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The methods and their abbreviations</figDesc><table><row><cell></cell><cell>(Section 4.1)</cell><cell>Cla</cell></row><row><cell cols="2">Cosine similarity (Sec. 4.1) Query Likelihood language model (Sec. 4.1) KL divergence language model (Sec. 4.1)</cell><cell>CS QL KL</cell></row><row><cell cols="2">Graph+Cosine similarity (Sec. 4.2) Graph+Query Likelihood language model (Sec. 4.2) Graph+KL divergence language model (Sec. 4.2)</cell><cell>G+CS G+QL G+KL</cell></row><row><cell></cell><cell>Graph(Classification) (Sec. 4.3) Classification(Graph) (Sec. 4.3)</cell><cell>G(Cla) Cla(G)</cell></row><row><cell>Method</cell><cell cols="2">All questions P@1(#) MRR MAP P@1 MRR MAP Question with answer</cell></row><row><cell>NA</cell><cell cols="2">0.525(806) 0.585 0.504 0.644 0.718 0.618</cell></row><row><cell>Lex Cla</cell><cell cols="2">0.529(812) 0.616 0.588 0.649 0.756 0.721 0.588(903) 0.667 0.631 0.722 0.818 0.774</cell></row><row><cell>CS QL KL</cell><cell cols="2">0.559(858) 0.643 0.601 0.686 0.789 0.737 0.568(872) 0.644 0.586 0.697 0.791 0.719 0.578(887) 0.659 0.621 0.709 0.809 0.762</cell></row><row><cell>G+CS G+QL G+KL</cell><cell cols="2">0.603(925) 0.677 0.639 0.739 0.830 0.784 0.620(952) 0.687 0.632 0.761 0.843 0.775 0.665(1,021) 0.719 0.686 0.816 0.882 0.842</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on A-TUnion data</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>failed to de-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on first question subset of A-TUnion data tect the questions without answers (deteriorated performance), i.e. all the methods identified wrong answers for all the 284 questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The evaluation of graph-based method on A-TUnion • Two propagation methods. Propagation without initial score (by default and denoted as G1) and Propagation with initial score (denoted as G 2 );</figDesc><table><row><cell>G K,1 +CS 0.563(864) 0.651 0.618 0.691 0.799 0.758</cell></row><row><cell>G K,1 +QL 0.584(897) 0.657 0.596 0.717 0.807 0.732</cell></row><row><cell>G K,1 +KL 0.626(961) 0.688 0.648 0.768 0.845 0.795</cell></row><row><cell>G A,1 +CS 0.603(925) 0.677 0.639 0.739 0.830 0.784</cell></row><row><cell>G A,1 +QL 0.620(952) 0.687 0.632 0.761 0.843 0.775</cell></row><row><cell>G A,1 +KL 0.665(1,021) 0.719 0.686 0.816 0.882 0.842</cell></row><row><cell>G K,2 +CS 0.541(831) 0.622 0.598 0.664 0.763 0.733</cell></row><row><cell>G K,2 +QL 0.546(838) 0.624 0.600 0.670 0.766 0.736</cell></row><row><cell>G K,2 +KL 0.546(838) 0.625 0.600 0.670 0.767 0.736</cell></row><row><cell>G A,2 +CS 0.616(946) 0.683 0.656 0.756 0.839 0.805</cell></row><row><cell>G A,2 +QL 0.623(956) 0.688 0.660 0.764 0.844 0.810</cell></row><row><cell>G A,2 +KL 0.625(959) 0.689 0.661 0.767 0.846 0.812</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Integration of graph-based method and classification</figDesc><table><row><cell>Forum</cell><cell>All questions</cell><cell>Subset</cell></row><row><cell>TripAdvisor</cell><cell>2,788,701</cell><cell>1,031,245</cell></row><row><cell>LonelyPlanet</cell><cell>194,672</cell><cell>59,242</cell></row><row><cell>BootsnAll</cell><cell>74,105</cell><cell>19,589</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The number of question answer pairs extracted</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The evaluation on other data</figDesc><table><row><cell>on subset</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.cogsci.ed.ac.uk/∼jamesc/taggers/MXPOST.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The thresholds were set to get thousands of patterns, and we did not try to search thresholds to optimize classification results in experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>To make sure that the Markov chains are always irreducible and aperiodic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We extracted 300,000 question-answer pairs from Yahoo! Answers as training set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://www.tripadvisor.com/ForumHome</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>http://www.lonelyplanet.com/thorntree/index.jspa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>http://boards.bootsnall.com/eve/ubb.x</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>We observed qualitatively similar results on A-TInter. Due to space limitation, we will not report the experimental results on A-TInter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_8"><p>questions on average and they may share the same answer paragraph. In addition, we will also investigate the effectiveness of our techniques in passage retrieval on TREC QA data.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was done when Gao Cong worked as a researcher at the Microsoft Research Asia, and Long Wang and Young-In Song were visiting students at the Microsoft Research Asia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bridging the lexical chasm: statistical approaches to answer-finding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW7</title>
		<meeting>of WWW7</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of the trec 2007 question answering track</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using conditional random fields to extract contexts and answers of questions from online forums</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An intelligent discussion-bot for answering student queries in threaded discussions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intelligent user interfaces</title>
		<meeting>of Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Methods for using textual entailment in open-domain question answering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting chatbot knowledge from online discussion forums</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding similar questions in large question and answer archives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrieving answers from frequently asked questions pages on the web</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying comparative sentences in text documents</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A probabilistic graphical model for joint answer ranking in question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pagerank without hyperlinks: structural re-ranking using links induced by language models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The process of question answering: A computer simulation of cognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Lehnert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using random walks for question-focused sentence retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/EMNLP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mortazavi-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical machine translation for query expansion in answer retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detection of question-answer pairs in email conversations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining sequential patterns and tree patterns to detect erroneous sentences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic generation of concise summaries of spoken dialogues in unrestricted domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A language modeling approach to passage question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Digesting virtual &quot;geek&quot; culture: The summarization of technical internet relay chats</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
