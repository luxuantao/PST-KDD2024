<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Entity Typing with Curriculum Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-18">18 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University ♣ Fudan-Aishu Cognitive Intelligence Joint Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinxi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University ♣ Fudan-Aishu Cognitive Intelligence Joint Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Entity Typing with Curriculum Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-18">18 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.02914v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity typing aims to assign types to the entity mentions in given texts. The traditional classification-based entity typing paradigm has two unignorable drawbacks: 1) it fails to assign an entity to the types beyond the predefined type set, and 2) it can hardly handle few-shot and zero-shot situations where many long-tail types only have few or even no training instances. To overcome these drawbacks, we propose a novel generative entity typing (GET) paradigm: given a text with an entity mention, the multiple types for the role that the entity plays in the text are generated with a pre-trained language model (PLM). However, PLMs tend to generate coarse-grained types after fine-tuning upon the entity typing dataset. In addition, only the heterogeneous training data consisting of a small portion of humanannotated data and a large portion of autogenerated but low-quality data are provided for model training. To tackle these problems, we employ curriculum learning (CL) to train our GET model on heterogeneous data, where the curriculum could be self-adjusted with the selfpaced learning according to its comprehension of the type granularity and data heterogeneity. Our extensive experiments upon the datasets of different languages and downstream tasks justify the superiority of our GET model over the state-of-the-art entity typing models. The code has been released on https://github.com/ siyuyuan/GET.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity typing aims to assign types to mentions of entities from a predefined type set, which enables machines to better understand natural languages and benefit many downstream tasks, such as entity linking <ref type="bibr" target="#b32">(Yang et al., 2019</ref>) and text classification <ref type="bibr" target="#b2">(Chen et al., 2019)</ref>. Traditional entity typing approaches follow the classification paradigm to classify (assign) the entity into a predefined set of types, which  have the following two unignorable drawbacks. 1) Closed Type Set: The classification-based approaches fail to assign the entity to the types out of the predefined set. 2) Few-shot Dilemma for Long-tail Types: Although fine-grained entity typing (FET) and ultra-fine entity typing approaches can classify entities into fine-grained types, they can hardly handle few-shot and zero-shot issues. In fact, there are many long-tail types only having few or even no training instances in the datasets. For example, more than 80% types have less than 5 instances and 25% types even never appear in the training data from the ultra-fine dataset <ref type="bibr" target="#b4">(Choi et al., 2018)</ref>.</p><p>To address these drawbacks, in this paper, we propose a novel generative entity typing (GET) paradigm: given a text with an entity mention, the multiple types for the role that the entity plays in the text are generated by a pre-trained language model (PLM). Compared to traditional classification-based entity typing methods, PLMbased GET has two advantages. First, instead of a predefined closed type set, PLMs can generate more open types for entity mentions due to their strong generation capabilities. For example, in Figure <ref type="figure" target="#fig_1">1</ref>, fine-grained types such as "large detergent company" and "large detergent manufacturer" can be generated by PLMs for entity P&amp;G, which con-tain richer semantics but are seldom included by a predefined type set. Second, PLMs are capable of conceptual reasoning and handling the few-shot and zero-shot dilemma <ref type="bibr" target="#b10">(Hwang et al., 2021)</ref>, since massive knowledge has been learned during their pre-training.</p><p>However, it is nontrivial to realize PLM-based GET due to the following challenges: 1) Entity typing usually requires generating fine-grained types with more semantics, which are more beneficial to downstream tasks. However, PLMs are biased to generate high-frequency vocabulary in the corpus due to their primary learning principle based on statistical associations. As a result, a typical PLM tends to generate high-frequent but coarse-grained types even if we carefully finetune the PLM on the fine-grained entity typing dataset (refer to Figure <ref type="figure" target="#fig_6">5</ref> in Section 4). Therefore, how to guide a PLM to generate high-quality and fine-grained types for entities is crucial. 2) It is costly for humans to annotate a great number of samples with fine-grained types. Therefore, most existing works adopt heterogeneous data consisting of a small portion (less than 10%) of human-annotated data and a large portion (more than 90%) of auto-generated lowquality data (e.g., by distant supervision), which greatly hurts the performance of entity typing models <ref type="bibr" target="#b8">(Gong et al., 2021)</ref>. How to train a PLM to generate desirable types on these low-quality heterogeneous data is also challenging.</p><p>The difficulty of using PLMs to generate highquality fine-grained types based on the low-quality heterogeneous training data motivates us to leverage the idea from curriculum learning (CL) <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref>, which better learns heterogeneous data by ordering the training samples based on their quality and difficulty <ref type="bibr" target="#b14">(Kumar et al., 2019)</ref>. In this paper, we propose a CL-based strategy to train our GET model. Specifically, we first define a fixed curriculum instruction and partition the training data into several subsets according to the granularity and heterogeneity of samples for model training. Based on the curriculum instruction, CL can control the order of using these training subsets from coarse-grained and lower-quality ones to finegrained and higher-quality ones. However, a fixed curriculum ignores the feedback from the training process. Thus, we combine the predetermined curriculum with self-paced learning (SPL) <ref type="bibr" target="#b15">(Kumar et al., 2010)</ref>, which can enforce the model dynamically self-adjusting to the actual learning order according to the training loss. In this way, our CLbased GET model can make the learning process move towards a better global optimum upon the heterogeneous data to generate high-quality and fine-grained types. Our contributions in this paper are summarized as follows:</p><p>• To the best of our knowledge, our work is the first to propose the paradigm of generative entity typing (GET). • We propose to leverage curriculum learning to train our GET model upon heterogeneous data, where the curriculum can be self-adjusted with self-paced learning. • Our extensive experiments on the data of different languages and downstream tasks justify the superiority of our GET model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Classification-based Entity Typing The traditional classification-based entity typing methods can be categorized into three classes. 1) Coarsegrained entity typing methods <ref type="bibr" target="#b29">(Weischedel and Brunstein, 2005;</ref><ref type="bibr" target="#b27">Tokarchuk et al., 2021)</ref> assign mentions to a small set of coarse types; 2) Finegrained entity typing (FET) methods <ref type="bibr" target="#b33">(Yuan and Downey, 2018;</ref><ref type="bibr" target="#b22">Onoe et al., 2021)</ref> classify mentions into more diverse and semantically richer ontologies; 3) Ultra-fine entity typing methods <ref type="bibr" target="#b4">(Choi et al., 2018;</ref><ref type="bibr" target="#b6">Ding et al., 2021;</ref><ref type="bibr" target="#b5">Dai et al., 2021)</ref> use a large open type vocabulary to predict a set of natural-language phrases as entity types based on texts. However, FET and ultra-fine entity typing methods hardly perform satisfactorily due to the huge predefined type set. They also hardly handle few-shot and zero-shot issues. Comparatively, our GET model can generate high-quality multigranularity types even beyond the predefined set for the given entity mentions.</p><p>Concept Acquisition Concept acquisition is very related to entity typing which also aims to obtain the types for the given entities, since entity types are often recognized as concepts. Concept acquisition can be categorized into the extractionbased or generation-based scheme. The extraction scheme cannot acquire concepts not existing in the given text <ref type="bibr" target="#b31">(Yang et al., 2020)</ref>. The existing approaches of concept generation <ref type="bibr" target="#b34">(Zeng et al., 2021)</ref> focus on utilizing the existing concept taxonomy or knowledge bases to generate concepts but neglect to utilize the large corpus. Our GET model can also achieve text-based concept generation.</p><p>Curriculum Learning According to the curriculum learning (CL) paradigm, a model is first trained with the easier subsets or subtasks, and then the training difficulty is gradually increased <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref> to improve model performance in difficult target tasks, such as domain adaption <ref type="bibr" target="#b28">(Wang et al., 2021)</ref> and training generalization <ref type="bibr" target="#b9">(Huang and Du, 2019)</ref>. The existing CL methods can be divided into predefined CL (PCL) <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref> and automatic CL (ACL) <ref type="bibr" target="#b15">(Kumar et al., 2010)</ref>. PCL divides the training data by the difficulty level with prior knowledge, while ACL, such as self-paced learning (SPL), measures the difficulty according to its losses or other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first formalize our task in this paper and overview the framework of our GET model. Then, we introduce the details of model implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formalization</head><p>Given a piece of text X and an entity mention M within it, the task of generative entity typing (GET) is to generate multiple types</p><formula xml:id="formula_0">T S = {T 1 , T 2 , ..., T K }, where each T k (1 ≤ k ≤ K) is a type for M w.r.t. the context of X.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>As most of the previous entity typing models <ref type="bibr" target="#b4">(Choi et al., 2018;</ref><ref type="bibr" target="#b16">Lee et al., 2020;</ref><ref type="bibr" target="#b8">Gong et al., 2021)</ref>, our GET model is also trained upon the heterogeneous data consisting of a small portion of human-annotated data and a large portion of autogenerated data, due to the difficulty and high cost of human annotation. We will introduce how to obtain our auto-generated data in Section 4. 3. CL-based Learning: In this step, our PLMbased GET model is trained with the designed curriculum, which is capable of adjusting its learning progress dynamically through selfpaced learning (SPL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt Construction</head><p>To generate the types of given entities by a PLM, we construct the prompts in cloze format from the Hearst patterns listed in Table <ref type="table" target="#tab_0">1</ref>. Specifically, each input text X including an entity mention M is concatenated with a cloze prompt constructed with M , and the PLM is asked to fill the blank within the cloze prompt. Recall the example in Figure <ref type="figure" target="#fig_1">1</ref>, the original text "In the early 1980s, P &amp; G tried to launch here a concentrated detergent under the Ariel brand name that it markets in Europe" can be concatenated with a cloze prompt such as "P &amp; G is a " to construct an input prompt for the PLM, which predicts "large detergent company", "large detergent manufacturer" and "company" as the types for P &amp; G to fill the blank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M is a such as M M is one of especially M M refers to</head><p>, including M M is a member of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Curriculum Instruction</head><p>Curriculum instruction is the core issue of CL, which requires estimating the difficulty of samples in terms of model learning, to decide the order of using samples for model training.</p><p>For our specific PLM-based GET model, we argue that the difficulty of a sample in terms of model learning greatly depends on the granularity of its type, which could be roughly measured by the length of the type term. To prove this, we generate two subsets of training samples in the same size from the auto-generated data according to their type length: one subset with type length=1, and the other subset with type length≥2<ref type="foot" target="#foot_0">1</ref> . Then, we train a typical PLM, T5 <ref type="bibr" target="#b24">(Raffel et al., 2019)</ref> for one epoch in the two subsets and record the landscapes of loss. As observed in Figure <ref type="figure" target="#fig_4">3</ref>, the length≥2 subset has lower converge and higher cross-entropy (CE) losses than the length=1 subset when training converges, which shows that it is more difficult for the PLM to fit the training samples of fine-grained types.</p><p>Based on this observation, we partition the autogenerated data into two subsets, i.e., the subset with one-word types (denoted as D A ), and the subset with types of more than one word (denoted as D B ), and D A is used for model training earlier than D B . The human-annotated data (denoted as D C ) is finally used, as it usually contains many ultra fine-grained types annotated by human annotators, which is harder for model learning. For easier presentation later, we denote the whole training data as</p><formula xml:id="formula_1">D = D A D B D C = {&lt; X (i) , M (i) , T S (i) &gt; } N</formula><p>i=1 , where N is the training sample size and a sample is denoted as</p><formula xml:id="formula_2">D (i) k = &lt; X (i) , M (i) , T (i) k &gt;.</formula><p>Based on the fixed curriculum instruction, CL can control the order in which data are used for model training, i.e., from D A to D B to D C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CL-based Learning</head><p>T5 Backbone To meet the cloze format and generate more fine-grained types, we choose T5 <ref type="bibr" target="#b24">(Raffel et al., 2019)</ref> as the backbone PLM of our GET model. T5 is an encoder-decoder pre-trained model, which can focus on the entire text and allow multiple words to fill in the blanks. To train the T5, in our settings, we define the loss function of sample</p><formula xml:id="formula_3">D (i) k as, L D (i) k = L CE T (i) k , f (X (i) , θ, M (i) ) (1)</formula><p>where L CE is a CE loss function that calculates the cost between the ground truth type</p><formula xml:id="formula_4">T (i)</formula><p>k and the predicted type f (X (i) , θ, M (i) ). θ denote the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPL-based Training Process</head><p>Our model training can be performed according to the above predefined curriculum, but the feedback from the learning process is inevitably ignored, which may lead to divergent solutions <ref type="bibr" target="#b12">(Jiang et al., 2015)</ref>. As an alternative, we adopt self-paced learning (SPL) to enforce the model to self-adjust the curriculum according to the feedback from the training loss. Formally, we define the objective of our CL as,</p><formula xml:id="formula_5">min θ,v E(θ, v; λ) = N i=1 K (i) k=1 v (i) k L D (i) k + g(v; λ) (2)</formula><p>where the binary variable v</p><formula xml:id="formula_6">(i) k ∈ [0, 1] indicates whether sample D (i)</formula><p>k should be incorporated into the calculation of the objective. Specifically,</p><formula xml:id="formula_7">v (i) k = 1 L D (i) k &lt; λ 0 L D (i) k ≥ λ (3) and g(v; λ) = −λ N i=1 K (i) k=1 v (i) k ,<label>(4)</label></formula><p>where K (i) is the number of types for &lt; X (i) , M (i) , T S (i) &gt;, and λ is the "age" of SPL to control the learning pace. The regularizer g(; ) is the binary self-paced function used to avoid overfitting <ref type="bibr" target="#b11">(Jiang et al., 2014)</ref>. In the training process, "easy" samples with small losses are used first for training. We update λ = µλ to increase λ gradually, where µ &gt; 1 is the step size. With the growth of λ, more samples with larger losses are gradually incorporated into model training to obtain a more "mature" model.</p><p>Prior Knowledge to Optimize SPL As mentioned in Section 3.4, we expect that the model is trained orderly by the three subsets according to the predetermined curriculum and generates more fine-grained types. However, the order of using the data for model training totally depends on the loss function (Eq. 1) in SPL. Thus, SPL is limited in incorporating predetermined curriculum and the type granularity into learning. Therefore, we treat the predetermined curriculum and the type granularity as prior knowledge to optimize SPL. Specifically, we increase the weight of the samples with finegrained types to let the model pay more attention to these data and assign each subset with different weights to ensure that the training process is executed according to the predetermined curriculum. In particular, given the sample D (i) k , we define its weight as</p><formula xml:id="formula_8">w(D (i) k ) = length(T (i) k ) * γ(D (i) k )<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">γ(D (i) k ) =        1 if D (i) k ∈ D A , 2 if D (i) k ∈ D B , 3 if D (i) k ∈ D C . (6)</formula><p>Then, the loss function L in Eq. 1 is updated as</p><formula xml:id="formula_10">L CE T (i) k , f (X (i) , θ, M (i) )) * w D (i) k , (7)</formula><p>which indicates that a sample with a large weight (more difficult) can be incorporated later into the training process since its v</p><formula xml:id="formula_11">(i)</formula><p>k is more likely to be 0 according to Eq. 3. We adopt an alternative convex search (ACS) to realize SPL, of which the algorithm is shown in Appendix A. We use Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2015)</ref> to update the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Type Generation</head><p>When the PLM in our GET model has been trained through the aforementioned CL-based learning process, we use it to generate the types for the given entity mention. Specifically, we let the PLM fill in the blank of the input text. To obtain more diverse candidate types, we apply the beam search <ref type="bibr" target="#b25">(Reddy et al., 1977)</ref> with beam size as b, and select the b most probable candidates. Then, we reserve the types with confidence scores bigger than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we verify the advantages of our GET model over the classification-based entity typing models through our experiments. We also explore the role of CL in guiding PLM-based type generation with different language data. We further display the effectiveness of our generated entity types in two downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>As we mentioned before, due to the expensive manual labeling of fine-grained types, the training dataset consists of a large portion of auto-generated data and a small portion of human-annotated data. Since PLMs have more difficulty fitting the training samples of fine-grained types (elaborated in Figure <ref type="figure" target="#fig_4">3</ref> in Section 3.4), we partition the autogenerated data into two subsets, and denote the subset with one-word types as D A while the other as D B . Furthermore, human-annotated data with ultra fine-grained types is denoted as D C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-generated Data</head><p>The auto-generated data used in our model is obtained from the abstracts of entities on Wikipedia (Vrandečić and Krötzsch, 2014). Specifically, we collect the abstract texts and their hyperlinks pointing to the web pages of mentioned entities, from which the type labels of these mentioned entities can be obtained. In this way, the obtained type labels are more consistent with the contexts of entities, and thus of much higher quality than those auto-generated with distant supervision <ref type="bibr" target="#b8">(Gong et al., 2021)</ref>.</p><p>To construct D A and D B from the autogenerated data, we collect 100,000 Chinese and English abstracts from Wikipedia, from which we randomly select 500 samples as our test set, and the rest are used as the training set. Then we split the training set into two subsets D A and D B , according to the length of the types as mentioned in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-annotated Data</head><p>To demonstrate that GET is superior to the classification-based approaches, we collect the human-annotated data from four different entity type datasets. The statistics of them are listed in Table <ref type="table" target="#tab_1">2</ref>. We compare our model with baselines on BNN, FIGER and Ultra-fine to demonstrate the superior performance of GET on entity typing. GT dataset is used to evaluate the effectiveness of CL upon the texts of different languages and heterogeneous data. Please note that we do sample the test set to reduce the cost of manual evaluation on assessing whether the newly-generated types are correct. However, the results from the baselines and our model are evaluated in the same test instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Language Size of D3 Size of test set BNN <ref type="bibr" target="#b29">(Weischedel and Brunstein, 2005)</ref> Coarse-grained English 10,000 500 FIGER <ref type="bibr" target="#b26">(Shimaoka et al., 2016)</ref> Fine-grained English 10,000 278 Ultra-Fine <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> Ultra fine-grained English 5,500 500</p><p>GT <ref type="bibr">(Lee et</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>The detailed information about the baselines and some experiment settings is shown in Appendix B.</p><p>Please note that the baselines on BNN and FIGER are different from those on Ultra-Fine.</p><p>For BNN and FIGER, we only reserve the type in the predefined type set with the highest probability predicted by the model since there is only one golden label in the dataset. Ultra-fine entity typing aims to predict a set of natural-language phrases that describe the types of entity mentions based on texts. Therefore, we reserve the types with the probability bigger than 0.5 for all models. Please note that previous work adopts a classification paradigm, while our GET model can generate new types not existing in the predefined type set. Therefore, annotators are invited to assess whether the generated types are correct. The annotation detail is shown in Appendix B.4.</p><p>We record the number of correct types assigned to the entity mentions (CT #) and strict macroaveraged precision (Prec.). Obviously, it is impossible to know all correct types generated based on the input text in advance due to incalculable search space. Therefore, we report the relative recall. Specifically, suppose CTs # is the total number of new types obtained by all models. Then, the relative recall (R-Recall) is calculated as CT # divided by CTs #. Accordingly, the relative F1 (R-F1) can also be calculated with Prec. and R-Recall. In addition, we also record the average length of types (Len.) under different training strategies to investigate the effectiveness of CL and prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Comparison Results</head><p>The comparison results on traditional entity typing and Ultra-fine entity typing are shown in Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_4">4</ref> <ref type="foot" target="#foot_2">2</ref> . The tables show that our model (Ours) achieves consistent performance improvements on these datasets. For BNN, our model significantly improves Prec. and covers more entity types. For FIGER, our model generates more types than other baselines. For Ultra-Fine, the existing models based on the classification paradigm are extremely difficult to select the appropriate types from the large predefined type set. Comparatively, our GET model has no classification constraint since it transforms multi-classification into a generation paradigm that is more suitable for PLMs. Therefore, our model greatly improves the precision of Ultra-Fine and covers more types of entities.</p><p>We further display the capability of our model to generate new types beyond the predefined type set, as shown in    New) is the total number of generated types beyond the predefined type set, MiNew (Micro-New) is the total number of generated types beyond the human-annotated type set (i.e., golden labeled set T S) of each instance, and R.New is the ratio of new generated types per sample. The listed results are counted upon the test sets in different datasets, from which we find that our model can generate abundant types that are not in the golden label set, thereby increasing the diversity of entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of CL</head><p>We further compare our model with the following ablated variants to verify the effectiveness of CL.</p><p>FT is fine-tuned directly with training data without CL; PCL adopts Baby Step <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref> instead of SPL, which inputs the subsets into the model in turn according to a fixed curriculum, but ignores the feedback from the learning process; SPL w/o PK adopts SPL but ignores prior knowledge in training.</p><p>In order to demonstrate the performance of the compared models on Chinese and English data, we only consider D C of GT in our ablation studies. Since the models are designed toward heterogeneous training data, we investigate their performance on both the auto-generated test set and human-annotated test set (in GT), which is displayed in Table <ref type="table" target="#tab_7">6</ref>. Please note that we only report the Prec. and R-F1 due to the limited space. Based on the results, the superiority of PCL and SPL over FT verifies the advantage of CL over the general training strategy, while the superiority of SPL w/o PK over PCL verifies the effectiveness of SPL. Furthermore, our GET model performs well on type generation upon abstract texts (auto-generated data) and common free texts (human-annotated data).</p><p>To explore the reason for the advantage of SPL, we record the ratio of incorporated training samples in each epoch. As shown in Figure <ref type="figure" target="#fig_5">4</ref>  be regarded as a pre-training process that helps model optimization and regularizes the training on the latter subsets. Thus, SPL can better guide the model to find a global minimum loss and make it more generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of Prior Knowledge</head><p>From the Table <ref type="table" target="#tab_7">6</ref>, we also find that Ours can generate more fine-grained types than SPL w/o PK. Without the prior knowledge, SPL only relies on the self-judgment of the model and treats all the selected samples equally, which ignores the data heterogeneity and type granularity during training and harms the model performance.</p><p>As shown in Figure <ref type="figure" target="#fig_6">5</ref>, compared to the original training dataset, there are more coarse-grained types (length=1) than fine-grained types (length≥2) generated by the directly fine-tuned GET model (Generation with FT), while the GET model with CL can generate more fine-grained types of the almost same ratio as the coarse-grained types. It is because that the prior knowledge about the type length is considered to re-weight the importance of samples, making the model pay more attention to fine-grained types. Thus, more fine-grained and high-quality types are generated. Based on these results, we believe that combining prior knowledge with SPL is an excellent way to optimize CL.</p><p>We also explore the influence of different λ and µ which is shown in Appendix B.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Applications</head><p>We further conduct experiments on the task of short text classification and entity linking to prove that the types generated by our model can promote the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Prec   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short Text Classification.</head><p>Existing short text classification approaches <ref type="bibr" target="#b2">(Chen et al., 2019)</ref> directly use KG as external knowledge to improve model performance. However, how to choose the context-consistent types for the roles that the entities play in the text relation is still a problem, which may lead to unsatisfactory results. GET can generate context-consistent types for entities, and thus it can be adopted to promote the classification performance. We conduct our experiments in the NLPCC2017 dataset<ref type="foot" target="#foot_3">3</ref> , the Chinese news title dataset with 18 classes (e.g., entertainment, game, food). We first use an NER model to identify entities in the text and directly apply our model upon NLPCC2017 dataset to generate types for the entities. Then we choose Bi-LSTM to achieve classification. we also collect corresponding types of entities in the representative KG CN-DBpedia for comparison. The results in Table <ref type="table" target="#tab_9">7</ref> show that external knowledge enhances the classification performance, and the types generated by our GET model are more effective than those directly obtained from KG.</p><p>Entity Linking. The representative entity linking model DCA-SL <ref type="bibr" target="#b32">(Yang et al., 2019)</ref> adopts the entity description and triples in KG as an external knowledge to enhance the performance of the model in the entity linking task. To prove that the types generated by our model are of high quality, we first adopt our model to generate types for entities based on texts. Then we replace the types in the original triples in KGs with the types we generated. From Table <ref type="table" target="#tab_10">8</ref> we find that the generated types by our model can improve the entity lining performance of DCA-SL effectively, indicating that the generated types are of high quality and meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel generative paradigm for entity typing, which employs a generative PLM trained with curriculum learning. Compared with the traditional classification-based entity typing methods, our generative entity typing (GET) model can generate new types beyond the predefined type set for given entity mentions. Our extensive experiments on several benchmark datasets justify that the curriculum learning with SPL and the prior knowledge of type length and subset order help our model generate more high-quality finegrained types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Although we have proven that our GET model can generate high-quality and new types beyond the predefined type set for given entity mentions, it also has some limitations. In this section, we analyze these limitations and hopefully advance future work in GET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Uncontrolled Generation</head><p>To delve into the model performance, we compare the types generated by our approach with the golden type labels in Ultra-fine entity typing datasets. Table <ref type="table" target="#tab_11">9</ref> lists three examples with the correct types generated by our model and the golden labeled type set of the entities in the Ultra-Fine dataset. The first example shows that our model can generate more fine-grained types which may not appear in the golden labeled set. The second and third examples demonstrate that although our model can generate new concepts, it may ignore some correct types in the golden label set, e.g., "administration". However, enforcing the model by constraint decoding to generate the types in the predefined type set may compromise the flexibility of our model to generate new concepts. Therefore, we hope that future work can handle this dilemma with better methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Type Selection</head><p>As mentioned in Sec. 3.6, T5 adopts beam search to generate the b most probable types with confidence scores. Then we reserve the types with confidence scores larger than the selection threshold. However, it can hardly achieve a satisfactory balance to reserve the types by choosing a specific threshold to directly truncate the output of T5. If we select a relatively big threshold, we can get more accurate types but may lose some correct types. If the recall is preferred, precision might be hurt. Therefore, we suggest that future work consider how to achieve a better trade-off between precision and recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In the early 1980s , P &amp; G tried to launch here a concentrated detergent under the Ariel brand name that it markets in Europe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example of entity typing through generation and classification paradigm, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our PLM-based GET framework trained with curriculum learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1. The frame-work of our model learning includes the following three steps, as shown in Figure 2. 1. Prompt Construction: To better leverage the knowledge obtained from the pre-training of PLM, we employ the prompt mechanism (Liu et al., 2021a) to guide the learning of our PLMbased GET model; 2. Curriculum Instruction: As a key component of CL, the curriculum instruction is responsible for measuring the difficulty of each sample in the heterogeneous training data, and then designing a suitable curriculum for the model training process;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The landscapes of CE loss comparison for the two training subsets with type length=1 and type length≥2.</figDesc><graphic url="image-4.png" coords="4,76.32,72.22,207.36,93.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The ratio of training samples of different learning strategies in each epoch.</figDesc><graphic url="image-5.png" coords="7,82.72,359.13,194.51,100.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The ratio of the types of length=1 and length ≥ 2 in the original training dataset, and the types generated by our GET model with fine-tuning (FT) or curriculum learning (CL).</figDesc><graphic url="image-6.png" coords="7,318.29,253.38,194.00,121.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Prompts constructed from Hearst patterns.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The statistic of different entity typing datasets.</figDesc><table><row><cell></cell><cell cols="2">al., 2020)</cell><cell cols="2">Multilingual</cell><cell>English Chinese</cell><cell cols="3">4,750 4,750</cell><cell>250 250</cell></row><row><cell>Model</cell><cell>CT #</cell><cell>Prec.</cell><cell>BNN R-Recall</cell><cell>R-F1</cell><cell>CT #</cell><cell>Prec.</cell><cell cols="2">FIGER R-Recall</cell><cell>R-F1</cell></row><row><cell cols="2">Zhang et al. (2018) 555</cell><cell>58.10%</cell><cell>50.49%</cell><cell>54.03%</cell><cell>348</cell><cell cols="2">62.00%</cell><cell>49.85%</cell><cell>55.26%</cell></row><row><cell>Lin and Ji (2019)</cell><cell>534</cell><cell>55.90%</cell><cell>48.58%</cell><cell>51.98%</cell><cell>353</cell><cell cols="2">62.90%</cell><cell>50.57%</cell><cell>56.07%</cell></row><row><cell cols="2">Xiong et al. (2019) 558</cell><cell>58.40%</cell><cell>50.75%</cell><cell>54.31%</cell><cell>350</cell><cell cols="2">62.30%</cell><cell>50.09%</cell><cell>55.53%</cell></row><row><cell>Ali et al. (2020)</cell><cell>697</cell><cell>73.00%</cell><cell>63.43%</cell><cell>67.88%</cell><cell cols="4">399 71.00% 57.08%</cell><cell>63.29%</cell></row><row><cell>Chen et al. (2020)</cell><cell>718</cell><cell>75.20%</cell><cell>65.35%</cell><cell>69.93%</cell><cell>388</cell><cell cols="2">69.10%</cell><cell>55.56%</cell><cell>61.59%</cell></row><row><cell cols="2">Zhang et al. (2021) 732</cell><cell>76.70%</cell><cell>66.65%</cell><cell>71.32%</cell><cell>394</cell><cell cols="2">70.10%</cell><cell>56.36%</cell><cell>62.48%</cell></row><row><cell>Li et al. (2021)</cell><cell>668</cell><cell>69.90%</cell><cell>60.74%</cell><cell>65.00%</cell><cell>397</cell><cell cols="2">70.60%</cell><cell>56.76%</cell><cell>62.93%</cell></row><row><cell>Ours</cell><cell cols="5">875 82.30% 79.62% 80.94% 444</cell><cell cols="4">66.20% 63.52% 64.83%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different approaches on the sample test set in coarse-grained and fine-grained entity typing dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>In the table, MaNew (Macro-</figDesc><table><row><cell>Model</cell><cell>CT #</cell><cell cols="2">Ultra-Fine Prec. R-Recall</cell><cell>R-F1</cell></row><row><cell>Xiong et al. (2019)</cell><cell>782</cell><cell>50.30%</cell><cell>24.28%</cell><cell>32.75%</cell></row><row><cell cols="2">Onoe and Durrett (2019) ELMo 884</cell><cell>51.50%</cell><cell>27.44%</cell><cell>35.81%</cell></row><row><cell>Onoe and Durrett (2019) BERT</cell><cell>884</cell><cell>51.60%</cell><cell>27.44%</cell><cell>35.83%</cell></row><row><cell>López and Strube (2020)</cell><cell>915</cell><cell>43.40%</cell><cell>28.41%</cell><cell>34.34%</cell></row><row><cell>Onoe et al. (2021)</cell><cell cols="2">1039 52.80%</cell><cell>32.26%</cell><cell>40.05%</cell></row><row><cell>Liu et al. (2021b)</cell><cell cols="2">1042 54.50%</cell><cell>32.35%</cell><cell>40.60%</cell></row><row><cell>Dai et al. (2021)</cell><cell cols="2">1213 53.60%</cell><cell>37.66%</cell><cell>44.24%</cell></row><row><cell>Ours</cell><cell cols="4">1275 87.10% 39.58% 54.43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of different approaches on the sample test set in Ultra-fine entity typing dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">MaNew MiNew R.New</cell></row><row><cell>BNN</cell><cell>4</cell><cell>100</cell><cell>11.61%</cell></row><row><cell>FIGER</cell><cell>25</cell><cell>137</cell><cell>26.81%</cell></row><row><cell>Ultra-Fine</cell><cell>73</cell><cell>543</cell><cell>42.14%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The number and ratio of new types generated by our model on different datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, SPL gradually incorporates the whole training data to train the model. The training on the former subsets can</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell>CT #</cell><cell>Chinese Prec. R-F1</cell><cell cols="2">Len. CT #</cell><cell>English Prec. R-F1</cell><cell>Len.</cell></row><row><cell>FT PCL SPL w/o PK Ours</cell><cell>Auto-generated data</cell><cell cols="3">690 646 672 92.18% 72.22% 2.75 84.46% 70.81% 2.80 91.76% 70.37% 2.75 714 90.04% 74.18% 2.86</cell><cell cols="3">870 864 900 928 87.14% 57.84% 1.62 75.85% 52.87% 1.48 85.97% 54.87% 1.32 87.12% 56.66% 1.54</cell></row><row><cell>FT PCL SPL w/o PK</cell><cell>Human-annotated data</cell><cell cols="3">383 370 383 409 83.64% 59.28% 2.63 72.54% 53.98% 2.65 77.24% 54.01% 2.64 78.64% 55.59% 2.61</cell><cell cols="3">352 375 370 373 90.75% 51.88% 1.82 84.82% 48.82% 1.72 88.03% 51.62% 1.69 90.46% 51.53% 1.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons of our model and its variants on the auto-generated and human-annotated test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance of short text classification based on Bi-LSTM without/with different external knowledge on NLPCC2017 dataset.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>F1</cell></row><row><cell>AIDA</cell><cell cols="2">triples (KG.) 94.58%</cell></row><row><cell>CoNLL-YAGO</cell><cell cols="2">triples (Gen.) 94.92%</cell></row><row><cell>ACE 2014</cell><cell cols="2">triples (KG) triples (Gen.) 90.54% 89.74%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Performance of entity linking model DCA-SL with different external knowledge.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Ultra-fine entity typing examples with the corresponding golden labels and generated types. Entity mentions are in bold and underlined.</figDesc><table><row><cell>Input Text</cell><cell>Golden</cell><cell>Generated</cell></row><row><cell>He was capped 42 times and scored 8 goals for Sweden, and he played at the 2002 FIFA World Cup</cell><cell>nation</cell><cell>nation, nordic country, scandinavian country</cell></row><row><cell>The Audit Bureau of Circulations was formed in 1914 to verify publication circulation figures and track media rates</cell><cell>administration, organization</cell><cell>organization, government agency, investigative service</cell></row><row><cell>Chapman retired from</cell><cell>tournament,</cell><cell>sport,</cell></row><row><cell>playing hockey after</cell><cell>event,contest,</cell><cell>contact sport,</cell></row><row><cell>the 1943 to 1944</cell><cell>game,activity,</cell><cell>team sport,</cell></row><row><cell>hockey season</cell><cell>sport,hockey</cell><cell>winter sport</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Here we chose length=1 and length≥2 to divide the training data since most of the types are no longer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">than 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The statistical significance test and data size analysis are provided in Appendix B.5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">http://tcci.ccf.org.cn/conference/2017/taskdata.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their valuable comments and suggestions for this work. This work is supported by the National Natural Science Foundation of China (No.62072323), Shanghai Science, the Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902) and Technology Innovation Action Plan (No.21511100401&amp; 22511104700).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CL Algorithm</head><p>We adopt an alternative convex search (ACS) to realize the SPL of our model training. As shown in Algorithm 1, we use Adam to update the model parameters.</p><p>Algorithm 1 Self-paced Learning with prior knowledge for types generation.  • <ref type="bibr" target="#b36">Zhang et al. (2018)</ref>: This approach uses a neural architecture to learn a distributional semantic representation to classify.</p><p>• <ref type="bibr" target="#b18">Lin and Ji (2019)</ref>: This approach proposes a two-step mention-aware attention mechanism to enable the model to focus on the important words in mentions and contexts to improve type classification performance.</p><p>• <ref type="bibr" target="#b30">Xiong et al. (2019)</ref>: This approach utilizes a graph propagation layer to capture label correlations for type classification.</p><p>• Ali et al. ( <ref type="formula">2020</ref>): This method adopts edgeweighted attentive graph convolution network to refine the noisy mention representations.</p><p>• Chen et al. ( <ref type="formula">2020</ref>): Under the undefined case, this approach does not modify the labels in the dataset.</p><p>• <ref type="bibr" target="#b35">Zhang et al. (2021)</ref>: This approach utilizes a probabilistic automatic relabeling method that treats all training samples uniformly to handle noisy samples.</p><p>• <ref type="bibr" target="#b17">Li et al. (2021)</ref>: This approach proposes a novel method based on a two-phase graph network for the Fine-Grained Entity Typing task to enhance the label representations via imposing the relational inductive biases of instanceto-label and label-to-label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Baselines of Ultra-Fine Entity Typing</head><p>For Ultra-Fine dataset, we compare our model with the following baselines:</p><p>• Onoe and Durrett ( <ref type="formula">2019</ref>): This approach adopts ELMo and BERT as the encoder to fine-tune on the crowdsourced train split or raw and denoised distantly-labeled data.</p><p>• <ref type="bibr" target="#b21">López and Strube (2020)</ref>: This approach proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space.</p><p>• <ref type="bibr" target="#b22">Onoe et al. (2021)</ref>: This approach adopts a BERT-based model with box embeddings to capture latent type hierarchies for type classification.</p><p>• <ref type="bibr" target="#b20">Liu et al. (2021b)</ref>: This approach discovers and exploits label dependencies knowledge entailed in the data to sequentially reason finegrained entity labels for type classification.</p><p>• <ref type="bibr" target="#b5">Dai et al. (2021)</ref>: This approach uses a BERT Masked Language Model to generate weak labels for ultra-fine entity typing to improve the performance of type classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Experiment Settings</head><p>Our experiments are conducted on a workstation of dual GeForce GTX 1080 Ti with 32G memory, and the environment of torch 1.7.1. We adopted a T5-base with 12 layers and 12 self-attention heads for the English dataset and mT5-small with 8 layers and 6 self-attention heads for the Chinese dataset.</p><p>The hyperparameter settings of training our PLMbased GET are: λ = 0.5, µ = 2. The beam size b is 8. The coefficient weight α in the loss function is 4.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Human Assessment</head><p>It is impossible to know all newly-generated types apriori. Thus human annotators are needed to assess whether the generated types are correct. We employ two annotators to ensure the quality of the assessment. Each predicated type is labeled with 0 or 1 by two annotators, where 0 means a wrong type for the given entity and 1 represents the right type for the given entity. If the results from the two annotators are different, the third annotator will be hired for a final check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Result Confidence</head><p>We also conduct a statistical significance test <ref type="bibr" target="#b7">(Dror et al., 2018)</ref> to show our experiment results are convincing. Specifically, we run our method on the test set of the Ultra-fine entity typing dataset twice with different random seeds. Then we implement a t-test on the two results with a 0.05 significance level. The result is not significant (p-value: 0.208) and thus we can not reject the null hypothesis (H0: result1-result2=0, where result i =(CT#, Prec., Rrecall, R-F1)). Based on the above hypothesis test, we believe that our experiment results are confident and reproducible.</p><p>Besides, we do a run with 50% auto-generated training data for the Ultra-fine entity typing task and the results are shown in Table <ref type="table">10</ref>. We find that our method suffers from a slight performance drop but still outperforms the baselines, which shows the effectiveness of auto-generated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Parameter Tuning Results</head><p>We explore the influence of different λ and µ on the performance of our model, as shown in Table <ref type="table">11</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained named entity typing over distantly supervised data based on refined representations</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7391" to="7398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep short text classification with knowledge powered attention</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6252" to="6259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02286</idno>
		<title level="m">Hierarchical entity typing via multi-level learning to rank</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04905</idno>
		<title level="m">Ultra-fine entity typing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ultra-fine entity typing with weak supervision from a masked language model</title>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04098</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Prompt-learning for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">and Roi Reichart</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
	<note>Australia</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Abusive language detection in heterogeneous contexts: Dataset collection and the role of supervised attention</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Valido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">M</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothy</forename><forename type="middle">L</forename><surname>Espelage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11119</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-attention enhanced cnns and collaborative curriculum learning for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yuyun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning based curriculum optimization for neural machine translation</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00041</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Chin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08825</idno>
		<title level="m">A chinese corpus for fine-grained entity typing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enhancing label representations with relational inductive bias constraint for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Jinqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An attentive fine-grained entity typing model with latent type representation</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6197" to="6202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05744</idno>
		<title level="m">Fine-grained entity typing via label reasoning</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A fully hyperbolic neural model for hierarchical multi-class classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00345</idno>
		<title level="m">Modeling fine-grained entity types with box embeddings</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01566</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech understanding systems: A summary of results of the five-year research effort. department of computer science</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01341</idno>
		<title level="m">Neural architectures for fine-grained entity type classification</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denny Vrandečić and Markus Krötzsch</title>
		<author>
			<persName><forename type="first">Evgeniia</forename><surname>Tokarchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Thulke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dugast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05892</idno>
	</analytic>
	<monogr>
		<title level="m">vestigation on data adaptation techniques for neural named entity recognition</title>
				<imprint>
			<date type="published" when="2014">2021. 2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Wikidata: a free collaborative knowledgebase. Communications of the ACM</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on curriculum learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">112</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Imposing label-relational inductive bias for extremely fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02591</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clinical concept extraction using transformers</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>William R Hogan</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1935" to="1942" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning dynamic context augmentation for global entity linking</title>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02117</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Otyper: A neural architecture for open named entity typing</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Enhancing taxonomy completion with concept generation via fusing relational representations</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02974</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with noise: improving distantlysupervised fine-grained entity typing via automatic relabeling</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3808" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fine-grained entity typing through increased discourse context and adaptive classification thresholds</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08000</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
