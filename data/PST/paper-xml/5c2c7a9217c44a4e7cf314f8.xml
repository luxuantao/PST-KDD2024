<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
							<email>andrew@pinterest.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Pinterest, Inc. San Francisco</orgName>
								<address>
									<region>US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pinterest, Inc. San Francisco</orgName>
								<address>
									<region>US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep metric learning aims to learn a function mapping image pixels to embedding feature vectors that model the similarity between images. Two major applications of metric learning are content-based image retrieval and face verification. For the retrieval tasks, the majority of current state-of-the-art (SOTA) approaches are triplet-based nonparametric training. For the face verification tasks, however, recent SOTA approaches have adopted classification-based parametric training. In this paper, we look into the effectiveness of classification based approaches on image retrieval datasets. We evaluate on several standard retrieval datasets such as CAR-196, CUB-200-2011, Stanford Online  Product, and  In-Shop datasets for image retrieval and clustering, and establish that our classification-based approach is competitive across different feature dimensions and base feature networks. We further provide insights into the performance effects of subsampling classes for scalable classification-based training, and the effects of binarization, enabling efficient storage and computation for practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Metric learning, also known as learning image embeddings, is a core problem of a variety of applications including face recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, fine-grained retrieval <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, clustering <ref type="bibr" target="#b30">[31]</ref>, and visual search <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. The goal of metric learning is that the learned embedding generalize well to novel instances during test time, an open-set setup in machine learning. This goal aligns well with practical applications in which the deployed machine learning system is required to handle large amount of unseen data.</p><p>Standard deep neural network metric learning methods train image embeddings through the local relationships between images in the form of pairs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or triplets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. A core challenge with metric learning is sampling informative samples for training. As described in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref>, negatives that are too hard can destabilize training, while negatives that are too easy result in triplets that have near zero loss leading to slow convergence.</p><p>Recent methods such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> focus on addressing this sampling problem, many of which utilize the relationships of all images within the batch to form informative triplets. These methods typically require a large batch size so that informative triplets can be selected c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</p><p>*These two authors contributed equally to this work arXiv:1811.12649v2 [cs.CV] 4 Aug 2019 within the batch. In practice, batch size is constrained by the hardware memory size. Therefore, as the dataset size grows, one still faces the challenge of the diminishing probability of a randomly sampled batch containing any informative triplet.</p><p>To alleviate the challenge of sampling informative triplets, another school of deep metric learning approaches propose to use classification-based training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. In contrast to triplet-based approaches, the classification-based approach can be viewed as approximating each class using a proxy <ref type="bibr" target="#b13">[14]</ref>, and uses all proxies to provide global context for each training iteration. Though classification-based metric learning simplify training by removing sampling, they have scalability limitations as in the extreme classification <ref type="bibr" target="#b17">[18]</ref> problem, where it is considered impractical to extend to much larger number of classes during training.</p><p>While generic deep metric learning approaches have been pushing SOTA through better negative sampling and ensembles of triplet-based approaches, face verification has in parallel seen SOTA results through classification-based loss functions. We set out to investigate if classification-based training can perform similarly well for general image retrieval tasks.</p><p>Our major contributions are as follows: 1) we establish that classification is a strong baseline for deep metric learning across different datasets, base feature networks and embedding dimensions, 2) we provide insights into the performance effects of binarization and subsampling classes for scalable extreme classification-based training 3) we propose a classification-based approach to learn high-dimensional binary embeddings that achieve state-of-the-art retrieval performance with the same memory footprint as 64 dimensional float embeddings across the suite of retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Metric Learning Losses Metric learning approaches aim to learn a good embedding space such that the similarity between samples are preserved as distance between embedding vectors of the samples. The metric learning losses, such as contrastive loss <ref type="bibr" target="#b1">[2]</ref> and triplet loss <ref type="bibr" target="#b5">[6]</ref>, are formulated to minimize intra-class distances and maximize inter-class distances. Recent approaches in metric learning design the loss function to consider the relationships of all the samples within the training batch <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>, and achieve state-of-the-art performance in image retrieval datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Training Sampling Sampling informative training samples plays an important role in metric learning as also suggested in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Semi-hard sampling in conjunction with tripletloss <ref type="bibr" target="#b15">[16]</ref> has been widely adopted for many tasks. Distanced-weighted sampling <ref type="bibr" target="#b26">[27]</ref> suggests that with a balanced mix of difficulties during training, the image retrieval performance can be further improved. Hierarchical Triplet Loss <ref type="bibr" target="#b23">[24]</ref> proposed that by merging similar classes dynamically during training into a hierarchical tree, more informative samples can be drawn from such a structure and the loss also provides global context for training.</p><p>Classification Losses for Metric Learning Classification losses are widely adopted in face verification applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and achieve state-of-the-art performance. The theoretical link between classification and metric learning is shown in <ref type="bibr" target="#b13">[14]</ref>, and image retrieval tasks have some success adopting classification loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Though classification-based training alleviates the need for carefully choosing sampling method, the parametric nature may cause issue in fine-grained open-set recognition setting <ref type="bibr" target="#b27">[28]</ref>.</p><p>Ensembling Ensembling embeddings has been the most recent focus to further improve image retrieval performance. The ensembled embeddings are trained via boosting <ref type="bibr" target="#b14">[15]</ref>, attending diverse spatial locations <ref type="bibr" target="#b25">[26]</ref>, or partitioning the training classes <ref type="bibr" target="#b28">[29]</ref>. However, such ensembled embeddings trade off image retrieval performance with higher dimensions. They also introduce additional hyperparameters into the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We study recent model architectures and triplet-based and classification losses in Section 4. Beyond standard classification training <ref type="bibr" target="#b3">[4]</ref>, we describe below techniques we used to achieve SOTA on the retrieval tasks including L2 normalization of embedding to optimize for cosine similarity, Layer Normalization of final pooled feature layer, and class balanced sampling of minibatch. An overview of our approach is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalized Softmax Loss</head><p>When training the classification network for metric learning, we remove the bias term in the last linear layer and add an L2 normalization module to the inputs and weights before softmax loss to optimize for cosine similarity. Temperature scaling is used for exaggerating the difference among classes and boosting the gradients as also used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. We follow the same derivation and notations as in <ref type="bibr" target="#b13">[14]</ref>. For embedding x of input image with class label y, the loss with temperature σ can be expressed with the weight of class y p y among all possible classes set Z:</p><formula xml:id="formula_0">L norm = − log exp(x T p y /σ )) ∑ z∈Z exp(x T p z /σ ))<label>(1)</label></formula><p>Normalized softmax loss fits into the proxy paradigm when we view the class weight as proxy and choose the distance metric as cosine distance function. A more general form of classification loss, Large Margin Cosine Loss (LMCL), has been proposed for face verification <ref type="bibr" target="#b22">[23]</ref> with an additional margin hyperparameter. We also evaluate the effect of LMCL in conjunction with our proposed techniques on image retrieval tasks in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Normalization</head><p>The layer normalization without affine parameters <ref type="bibr" target="#b9">[10]</ref> is added immediately after the final pooling layer of the feature model (e.g. GoogleNet <ref type="bibr" target="#b19">[20]</ref>'s pool5 layer) to normalize the feature dimension of our embeddings to have a distribution of values centered at zero. This allows us to easily binarize embeddings via thresholding at zero. We also show empirically through ablation experiments in Section 4.4.1 that layer normalization helps the network better initialize new parameters and reach better optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Class Balanced Sampling</head><p>Based on the proxy explanation of using classification loss for metric learning, the loss is bounded by the worst approximated examples within the class <ref type="bibr" target="#b13">[14]</ref>. A simple way to alleviate this is by including multiple examples per class when constructing the training mini batch.</p><p>For each training batch, we first sample C classes, and sample S samples within each class. The effect of class balanced sampling is studied through ablation experiment in Section 4.4.2 and is a common approach to retrieval tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We follow the same evaluation protocol commonly used in image retrieval tasks with the standard train/test split on four datasets: CARS-196 <ref type="bibr" target="#b11">[12]</ref>, CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref>, Stanford Online Products (SOP) <ref type="bibr" target="#b17">[18]</ref>, and In-shop Clothes Retrieval <ref type="bibr" target="#b33">[34]</ref>. We compare our method using Recall@K to measure retrieval quality. To compute Recall@K, we use cosine similarity to retrieve the top K images from the test set, excluding the query image itself. We first investigate how our embeddings trained with normalized softmax loss compare against embeddings trained with existing metric learning losses using the same featurizer and embedding dimension. We then study in detail how the dimensionality of our embeddings (Section 4.3) affects its performance and the relative performance between float and binary embeddings. Ablation studies on different design choices of our approach on CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> (Section 4.4) are provided as empirical justifications. Next, we investigate how our embeddings are affected by class subsampling in Section 4.5, addressing the key scalability concern of softmax loss where training complexity is linear with the number of classes. Finally in Section 4.6, we show that our method outperforms state-of-the-art methods on several retrieval datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>All experiments were executed using PyTorch and a Tesla V100 graphic card. We compare our method with common architectures used in metric learning including GoogleNet <ref type="bibr" target="#b19">[20]</ref>, GoogleNet with Batch Normalization <ref type="bibr" target="#b7">[8]</ref>, and ResNet50 <ref type="bibr" target="#b3">[4]</ref>. We initialize our networks with pre-trained ImageNet ILSVRC-2012 <ref type="bibr" target="#b2">[3]</ref> weights. We add a randomly initialized fully connected layer to the pool5 features of each architecture to learn embeddings of varying dimensionality. To simplify the sensitivity to the initialization of the fully connected layer, we add a Layer Normalization <ref type="bibr" target="#b9">[10]</ref> without additional parameters between the pool5 and fully connected layer (See Section 4.4.1 for the ablation study). We L2 normalize the embedding and class weights before Softmax and use a temperature of 0.05 consistently. During training, we apply horizontal mirroring and random crops from 256x256 images; during testing we center crop from the 256x256 image. Following <ref type="bibr" target="#b8">[9]</ref> [14], we crop to 227x227 for GoogleNet, otherwise 224x224. Complete implementation details can be found in our source code repository. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function Comparisons</head><p>We compare our normalized softmax loss against existing metric learning losses. To focus on contributions from the loss functions, we leave comparisons against methods that ensemble models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, modify the feature extractor architecture <ref type="bibr" target="#b25">[26]</ref>, or propose complex activation paths between the featurizer and final embedding <ref type="bibr" target="#b14">[15]</ref> for Section 4.6.</p><p>We present Recall@K and NMI results on three standard retrieval datasets in Table <ref type="table" target="#tab_0">1</ref>, Table <ref type="table" target="#tab_1">2</ref>, and Table <ref type="table">3</ref>, comparing against reported performance of methods trained with model architectures of GoogleNet, GoogleNet with Batch Normalization (BNInception), and ResNet50 respectively. For GoogleNet with Stanford Online Products only, we saw around a 1% Recall@1 improvement by training all parameters from start with 10x learning rate on new parameters when compared with models trained with our standard finetuning procedure.</p><p>As shown, our approach compares very strongly against existing baselines. When fixing dimensionality to 512, we see that the performance improvements of our softmax embeddings across architectures mirror classification performance on ImageNet ILSVRC-2012. We hope our results help disentangle performance improvements of existing metric learning methods due to advancements in methodology versus changes of base feature models. Table <ref type="table">3</ref>: Recall@K and NMI across standard retrieval tasks. All methods are trained using ResNet50 for a fair comparison</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Dimensionality</head><p>We study the effects of dimensionality on our classification-based embeddings by varying only the embedding dimension while keeping all other optimization parameters fixed. We have consistently observed that dimensionality is directly related to retrieval performance. Two examples of this across different datasets <ref type="bibr">(CARS-196 and CUB-200-2011)</ref> and model architectures (ResNet50 and GoogleNet) are shown in Figure <ref type="figure" target="#fig_1">2</ref>. Interestingly, this contradicts to reported behaviors for previous non-parametric metric learning methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, showing that dimensionality does not significantly affect retrieval performance. This difference is seen clearly when comparing R@1 across dimensionality for CUB-200-2011 with GoogleNet in Figure <ref type="figure" target="#fig_1">2</ref> with the same dataset and model combination from <ref type="bibr" target="#b17">[18]</ref>.</p><p>Higher dimensional embeddings lead to an increase in retrieval performance. Lower dimensional embeddings however are preferred for scalability to reduce storage and distance computation costs especially in large scale applications such as visual search <ref type="bibr" target="#b10">[11]</ref>. We observe however that as we increase dimensionality of our embeddings, the optimizer does not fully utilize the higher dimensional metric space. Instead, we see that feature dimensions start relying less on the magnitude of each feature dimension and instead rely on the sign value. In Figure <ref type="figure" target="#fig_1">2</ref>, we see for both datasets that the Recall@1 performance of binary features (thresholding the feature value at zero) converges with the performance of the float embeddings. This is a consistent result we see across datasets and model architectures. We show that training high dimensional embeddings and binarizing leads to the best trade-off of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this set of experiments we report the effect on Recall@1 with different design choices in our approach on CUB-200-2011. We train the ResNet50 with embedding dimension of 512 variant as in Table <ref type="table">3</ref>. We fix all hyperparameters besides the one of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Layer Normalization</head><p>We utilize Layer Normalization <ref type="bibr" target="#b9">[10]</ref> without parameters to standardize activation values after pooling to help the initialization of our training. With 100 classes in CUB-200-2011, we expect that a random classifier would value of roughly − ln( 1 100 ) = 4.6. As shown in Table <ref type="table">3</ref>, this occurs when training with Layer Normalization, but not without. We have found incorporating Layer Normalization in our training allows us to be robust against poor weight initialization of new parameters across model architectures, alleviating the need for careful weight initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Class Balanced Sampling</head><p>As seen in Table <ref type="table">4</ref>, class balanced sampling is beneficial over random sequential iteration over the dataset. Generally we've observed that performance improves when we have more samples per class until too few distinct classes exist in the minibatch where the bias of separating few distinct classes may introduce too much noise to the optimization, resulting Figure <ref type="figure">4</ref>: Recall@K for SOP with ResNet50 across class sampling ratios. We see that with sampling only 10% of classes per iteration (∼1K classes), we converge to a R@1 that is less than 1% absolute drop in performance from using all classes. in lower performance. For SOP and InShop, we simply sample roughly how many images exist per class on average (≈ for Classification Scalability</p><p>To show that classification-based training is scalable, we apply the subsampling methodology to the Stanford Online Products dataset, which has the largest number of classes among datasets we tested. By subsampling, for each training minibatch the network only need to classify among a subset of all classes (randomly sampled subset which includes all classes within the training minibatch). We present our findings in Figure <ref type="figure">4</ref>, showing that with only 10% of the classes available during the forward pass of training, we can reach a R@1 performance comparable to using all classes (1% drop in performance). When using 1% of classes, we reach a R@1 of 75.7. When using 0.1% of classes, we reach a R@1 of 72.0. As we can see, subsampling classes during training is an effective method of scaling softmax embedding training with little drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison against State of the Art</head><p>Finally we compare our best performing softmax embedding model against state-of-the-art metric learning approaches on Stanford Online Products, In-Shop, CARS-196, and CUB-200-2011. We reproduced the state-of-the-art method in face verification literature, Large Maring Cosine Loss (LMCL) <ref type="bibr" target="#b22">[23]</ref>, and trained two variants of the networks: one with their recommended network architecture of 512 dimensional embeddings, and one with our modifications of 2048 dimensional embeddings.</p><p>As shown in Table <ref type="table" target="#tab_3">5</ref> and Table <ref type="table" target="#tab_4">6</ref>, our 2048 dimensional ResNet50 embedding outperforms previous approaches. Considering the higher dimensionality of our embeddings, we also show that our 2048 binary embedding, sharing the same memory footprint of a 64 dimensional float embedding, similarly outperforms state-of-the-art baselines. These binary features were obtained by thresholding the float embedding features at zero as in Figure <ref type="figure" target="#fig_1">2</ref>. Considering the scalability and performance of our binary softmax features along the simplicity of our approach, we believe softmax embeddings should be a strong baseline for future metric learning work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we show that classification-based metric learning approaches can achieve stateof-the-art not only in face verification but general image retrieval tasks. In the metric learning community, a diverse set of base networks for training embedding of different sizes are compared to one another. In our work, we conducted comparisons through extensive experimentation, and establish that normalized softmax loss is a strong baseline in a wide variety of settings. We investigate common critiques of classification training and high dimensionality for metric learning through subsampling, making our approach viable even for tasks with a very large number of classes and binarization, allowing us to achieve SOTA performance with the same memory footprint as 64 dimensional float embeddings across the suite of retrieval tasks. We believe these practical benefits are valuable for large scale deep metric learning and real world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture overview for training high dimensional binary embedding</figDesc><graphic url="image-1.png" coords="2,62.75,28.96,274.45,143.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Recall@1 for CARS-196 (left) and CUB-200-2011 (right) across varying embedding dimensions. Softmax based embeddings improve performance when increasing dimensionality. The performance gap between float and binary embeddings converge when increasing dimensionality, showing that when given enough representational freedom, Softmax learns bit like features.</figDesc><graphic url="image-2.png" coords="5,47.93,28.96,156.44,105.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Loss and R@1 trends for training CUB-200 ResNet50 with and without Layer Normalization. Layer Normalization helps initialize learning, leading to better training convergence and R@1 performance. S -1 3 12 25 37 75 C -75 25 6 3 2 1 R@1 59.5 59.6 60.0 60.8 61.3 59.6 40.9 Table 4: ResNet50 Recall@1 on CUB-200-2011 dataset across varying samples per class for batch size of 75. (S) Samples per class in batch. (C) Distinct classes in batch. First column shows no class balancing in batch</figDesc><graphic url="image-4.png" coords="7,80.41,28.96,256.17,129.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Recall@K and NMI across standard retrieval tasks. All methods are trained using GoogleNet for a fair comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell></cell><cell>CARS-196</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB-200</cell><cell></cell></row><row><cell>Recall@K</cell><cell>1</cell><cell>10</cell><cell cols="2">100 NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell></row><row><cell>Contras. 128 [18]</cell><cell cols="3">42.0 58.2 73.8</cell><cell>-</cell><cell cols="4">21.7 32.3 46.1 58.9</cell><cell>-</cell><cell cols="4">26.4 37.7 49.8 62.3</cell><cell>-</cell></row><row><cell>Lift. Struc 128 [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">49.0 60.3 72.1 81.5 55.0 47.2 58.9 70.2 80.2 55.6</cell></row><row><cell cols="4">Lift. Struc 512 [18] 62.1 79.8 91.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hist Loss 512 [21]</cell><cell cols="3">63.9 81.7 92.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">50.3 61.9 72.6 82.4</cell><cell>-</cell></row><row><cell>Bin. Dev 512 [21]</cell><cell cols="3">65.5 82.3 92.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">52.8 64.4 74.7 83.9</cell><cell>-</cell></row><row><cell>Npairs 512 [17]</cell><cell cols="4">67.7 83.8 93.0 88.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Npairs 64 [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">71.1 79.7 86.5 91.6 64.0 51.0 63.3 74.3 83.2 60.4</cell></row><row><cell>Angular 512 [9]</cell><cell cols="14">70.9 85.0 93.5 88.6 71.4 81.4 87.5 92.1 63.2 54.7 66.3 76.0 83.9 61.1</cell></row><row><cell>NormSoftmax 512</cell><cell cols="14">69.0 84.5 93.1 88.2 75.2 84.7 90.4 94.2 64.5 55.3 67.0 77.6 85.4 62.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell></cell><cell>CARS-196</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB-200</cell><cell></cell></row><row><cell>Recall@K</cell><cell>1</cell><cell>10</cell><cell cols="2">100 NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell></row><row><cell>Clustering 64 [19]</cell><cell cols="14">67.0 83.7 93.2 89.5 58.1 70.6 80.3 87.8 59.0 48.2 61.4 71.8 81.9 59.2</cell></row><row><cell cols="2">Proxy NCA 64 [14] 73.7</cell><cell>-</cell><cell>-</cell><cell cols="11">90.6 73.2 82.4 86.4 88.7 64.9 49.2 61.9 67.9 72.4 59.5</cell></row><row><cell>HTL 512 [24]</cell><cell cols="3">74.8 88.3 94.8</cell><cell>-</cell><cell cols="4">81.4 88.0 92.7 95.7</cell><cell>-</cell><cell cols="4">57.1 68.8 78.7 86.5</cell><cell>-</cell></row><row><cell>NormSoftmax 512</cell><cell cols="14">73.8 88.1 95.0 89.8 81.7 88.9 93.4 96.0 70.5 59.6 72.0 81.2 88.4 66.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recall@K and NMI across standard retrieval tasks. All methods are trained using BNInception for a fair comparison NormSoftmax 128 75.2 88.7 95.2 90.4 81.6 88.7 93.4 96.3 72.9 56.5 69.6 79.9 87.6 66.9 NormSoftmax 512 78.2 90.6 96.2 91.0 84.2 90.4 94.4 96.9 74.0 61.3 73.9 83.5 90.0 69.7</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell>CARS-196</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB-200</cell><cell></cell><cell></cell></row><row><cell>Recall@K</cell><cell>1</cell><cell>10</cell><cell>100 NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell></row><row><cell>Margin 128 [27]</cell><cell cols="13">72.7 86.2 93.8 90.7 79.6 86.5 91.9 95.1 69.1 63.6 74.4 83.1 90.0 69.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Recall@K on Stanford Online Products (SOP) and In-Shop. R -ResNet, G -GoogleNet, B -BNInception, † refers to refers to additional attention parameters, LMCL * is our method trained with the Loss .<ref type="bibr" target="#b3">4</ref> 94.4 96.9 61.3 73.9 83.5 90.0 NormSoftmax 2048 R50 89.3 94.1 96.4 98.0 65.3 76.7 85.4 91.8 NormSoftmax 2048bits R50 88.7 93.7 96.4 98.0 63.3 75.2 84.3 91.0</figDesc><table><row><cell></cell><cell>Net.</cell><cell></cell><cell cols="2">CARS-196</cell><cell></cell><cell></cell><cell cols="2">CUB-200</cell></row><row><cell>Recall@K</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>Contrastive 128 [18]</cell><cell>G</cell><cell cols="8">21.7 32.3 46.1 58.9 26.4 37.7 49.8 62.3</cell></row><row><cell>Lifted Struct 128 [18]</cell><cell>G</cell><cell cols="8">49.0 60.3 72.1 81.5 47.2 58.9 70.2 80.2</cell></row><row><cell>Clustering 64 [19]</cell><cell>B</cell><cell cols="8">58.1 70.6 80.3 87.8 48.2 61.4 71.8 81.9</cell></row><row><cell>Npairs 64 [17]</cell><cell>G</cell><cell cols="8">71.1 79.7 86.5 91.6 51.0 63.3 74.3 83.2</cell></row><row><cell>Angular Loss 512 [9]</cell><cell>G</cell><cell cols="8">71.4 81.4 87.5 92.1 54.7 66.3 76.0 83.9</cell></row><row><cell>Proxy NCA 64 [14]</cell><cell>B</cell><cell cols="8">73.2 82.4 86.4 88.7 49.2 61.9 67.9 72.4</cell></row><row><cell>HDC 384 [30]</cell><cell>G</cell><cell cols="8">73.7 83.2 89.5 93.8 53.6 65.7 77.0 85.6</cell></row><row><cell>Margin 128 [27]</cell><cell cols="9">R50 79.6 86.5 91.9 95.1 63.6 74.4 83.1 90.0</cell></row><row><cell>HTL 512 [24]</cell><cell>B</cell><cell cols="8">81.4 88.0 92.7 95.7 57.1 68.8 78.7 86.5</cell></row><row><cell>A-BIER 512 [15]</cell><cell>G</cell><cell cols="8">82.0 89.0 93.2 96.1 57.5 68.7 78.3 86.2</cell></row><row><cell>ABE-8 512 [26]</cell><cell cols="9">G † 85.2 90.5 94.0 96.1 60.6 71.5 79.8 87.4</cell></row><row><cell>DREML 576 [29]</cell><cell cols="9">R18 86.0 91.7 95.0 97.2 63.9 75.0 83.1 89.7</cell></row><row><cell>LMCL 512 [23]</cell><cell cols="9">R50 73.9 81.7 87.4 91.5 58.7 70.3 79.9 86.9</cell></row><row><cell>LMCL  * 2048 [23]</cell><cell cols="9">R50 88.3 93.1 95.7 97.4 61.2 71.4 80.4 87.4</cell></row><row><cell>NormSoftMax 1024</cell><cell>B</cell><cell cols="8">87.9 93.2 96.2 98.1 62.2 73.9 82.7 89.4</cell></row><row><cell>NormSoftmax 128</cell><cell cols="9">R50 81.6 88.7 93.4 96.3 56.5 69.6 79.9 87.6</cell></row><row><cell>NormSoftmax 512</cell><cell cols="3">R50 84.2 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Recall@K on CARS-196 and CUB-200-2011. R -ResNet, G -GoogleNet, B -BNInception, † refers to additional attention parameters, LMCL * is our method trained with the Loss</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code: https://github.com/azgo14/classification_metric_learning.git</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1">ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1703.07737.pdf" />
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
				<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6622</idno>
		<ptr target="http://arxiv.org/abs/1412.6622" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Web-scale responsive visual search at bing</title>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Komlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapei</forename><surname>Stephen) Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meenaz</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><surname>Sacheti</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219843</idno>
		<ptr target="http://doi.acm.org/10.1145/3219819.3219843" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK, Au</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
	<note>gust 19-23</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual search at pinterest</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<meeting>the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
				<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<idno>CoRR, abs/1703.07464</idno>
		<ptr target="http://arxiv.org/abs/1703.07464" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno>arXiv:cs/1801.04815</idno>
		<title level="m">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName><forename type="first">Dengke</forename><surname>Dong Weifeng Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentionbased ensemble for deep metric learning</title>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krähenbühl</surname></persName>
		</author>
		<idno>CoRR, abs/1706.07567</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="712" to="728" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05720</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Google image swirl: a large-scale content-based image visualization system</title>
		<author>
			<persName><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
				<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04680</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Visual discovery at pinterest. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual search at alibaba</title>
		<author>
			<persName><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219820</idno>
		<ptr target="http://doi.acm.org/10.1145/3219819.3219820" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-19">2018. August 19-23. 2018. 2018</date>
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang Ziwei Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
