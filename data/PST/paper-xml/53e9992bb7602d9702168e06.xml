<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-Robot Collaborative Manipulation Planning Using Early Prediction of Human Motion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jim</forename><surname>Mainprice</surname></persName>
							<email>jmainprice@wpi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Jim Mainprice and Dmitry Berenson are with the Robotics Engineering Program</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
								<address>
									<addrLine>100 Institute Rd</addrLine>
									<postCode>01609</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dmitry</forename><surname>Berenson</surname></persName>
							<email>dberenson@cs.wpi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Jim Mainprice and Dmitry Berenson are with the Robotics Engineering Program</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
								<address>
									<addrLine>100 Institute Rd</addrLine>
									<postCode>01609</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human-Robot Collaborative Manipulation Planning Using Early Prediction of Human Motion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">968FE04F3E76829FBE9ED42F863B45CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a framework that allows a human and a robot to perform simultaneous manipulation tasks safely in close proximity. The proposed framework is based on early prediction of the human's motion. The prediction system, which builds on previous work in the area of gesture recognition, generates a prediction of human workspace occupancy by computing the swept volume of learned human motion trajectories. The motion planner then plans robot trajectories that minimize a penetration cost in the human workspace occupancy while interleaving planning and execution. Multiple plans are computed in parallel, one for each robot task available at the current time, and the trajectory with the least cost is selected for execution. We test our framework in simulation using recorded human motions and a simulated PR2 robot. Our results show that our framework enables the robot to avoid the human while still accomplishing the robot's task, even in cases where the initial prediction of the human's motion is incorrect. We also show that taking into account the predicted human workspace occupancy in the robot's motion planner leads to safer and more efficient interactions between the user and the robot than only considering the human's current configuration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Factory automation has revolutionized manufacturing over the last 50 years, but there is still a large set of manufacturing tasks that are tedious or strenuous for humans to perform. Some of these tasks, such as electronics or aircraft assembly, are difficult to automate because they require workers to collaborate in close proximity and adapt to each other's decisions and motions, which robots cannot currently do. Rather than automating such tasks fully (which may not be possible and/or cost-effective), we believe that human-robot collaboration can enable safe and effective task execution while reducing tedium and strain of the human.</p><p>In this paper we address an important step toward humanrobot collaboration: allowing a robot and human to safely perform simultaneous manipulation motions in close proximity to one another. Given two sets of tasks M and K that the human and the robot could perform, respectively, at a given time, we seek to create a method that selects the robot task and plans the robot motion while 1) minimizing the physical interference between the human and the robot during task execution, and 2) minimizing robot execution time. This involves sensing and predicting the movements of the human, adapting the motion of the robot to avoid the human, and choosing the robot task which interferes the least with the human. A popular approach to maintaining safety in the presence of moving humans is to interleave planning and execution within a motion planning framework <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Humans are treated as dynamic obstacles and the human motion is predicted in the replanning window by using a bounded velocity model. However future motions are unaccounted for, which may lead to inefficiency in the robot behavior. On the other end, research on human-robot collaborative task planning has been focused on taking into account the human explicitly by maintaining a model of affordances <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and using such analysis at the decision and the motion planning layers. Although these approaches reason on human abilities, they only account for a static model of the human. In this work we propose to account for human motion by learning a task-specific model of human motion and using it to infer the humans future motion and compute safe robot trajectories.</p><p>An overview of the framework we have developed to address the above problem is presented in Figure <ref type="figure" target="#fig_0">1</ref>. Our method is divided in two phases. In the offline phase, we gather data about how the human moves to perform the set of tasks we are interested in. In the online phase, we use the models learned in the offline phase to predict the human's motion and simultaneously select a task that interferes the least with the human while producing a robot motion that avoids the human.</p><p>An important feature of our framework is early detection of the human's intent. This is performed by querying the probabilistic models learned in the offline phase. The result of this query is used to predict the workspace occupancy of the human's future motions, using the swept volume of the representative motions obtained by regression.</p><p>The motion planning component of the framework computes a plan for the K robot tasks in parallel as the robot moves, and falls back to the best solution at each re-planning step, which gives it an anytime property. Hence as we interleave prediction, planning, and execution, the robot is able to adapt its motion to the human's intent, minimize interference, and change tasks quickly in order to avoid the human.</p><p>This paper focuses on the core algorithms and framework structure that allow safe simultaneous human-robot motion in close proximity. We thus implement, test, and analyze our method in simulation, which is necessary before any human trials are conducted. Given the success of the simulation study presented in this paper, we will pursue physical experiments with human subjects in future work.</p><p>The rest of the paper is organized as follows: After discussing related work in section II, we give a detailed overview of the framework in section III. In section IV we present the method for early human motion recognition and prediction of workspace occupancy. Then in section V we present the motion planning component before going over the experimental results in section VI. We end the paper with conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work contributes to the field of safe robot manipulation in the presence of humans, it is similar to <ref type="bibr" target="#b5">[6]</ref>, however their work only considers motions of the human's hands and does not fall back to the plan of least interference. Older works in the area of safe robot manipulation in proximity to humans, e.g. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, have led to the characterization of safety criteria. However, though safety is the most important factor of robotic design, it may be desirable to account for other constraints to allow for more comfortable userinteractions and more efficient robot behaviors.</p><p>In the closely-related topic of robot navigation in the presence of humans, early works were inspired by the social behavior demonstrated by humans <ref type="bibr" target="#b8">[9]</ref>. In those work, the motion planner not only reasons about the human safety, i.e. clearance, but also about how visible the robot is along the robot motion to avoid the effect of surprise <ref type="bibr" target="#b9">[10]</ref> leading to more legible navigation plans. Recently this approach was extended to combine planning and more reactive schemes <ref type="bibr" target="#b10">[11]</ref>, allowing more natural crossings between the robot and the human. Similar to our approach, researchers have explored how to learn models of human motion and use them for online online navigation planning <ref type="bibr" target="#b11">[12]</ref>. While we believe it is possible to apply our framework to navigation, our approach is intended for manipulation tasks which require planning in a much higher-dimensional space as well as predicting human motion in that space.</p><p>Motion planning for robot manipulation in close proximity to humans poses two main challenges: first on the algorithmic techniques to be used and second on the definition of safety and comfort metrics. Recent work has explored computing more human-like motions <ref type="bibr" target="#b12">[13]</ref> using a reachability map representation. Other work <ref type="bibr" target="#b4">[5]</ref> has focused on considering the human's safety, visibility, and musculoskeletal comfort directly in the robot configuration space and solving motion planning using sampling-based and stochastic-optimization techniques. The recent work by Dragan and Srinivasa <ref type="bibr" target="#b13">[14]</ref> proposes a legibility metric inspired by the psychology of action interpretation applied to robot motion planning using functional gradient optimization. The resulting motion trajectories purposefully deviate from what an observer would expect in order to better convey intent. This work in particular can be seen as the counter part of our approach, where the robot tries to interpret the human's intent as early as possible.</p><p>Simultaneous motion for human-robot collaboration has been studied in terms of hand-over tasks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. A motion planning method that allows to share the effort by exploring the combined configuration space of the human and the robot has been proposed in <ref type="bibr" target="#b17">[18]</ref>. While hand-over is an important instance of collaboration, the work presented here is meant to apply to a broader class of tasks, although it could be adapted for hand-over tasks as well.</p><p>A major part of our framework is early prediction of human motion. Researchers have investigated early recognition of gestures <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b19">[20]</ref>, where the goal is to classify a type of gesture being performed by the human. While classifying the type of motion is the first step of our framework, we are also concerned with predicting the human's future motion and ultimately computing the probability of the human occupying a given voxel in the environment. Our method of learning the Gaussians Mixture Model (GMM) representation of the human's motion and querying it using Gaussian Mixture Regression (GMR) is similar to <ref type="bibr" target="#b20">[21]</ref>, although, again, we extend this work to predict the workspace occupancy of the human, as well as integrating the prediction with the robot motion planning.</p><p>We rely on recent developments in trajectory optimization for motion planning <ref type="bibr" target="#b0">[1]</ref> [22] <ref type="bibr" target="#b22">[23]</ref> to compute low-cost robot motion plans quickly. Our framework uses the STOMP algorithm, which has proven effective for the type of manipulator motion planning we consider <ref type="bibr" target="#b22">[23]</ref>. Recently, STOMP was adapted to run faster than real-time <ref type="bibr" target="#b2">[3]</ref>, and we plan to employ this new method in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK OVERVIEW</head><p>Our framework (Figure <ref type="figure" target="#fig_0">1</ref>) is composed of two stages: 1) Offline construction of a probabilistic representation of the human workspace occupancy from a motion trajectory library, and 2) online recognition and prediction of human motion and computation of robot motion plans. The online stage also simultaneously computes plans for each possible robot task. The one that minimizes interference with the human is then selected for execution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Offline phase</head><p>A library of motions is first gathered from human demonstration. The type of motion depends on the type of task being executed by the human, here we consider manipulation motion on a table. Each motion in the library is then tagged depending on the intent of the human. For instance, in the manipulation tasks considered in the experiments of section VI, the intent corresponds to the goal position of the hand as the human reaches for an object on the table. In order to perform early motion recognition and predict the subsequent human motion, we fit a GMM to each class of motion. We then use GMR to extract a new motion that best fits the class. Figure <ref type="figure" target="#fig_1">2</ref> shows the regressed motion trajectories used in the experiments. These trajectories are used to compute a swept volume that corresponds to the class which is then stored in a 3D voxel grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Online phase</head><p>The online phase interleaves prediction, planning and execution. The GMM of each class is first queried with the partial human motion trajectory observed at the current time, yielding likelihood values for each class. The probability of occupancy of a given voxel is then estimated by summing the likelihood of all motion classes whose swept volumes occupy the voxel.</p><p>At each replanning step, the cost assessed by the motion planner is updated according to the prediction of human workspace occupancy. We define a penetration cost in the occupancy grid where high cost corresponds to penetration in regions very likely to be occupied by the human's subsequent motion. This cost can be viewed as an interference cost. We compute robot motions that minimize this cost along the robot trajectory using STOMP <ref type="bibr" target="#b22">[23]</ref>. We also plan for each task available simultaneously by running STOMP in parallel, one instance per task, and then select the trajectory for the task with the least interference cost at the end of the replanning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EARLY RECOGNITION AND PREDICTION OF HUMAN MOTION AND WORKSPACE OCCUPANCY</head><p>Offline, we fit one GMM per class of motion that we are considering in the human motion library. Intuitively, fitting Gaussians to the data set allows us to restrict each class to a small set of parameters, which is straightforward to query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gaussian mixture model</head><p>Each motion trajectory in our gathered data set is represented by a matrix ξ composed of T vectors of dimension D, with each vector representing a posture.</p><p>In gesture recognition, an important problem resides in constructing the feature space, i.e. what values are considered in the posture vector. In <ref type="bibr" target="#b19">[20]</ref> the author studies three types of features space, euclidean i.e. position of interest points on the kinematic structure of the human, joint angles, or amorpholgical i.e. combination of joint angles and relative positions between links, as well as the derivative of the feature vector. We chose joint angles to represent postures, as our aim is to both predict and recognize the motion. Joint angles enable a straightforward reconstruction of the regressed motion with no need for inverse kinematics (which may be difficult due to redundancy). In our experiments 12 DoFs are taken into account: pelvis position and orientation as well as the joint values of the arm and torso.</p><p>In the library we define M classes (which correspond to 8 goal positions in our experiments), the set of motions of class m is denoted C m . Each class consists of N trajectories. We then fit N g gaussians per class; e.g. 20 in our experiments. Thus, the probability density of any posture in a class represented by N g gaussians is given by:</p><formula xml:id="formula_0">p(ξ t ) = Ng g=1 p(g)p(ξ t |g),</formula><p>where ξ t is the feature vector corresponding to the posture along the motion trajectory ξ at index t and p(g) is the prior probability of component g. The conditional probability for g is defined as follows:</p><formula xml:id="formula_1">p(ξ t |g) = N (μ g , Σ g ) = 1 (2π) D |Σ g | e -1</formula><p>where and {μ g , Σ g } are the mean and covariance parameters of the Gaussian component g. This probabilistic representation of the data set enables two things: first to extract a motion trajectory for each class using GMR, and second, to compute the likelihood that any new motion trajectory belongs to any class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training</head><p>To fit the GMM to a set of motion trajectories, we have to maximize the likelihood for the mixture parameters. For a given class C m comprising of N motion trajectories, the objective is to maximize the likelihood defined as follows:</p><formula xml:id="formula_2">p(ξ|C m ) = N n=1 T t=1 p(ξ n t |C m )</formula><p>Since the number of considered feature vectors is usually quite high (e.g. 2500 postures per class in our experiments) p(ξ|C m ) can exceed the machines precision. In order to avoid such situations we used the log-likelihood which is a common procedure for mixture model fitting.</p><p>No analytical method exists for maximizing the likelihood. So we use the widely known expectancy-maximization (EM) algorithm which is a simple search technique that monotonically increases the log-likelihood during optimization. The obtained distribution corresponds to a local minimum.</p><p>EM consists of two steps. The E step computes the loglikelihood for the parameters (p(g), μ g , Σ g ), and the M steps performs their adjustment through computation of the partial derivatives of the log-likelihood function. We initialize the procedure with an estimate provided by a k-means clustering applied to the data set. The algorithm stops when a negligible improvement of the log-likelihood is attained. The reader may refer to <ref type="bibr" target="#b23">[24]</ref> for a more detailed explanation of this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Query phase</head><p>Once the GMMs are fitted for each class, the loglikelihood of all C m can be computed as follows:</p><formula xml:id="formula_3">ln(p(ξ|C m )) = T t=1 ln(p(ξ t |C m ))</formula><p>Note that for the classification problem we would only consider the class with highest log-likelihood. Here we use the likelihood of each class to weigh the voxel occupancy as shown in section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extraction of the swept volume</head><p>To obtain a motion reconstructed from the GMMs that represents the predicted trajectory for each class, we use the GMR procedure <ref type="bibr" target="#b24">[25]</ref>. More precisely, we apply the technique as established in <ref type="bibr" target="#b20">[21]</ref>, as it provides a way to reconstruct a general motion for the class.</p><p>The swept volume of a given class is computed using the regressed trajectory of the class. To compute the swept volume of the trajectory, we first sample points on the human model 3D surface. We then set the human model to each configuration along the trajectory. The voxel occupancy for one configuration is computed by looking up the voxels that contain the sampled points and marking those as occupied. The sampling has to be dense enough not to miss any voxel. Thus, the swept volume of a given class is computed as the union of the voxel occupancy of discretized configurations along the regressed motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Likelihood estimation of workspace occupancy</head><p>The motion recognition systems relies on the loglikelihood computed by querying the GMMs which represent the probability of the motions belonging to each class given the partial trajectory observed so far (see Section IV-D). To obtain a prediction p(x|ξ) for each voxel x to be occupied by the human motion ξ, we first retrieve the likelihood p(ξ|C m ), the output of the GMM query, of the M motion classes by exponentiation. We then compute p(x|ξ) by summing the contribution of the M motion classes C m in the following manner:</p><formula xml:id="formula_4">p(x|ξ) = M m=1 p(x|C m )p(C m |ξ)</formula><p>where:</p><formula xml:id="formula_5">p(C m |ξ) = p(ξ|C m ) M m=1 p(ξ|C m ) and: p(x|C m ) = 1 if x is occupied by C m 0 otherwise</formula><p>Intuitively, this procedure returns a score p(x|ξ) ∈ [0 , 1] for all voxels that is higher for regions more likely to be occupied by the subsequent human motion, and thus predicts workspace occupancy. This score will be used by the motion planner to avoid colliding with the human and adapt the robot motion to the human's intent. Figure <ref type="figure" target="#fig_2">3</ref> shows the evolution of the estimation of workspace occupancy during the execution of a human motion recorded with the Kinect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MOTION PLANNING WITH PREDICTED HUMAN WORKSPACE OCCUPANCY</head><p>In this section, we describe the motion planning approach based on STOMP. A similar approach has been proposed and studied recently in <ref type="bibr" target="#b2">[3]</ref>. Our approach differs from [3] by using workspace occupancy prediction and planning simultaneously for distinct goals.</p><p>A. Planning with predicted human workspace occupancy STOMP <ref type="bibr" target="#b22">[23]</ref> is a trajectory optimizer that iteratively deforms an initial solution by estimating stochastically the gradient in trajectory space. It internally represents the trajectory by an m by n matrix, where m is the number of DoFs and n the number of waypoints. At each iteration, trajectories are sampled in the neighborhood of the current solution and combined to generate the update. Thus it does not require the analytical gradient of the cost function to be known. The convergence rate to a local minima depends on the standard deviation with which the neighboring trajectories are sampled.</p><p>The initial algorithm presented in <ref type="bibr" target="#b22">[23]</ref> optimizes a combination of two classical criteria, namely obstacle cost and smoothness cost. The first is estimated by computing a penetration distance in the static obstacles for every waypoint using a signed Euclidean Distance Transformed (EDT). The second is estimated by summing the squared accelerations along the trajectory using finite differencing.</p><p>In order to account for the human's intent and minimize interference, we combine a third cost criterion that penalizes configurations penetrating the human predicted workspace occupancy. The penetration cost of a given configuration q is estimated in the following manner: after sampling points on the robot structure in an initialization phase, we place the robot at q and evaluate the probability of occupancy of the sampled points. The total cost of q is then simply the sum of the probabilities of occupancy p(x|ξ) (see section IV) of the human occupancy-map voxels x, that contain sampled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Planning with multiple goals</head><p>Our approach consists of interleaving prediction, planning and execution (see Figure <ref type="figure" target="#fig_3">4</ref>). At each replanning step K STOMPs are run in parallel, one per manipulation task (e.g. pick object1, pick object2, place object1 at p, ...). At replanning step n, the robot executes the motion planned in  step n -1, it also records the current human motion and predicts the human workspace occupancy. This workspace occupancy prediction will be used in replanning step n + 1.</p><p>The complete planner is described in algorithm 1. The planner first starts by initializing straight line trajectories between the current configuration and each goal in initStraightLines. It then loops over the three following steps. First, the updated human motion ξ yields a new voxel occupancy in the predictVoxelOccupancy function. This updates the cost function taken into account by STOMP. Then for each goal, the reconnectPrevious function adapts the input trajectory in order to cope with the interleaving of the execution step (discussed in section V-C). The planner then launches each STOMP in a different thread. Finally, once each trajectory has been optimized by STOMP, the algorithm executes the trajectory τ best that minimizes the overall cost combining human interference, obstacle cost and smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interleaving planning and execution when considering multiple goals</head><p>When planning with multiple tasks, the execution of the trajectory τ best brings the robot away from the trajectories planned for the other tasks. Hence, in order to reuse part of the previously optimized trajectories, the reconnectPrevious function reconnects the current robot configuration to the previous trajectory by selecting the best feasible solution.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> depicts the procedure for two goals g 1 and g 2 where a trajectory has been planned from q cur to each goal. All straight lines from the future configuration q t on τ best to the previous trajectory are tested and the trajectory which minimizes cost is kept to provide the next replanning step preformed by STOMP with a good initial input leading to goal g 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head><p>We have performed a series of test and experiments on the framework to asses the efficacy of the motion recognition system and the capacity of the framework to minimize interference with the human and switch quickly between robot tasks when necessary. We first evaluate the classification system, then we show the overall efficacy of the framework on two examples. The GMM fitting and GMR algorithms have been implemented in Matlab. The online classifier, STOMP, and a simulated version of the execution framework have been implemented in Move3D <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classification</head><p>We first evaluated the performance of the classification component of our framework by gathering a library of M = 8 classes of human motion trajectories each containing N = 25 trajectories using the Kinect sensor. We converted the raw Kinect data into joint-space trajectories using a custom inverse kinematics procedure. The regressed trajectories of each class are presented in Figure <ref type="figure" target="#fig_1">2</ref>. In order to verify that this system is adequate for predicting human motions, we evaluated the classification by querying the GMM using leave-one-out testing.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> presents the average recognition rate for the 200 motion trajectories as a function of the executed fraction of the motion trajectory. The recognition rate increases as the fraction of the motion trajectory, until reaching a plateau at 92% of the trajectories being correctly classified which occurs at 80% of trajectory execution.</p><p>The classifier crosses 50% of good classification with 43% of the motion trajectory and reaches a high percentage of  good classification results (approximately 80%) with 60% of the motion trajectory. This shows that the GMM fitting is well suited for early motion recognition and prediction, and can be used for collaborative manipulation. We expect that recognition would decrease as the number of classes taken into account increases. However, this could be compensated with a larger motion trajectory library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interference in a manipulation scenario</head><p>We study the manipulation task depicted in Figure <ref type="figure" target="#fig_7">7</ref> where the robot and the human reach for two cups set on a table. At each replanning step, we move the human forward along the recorded motion trajectory by a fixed amount, estimate the workspace occupancy and replan the robot motion. We execute 20 replanning steps in total for each run.</p><p>A replanning step comprises 50 iterations of STOMP which take approximately 0.5 sec. We plan to make use of the GPU accelerated framework described by <ref type="bibr" target="#b2">[3]</ref> to decrease planning time. Given their speed-up description, we can expect to reduce planning time by a factor of 20 to 30.</p><p>We compare two versions of the planner. 1) A naive version that computes a path without considering the predicted workspace occupancy. In this case the intermediate path is planed by only considering smoothness and obstacle constraints. 2) A planner that considers the supplementary constraint of minimizing the penetration in the predicted workspace occupancy along the path (our method).</p><p>Table <ref type="table" target="#tab_2">I</ref> presents the integral cost of the executed robot trajectories considering the final and most accurate prediction of the workspace occupancy. The results are averaged values over 10 runs for 5 human motion trajectories taken from a human motion trajectory library similar to the one described above. The GMMs used in the experiment have been trained leaving those 5 trajectories out.</p><p>In Figure <ref type="figure" target="#fig_7">7</ref> where the robot reaches for the the red cup while the human reaches for the blue cup. As one can see taking into account the early prediction of workspace occupancy in the planning phase produces minimal interference with the human and provides good clearance.</p><p>This fact is also confirmed by the results presented in table I. Indeed, even though the workspace occupancy predicted with a minimal execution of the human trajectory is inaccu-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Example with two robot tasks</head><p>We now study a slightly different scenario where the human and the robot manipulate objects facing each other across the table (see Figure <ref type="figure" target="#fig_9">8</ref>). Two cups are present on the table similarly to the example of Figure <ref type="figure" target="#fig_7">7</ref>, and this time the human reaches for the red cup. The robot plans for both tasks (i.e. reaching for both cups), and the goal configurations of the robot are shown on the left side of Figure <ref type="figure" target="#fig_9">8</ref>. The trajectories being optimized for each goal configuration are depicted in green and orange. Green for the current τ best and orange for the other.</p><p>We compare two versions of the algorithm, one that only accounts for the current workspace occupancy of the human (Figure <ref type="figure" target="#fig_9">8</ref>.b, 8.c, 8.d) and another that accounts for the predicted workspace occupancy (Figure <ref type="figure" target="#fig_9">8</ref>.f, 8.g, 8.h). In the latter case, the early recognition system provides the robot with an initial workspace occupancy prediction that shifts as the human reaches towards the object. This is also depicted in Figure <ref type="figure" target="#fig_9">8</ref> with the same color convention as Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> presents the evolution of the cost of the two candidate trajectories from the current configuration to the target configuration. This cost is evaluated at the end of each replanning step. The top graph corresponds to using the current observed workspace occupancy version while the bottom graph corresponds to the predicted workspace occupancy version of the algorithm. The integral cost of the trajectory reaching for the blue cup is depicted in blue and in red for the red cup. As shown in Figure <ref type="figure" target="#fig_8">9</ref>, the robot switches tasks in step 4 when considering prediction of the workspace occupancy while only switching in step 13 otherwise. Switching task later results in less efficient collaboration as time taken between step 4 and 13 could have been better allocated to perform another task. As the ambiguity in the human intent lessens, depicted in Figure <ref type="figure" target="#fig_9">8</ref> by the shift of workspace occupancy prediction, the robot changes the target to the one that minimizes the penetration cost in the predicted workspace occupancy thus leading to more efficient collaborative behavior than produced using only the human's current configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE WORK</head><p>We have presented a framework that allows the human and the robot to perform simultaneous manipulation safely to enable collaborative tasks in close proximity.</p><p>This framework is based on early prediction of the human motion. The prediction system, which builds on previous work in the area of gesture recognition, yields a workspace occupancy prediction by computing the swept volume of learned motion trajectories. The motion planner then plans trajectories that minimize a penetration cost in the workspace occupancy and interleaves planning and execution. Plans for multiple tasks can be considered in parallel to select the leastcost task.</p><p>Our results show that taking into account workspace occupancy prediction in the motion generation leads to safer, more efficient interactions between the user and the robot than only considering the current configuration. Future work concerns testing this hypothesis on real users with a real PR2. We also intend to investigate a wider range of tasks such as manipulation in shelves or incorporating human navigation.</p><p>Finally, our framework considers all available robot actions at the current time to have equal value to the overall process, which may not be the case. To account for varying action costs at the process level, we seek to explore integration with task-level planning for human-robot teams such as <ref type="bibr" target="#b26">[27]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data flow of the framework: In the offline stage the human motion library is encoded in a gaussian mixture model and the swept volumes of regressed motions are computed. In the online stage, the workspace occupancy prediction module makes use of the mixture model to produce plans for the robot to minimize interference with the human.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A motion library gathered using kinect data is regressed to 8 motion trajectories using GMR. Each motion type corresponds to a different reaching goal position on the table.</figDesc><graphic coords="3,193.31,139.73,111.02,91.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Evolution of the workspace occupancy prediction stored in a voxel map during a manipulation task. Red spheres correspond to high probability of occupancy and blue to low values. The prediction switches from right to left of the human as he/she progresses towards the goal position.</figDesc><graphic coords="4,256.79,56.69,98.42,77.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Interleaving of prediction, planning and execution. The planner uses the prediction that has been established at the previous replanning step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Depiction of the procedure performed in the reconnectPrevious function of algorithm 1 with two goals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Leave-one-out testing of the recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7.Left, the motion is planned without considering workspace occupancy, right, considering it as an additional constraint of STOMP.</figDesc><graphic coords="7,59.51,53.93,110.30,96.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Integral cost of the two cups example as more replanning steps are executed. Top: only considering current workspace occupancy. Bottom: using workspace occupancy prediction. Red curves correspond to red cups and blue curves to blue cups respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Optimization of two trajectories and switch between the initial trajectory and the goal. The current trajectory is depicted in green and the other is yellow. The sequence on top (b,c,d) corresponds to planning with the current human workspace occupancy while the bottom sequence (f,g,h) makes use of the predicted human workspace occupancy.</figDesc><graphic coords="8,131.27,175.01,131.39,103.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) November 3-7, 2013. Tokyo, Japan 978</head><label></label><figDesc></figDesc><table /><note><p>-1-4673-6358-7/13/$31.00 ©2013 IEEE</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I AVERAGED</head><label>I</label><figDesc>COST OF 10 RUNS OF THE MANIPULATION TASK FOR 5</figDesc><table><row><cell cols="3">DISTINCT MOTION TRAJECTORIES OF THE HUMAN</cell></row><row><cell></cell><cell>Cost (basic STOMP)</cell><cell>Cost (least penetration)</cell></row><row><cell>Traj. 1</cell><cell>24.41</cell><cell>0.41</cell></row><row><cell>Traj. 2</cell><cell>21.47</cell><cell>0.49</cell></row><row><cell>Traj. 3</cell><cell>27.52</cell><cell>0.60</cell></row><row><cell>Traj. 4</cell><cell>24.44</cell><cell>0.41</cell></row><row><cell>Traj. 5</cell><cell>25.13</cell><cell>0.74</cell></row><row><cell cols="3">rate in the first replanning steps, the robot trajectory produced</cell></row><row><cell cols="3">minimizes the penetration on the final workspace occupancy.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>((ξt-μg) T Σ -1 g (ξt-μg))</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Elastic strips: A framework for motion generation in human environments</title>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inevitable collision states-a step towards safer robots?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fraichard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time optimization-based planning in dynamic environments using gpus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mightability maps: A perceptual level decisional framework for co-operative and competitive human-robot interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Planning human-aware motions using a sampling-based costmap planner</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sisbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName><surname>Siméon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pre-collision safety strategies for human-robot interaction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Requirements for Safe Robots: Measurements, Analysis and New Insights</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haddadin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albu-Schäffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A system for the notation of proxemic behavior</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American anthropologist</title>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A human aware mobile robot motion planner</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sisbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Marin-Urias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Legible robot navigation in the proximity of moving humans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Glasauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Robotics and its Social Impacts</title>
		<imprint>
			<date type="published" when="2012">2012 Workshop on, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Planning-based prediction for pedestrians</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making planned paths look more humanlike in humanoid robot manipulation planning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zacharias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating legible motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Humanrobot interaction in handing-over tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Glasauer</surname></persName>
		</author>
		<editor>ROMAN</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Human-robot interaction for cooperative manipulation: Handing objects to one another</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edsinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
		<editor>ROMAN</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Using spatial and temporal contrast for fluent robot-human hand-overs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Forlizzi</surname></persName>
		</author>
		<editor>HRI</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sharing effort in planning human-robot handover tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Siméon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<editor>ROMAN</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early recognition and prediction of gestures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurazume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>-I. Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gestion de la variabilité morphologique pour la reconnaissance de gestes naturels à partir de données 3D</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Université Rennes 2</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On learning, representing, and generalizing a task in a humanoid robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chomp: Gradient optimization techniques for efficient motion planning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stomp: Stochastic trajectory optimization for motion planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning (information science and statistics)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Move3d: A generic platform for path planning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Siméon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Laumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lamiraux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assembly and Task Planning, Proceedings of the International Symposium on</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved humanrobot team performance using chaski, a human-inspired plan execution system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<editor>HRI</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
