<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
							<email>davide.cozzolino@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Poggi</surname></persName>
							<email>poggi@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
							<email>verdoliv@unina.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">DIETI University Federico II of Naples</orgName>
								<address>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">DIETI University Federico II of Naples</orgName>
								<address>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">DIETI University Federico II of Naples</orgName>
								<address>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IH&amp;MMSec &apos;17</orgName>
								<address>
									<addrLine>June 20-22</addrLine>
									<postCode>2017</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B17FED15A4E70D59C40AA02AE487228C</idno>
					<idno type="DOI">10.1145/3082031.3083247</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Local descriptors</term>
					<term>bag-of-words</term>
					<term>CNN</term>
					<term>image forgery detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local descriptors based on the image noise residual have proven extremely e ective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shi ing on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). en, by relaxing the constraints, and ne-tuning the net on a relatively small training set, we obtain a signi cant performance improvement with respect to the conventional detector.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>may raise suspects and suggest deeper inquiry. In fact, most of the times, copy-moves and splicing are accompanied by various forms of elaboration aimed at removing the most obvious traces of editing. ese include, for example, resizing, rotation, linear and non-linear ltering, contrast enhancement, histogram equalization and, eventually, re-compression.</p><p>A number of papers have been proposed to detect one or the other of such elaborations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. ese methods, however, are sensitive to just some speci c manipulations. A more appealing line of research is to detect all possible manipulations, an approach that has been followed in several papers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. Notably, in the 2013 IEEE Image Forensics Challenge, the most e ective techniques for both image forgery detection <ref type="bibr" target="#b5">[6]</ref> and localization <ref type="bibr" target="#b6">[7]</ref> used this approach, relying on powerful residual-based local descriptors. ese features, such as SPAM (subtractive pixel adjacency matrix) <ref type="bibr" target="#b20">[21]</ref> or SRM (spatial rich models) <ref type="bibr" target="#b10">[11]</ref>, inspired to previous work in steganalysis, are extracted from the so-called residual image. In fact, the noise residual, extracted through some high-pass ltering of the image, contains a wealth of information on the in-camera and out-camera processes involved in the image formation. Such subtle traces, hardly visible without enhancement, may reveal anomalies due to object insertion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or allow detecting di erent types of image editing operations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Very recently, inspired by impressive results in the closely related elds of computer vision and pa ern recognition <ref type="bibr" target="#b16">[17]</ref>, the multimedia forensics community began focusing on the use of deep learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, especially convolutional neural networks (CNN) <ref type="bibr" target="#b26">[27]</ref>. Taking advantage of the lesson learnt from SPAM/SRM features, constrained CNN architectures have been proposed both for steganalysis <ref type="bibr" target="#b22">[23]</ref> and manipulation detection <ref type="bibr" target="#b1">[2]</ref>, where the rst convolutional layer is forced to perform a high-pass ltering.</p><p>In this paper we show that there is no real contraposition between residual-based features and CNNs. Indeed, these local features can be computed through a CNN with architecture and parameters selected so as to guarantee a perfect equivalence. Once established this result, we go beyond emulation, removing constraints on parameters, and ne-tuning the net to further improve performance. Since the resulting network has a lightweight structure, ne-tuning can be carried out through a small training set, limiting computation time and memory usage. A signi cant performance gain with respect to the conventional feature is observed, especially in the most challenging situations.</p><p>In the following we describe in more detail the residual-based local features (Section 2), recast them as a constrained CNN, to be further trained a er removing constraints (Section 3), show experimental results (Section 4), and draw conclusions (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Session: Deep Learning for Media Forensics</head><p>IH&amp;MMSec <ref type="bibr">'17, June 20-22, 2017</ref>, Philadelphia, PA, USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RESIDUAL-BASED LOCAL DESCRIPTORS</head><p>Establishing which type of processing an image has undergone calls for the ability to detect the subtle traces le by these operations, typically in the form of recurrent micropa erns. is problem has close ties with steganalysis, where weak messages hidden in the data are sought, so it is no surprise that the same tools, residualbased local descriptors, prove successful in both cases. To associate a residual-based feature to an image, or an image block, the following processing chain has been successfully used in steganalysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>:</p><p>(1) extraction of noise residuals (2) scalar quantization (3) computation of co-occurrences (4) computation of histogram</p><p>In the following, we describe in some more depth all these steps, taking a speci c model out of the 39 proposed in <ref type="bibr" target="#b10">[11]</ref> as running example.</p><p>Extraction of noise residual. e goal is to extract image details, in the high-frequency part of the image, which enables the analysis of expressive micropa erns. As the name suggests, this step can be implemented by resorting to a high-pass lter. In <ref type="bibr" target="#b10">[11]</ref> a number of di erent high-pass lters have been considered, both linear and nonlinear, with various supports. Here, as an example, we focus on a single 4-tap mono-dimensional linear lter, with coe cients w = [1, -3, 3, -1]. e lter extracts image details along one direction, but is applied also on the image transpose (assuming vertical/horizontal invariance) to augment the available data. Choosing a single lter rather than considering all models proposed in <ref type="bibr" target="#b10">[11]</ref> is motivated not only by the reduced complexity but also by the very good performance observed in the context of image forgery detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Scalar quantization. Residuals are conceptually real-valued quantities or, in any case, high-resolution integers, so they must be quantized to reduce cardinality and allow easy processing. In <ref type="bibr" target="#b10">[11]</ref> a uniform quantization is used with an odd number of levels (to ensure that 0 is among the possible outputs). erefore, the only parameters to set are the number of quantization levels, L, and the quantization step ∆. In our example we set L = 3 and ∆ = 4.5.</p><p>Co-occurrences. e computation of co-occurrences is the core step of the procedure. In fact, this is a low-complexity means for taking into account high-order dependencies among residuals and hence gather information on recurrent micropa erns. Following <ref type="bibr" target="#b10">[11]</ref> we compute co-occurrences on N = 4 pixels in a row, both along and across the lter direction. With these values, two cooccurrence matrices with 3 4 = 81 entries are obtained. Of course, the image or block under analysis must be large enough to obtain meaningful estimates. All the co-occurrence N -dimensional bins are eventually coded as integers.</p><p>Feature formation. Counting co-occurrences one obtains the nal feature vector describing the image. Neglecting symmetries, our nal feature has length equal to 162. e nal classi cation phase is performed by a linear SVM classi er.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>filter SQ </p><formula xml:id="formula_0">z 0 z -1 . . . z 1-N coder hist h R R R1 R2 R3 I * X filter z 0 z -1 . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECASTING LOCAL FEATURES AS CNN</head><p>We will now show that local residual-based features can be extracted by means of a convolutional neural network. Establishing this equivalence leaves us with a CNN architecture and a set of parameters that are already known to provide an excellent performance for the problem of interest. en, given this good starting point, we can move a step forward and ne-tune the network through a sensible training phase with labeled data. Note that in this way we will carry out a joint optimization of both the feature extraction process and classi cation. In the following we will rst move from local features to a Bag-of-Words (BoW) paradigm, and then proceed to the implementation by means of Convolutional Neural Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From local features to Bag-of-Words</head><p>In Fig. <ref type="figure" target="#fig_0">1</ref> (top) we show the basic processing scheme used to extract the single model SRM feature. Let X be the input image <ref type="foot" target="#foot_0">1</ref> , R the residual image, and R the quantized residual image. To compute the output feature, the input image is high-pass ltered, then the residual image is quantized, and N versions of it are generated, shi ed one pixel apart from one another. For each pixel s, the values r 1,s , . . . , r N ,s are regarded as base-L digits and encoded as a single scalar i * s , nally, the histogram h of this la er image is computed.</p><p>e scheme at the bo om of Fig. <ref type="figure" target="#fig_0">1</ref> is identical to the former except for the inverted order of scalar quantization (SQ) and shi ing.</p><p>is inversion, however, allows us to focus on the two groups of blocks highlighted at the top of Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>e lter-shi er group can be replaced by a bank of N lters, all identical to one another except for the position of the non-zero weights. So, with reference to our running example, the n-th lter will have non-zero weights, [1, -3, 3, -1], only on the n-th row, and  <ref type="formula" target="#formula_3">1</ref>) can be replaced by a bank of lters, while the bank of independent SQ's + coder can be replaced by product VQ. e resulting scheme (2) ts the Bag-of-Words paradigm. zero weights everywhere else. Turning to the second group, the combination of N scalar quantizers can be regarded as a constrained form of vector quantization (VQ). More speci cally, it is a product VQ, since the VQ codebook is obtained as the cartesian product of the N SQ codebooks. On one hand, product quantization is much simpler and faster than VQ. On the other hand, its strong constraints are potentially detrimental for performance. Its K = L N codewords are forced to lie on a truncated N -dimensional square la ice <ref type="bibr" target="#b11">[12]</ref> and cannot adapt to the data distribution. Many of them will be wasted in empty regions of the feature space, causing a sure loss of performance with respect to unconstrained VQ. However, the most interesting observation about the new structure at the bo om of Fig. <ref type="figure" target="#fig_1">2</ref> is that it implements the Bag-of-Words (or also Bag-of-Features) paradigm. e lter bank extracts a feature vector for each image pixel, based on its neighborhood. ese features are then associated, through VQ, with some template features. Finally, the frequency of occurrence of the la er, computed in the last block, provides a synthetic descriptor of the input image.</p><formula xml:id="formula_1">X filter z 0 z -1 . . .</formula><formula xml:id="formula_2">r s (c 1 , c 2 , . . . , c K ) VQ i * s r s c 1 , 1 c 2 , 2 . . . c K , K argmax i * s m 1,s m 2,s m K,s</formula><p>e fact that lters and vector quantizer are largely sub-optimal impacts only on performance, not on interpretation. Needless to say, they could be both improved through supervised training. e lter bank is replaced by a convolutional layer, VQ is replaced by convolutional-hardmax layers, the histogram can be computed through an average pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From Bag-of-Words to CNN</head><p>We now show that the processing steps of Fig. <ref type="figure" target="#fig_1">2</ref> can be all implemented through a CNN. First of all, the bank of linear lters used to extract noise residuals can be replaced by a pure convolutional layer, with neurons computing the residuals as</p><formula xml:id="formula_3">r n,s = f (w n,s * x s + b n ) n = 1, • • • , N<label>(1)</label></formula><p>with s used for image spatial location and n to identify neurons. e neuron weights coincide with lter coe cients, biases b n are all set to zero, and the non-linearity f (•) is set to identity.</p><p>As for the vector quantizer, assuming the usual minimum distance hard-decision rule, it can be implemented by means of a convolutional layer followed by a hard-max layer. Let r s be a vector formed by collecting a group of residuals at site s, and c k the k-th codeword of the quantizer. eir squared Euclidean distance, d 2 k,s , can be expanded as</p><formula xml:id="formula_4">||r s -c k || 2 = ||r s || 2 + ||c k || 2 -2 &lt; r s , c k &gt; = ||r s || 2 -2ϵ k -2 &lt; r s , c k &gt;<label>(2)</label></formula><p>with || • || 2 and &lt; •, • &gt; indicating norm and inner product, respectively. Hence, neglecting the irrelevant ||r s || 2 term:</p><formula xml:id="formula_5">i * s = argmin k =1, ••• , K d k,s = argmax k =1, ••• , K (&lt; r s , c k &gt; +ϵ k ) = argmax k =1, ••• , K m k,s<label>(3)</label></formula><p>with m k,s interpreted as a matching score between the feature vector at site s and the k-th codeword. is equivalence is depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. e matching scores m k,s are computed through a convolutional layer, equipped with K = L N lters (remember that L is the number of quantization levels), one for each codeword, having weights c k , bias ϵ k and, again, an identity as activation function. e best matching codeword is then selected through a hardmax processing.  <ref type="formula" target="#formula_5">3</ref>) extracts the features which feed an external classi er. In scheme (4) this is replaced by an internal fully connected layer, and all constraints are removed. By ne-tuning on training data, all layers can be optimized jointly.</p><p>e dashed lines are to remind that this net provides only half the feature, a twin net (not shown for clarity) provides the other half.</p><p>e evolution of the whole network is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. As already said, the rst lter bank is replaced by a convolutional layer. en, the VQ is replaced by another convolutional layer followed by a hardmax layer. e former outputs K feature maps, M k , with the matching scores.</p><p>e la er outputs K binary maps, P k , where p k,s = 1 when the corresponding matching score m k,s is maximum over k, and 0 otherwise. Finally, the histogram computation is replaced by an average pooling layer operating on the whole feature maps, that is, h k = s p k,s . e resulting scheme is shown at the bo om of Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>is net computes only half the desired feature, the part based on across-lter co-occurrences. e twin half is computed similarly, and the complete feature is eventually fed to the SVM classi er, as shown in Fig. <ref type="figure">5</ref> (top). In this network, weights and biases of the convolutional layers are all hard-wired to reproduce exactly the behavior of the residual-based local feature described in Section 2, thereby ensuring the good performance observed in the literature. We now proceed to remove all constraints and allow the net to learn on a suitable training set. First of all, the classi er itself can be implemented as part of the CNN architecture by including a fully connected layer at the end, obtaining the architecture of Fig. <ref type="figure">5</ref> (bo om). Now, to exploit the full potential of deep learning, all parameters must be optimized by appropriate training, thereby overcoming all the impairing constraints mentioned before. Note that the learning phase allows us not only to optimize all layers, which could be done also in the BoW framework, but to optimize them jointly, taking full advantage of the CNN structural freedom. Moreover, the lightweight architecture of the network is instrumental to achieve good results even with a limited training set.</p><p>However, before proceeding with the training, it is necessary to replace the hard-max layer, with a so -max layer that approximates it, so as to avoid non-di erentiable operators. Given the input vector </p><p>With the aim to preserve a close correspondence with the original descriptor, we should choose a very large α parameter, so as to obtain a steep nonlinearity. However, as said before, this is not really necessary, since our goal is only to improve performance. Hence, we select a relatively small value for α in order not to slow down learning. Likewise, to implement minimum distance VQ exactly, the biases in the second convolutional layer should depend on the lter weights, but there is no practical reason to enforce this constraint, and we allow also the biases to adapt freely. Now, the nal CNN can be trained as usual with stochastic gradient descent <ref type="bibr" target="#b16">[17]</ref> to adapt to the desired task. It should be clear that a number of architectural modi cations could be also tested starting from this basic structure, but this goes beyond the scope of the present paper, and will be the object of future research. We must underline that the equivalence between CNN and BoW has been noticed before in the literature, for example in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL ANALYSIS</head><p>To test the performance of the proposed CNN architecture we carry out a number of experiments with typical manipulations. Our synthetic dataset includes images taken from 9 devices, 4 smartphones (Apple iPhone 4S, Apple iPhone 5s, Huawei P7 mini, Nokia Lumia 925) and 5 cameras (Canon EOS 450D, Canon IXUS 95 IS, Sony DSC-S780, Samsung Digimax 301, Nikon Coolpix S5100). Each device contributes 200 images, and from each image non-overlapping patches of dimension 128 × 128 are sampled. We select at random 6 devices to form the training set, while the remaining 3 are used as a testing set. erefore, the patches used for testing come from devices that are never seen in the training phase. For each pristine patch, the corresponding manipulated patch is also included in the set. Overall, our training set comprises a total of just 10800+10800 patches, quite a small number for deep learning applications. We consider 5 types of image manipulation: median ltering, gaussian blurring, AWGN noise addition, resizing, and JPEG compression, with three di erent se ings for each case (see Tab.1) corresponding to increasingly challenging tasks. For example, JPEG compression with quality factor Q=70 is always easily detected, while a quality factor Q=90 makes things much harder.</p><p>In the proposed CNN the rst convolutional layer includes 4 lters of size 5 × 5 × 1 operating on the monochrome input (we use only the green band normalized in [0, 1]. In the second layer there are 81 lters of size 1 × 1 × 4. Filters are initialized as described in Section 3 and the α parameter of so -max is set to 2 16 . e code is implemented in Tensorfow and runs on a Nvidia Tesla P100 with 16GB RAM. We set the learning rate to 10 -6 , with decay 5 • 10 -4 , batch size 36 and Adam <ref type="bibr" target="#b13">[14]</ref> optimization method, using the cross-entropy loss function. Together with the proposed CNN we consider also the basic solution, with the handcra ed feature followed by linear SVM, and the CNN proposed in Bayar2016 <ref type="bibr" target="#b1">[2]</ref> based on the use of a preliminary high-pass convolutional layer.</p><p>Results in terms of probability of correct decision for each binary classi cation problem are reported in the le part of Tab.2 (small training set). With "easy" manipulations, e.g. JPEG@70, all methods provide near-perfect results and there is no point in replacing the SRM+SVM solution with something else. In the presence of more challenging a acks, however, the performance varies signi cantly across methods. A er just 15 epochs of ne tuning, the proposed CNN improves over SRM+SVM of about 2 percent points for JPEG compression, resizing, and noising, and more than 8 points for blurring, while median ltering is almost always detected in any case. In the same cases, the CNN architecture proposed in <ref type="bibr" target="#b1">[2]</ref> provides worse results, sometimes close to 50%, even a er 60 epochs of training. Our conjecture is that a deep CNN is simply not able to adapt correctly with a small training set. In this condition a good hand-cra ed feature can work much be er.</p><p>e proposed CNN builds upon this result and takes advantage of the available limited training data to ne-tune its parameters.</p><p>To carry out a fair comparison we also considered a case in which a much larger training set is available, comprising 460800 patches, that is more than 20 times larger than before. Results are reported in the right part of Tab.2. As expected the performance does not change much for the SRM+SVM solution, since the SVM needs limited training anyway. For the proposed CNN, some improvements are observed for the more challenging tasks. As an example, for JPEG@90 the accuracy grows from 92.08 to 94.59. Much larger improvements are observed for the network proposed in <ref type="bibr" target="#b1">[2]</ref>, which closes almost always the performance gap and sometimes outperforms slightly the proposed CNN. Nonetheless in a few challenging cases, like the already mentioned JPEG@90 or the addition of lowpower white noise, there is still a di erence of more than 10 percent point with our proposal. It is also interesting to compare the two adjacent columns, proposed CNN at 15 epochs and the CNN architecture proposed in <ref type="bibr" target="#b1">[2]</ref> at 60 epochs, which speak clearly in favor of the rst solution, in terms of both complexity and performance.</p><p>e ability to reliably classify small patches may be very valuable in the presence of spatially localized a acks.</p><p>is is the case of image copy-move or splicing, where only a small part of the image is tampered with. In these cases, a descriptor computed on small patches can more reliably detect manipulations, and even localize the forgery by working in sliding-window modality. Fig. <ref type="figure" target="#fig_5">6</ref> shows two examples of forgery localization with a slightly blurred splicing and a resized copy-move, respectively. In both cases using the proposed CNN in sliding-window modality (block size equal to 128 × 128), a sharp heat map is obtained. e SRM+SVM solution also provides good results, but more false alarms are present. It is worth underlining again that the images used for these tests come from cameras that did not contribute to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Residual-based descriptors have proven extremely e ective for a number of image forensic applications. Improving upon the current state of the art, however, is slow and costly, since the design of be er hand-cra ed features is not trivial. We showed that a class of residual-based features can be regarded as compact constrained CNNs. is represent a precious starting point to exploit the huge potential of deep learning, as testi ed by the promising early results. However, this is only a rst step, and there is much room for improvements, especially through new architectural solutions.</p><p>is will be the main focus of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>is material is based on research sponsored by the Air Force Research Laboratory and the Defense Advanced Re search Projects Agency under agreement number FA8750-16-2-0204. e U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. e views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the o cial policies or endorsements, either expressed or implied, of the Air Force Research Laboratory and the Defense Advanced Research Projects Agency or the U.S. Government.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Basic processing scheme (0) for extracting the single model SRM feature, and equivalent scheme (1) with inverted order of scalar quantization (SQ) and n-pixel shi ing (z -n ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: e cascade of lter and shi ers of scheme (1) can be replaced by a bank of lters, while the bank of independent SQ's + coder can be replaced by product VQ. e resulting scheme (2) ts the Bag-of-Words paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A vector quantizer (le ) can be implemented through a bank of lter followed by argmax (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: e whole scheme (2) can be converted in the CNN (3).e lter bank is replaced by a convolutional layer, VQ is replaced by convolutional-hardmax layers, the histogram can be computed through an average pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 Figure 5 :</head><label>45</label><figDesc>Figure 5:e constrained CNN of scheme (3) extracts the features which feed an external classi er. In scheme<ref type="bibr" target="#b3">(4)</ref> this is replaced by an internal fully connected layer, and all constraints are removed. By ne-tuning on training data, all layers can be optimized jointly.e dashed lines are to remind that this net provides only half the feature, a twin net (not shown for clarity) provides the other half.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top: splicing (blurred with st. dev. 0.5). Bottom: copy-move (resized with scale 1.125). From le to right: original image, forged image, SRM+SVM heat map, proposed CNN heat map. Images are of dimension 768 × 1024.</figDesc><graphic coords="6,309.49,171.89,110.95,82.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image manipulations under test. , k = 1, . . . , K }, the so -max computes the quantities p k,s = e αm k,s /</figDesc><table><row><cell>Manipulation</cell><cell>Parameters</cell></row><row><cell>Median Filtering</cell><cell>kernel: 7×7, 5×5, 3×3</cell></row><row><cell>Gaussian Blurring</cell><cell>st. dev: 1.1, 0.75, 0.5</cell></row><row><cell cols="2">Additive Noise (AWGN) st. dev.: 2.0, 0.5, 0.25</cell></row><row><cell>Resizing</cell><cell>scale: 1.5, 1.125, 1.01</cell></row><row><cell>JPEG Compression</cell><cell>quality factor: 70, 80, 90</cell></row><row><cell>{m k,s</cell><cell></cell></row></table><note><p>l e αm l,s</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection Accuracy for binary classi cation tasks.</figDesc><table><row><cell>Manipulation</cell><cell></cell><cell cols="2">Small Training Set</cell><cell></cell><cell cols="2">Large Training Set</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Bayar2016 SRM+SVM prop. CNN</cell><cell cols="3">Bayar2016 SRM+SVM prop. CNN</cell></row><row><cell></cell><cell></cell><cell>60 epochs</cell><cell cols="2">15 epochs</cell><cell>60 epochs</cell><cell cols="2">15 epochs</cell></row><row><cell></cell><cell>7x7</cell><cell>98.23</cell><cell>99.61</cell><cell>99.07</cell><cell>99.69</cell><cell>99.68</cell><cell>99.55</cell></row><row><cell>Median Filtering</cell><cell>5x5</cell><cell>96.66</cell><cell>99.67</cell><cell>99.47</cell><cell>99.78</cell><cell>99.68</cell><cell>99.60</cell></row><row><cell></cell><cell>3x3</cell><cell>94.56</cell><cell>99.83</cell><cell>99.35</cell><cell>99.80</cell><cell>99.87</cell><cell>99.75</cell></row><row><cell></cell><cell>1.1</cell><cell>99.65</cell><cell>99.93</cell><cell>99.79</cell><cell>99.98</cell><cell>99.97</cell><cell>99.95</cell></row><row><cell>Gaussian Blurring</cell><cell>0.75</cell><cell>98.52</cell><cell>99.90</cell><cell>99.82</cell><cell>99.94</cell><cell>99.77</cell><cell>99.93</cell></row><row><cell></cell><cell>0.5</cell><cell>83.10</cell><cell>87.10</cell><cell>95.70</cell><cell>94.57</cell><cell>87.55</cell><cell>96.56</cell></row><row><cell></cell><cell>2.0</cell><cell>97.08</cell><cell>99.94</cell><cell>99.95</cell><cell>99.56</cell><cell>99.94</cell><cell>99.94</cell></row><row><cell>Additive Noise</cell><cell>0.5</cell><cell>82.93</cell><cell>99.37</cell><cell>99.36</cell><cell>93.83</cell><cell>99.34</cell><cell>99.66</cell></row><row><cell></cell><cell>0.25</cell><cell>51.83</cell><cell>85.06</cell><cell>88.81</cell><cell>80.28</cell><cell>84.01</cell><cell>90.79</cell></row><row><cell></cell><cell>1.5</cell><cell>99.22</cell><cell>99.99</cell><cell>100.00</cell><cell>99.72</cell><cell>99.87</cell><cell>100.00</cell></row><row><cell>Resizing</cell><cell>1.125</cell><cell>91.06</cell><cell>98.94</cell><cell>99.56</cell><cell>97.02</cell><cell>96.00</cell><cell>99.78</cell></row><row><cell></cell><cell>1.01</cell><cell>80.51</cell><cell>96.01</cell><cell>97.81</cell><cell>98.44</cell><cell>95.11</cell><cell>97.20</cell></row><row><cell></cell><cell>70</cell><cell>96.04</cell><cell>99.99</cell><cell>99.99</cell><cell>99.43</cell><cell>99.99</cell><cell>99.94</cell></row><row><cell>JPEG Compression</cell><cell>80</cell><cell>77.01</cell><cell>99.73</cell><cell>99.37</cell><cell>98.12</cell><cell>99.94</cell><cell>99.86</cell></row><row><cell></cell><cell>90</cell><cell>63.77</cell><cell>90.86</cell><cell>92.08</cell><cell>79.69</cell><cell>90.90</cell><cell>94.59</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use capital boldface for images, lowercase boldface for vectors, and simple lowercase for scalars. e value of image X at spatial site s, will be denoted by x s .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Session: Deep Learning for Media Forensics IH&amp;MMSec'17, June 20-22, 2017, Philadelphia, PA, USA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Session: Deep Learning for Media Forensics IH&amp;MMSec'17, June 20-22, 2017, Philadelphia, PA, USA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Session: Deep Learning for Media Forensics IH&amp;MMSec'17, June 20-22, 2017, Philadelphia, PA, USA</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach To Universal Image Manipulation Detection Using A New Convolutional Layer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Information Hiding and Multimedia Security</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable Processing History Detector for JPEG Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T Electronic Imaging -Media Watermarking, Security, and Forensics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manipulation Detection on Image Patches Using FusionBoost</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="992" to="1002" />
			<date type="published" when="2012-06">2012. june 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image forgery detection through residual-based local descriptors and block-matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gragnaniello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5297" to="5301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image forgery localization through the fusion of camera-based, feature-based and pixel-based techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gragnaniello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5302" to="5306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Splicebuster: a new blind image splicing detector</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single-image splicing localization through autoencoder-based anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">General-purpose image forensics using patch likelihood under image statistical models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cayre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich models for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2012-06">2012. june 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuho</surname></persName>
		</author>
		<title level="m">IEEE Transactions on Information eory</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2325" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting double JPEG compression with the same quantization matrix</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="848" to="856" />
			<date type="published" when="2010-12">2010. dec 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and Reliable Resampling Detection by Spectral Analysis of Fixed Linear Predictor Residue</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Multimedia and Security Workshop</title>
		<meeting>the Multimedia and Security Workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Detection of Median Filtering in Digital Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE, Electronic Imaging, Media Forensics and Security XII</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Classi cation with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05045</idno>
		<title level="m">Local handcra ed features are convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identi cation of Various Image Operations Using Residual-based Features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Steganalysis by subtractive pixel adjacency matrix</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries by detecting traces of resampling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="758" to="757" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for steganalysis via convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging. 94090J-94090J</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bag-of-words equivalent recurrent neural network for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Forensic Detection of Image Manipulation Using Statistical Intrinsic Fingerprints</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="506" />
			<date type="published" when="2010-09">2010. september 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A feature-based approach for image tampering detection and localization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
