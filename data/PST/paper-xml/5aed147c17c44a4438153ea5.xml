<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domino Temporal Data Prefetcher</title>
				<funder>
					<orgName type="full">research deputy of Sharif University of Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">Iran National Science Foundation</orgName>
					<orgName type="abbreviated">INSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domino Temporal Data Prefetcher</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2018.00021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big-data server applications frequently encounter data misses, and hence, lose significant performance potential. One way to reduce the number of data misses or their effect is data prefetching. As data accesses have high temporal correlations, temporal prefetching techniques are promising for them. While state-of-the-art temporal prefetching techniques are effective at reducing the number of data misses, we observe that there is a significant gap between what they offer and the opportunity.</p><p>This work aims to improve the effectiveness of temporal prefetching techniques. We identify the lookup mechanism of existing temporal prefetchers responsible for the large gap between what they offer and the opportunity. Existing lookup mechanisms either not choose the right stream in the history, or unnecessarily delay the stream selection, and hence, miss the opportunity at the beginning of every stream. In this work, we introduce Domino prefetching to address the limitations of existing temporal prefetchers. Domino prefetcher is a temporal data prefetching technique that logically looks up the history with both one and two last miss addresses to find a match for prefetching. We propose a practical design for Domino prefetcher that employs an Enhanced Index Table that is indexed by just a single miss address. We show that Domino prefetcher captures more than 90% of the temporal opportunity. Through a detailed evaluation targeting a quadcore processor and a set of server workloads, we show that Domino prefetcher improves system performance by 16% over the baseline with no data prefetcher and 6% over the state-ofthe-art temporal data prefetcher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Server workloads have vast datasets beyond what can be captured by on-chip caches of modern processors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Consequently, server workloads encounter frequent cache misses during their execution. The cache misses prevent server processors from reaching their peak performance because cores are idle waiting for the data to arrive.</p><p>Data prefetching is a widely-used approach to eliminate cache misses or reduce their effect. While it has been shown that simple prefetching techniques, such as stride prefetching <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, are ineffective for server workloads <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, more advanced data prefetchers may eliminate or reduce the negative effect of data misses. One of the promising prefetching techniques is temporal prefetching <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. 1 This work was done while the author was at Sharif University of Technology. He is currently affiliated with the Institute for Research in Fundamental Sciences (IPM). Temporal data prefetchers record the sequence of past cache misses and use them to predict future cache misses. Temporal prefetching works because programs consist of loops, and hence, the sequence of addresses, and consequently, miss addresses repeat <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Upon a miss, temporal prefetchers look up the history to find a match (usually the most recent match) and replay the sequence of misses after the match for eliminating future misses. Many pieces of prior work <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> demonstrated the effectiveness of temporal prefetching in reducing data misses and boosting the performance of processors. A variant of temporal prefetching has been implemented in IBM Blue Gene/Q, where it is named List Prefetching <ref type="bibr" target="#b23">[24]</ref>.</p><p>Temporal prefetching is suitable for accelerating chains of dependent data misses, which are common in pointerchasing applications <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> (e.g., OLTP <ref type="bibr" target="#b26">[27]</ref>). A dependent data miss refers to a data access that results in a cache miss while the access is dependent on a piece of data from a prior cache miss. These misses have a negative effect on the performance of processors, as they usually stall the core because both misses are fetched serially <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The length of the chain of dependent misses varies across applications and across different chains in a particular application, ranging from a couple to hundreds of thousands of misses <ref type="bibr" target="#b17">[18]</ref>. While stride <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or spatial <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> prefetchers are usually incapable of prefetching dependent misses <ref type="bibr" target="#b21">[22]</ref> due to the lack of stride/spatial access patterns among dependent misses, temporal prefetchers can capture such misses, and hence, boost performance through substantially increasing the memory-level parallelism (MLP). While existing temporal data prefetchers are useful at eliminating cache misses, we observe that there is a significant gap between what they offer and what opportunity analysis shows for temporal prefetching. Figure <ref type="figure" target="#fig_0">1</ref> shows the opportunity of temporal prefetching and what STMS <ref type="bibr" target="#b9">[10]</ref> and ISB <ref type="bibr" target="#b12">[13]</ref>, two state-of-the-art temporal data prefetchers, offer for several big-data server applications. Like prior studies of measuring repetitiveness in data misses <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify the opportunity of temporal prefetching. While STMS looks for temporal correlation in the global miss sequence, ISB applies temporal correlation to the PC-localized miss sequences. As shown, there is a significant gap between the opportunity and what existing state-of-the-art temporal prefetchers offer across all workloads. On average, the best-performing temporal prefetcher, i.e., STMS, captures less than 47% of data misses.</p><p>This work aims to bridge the gap between what temporal prefetching techniques offer and the opportunity. Corroborating prior work <ref type="bibr" target="#b20">[21]</ref>, our studies show that PC localization is not useful for temporal correlation in server workloads. This fact is evident in Figure <ref type="figure" target="#fig_0">1</ref>: ISB, a PC-localized temporal prefetcher, offers lower coverage than STMS that benefits from the sequence of global misses for prefetching <ref type="foot" target="#foot_0">1</ref> .</p><p>We observe that the lookup mechanism is responsible for the gap between what the state-of-the-art global-misssequence-based prefetcher (i.e., STMS) offers and the opportunity. STMS relies on a single miss address to identify a stream in the history. Unfortunately, a single miss address cannot distinguish two streams that begin with the same miss address. Consequently, STMS frequently picks a wrong stream for prefetching, as is evident from Figure <ref type="figure" target="#fig_0">1</ref>. We observe that if instead of a single miss address, two consecutive miss addresses are used in the lookup mechanism, the chosen streams will be longer, and hence, more useful for prefetching. Figure <ref type="figure" target="#fig_1">2</ref> shows the average stream length for Sequitur, which picks the longest stream, STMS that selects a stream based on the last occurrence of the miss address, and Digram <ref type="bibr" target="#b20">[21]</ref> that chooses a stream based on the last appearance of two previous misses for several server workloads. In this experiment, we refer to a stream as the sequence of consecutive correct prefetches. The figure shows that using two miss addresses in the lookup mechanism (i.e., Digram) instead of one (i.e., STMS) results in longer streams. While average stream length with Sequitur is 7.6, the stream length reduces to 1.4 for STMS. As a result of shorter stream length, STMS looks up 2.7? more streams than Sequitur and at the end of each stream inevitably encounters a cache miss.</p><p>The idea of using last two consecutive misses in the lookup mechanism is evaluated in Wenisch's Ph.D. thesis <ref type="bibr" target="#b20">[21]</ref> and is discarded due to not being effective, and has never been pursued for publication. Prior work <ref type="bibr" target="#b20">[21]</ref> evaluated the effect of lookups with the last two consecutive misses on temporal prefetching with a prefetcher named Digram <ref type="bibr" target="#b20">[21]</ref> and concluded that using a single miss address is more reasonable. Lookups with the last two consecutive misses ensure that a temporal prefetcher cannot issue prefetch requests for the first two addresses of a stream. As the length of streams in server workloads is short (7.6 on average), the benefit of having longer streams is compensated with the fact that we issue one fewer prefetch request for every stream. As single-address lookup is simple, prior work discarded the idea of using two consecutive miss addresses in the lookup mechanism of temporal prefetchers.</p><p>To address the problem, we use a combination of one and two last misses in the lookup mechanism of the proposed temporal prefetcher, named Domino. When a new stream begins, we use a single miss address to prefetch the next miss, and when the next miss or prefetch hit occurs, we use the last two cache accesses to identify a stream. We propose a practical design for Domino that benefits from an Enhanced Index Table (EIT) that is indexed by a single miss address. Unlike a conventional Index Table <ref type="bibr" target="#b10">[11]</ref> that solely stores a pointer for each address in the history, EIT, in addition, keeps the subsequent miss of each address. Having the next miss of every address in the EIT enables Domino to (1) find the correct stream in the history using the last two misses, and (2) issue the first prefetch request of a stream in one round-trip memory access latency. Therefore, unlike STMS that requires two accesses to the off-chip memory to issue the first prefetch request of a stream, Domino prefetcher issues the first prefetch of a stream after one access to the memory, as it stores the next miss in its EIT. This improves the timeliness of the proposed prefetcher.</p><p>In this paper, we make the following contributions:</p><p>? We observe that the lookup mechanism of the state-ofthe-art temporal prefetcher is responsible for the large gap between its coverage and the opportunity. Figure <ref type="figure">3</ref>. The fraction of lookups that result in a correct prediction over all lookups that find a match in the history, as a function of the number of addresses that they attempt to match. based on the last two miss addresses results in larger streams but at the cost of one fewer prefetch per stream, as compared to a single-address lookup. The net results show no significant difference between the two lookup mechanisms for server workloads.</p><p>? We propose a lookup mechanism that benefits from both one and two previous miss addresses to find larger streams without imposing fewer prefetch requests per stream. ? We incorporate the proposed lookup mechanism in a practical temporal prefetcher named Domino. The practical design has just one Index Table that is indexed by a single miss address (instead of two). Moreover, Domino prefetcher issues the first prefetch for a stream after one round-trip off-chip memory access latency (instead of two). ? We use a full-system simulation infrastructure to evaluate our proposal in the context of a four-core server processor on a set of server workloads. Our results show that our proposal offers, on average, 16% speedup over the baseline with no prefetcher, and 6% over the stateof-the-art temporal data prefetcher (i.e., STMS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION</head><p>Prefetching is a technique that aims to eliminate cache misses by bringing a piece of data to the cache before a processor's request for the piece of data. Temporal prefetching techniques rely on the repetitiveness of cache miss sequences. In the rest of this paper, we use the term temporal prefetching to refer to a technique that relies on the repetitiveness of the global cache misses for prefetching.</p><p>Upon a cache miss, a temporal prefetcher should look up the history to find a stream, i.e., a sequence of cache misses that tend to occur together, for the purpose of predicting future cache misses. Temporal prefetchers commonly locate the last occurrence of the missed address in the history and prefetch the addresses that follow the missed address in the  history <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As it is evident from Figure <ref type="figure" target="#fig_0">1</ref>, the coverage of such a prefetcher is considerably less than that of an oracle that upon a miss, always picks the longest stream in the history.</p><p>In this section, we reduce the problem of temporal prefetching to identifying the next miss based on the previously-observed miss sequence. We offer justification for looking up the history with both last one and two miss addresses.</p><p>As the number of addresses that the lookup mechanism of a temporal prefetcher attempts to match increases, the prefetcher becomes more accurate. Figure <ref type="figure">3</ref> shows the fraction of lookups that lead to a correct prediction over all lookups that find a match in the history, as a function of the number of addresses that lookups attempt to match. As shown, the fraction of lookups that lead to a correct prediction is low if they match only a single address. Moreover, as the number of addresses that lookups match increases, the fraction of useful lookups also increases. However, increasing the number of addresses that a lookup matches beyond three yields only little improvements in the fraction of useful lookups. These results clearly show that temporal prefetchers that rely on just one miss address to look up the history (e.g., STMS <ref type="bibr" target="#b9">[10]</ref>), frequently prefetch incorrectly.</p><p>Another interesting data point, as we increase the number of addresses that lookups match, is the fraction of lookups that find a match in the history. Only for lookups that find a match in the history, the temporal prefetcher issues a prefetch request. Figure <ref type="figure" target="#fig_2">4</ref> shows the fraction of lookups that find a match in the history as a function of the number of addresses that they match. As expected, the number of lookups that find a match reduces when the number of matched addresses increases. So temporal prefetchers that rely on a couple of misses to lookup the history (e.g., Digram <ref type="bibr" target="#b20">[21]</ref>) miss a significant opportunity due to lack of finding a match in the previously-observed misses.  <ref type="table">1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4</ref>   This observation motivates using more than one miss address for lookup. A prefetcher can look up the history with the last N misses; if a match is found, the prefetcher issues a prefetch based on the match; otherwise, it looks up the history with one fewer miss in a recursive manner. This way, the prefetcher benefits from both high accuracy and high opportunity, overcoming the limitations of previouslyproposed temporal prefetchers.</p><p>To show the importance of using more than one address for lookup, Figure <ref type="figure" target="#fig_4">5</ref> shows the coverage and overpredictions of a temporal prefetcher with lookups of varying number of addresses. In all cases, if a prefetcher uses N addresses for lookup, it attempts to find a match with 1, 2, 3,...,N addresses and picks the match with the largest number of addresses. As shown, the coverage with a single-address lookup is low across all workloads while the overpredictions are high. As the number of looked up addresses increases, both coverage and overpredictions improve. However, only few workloads benefit from a temporal prefetcher that has a lookup mechanism that matches more than two addresses. Even on these workloads, the benefit of matching more than two addresses is insignificant.</p><p>We conclude that using two previous miss addresses is sufficient to correctly identify the right group of addresses in the history. As such, a temporal prefetcher should use both one and two last misses to have high coverage and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSAL</head><p>Domino prefetcher is a temporal prefetching technique, and consequently, relies on past misses to predict and prefetch future memory references. Domino prefetcher performs its actions on cache misses and prefetch hits, which we refer to as triggering events.</p><p>Logically, every time a miss occurs, Domino prefetcher looks up the history with both the last two triggering events and the current triggering event to find a match. If the lookup using the last two triggering events finds a match, Domino uses the stream of misses that follows the match in the history for prefetching. Otherwise, if the lookup using the current triggering event finds a match, it just prefetches the first address after the match. Then, Domino waits for the next triggering event to occur. As looking up the history takes a long time, the following triggering event usually happens quickly, and Domino does not need to wait for a long time. If the triggering event is the hit of the prefetched cache block, Domino continues to prefetch from the alreadyfound stream, and otherwise, Domino attempts to find a new stream. In case no match is located in the history for the two lookups, Domino does nothing and waits for the next triggering event to occur.</p><p>At any point in time, Domino has several active streams. If a miss occurs, Domino finds a new stream and replaces one of the old streams with it (round robin). In case a prefetch hit occurs, Domino continues to prefetch from the active stream that is responsible for the prefetch hit. Moreover, this stream becomes the most-recently-used stream in the LRU stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>STMS is the state-of-the-art temporal prefetcher, and Domino is built upon it. STMS has a dedicated per-core History Table (HT) that stores the sequence of misses observed by the core and a dedicated Index Table (IT) that for every observed miss address has a pointer to its last occurrence in the HT. The HT is a circular buffer, and the IT is a bucketized hash table <ref type="bibr" target="#b35">[36]</ref> managed with LRU replacement policy. As both HT and IT require multi-megabyte storage for STMS to have a reasonable coverage, both tables are placed off chip in the main memory <ref type="bibr" target="#b9">[10]</ref>. Consequently, every access to these tables (read or update) should be sent to the main memory, which is slow, and brings/updates a cache block worth of data.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the timing of events with STMS <ref type="bibr" target="#b9">[10]</ref>. Upon a cache miss, a request is sent to the main memory to bring an entry of the IT that points to the last occurrence of the cache miss in the HT. Whenever the entry is received, STMS finds the pointer to the HT. Having the pointer, another request is sent to the main memory to bring a cache block worth of data from the HT. The data contains several consecutive miss addresses that immediately followed the last occurrence of the missed address. Upon receiving the data, STMS sends prefetch requests for the addresses that follow the miss address.</p><p>With this implementation and for every stream, the temporal prefetcher needs to wait for two (serial) memory requests to be sent (one to read the IT and one to read the correct location of the HT) and their responses to come back to the prefetcher before issuing prefetch requests for the stream. The delay of the two off-chip memory accesses is compensated over several prefetch requests of a stream if the stream is long enough.</p><p>A row of the HT, which has a cache block worth of data, contains a sequence of consecutive data misses as observed by the core. As compared to the HT, IT is more complicated. IT is indexed by a hash of a single miss address (e.g., STMS) or two miss addresses (e.g., Digram). A row of the IT has some tag-pointer pairs. The tag along with the row number identifies a single miss address (or two miss addresses in Digram) and the pointer points to the last occurrence of the miss address(es) in the HT.</p><p>Every time a miss address is recorded in the end of the HT, its pointer in the IT needs to be updated to point to the last row of the HT. Unfortunately, the misses that are observed close to each other are usually mapped to different rows in the IT due to not being spatially correlated. As such, updating the pointers requires several accesses to the IT, which imposes off-chip bandwidth overhead and is timeconsuming. To reduce the pressure on the off-chip memory, the state-of-the-art implementation of a temporal prefetcher benefits from statistical updates for the IT. Randomly, for every several index updates (e.g., eight), only one of them is recorded in the IT. It has been shown that this implementation offers the level of performance similar to that of the non-practical always-update implementation <ref type="bibr" target="#b9">[10]</ref>.</p><p>As both metadata tables are off-chip, the on-chip storage requirement of the prefetcher is negligible. The prefetcher needs only few kilobytes of per-core storage for recording misses and replaying prefetch candidates. Domino uses both one and two last triggering events for prefetching. A naive implementation of Domino requires two ITs and one HT. One IT points to the last occurrence of every triggering event in the HT, and the other one points to the last appearance of every pair of consecutive triggering events in the HT. Compared to STMS and Digram, the naive implementation of Domino requires one more off-chip access per stream due to having two ITs, and as such, significantly wastes precious off-chip bandwidth. In this section, we detail a practical design for Domino that requires a single IT that is indexed by a single triggering event (and not two). Moreover, unlike STMS and Digram, the practical design can prefetch the first address of a stream in one round-trip memory access latency, which improves the timeliness of the prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Practical Implementation</head><p>Domino prefetcher relies on an Enhanced Index Table (EIT) and an HT to record past triggering events and replay the sequence of future data misses. As the size of these two tables is very large (several megabytes), just like prior work <ref type="bibr" target="#b9">[10]</ref>, both tables are stored in the main memory.</p><p>Domino prefetcher allocates a contiguous portion of the physical address space for the two tables. Each core has a dedicated address space for the tables. The allocated address space is hidden from the operating system. The size of the allocated address space depends on the requirements of the workloads (it is usually several megabytes per core). As Domino prefetcher requires two tables, it statically divides the allocated address space into two parts. The start address of the EIT is recorded in a register named EIT-Start, and the start address of the HT is recorded in a register named HT-Start.</p><p>The memory system has a particular read request that, like an ordinary read request, fetches a block from memory but instead of placing the fetched block into the cache, puts the block into the prefetcher's on-chip metadata storage. There is no need to cache the content of the two tables in the cache hierarchy because metadata accesses exhibit neither spatial nor temporal locality.</p><p>Just like STMS and unlike Digram, the EIT in Domino is indexed by a single miss address. Associated with every tag, there are several address-pointer pairs, where the address is a miss experienced by the core and the pointer is a location in the HT. An (a, p) pair associated to a tag t indicates that the pointer to the last occurrence of miss address t followed by a is p. We refer to a tag and its associated address-pointer pairs as a super-entry, and to an address-pointer pair as an entry. Figure <ref type="figure">7</ref> shows the organization of Domino's EIT. Every row of the EIT has several super-entries, and each super-entry has several entries (three in our configuration). Domino keeps the LRU stack among both the super-entries and entries within each super-entry. To better understand the differences of Domino prefetcher's organization and those of the prior work, Figure <ref type="figure">8</ref> shows the structure of the metadata tables of STMS, Digram, and Domino.</p><formula xml:id="formula_0">A B L D F A Q B A X C U Row C (U, P7)</formula><p>A (X, P6), (Q, P4), (B, P1) B (A, P5), (L, P2)</p><formula xml:id="formula_1">F (A, P3)</formula><p>The most recent super-entry in this row The most recent entry of 'A' P1 P2 P3 P4 P5 P6 P7 Domino benefits from several storage elements next to each core. These elements are (1) a buffer to record the sequence of triggering events named LogMiss, (2) a buffer to keep the prefetched cache blocks named Prefetch Buffer, (3) a buffer that holds the sequence of addresses of an active stream named PointBuf, and (4) a buffer named FetchBuf.</p><p>Recording. Upon a triggering event, the address of the triggering event is appended to LogMiss. LogMiss has the capacity of two cache blocks. When one cache block worth of data is in LogMiss, Domino writes it to the end of the HT in the main memory and statistically updates the pointers of the written miss addresses in the EIT.</p><p>To update the EIT, for one out of every N triggering events (e.g., eight) written into the HT, the corresponding row of the EIT is fetched into FetchBuf. If a super-entry for the triggering event is not found in the fetched row, a new super-entry is allocated with the LRU policy. In the chosen super-entry, Domino attempts to find an entry for the address following the triggering event. If no match is found, a new entry is allocated with LRU policy. The pointer field of the entry is updated to point to the last row of the HT. Finally, Domino updates the LRU stack of entries and super-entries. Once Domino is done with the row, it is written back to the EIT.</p><p>Replaying. Upon a successful use of a prefetched block, Domino uses the active stream responsible for the prefetch hit and issues the next item of the stream (using PointBuf). Upon a cache miss, however, Domino prefetcher attempts to find a new stream and replaces the least-recently-used stream with it (which means discarding the contents of the prefetch buffer and PointBuf related to the replaced stream).</p><p>To find a new stream, Domino uses the missed address to fetch a row of the EIT. When the row is brought into PointBuf, Domino attempts to find the super-entry associated with the missed address. The delay of the search is tolerable because it is considerably smaller than the off-chip latency <ref type="bibr" target="#b9">[10]</ref>. In case a match is not found, nothing will be done, and otherwise, a prefetch will be sent for the address field of the most recent entry in the found super-entry to be brought into the Prefetch Buffer.</p><p>When the next triggering event occurs (miss or prefetch hit), Domino searches the super-entry and picks the entry for which the address field matches the triggering event (might not be the most recent entry). In case no match is found, the stream is discarded and Domino uses the triggering event to bring another row from the EIT, and otherwise, Domino creates an active stream using the matched entry. It means that Domino sends a request to read the row of the HT pointed to by the pointer field of the matched entry to be brought into PointBuf. Once the sequence of miss addresses from the row of the HT arrives, Domino issues prefetch requests to be appended to the Prefetch Buffer.</p><p>As both recording and replaying may require accessing the tables, and since replaying is on the critical path but the recording is not, Domino prefetcher prioritizes replaying over recording. Only when replaying is done, Domino prefetcher attempts to follow the steps necessary for recording if it wants to access the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>Table <ref type="table" target="#tab_3">I</ref> summarizes key elements of our methodology, with the following sections detailing the processor parameters, workloads, simulator, and evaluated designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Processor Parameters</head><p>The evaluated processor has four cores with 4 MB of lastlevel cache (LLC). The cache hierarchy of each core includes a 64 KB data and a 64 KB instruction cache. The 4 MB LLC </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Workloads</head><p>We simulate systems running Solaris and executing the workloads listed in Table <ref type="table" target="#tab_4">II</ref>. We include a variety of server workloads from competing vendors, including online transaction processing, CloudSuite <ref type="bibr" target="#b36">[37]</ref>, and Web server benchmarks. Prior work <ref type="bibr" target="#b0">[1]</ref> has shown that these workloads have characteristics representative of the broad class of server workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Simulation Infrastructure</head><p>We use a combination of trace-based and timing fullsystem simulations to evaluate our proposal. Our tracebased analyses use traces collected from Flexus with in-order execution, no memory system stalls, and a fixed instructionper-cycle (IPC) of 1.0.</p><p>We estimate the performance of various designs using Flexus full-system timing simulator <ref type="bibr" target="#b37">[38]</ref>. Flexus timing simulator extends the Virtutech Simics functional simulator with timing models of cores, caches, on-chip protocol controllers, and interconnect. Flexus models the SPARC v9 ISA and is able to run unmodified operating systems and applications.</p><p>We use the SimFlex multiprocessor sampling methodology <ref type="bibr" target="#b38">[39]</ref>. For each measurement, we launch simulations from checkpoints with warmed caches and branch predictors and run 300 K cycles to achieve a steady state of detailed cycle-accurate simulation before collecting measurements for the subsequent 150 K cycles. We use the ratio of the number of application instructions to the total number of cycles (including the cycles spent executing operating system code) to measure performance; this metric has been shown to accurately reflect overall system throughput of multiprocessors <ref type="bibr" target="#b38">[39]</ref>. Performance measurements are computed with 95% confidence and an error of less than 4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Prefetchers' Configurations</head><p>We consider seven systems, as follows: Baseline. As a recent study <ref type="bibr" target="#b0">[1]</ref> showed that simple data prefetchers do not work for server workloads, the baseline has no data prefetcher. The baseline benefits from a nextline instruction prefetcher. All evaluated prefetchers are implemented on top of the baseline.</p><p>Variable Length Delta Prefetcher. We include VLDP <ref type="bibr" target="#b33">[34]</ref> because it has similarities with the lookup mechanism of our proposal. VLDP is a prefetcher that relies on spatial correlation for prefetching and benefits from multiple previous deltas (the difference of two successive miss addresses in a page) for lookup. We equip VLDP with 16-entry DHB, 64-entry OPT, and three infinite-size DPTs. As this prefetcher works based on spatial correlation, it is orthogonal to our work and can be used together <ref type="bibr" target="#b21">[22]</ref>.</p><p>Irregular Stream Buffer. ISB <ref type="bibr" target="#b12">[13]</ref> combines the use of PC localization and address correlation. We implement idealized PC/AC with an infinite-size history table. It has been shown that the idealized PC/AC has significantly better performance as compared to its practical implementation <ref type="bibr" target="#b12">[13]</ref>.</p><p>Sampled Temporal Memory Streaming. STMS <ref type="bibr" target="#b9">[10]</ref> records miss sequences in a global per-core HT and locates streams through an IT. It benefits from a stream-end detection heuristic <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b39">[40]</ref> to reduce useless prefetches. We implement STMS with infinite-size metadata tables. Both HT and IT are located in the main memory. STMS can track four active streams at any given point in time. The sampling probability is set to 12.5% as suggested by the original proposal. Other parameters are taken from the original proposal.</p><p>Digram. Like STMS, Digram <ref type="bibr" target="#b20">[21]</ref> stores misses in an HT and locates streams through an IT. We include Digram because, like Domino, it uses two misses for locating streams (but unlike Domino it does not look up the history with one miss address). We equip Digram with an infinite-size HT and IT. Metadata tables are located in the main memory. Digram can track four active streams at any given point in time. Digram also benefits from the stream-end detection mechanism. The sampling probability is set to 12.5%.</p><p>Domino. The EIT and HT are located off-chip in the main memory. We set the size of the metadata tables based on sensitivity analysis. LogMiss, Prefetch Buffer, PointBuf, and FetchBuf are 128 B, 2 KB, 256 B, and 64 B, respectively. Domino can track four active streams at any given point in time. Domino benefits from the stream-end detection mechanism. The sampling probability is set to 12.5%, similar to STMS and Digram.</p><p>Sequitur. Like prior studies of measuring repetitiveness of access sequences <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify temporal prefetching opportunity for data miss sequences.</p><p>Sequitur is an algorithm that builds a grammar based on the input. The production rules of the grammar are formed in a way that capture repetitions in the input. Sequitur repeatedly reads a symbol from the input and extends its grammar accordingly. When a symbol is added, the grammar is adjusted in a way that captures new repetitions caused by the added symbol. We compare our prefetcher against Sequitur to see what fraction of the opportunity it has covered.</p><p>To have a fair evaluation, all prefetchers are trained using L1-D miss sequences, and all prefetchers prefetch into a small prefetch buffer near the L1-D cache with the capacity of 32 cache blocks. The degree of prefetching for all prefetchers is set to four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensitivity Analysis</head><p>The storage requirement. The effectiveness of Domino prefetcher, or any other temporal prefetcher, directly depends on the size of the history on which the predictions are made. Figure <ref type="figure">9</ref> shows the coverage of Domino prefetcher for different numbers of entries (i.e., cache misses) in the HT for each workload. In this experiment, there is no limit on the size of the EIT. As the number of entries in the HT increases, the coverage of Domino prefetcher also increases because the prefetcher is able to make more correct predictions. Beyond 16 M entries, the coverage reaches its peak, effectively exploiting the opportunity. As HT is placed in the main memory, the space requirement is less important than the coverage of the prefetcher. So, we decide to have 16 M entries (85 MB) in the HT. Note that every 12 entries (addresses of triggering events) are placed into a row of the HT.</p><p>Having determined the size of the HT, Figure <ref type="figure" target="#fig_0">10</ref> shows the coverage of Domino prefetcher for different numbers of rows in the EIT for each workload. HT has 16 M entries. The coverage of Domino prefetcher reaches its peak when the size of the EIT becomes 2 M rows. So we decide to have an EIT with 2 M rows (128 MB). Note that both metadata tables are stored in the main memory, and as such, they do not impose space overhead to the processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Trace-based Evaluation</head><p>We compare Domino prefetcher against ISB <ref type="bibr" target="#b12">[13]</ref>, VLDP <ref type="bibr" target="#b33">[34]</ref>, STMS <ref type="bibr" target="#b9">[10]</ref>, and Digram <ref type="bibr" target="#b20">[21]</ref>. As a point of reference, we also include the temporal opportunity measured using Sequitur <ref type="bibr" target="#b34">[35]</ref>. VLDP is a spatial prefetching technique while ISB, STMS, Digram, and Domino are temporal prefetchers. For all temporal prefetchers except Domino, we assume unlimited-size storage for metadata. For Domino prefetcher, we limit the size of EIT to 2 M rows and HT to 16 M entries.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> shows the coverage and overpredictions for the competing prefetching techniques when prefetching degree is one. Covered misses are the ones that are successfully eliminated by a prefetcher. Overpredictions are incorrectly prefetched cache blocks, which cause bandwidth overhead and potentially pollute the prefetch buffer. The incorrect prefetches are normalized against the number of cache misses in the baseline system.</p><p>On average, Domino offers 8% higher coverage as compared to the second-best prefetcher (i.e., STMS) and covers 56% of data misses. With respect to overpredictions, Domino is the second-best prefetcher (after Digram) across all workloads, except SAT Solver. Comparing Domino prefetcher against Sequitur, which identifies the temporal opportunity in the data miss sequence, Domino prefetcher captures slightly more than 90% of the opportunity.</p><p>While Figure <ref type="figure" target="#fig_1">2</ref> shows that Digram, which benefits from two-address lookups, has longer streams as compared to STMS, Figure <ref type="figure" target="#fig_0">11</ref> shows that STMS has slightly higher coverage as compared to Digram. To shed light on the reasons behind this phenomenon, Figure <ref type="figure" target="#fig_7">12</ref>   histogram of stream length for various workloads obtained using Sequitur analysis. Across all workloads, 10% to 47% of streams have a length of less than or equal to two, for which Digram cannot act. The significant majority of other streams are also short (have a length of less than eight). Unlike STMS, Digram consumes two accesses of a stream before issuing prefetch requests. As most of the streams are short, losing one prefetch request per stream has a significant negative effect on the coverage of the prefetcher. While the average stream length of Digram is larger than STMS, its coverage is slightly less than that of STMS. That is why prior work <ref type="bibr" target="#b20">[21]</ref> picked STMS over Digram and concluded that two-address lookup has no practical advantage over singleaddress lookup. Corroborating prior work <ref type="bibr" target="#b20">[21]</ref>, our results show that PClocalized temporal prefetchers (e.g., ISB) are not useful in the context of server workloads. PC-localized temporal prefetchers suffer from two fundamental obstacles: (1) PC localization breaks the strong temporal correlation among the global miss addresses (see the results of Sequitur), and (2) PC localization makes prefetchers predict the following misses of a memory instruction, which may not be the subsequent misses of the workload. Since server workloads have extensive instruction working sets <ref type="bibr" target="#b0">[1]</ref>, the re-execution of a specific memory instruction in the execution sequence may take a long time. Therefore, the prefetched blocks may be evicted before re-execution of the memory instruction.</p><p>VLDP performs poorly because, unlike LLCs, L1 caches cannot significantly exploit the spatial correlation of data accesses due to the low residency of data in the cache <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Sharing the metadata of different pages in a unified history is another reason for the low accuracy of VLDP.</p><p>Prefetchers usually benefit from a prefetching degree greater than one to improve the timeliness of the prefetch requests (i.e., to have the prefetched blocks ready before the processor actually requests for them). Figure <ref type="figure" target="#fig_0">13</ref> shows the coverage and overpredictions of the competing prefetching techniques when the prefetching degree is four. As compared to Figure <ref type="figure" target="#fig_0">11</ref>, overpredictions of many prefetchers have increased significantly.</p><p>Just like the prefetching degree of one, Domino prefetcher outperforms other prefetchers with respect to coverage. The second-best prefetcher is STMS. In all workloads, Domino either significantly outperforms STMS (e.g., 19% in OLTP) or with similar coverage substantially lowers the overpredictions. On average, Domino's overpredictions are one-third of those of STMS, and are close to those of Digram. The gap between the overpredictions of STMS and Domino-Digram grows with increasing the prefetching degree. It mainly comes from the lookup mechanism of STMS. STMS looks up history with a single miss address, and as such, frequently picks wrong streams. At the beginning of a wrongly-chosen stream, STMS prefetches as many incorrect cache blocks as the prefetching degree. Meanwhile, Domino and Digram often locate the correct stream using the two miss addresses and avoid the overpredictions.</p><p>Increasing the degree of prefetching significantly increases the possibility of early-eviction of prefetched cache blocks in ISB. Therefore, compared to degree one, both coverage and overpredictions have been worsen in this prefetcher.</p><p>With prefetching degree greater than one, upon predicting the next access in a page, VLDP uses the prediction as input to the metadata tables to make more predictions. We found that this approach is inaccurate for server workloads and the results become less accurate as the prefetching degree increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cycle-Accurate Evaluation</head><p>Figure <ref type="figure" target="#fig_2">14</ref> shows the performance improvement of Domino prefetcher along with ISB, VLDP, STMS, and Digram, over a baseline with no prefetcher. All temporal prefetchers except Domino have unlimited storage for metadata in the main memory. For Domino prefetcher, we limit the size of EIT to 2 M rows and HT to 16 M entries. The prefetching degree for all prefetchers is four. The figure clearly shows the ability of Domino prefetcher in boosting performance.</p><p>In eight out of nine workloads, Domino outperforms other temporal prefetchers due to its higher coverage and/or better timeliness. The average performance improvement of Domino prefetcher over the baseline is 16%. The secondbest prefetcher is STMS with the average performance improvement of 10%. As compared to VLDP, which is a recently-proposed spatial prefetcher, Domino offers 7% higher performance.</p><p>For most of the workloads, Domino provides a significant performance improvement. Web Search and Media Streaming have relatively high MLP, and hence, many of the misses that prefetchers capture, are already fetched in parallel with the out-of-order execution mechanism. Therefore, despite high coverage, prefetchers are unable to boost the performance of these workloads significantly. In MapReduce-W, temporal streams identified by the examined prefetching techniques are drastically short, and hence, the delay of fetching metadata from memory cannot be amortized over subsequent prefetches, resulting in less performance enhancement. SAT Solver produces its dataset on-the-fly during the execution, and thus, does not have a static and wellstructured dataset <ref type="bibr" target="#b40">[41]</ref>. Consequently, its memory accesses are hard-to-predict and all techniques manifest low coverage and high overpredictions, and accordingly, low performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Off-chip Bandwidth Overhead</head><p>Figure <ref type="figure" target="#fig_8">15</ref> shows the off-chip bandwidth overhead of STMS, Digram, and Domino over the baseline with no prefetcher. Out of the three prefetchers, STMS has the highest and Digram and Domino have the lowest overhead. STMS has the highest off-chip traffic because of its high overprediction rate, as shown in Figure <ref type="figure" target="#fig_0">13</ref>. Compared to STMS, Domino consumes less off-chip bandwidth due to (1) its low overprediction rate and (2) fewer metadata fetches, as Domino finds correct streams with fewer memory accesses than STMS. Compared to Digram, while Domino has slightly higher overprediction rate, it brings less metadata because more often lookups can find a match in its EIT (cf., Figure <ref type="figure" target="#fig_2">4</ref>).</p><p>Compared to other prefetching techniques, the bandwidth requirement of temporal prefetchers is relatively high. Fortunately, server workloads consume only a small fraction of the available off-chip bandwidth offered by today's commercial processors <ref type="bibr" target="#b0">[1]</ref>. Today's quad-core processors provide off-chip bandwidth, typically in the range of 37.5 GB/s <ref type="bibr" target="#b42">[43]</ref> to 85 GB/s <ref type="bibr" target="#b43">[44]</ref>. Meanwhile, the most bandwidth-hungry server workload (i.e., Web Apache) consumes only 8 GB/s of the available bandwidth.</p><p>The unused bandwidth can be utilized by a temporal prefetcher, like Domino, to improve the execution of server workloads. Using Domino, the bandwidth utilization ranges from 8.7% in MapReduce-C to 32.8% in Web Apache. We conclude that Domino offers the highest performance improvement with the lowest off-chip bandwidth overhead as compared to the state-of-the-art global-miss-based temporal prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Spatio-Temporal Prefetching</head><p>VLDP relies on spatial correlation for prefetching while Domino captures temporal correlation of data accesses. As each technique targets different subset of misses, they can be used orthogonally <ref type="bibr" target="#b21">[22]</ref>. VLDP uses patterns that fall into  a single page and is able to prefetch unobserved misses but is incapable of capturing consecutive misses that fall across pages. Domino replays previously-observed miss sequences, regardless of their spatial region in the memory, but is unable to prefetch cold misses. Each technique captures a particular type of misses, leaving the other type unpredicted.</p><p>To demonstrate the orthogonality of these techniques, we implement both of them in a single system. We stack Domino to a system that has VLDP. Domino trains and prefetches on misses that VLDP cannot capture. As Figure <ref type="figure" target="#fig_9">16</ref> shows, there is a large fraction of misses that are predictable solely by one of these techniques. On average, the combination of VLDP and Domino can cover 43%/20% more misses than VLDP/Domino alone.</p><p>The effectiveness of spatio-temporal prefetching drastically varies across workloads. In Data Serving, spatiotemporal prefetching efficiently increases the coverage of VLDP and Domino by 37% and 30%, respectively. Meanwhile, spatio-temporal prefetching provides almost no advantage over Domino in OLTP. An extreme behavior is observed in MapReduce-W, where the coverage of the combination of VLDP and Domino is higher than the arithmetic sum of the individual coverage of the two prefetchers. We found that, in this case, the remaining misses of a system with VLDP has higher temporal correlation than the original misses of the system without VLDP. With VLDP as the baseline prefetcher, the length of temporal streams extracted by Domino roughly doubles, which results in higher coverage and effectiveness of the temporal prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Data prefetching is an active research area in computer architecture. Thread-based prefetching techniques <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> exploit idle thread contexts to execute threads that prefetch for the main program thread. However, the extra resources the prefetcher threads need may not be available when the processor is fully utilized.</p><p>Software-based techniques <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> use compiler or programmer hints to issue prefetch operations. However, the complicated access patterns and fast changes in the dataset of big-data server applications make prefetching more difficult for such approaches.</p><p>Complex access patterns in the context of hardware prefetching are considered in many pieces of recent work <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Most of prior work is either not temporal and/or is far from covering most of the temporal opportunity. In this work, we proposed a practical temporal data prefetcher and showed that it covers a significant fraction of the temporal opportunity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Data misses are a major source of performance degradation in server applications. Data prefetching is a technique for reducing the number of cache misses or their negative effect. Among data prefetching techniques, temporal prefetching has high potential in eliminating data misses due to existence of high temporal correlation among data accesses. Unfortunately, existing temporal prefetching techniques fall significantly short of efficiency and cannot capture the opportunity and minimize the number of data misses. In this paper, we proposed Domino, a temporal data prefetcher, and showed that it achieves more than 90% of the theoretical opportunity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.Read miss coverage of two state-of-the-art temporal data prefetchers with unlimited storage versus the opportunity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Average stream length with STMS, Digram, and Sequitur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The fraction of lookups that find a match in the history over all lookups, as a function of the number of addresses that they attempt to match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Coverage and overpredictions of a temporal prefetcher with unlimited storage for lookups with varying number of addresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Timing of events in state-of-the-art temporal data prefetcher (i.e., STMS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. The details of the Enhanced Index Table in Domino prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Histogram of stream length with Sequitur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Bandwidth overhead of STMS, Digram and Domino over the baseline with no data prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Spatio-Temporal prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I EVALUATION</head><label>I</label><figDesc>PARAMETERS.   </figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Chip</cell><cell>Four cores, 4 GHz</cell></row><row><cell>Core</cell><cell>SPARC v9 ISA, 8-stage pipeline, out-of-order execution, 4-wide issue, 128-entry ROB, 64-entry LSQ</cell></row><row><cell>I-Fetch Unit</cell><cell>64 KB, 2-way, 2-cycle load-to-use, next-line prefetcher, hybrid branch predictor, 16 K gShare &amp; 16 K bimodal</cell></row><row><cell>L1-D Cache</cell><cell>64 KB, 2-way, 2-cycle load-to-use, 4 ports, 32 MSHRs</cell></row><row><cell>L2 Cache</cell><cell>4 MB, 16-way, 18-cycle hit latency, 64 MSHRs</cell></row><row><cell>Memory</cell><cell>45 ns delay, 37.5 GB/s peak bandwidth</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table II APPLICATION</head><label>II</label><figDesc>PARAMETERS.   </figDesc><table><row><cell></cell><cell>CloudSuite</cell></row><row><cell>Data Serving</cell><cell>Cassandra 0.7.3 Database 15GB Yahoo! Cloud Serving Benchmark</cell></row><row><cell>MapReduce-C</cell><cell>Hadoop 0.20.2 Bayesian Classification Algorithm</cell></row><row><cell>MapReduce-W</cell><cell>Hadoop 0.20.2 Mahout 0.4 Library</cell></row><row><cell>SAT Solver</cell><cell>Cloud9 Parallel Symbolic Execution Engine Four 5-byte and One 10-byte Arguments</cell></row><row><cell>Media Streaming</cell><cell>Darwin Streaming Server 6.0.3 7500 Clients, 60GB Dataset, High Bitrate</cell></row><row><cell>Web Search</cell><cell>Nutch 1.2/Lucene 3.0.1, 230 Clients 1.4 GB Index, 15 GB Data Segment</cell></row><row><cell></cell><cell>Web Server (SPECweb99)</cell></row><row><cell>Web Apache</cell><cell>Apache HTTP Server v2.0, 16 K Connections FastCGI, Worker Threading Model</cell></row><row><cell>Web Zeus</cell><cell>Zeus Web Server v4.3 16 K Connections, FastCGI</cell></row><row><cell cols="2">Online Transaction Processing (TPC-C)</cell></row><row><cell>OLTP</cell><cell>Oracle 10g Enterprise Database Server 100 Warehouses (10 GB), 1.4 GB SGA</cell></row><row><cell cols="2">is distributed among four slices. Cache line size is 64 bytes.</cell></row><row><cell cols="2">The chip has two memory controllers that provide up to</cell></row><row><cell cols="2">37.5 GB/s of off-chip bandwidth.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>shows theFigure 11. Domino prefetcher compared to other prefetchers. Prefetching degree of all prefetchers is one. For all temporal prefetchers except Domino, we assume unlimited-size history. For Domino prefetcher, we limit the size of EIT to 2 M rows and the HT to 16 M entries, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Coverage</cell><cell></cell><cell cols="4">Uncovered</cell><cell></cell><cell></cell><cell cols="6">Overpredictions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">172%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>150%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>% Consumptions</cell><cell>50% 75% 100% 125%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>Sequitur</cell></row><row><cell></cell><cell></cell><cell cols="5">Data Serving</cell><cell></cell><cell cols="5">MapReduce-C</cell><cell></cell><cell cols="6">MapReduce-W</cell><cell cols="6">Media Streaming</cell><cell></cell><cell></cell><cell cols="2">OLTP</cell><cell></cell><cell></cell><cell></cell><cell cols="4">SAT Solver</cell><cell></cell><cell></cell><cell cols="4">Web Apache</cell><cell></cell><cell></cell><cell cols="4">Web Search</cell><cell></cell><cell></cell><cell cols="4">Web Zeus</cell><cell></cell><cell></cell><cell cols="3">Average</cell><cell></cell></row><row><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Data Serving</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MapReduce-C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MapReduce-W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cum % of All Streams</cell><cell>40% 60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Media Streaming OLTP SAT Solver Web Apache</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Web Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Web Zeus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell></cell><cell cols="2">16</cell><cell></cell><cell>32</cell><cell></cell><cell>64</cell><cell></cell><cell cols="4">128 128+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Temporal Stream Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 13. Domino prefetcher compared to other prefetchers. Prefetching degree of all prefetchers is four. For all temporal prefetchers except Domino, we assume unlimited-size history. For Domino prefetcher, we limit the size of EIT to 2 M rows and the HT to 16 M entries, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Coverage</cell><cell></cell><cell cols="4">Uncovered</cell><cell></cell><cell cols="5">Overpredictions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>200%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">204%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">204%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">218%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">208% 273% 202% 213%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">215% 203%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>175%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>% Consumptions</cell><cell>50% 75% 100% 125% 150%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Data Serving</cell><cell></cell><cell cols="5">MapReduce-C</cell><cell cols="5">MapReduce-W</cell><cell cols="5">Media Streaming</cell><cell></cell><cell></cell><cell>OLTP</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SAT Solver</cell><cell></cell><cell></cell><cell cols="3">Web Apache</cell><cell></cell><cell></cell><cell cols="3">Web Search</cell><cell></cell><cell></cell><cell cols="3">Web Zeus</cell><cell></cell><cell></cell><cell cols="3">Average</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure 14. Performance improvement of Domino prefetcher compared with VLDP, ISB, STMS, and Digram. For temporal prefetchers except Domino, we assume unlimited-size storage for history in the main memory. For Domino prefetcher, we limit the size of EIT to 2 M rows and the HT to 16 M entries, respectively.</figDesc><table><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speedup over Baseline</cell><cell>10% 20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell><cell>VLDP</cell><cell>ISB</cell><cell>STMS</cell><cell>Digram</cell><cell>Domino</cell></row><row><cell></cell><cell></cell><cell cols="3">Data Serving</cell><cell></cell><cell></cell><cell cols="3">MapReduce-C</cell><cell></cell><cell cols="4">MapReduce-W</cell><cell></cell><cell cols="5">Media Streaming</cell><cell></cell><cell cols="2">OLTP</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SAT Solver</cell><cell></cell><cell></cell><cell cols="3">Web Apache</cell><cell></cell><cell></cell><cell cols="3">Web Search</cell><cell></cell><cell></cell><cell cols="3">Web Zeus</cell><cell></cell><cell></cell><cell cols="3">GMean</cell></row><row><cell></cell><cell>200%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Incorrect Prefetches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Off-Chip Traffic Overhead</cell><cell>50% 100% 150%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Metadata Update Metadata Read</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">STMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Digram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Domino</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Prefetching Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Later in Section V, we discuss why PC localization is ineffective for server workloads in the context of temporal prefetching.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank the anonymous reviewers for their valuable comments. We appreciate the reviewers of <rs type="institution">IEEE CAL</rs> for the positive feedback on the preliminary version of this work [66]. The authors would like to thank members of the <rs type="institution">IPM HPC center</rs>, especially <rs type="person">Armin Ahmadzadeh</rs>, for maintaining and managing the cluster that is used to conduct the experiments. This work was supported in part by a grant from <rs type="funder">Iran National Science Foundation (INSF)</rs>. The research of the third author was partially supported by the <rs type="funder">research deputy of Sharif University of Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Clearing the Clouds: A Study of Emerging Scale-Out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scale-Out Processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Effective On-chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Predictor-Directed Stream Buffers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-associative Cache and Prefetch Buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal Streaming of Shared Memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic Hot Data Stream Prefetching for General-purpose Programs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prefetching Using Markov Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Low-Cost Epoch-Based Correlation Prefetching for Commercial Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical Off-chip Meta-data for Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data Cache Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using a User-level Memory Thread for Correlation Prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Linearizing Irregular Memory Accesses for Improved Correlated Prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Last-Touch Correlated Data Streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TCP: Tag Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dead-block Prediction &amp; Dead-block Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal Streams in Commercial Server Applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IISWC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the Stability of Temporal Data Reference Profiles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Microlib: A Case for The Quantitative Comparison of Micro-Architecture Mechanisms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<title level="m">Temporal Memory Streaming</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Making Address-Correlated Prefetching Practical</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The IBM Blue Gene/Q Compute Chip</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Address-Value Delta (AVD) Prediction: Increasing the Effectiveness of Runahead Execution by Exploiting Regular Memory Allocation Patterns</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependence Based Prefetching for Linked Data Structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Performance of Database Workloads on Shared-Memory Systems with Out-of-Order Processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerating Dependent Cache Misses with an Enhanced Memory Controller</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microarchitecture Optimizations for Exploiting Memory-Level Parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stealth Prefetching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate and Complexity-Effective Spatial Pattern Prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting Spatial Locality in Data Caches Using Spatial Footprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficiently Prefetching Complex Address Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identifying Hierarchical Structure in Sequences: A Linear-time Algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>McGraw-Hill Higher Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="http://cloudsuite.ch" />
		<title level="m">CloudSuite</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="http://parsa.epfl.ch/simflex/flexus.html" />
		<title level="m">Flexus</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SimFlex: Statistical Sampling of Computer System Simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Temporal Instruction Fetch Streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Die-Stacked DRAM Caches for Servers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Intel R Xeon R Processor E</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/processors/xeon/e3-processors/e3-1220-v6.html" />
		<imprint>
			<biblScope unit="page" from="3" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Intel R Xeon R Processor E</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/processors/xeon/e7-processors/e7-8893-v4.html" />
		<imprint>
			<biblScope unit="page" from="7" to="8893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tolerating memory latency through software-controlled preexecution in simultaneous multithreading processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Data Prefetching by Dependence Graph Precomputation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dynamic Speculative Precomputation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Speculative Precomputation: Long-range Prefetching of Delinquent Loads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Inter-core Prefetching for Multicore Processors Using Migrating Helper Threads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamruzzaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Execution-based Prediction Using Speculative Slices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-contained, accurate precomputation prefetching</title>
		<author>
			<persName><forename type="first">I</forename><surname>Atta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Design and Evaluation of a Compiler Algorithm for Prefetching</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Compiler-Based Prefetching for Recursive Data Structures</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Effective Jump-pointer Prefetching for Linked Data Structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">SPAID: Software Prefetching in Pointer-and Call-Intensive Environments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Loop-aware memory prefetching using code block working sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fuchs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The Performance of Runtime Data Cache Prefetching in a Dynamic Optimization System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Guided Region Prefetching: A Cooperative Hardware/Software Approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Path Confidence Based Lookahead Prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">B-Fetch: Branch Prediction Directed Prefetching for Chip-Multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadjo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Best-Offset Hardware Prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<title level="m">Sandbox Prefetching: Safe Run-time Evaluation of Aggressive Prefetchers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>HPCA</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">IMP: Indirect Memory Prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Access Map Pattern Matching for Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An Efficient Temporal Data Prefetcher for L1 Caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
