<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Model Similarity Propagation and its Application for Web Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
							<email>wyma@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
							<email>grxue@sjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xin-Jing</orgName>
								<address>
									<addrLine>Wang 1</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Model Similarity Propagation and its Application for Web Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">588883F0EB552C30BC78B699CD52E404</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H3.3 [Information Search and Retrieval]: Retrieval models Algorithms</term>
					<term>Design</term>
					<term>Performance Multimedia Retrieval</term>
					<term>Mixture Model</term>
					<term>Mutual Reinforcement</term>
					<term>Iterative Similarity Propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an iterative similarity propagation approach to explore the inter-relationships between Web images and their textual annotations for image retrieval. By considering Web images as one type of objects, their surrounding texts as another type, and constructing the links structure between them via webpage analysis, we can iteratively reinforce the similarities between images. The basic idea is that if two objects of the same type are both related to one object of another type, these two objects are similar; likewise, if two objects of the same type are related to two different, but similar objects of another type, then to some extent, these two objects are also similar. The goal of our method is to fully exploit the mutual reinforcement between images and their textual annotations. Our experiments based on 10,628 images crawled from the Web show that our proposed approach can significantly improve Web image retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multimodal Image Retrieval attempts to leverage simultaneously several data types (e.g. image contents, surrounding texts, and links) to improve retrieval performance <ref type="bibr" target="#b2">[2]</ref>[4] <ref type="bibr" target="#b7">[7]</ref>[8] <ref type="bibr" target="#b9">[9]</ref>[17] <ref type="bibr" target="#b20">[20]</ref> <ref type="bibr" target="#b22">[22]</ref>. A major technical challenge in Multimodal Image Retrieval is how to combine different retrieval models in order to achieve best performance. As the data are heterogeneous and inter-related, it is difficult to evaluate the contribution of each individual data type, and therefore, the optimal combination of different models is unclear. Current approaches for combing different models include simple linear combination <ref type="bibr">[8]</ref> <ref type="bibr" target="#b17">[17]</ref>, resorting to human interaction <ref type="bibr" target="#b9">[9]</ref> <ref type="bibr" target="#b22">[22]</ref>, and probabilistic models <ref type="bibr" target="#b2">[2]</ref>[4] <ref type="bibr" target="#b20">[20]</ref>.</p><p>There are two big drawbacks in these existing approaches. First, they are greatly affected by the features of data <ref type="bibr">[8]</ref>[17] <ref type="bibr" target="#b20">[20]</ref>. Two semantically similar images may have entirely different visual features. Although <ref type="bibr" target="#b9">[9]</ref> <ref type="bibr" target="#b22">[22]</ref> proposed methods that partly solve this problem by discovering the semantically similar terms/images through user interaction, the computational cost of these methods is high. Second, in these approaches, the relationships among different data types are treated as additional features, and these features remain unchanged during the learning process. The mutual reinforcement across sets of related data types is not fully explored. Figure <ref type="figure">1</ref> shows an example when the former approaches may possibly fail. A and B are two web-pages. The left two images in Figure <ref type="figure">1</ref> are categorized by the author of web-page A as relevant images (i.e. "bulbs"). However, their visual and textual features (i.e. surrounding text) are both quite different. It is obvious that using the linear combination or probabilistic combination methods, these two images will most probably be regarded as dissimilar.</p><p>Recently, many applications in text retrieval and Web mining have indicated that relational links between objects provide a useful source of information <ref type="bibr" target="#b9">[9]</ref>[12] <ref type="bibr" target="#b13">[13]</ref>[14] <ref type="bibr" target="#b16">[16]</ref> <ref type="bibr" target="#b19">[19]</ref> <ref type="bibr" target="#b21">[21]</ref>. In <ref type="bibr" target="#b19">[19]</ref> the authors use relationships among different types of objects to improve the cluster quality of interrelated data. In <ref type="bibr" target="#b21">[21]</ref>, a method is proposed for spreading the similarities inside and across sets of interrelated objects to discover implicitly similar objects. Both of these methods leverage the inter-type relations by iteratively projecting the clustering results or similarities of one data type to another.</p><p>In this paper, we attempt to learn the semantic similarities of images using the relations between Web images and their textual annotations. We propose a method called Iterative Similarity Propagation which iteratively reinforces the similarities between images by their textual annotations, and vice versa. Our goal is to explore the interrelation between multiple modalities, and by this way, to discover the intrinsic similarity of images and improve the performance of image retrieval. The assumption is that if two objects of the same type are both related to one of another type, these two objects are similar; likewise, if two objects of the same type are related to two different, but similar objects of another type, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. MM'04, October 10-16, 2004, New York, New York, USA.</p><p>Copyright 2004 ACM 1-58113-893-8/04/0010...$5.00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. The two modalities, image content and textual information, can together help group similar Web images</head><p>then to some extent, these two objects are also similar. Note that the meaning of "similarity propagation" is two-fold: enhancing or reducing the similarity of two objects (image or text). By enhancing, we mean that the similarity of two objects will be increased if they relate to similar objects. Quite the reverse, by reducing we mean that the similarity of two objects is decreased if they relate to dissimilar objects. Figure <ref type="figure">1</ref> shows a case of similarity enhancement. Consider a segment of the web-page B which contains an image and its surrounding texts. The image in B has similar visual features to the upper image in A and has similar surrounding text to the bottom image. Thus using B as a bridge, the similarity between two images in A should be increased. However, using the former approaches <ref type="bibr">[8]</ref>[17] <ref type="bibr" target="#b20">[20]</ref> will not be able to cluster these two images together.</p><p>It is valuable to highlight some key-points of our method here:</p><p>1. Instead of treating the image textual annotations as an additional feature for image retrieval, we use an iterative approach to explore the mutual reinforcement between images and their textual annotations. This approach avoids the bias of features mentioned above, and provides a better combination of the image and text retrieval modality.</p><p>2. In our proposed approach, it is the similarity that is propagated between different objects (i.e. images and texts). It can deal with data sparseness problem <ref type="bibr">[8]</ref>[17] <ref type="bibr" target="#b20">[20]</ref> and reduce space complexity since the visual and textual features are often of high dimensional. The intra-and inter-object similarities are refined during the process, which can reduce both false positives and false negatives and reveal the intrinsic similarities in the semantic level.</p><p>3. Our method is an iterative process. The effect of each retrieval modality is propagated to its related modalities in each iteration, by which the interactions inside and across the sets of relational data are explored during the mutual reinforcement.</p><p>4. Fundamentally, our approach can be seen as a non-linear combination of different retrieval modalities, which better exploits the relationships among different data types and use these relationships to discover the implicit but semantic object similarities.</p><p>This paper is organized as follows. We discuss some related works in Section 2. In Section 3, we present the algorithm of iterative similarity propagation and detail its usage in image retrieval in Section 4. Section 5 gives the experiment evaluation of our method. We conclude our work in Section 6 with discussions and possible future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>A number of researchers have introduced systems for searching image databases/Web with combined text and image data. Chen et al. <ref type="bibr">[8]</ref> linearly combine the dot product similarities on textual features and Euclidean distances on visual features and set the two models equal weight. Srihari et al. <ref type="bibr" target="#b17">[17]</ref> intend to find the optimal weight set for the multi-modalities by involving a training phase to learn a group of optimal weight set for the selected set of representative queries. And in the retrieval phase, they linearly combine the individual models using the weight set of the representative query which is the most similar to the current user submitted query. In their work, not only image and text, but face detection and recognition etc. are adopted. Cascia et al <ref type="bibr" target="#b7">[7]</ref> represent each image by a composite of visual and textual features. They use PCA to reduce the dimension of visual features and use LSI to address problems with synonyms, word sense, lexical matching and term omission. <ref type="bibr" target="#b2">[2]</ref>[4] <ref type="bibr" target="#b20">[20]</ref> propose probabilistic models to integrate information provided by associated text and image features. <ref type="bibr" target="#b22">[22]</ref> assumes that images in the database have precise keyword annotations and resorting to users' relevance feedback to discover the common keywords from the keyword vectors of those "positive" images. The query concept is then inferred from these common keywords. <ref type="bibr" target="#b9">[9]</ref> implements the imagetext interaction by generating a thesaurus which establishes the relationships between keywords and visual features. In all these approaches, the relationships or interactions between objects are considered as additional features and these features remain unchanged during the process.</p><p>Recently, many works in text retrieval and Web mining have indicated the effectiveness of iterative reinforcement among different data types for various applications. <ref type="bibr" target="#b14">[14]</ref> proposes an iterative classification procedure which exploits the characteristic of relational data. <ref type="bibr" target="#b12">[12]</ref> leverages the relationship between text and document to exploit the semantic similarity between terms. Wang et al. <ref type="bibr" target="#b19">[19]</ref> use relationships among data objects to improve the cluster quality of interrelated data objects through an iterative reinforcement clustering process. Though effective, these methods suffer from the data sparseness problem. And <ref type="bibr" target="#b19">[19]</ref> partly solves this problem by propagating the cluster centroid instead of the data points enclosed in the clusters of one data type. Xue et al. <ref type="bibr" target="#b21">[21]</ref> improves retrieval effectiveness by iteratively "spread" the interand intra-object similarities through hyperlinks and click-through logs between queries and web-pages.</p><p>Our approach is motivated by <ref type="bibr">[12][19]</ref>[21] on similarity propagation. And we extend the idea of iterative reinforcement to multi-model image retrieval area. To make it feasible, we convert the two kinds of features (i.e. the low-level visual and textual features of images) to two types of objects (i.e. images and webblocks) and use the block-image containerships to represent their relationships. In this way, the mutual reinforcement approach can be implemented on a single type of objects (i.e. images) other than multi-types of objects (such as web-pages, users, queries) required by the former approaches <ref type="bibr" target="#b19">[19]</ref> <ref type="bibr" target="#b21">[21]</ref>. In contrast with the former linear combination approaches <ref type="bibr" target="#b2">[2]</ref> <ref type="bibr" target="#b20">[20]</ref>, our approach provides a non-linear combination of different modalities.</p><formula xml:id="formula_0">[4][7][8][17]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTI-MODEL ITERATIVE SIMILARITY PROPAGATION</head><p>We discuss our iterative similarity propagation approach in this section. First we give an overview of the procedure in Section 3.1, and formularize it in Section 3.2. The convergence of this approach is proved in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the Approach</head><p>The basic idea of iterative similarity propagation is that the similarities among heterogeneous object types can mutually influence each other, by enhancing or reducing the similarity between two objects. Figure <ref type="figure">2</ref> shows an example. Let T and S denote two heterogeneous object spaces. Let i t and j s represent two specific objects in these spaces respectively. The dotted lines represent links among objects (i.e. inter-object relation) and the real lines represent similarities (i.e. intra-object relation). The length of the real line represents the degree of similarities. The left part shows the original object relationships, i.e. in space S, <ref type="bibr" target="#b1">1</ref>  Hence the resulted object relationships turn to those in the right part of Figure <ref type="figure">2</ref>, which better reflects the intrinsic similarities between objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Algorithm</head><p>Without loss of generality, we describe the algorithm of iterative similarity propagation using two types of objects.</p><p>Assume the dimension of space S is M while the dimension of space T is N. Let M M K × and N N G × denote the intra-object similarity matrices based on the content features in space S and T respectively. Let ˆM M K × and denote the intra-object similarity matrices after similarity propagation and both of which are normalized at the end of each iteration. Let</p><formula xml:id="formula_1">ˆN N G × M N</formula><p>Z × be the link matrix from S to T (its transpose, i.e. Z′ , is the link matrix from T to S ) whose elements satisfy</p><formula xml:id="formula_2">1 0 i i ij link s t Z otherwise θ ⎧ ∃ → ⎪ = ⎨ ⎪ ⎩ j (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>i s and j t denote the i th and j th data in S and T respectively. i θ is the number of non-zero elements (i.e. the out-links of i s ) in the i th row of Z .</p><p>The iterative similarity propagation process can be described as follows</p><formula xml:id="formula_4">( ) ( ) 1 ˆ1 K K Z G G ZKZ α α λ β β λ ⎧ ′ = + - ⎪ ⎨ ′ = + - ⎪ ⎩ G Z (2)</formula><p>where α and β are the weights. λ is a decay factor to ensure that the propagated similarities are weaker than the original similarities. 0 , , 1</p><formula xml:id="formula_5">α β λ &lt; &lt; .</formula><p>The physical meaning of equation ( <ref type="formula" target="#formula_2">1</ref>) is obvious: , K K ( ) are the intra-object similarity matrices where , G G K ( ) is fully determined by the content feature of objects in S ( T ).</p><formula xml:id="formula_6">G ẐGZ′ ( Ẑ KZ ′</formula><p>) is inter-object similarity matrix, i.e. the part of intra-object similarities ( G K ) which are propagated from T ( S ) to S ( T ) through the links Z ( Z′ ). And the similarities are decayed during this propagation of λ times. Equation ( <ref type="formula" target="#formula_11">2</ref>) combines both the intra-and inter-object similarities and addresses such mutual reinforcement in an iterative way. It points out that the similarities of one type of objects are affected by other types of objects related to them. It is, fundamentally, a non-linear combination method for the effects of different modalities on relational data.</p><p>Interestingly, the traditional single-modality image retrieval method and linear combination method can be seen as two special cases of equation ( <ref type="formula" target="#formula_11">2</ref>):</p><formula xml:id="formula_7">1. If ( ) 1 or α β = ( ) , K K or G G = =</formula><p>, then equation ( <ref type="formula" target="#formula_2">1</ref>) is reduced to the traditional single-modality retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>In the initial phase, if we set</p><formula xml:id="formula_8">(0) (0) ( ) K K G G =</formula><p>= and 1 λ = , then (1) becomes the traditional linear combination method.</p><p>The superiority of our method to the traditional singlemodality retrieval method and linear combination method is obvious. It is already proved by recent research that relational links between objects are helpful since they provide a unique source of information <ref type="bibr" target="#b9">[9]</ref>[13] <ref type="bibr" target="#b16">[16]</ref>. Hence it is almost definitely true that our method will surpass the traditional content-based image retrieval methods. Moreover, the interactions among heterogeneous objects are most probably non-linear, which can not be well approached by a simple linear combination method, no matter how optimal the weight set is selected <ref type="bibr">[8]</ref>[17] <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convergence of the Algorithm</head><p>In this section, we prove that the equation ( <ref type="formula" target="#formula_11">2</ref>) will converge at the end. We denote the K and in the n-th iteration as Ĝ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>Assume the process begins with the propagation from S to T.</p><p>From equation (2), we have:</p><formula xml:id="formula_9">( ) ( ) ( ) ( ) ( 1) ( )<label>( 1) ( ) ( 1) ( 1 ) ( 1 ˆ1 (</label></formula><p>)</p><formula xml:id="formula_10">n n n n n n ˆ) K K K Z G Z K Z G Z G G Z α α λ α α λ α λ - - - ′ - = + - - + - ′ = - - Z ′ ) - Because likewise, ( ) ( ) ( ) ( ) ( 1)<label>( 1) ( 2)</label></formula><p>( 1)</p><p>ˆˆˆ( <ref type="formula" target="#formula_2">1</ref>) ( 1 ˆ1 ( )</p><formula xml:id="formula_12">n n n n n n G G G ZK Z G ZK Z Z K K Z β β λ β β λ β λ - - - - ′ ′ - = + - - + - ′ = - - Replace ( )<label>( 1)</label></formula><p>ˆn</p><formula xml:id="formula_13">n G G - - in the equation of ( )<label>( 1)</label></formula><p>ˆn</p><formula xml:id="formula_14">n K K - -</formula><p>, we obtain that ( )( )</p><formula xml:id="formula_15">( )( ) 2 (0) ( )<label>( 1) 2 ( 1)</label></formula><p>( 2)</p><formula xml:id="formula_16">1 1 ,<label>( 1)</label></formula><p>( 2)</p><formula xml:id="formula_17">1 1 ( 1 ) ( 0 ) 1 ˆ1 1 ( 1 ) 1 ˆˆˆ1 1 ( ) ˆ( ) ˆ( ) ( ) n n n n A ZZ n n n n n K K n n n K K ZZ K K ZZ A K K A A K K A A K K A ω α β λ α β λ ω ω ω - - ′ = - - = - - - - - = - - - ′ ′ - = - - - = - = = - = - - Denote ij M M A a × ⎡ ⎤ = ⎣ ⎦</formula><p>, because of A ZZ′ = , according to the definition of Z given in equation ( <ref type="formula" target="#formula_2">1</ref>), we have</p><formula xml:id="formula_18">min( , ) 1 1 , 0 max( , ) 0 0 i j i j i j i j ij i j a or θ θ θ θ θ θ θ θ θ θ ⎧ = ≤ &gt; ⎪ = ⎨ ⎪ 0 = = ⎩ Hence we have . 1 0 n n A - → ∞ ⎯⎯⎯ →</formula><p>On the other hand, because (1)  K K is a constant matrix and 1 ω &lt; , we have</p><formula xml:id="formula_19">( )<label>( 1)</label></formula><p>ˆˆ0</p><formula xml:id="formula_20">n n n K K - → ∞ - ⎯⎯⎯ → ,</formula><p>which proves the convergence of equation ( <ref type="formula" target="#formula_2">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMAGE RETRIEVAL USING ITERATIVE SIMILARITY PROPAGATION</head><p>We have detailed in Section 3 the iterative similarity propagation algorithm. In this section, we present how we improve the performance of image retrieval using this mutual reinforcement approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Link Graph Construction</head><p>We crawled 10,628 images mainly from the websites listed in Table <ref type="table">1</ref>, and use an effective page segmentation technique called VIPS (VIsion-based Page Segmentation) <ref type="bibr" target="#b5">[5]</ref>[6] to obtain the "blocks", i.e. the web-page segments that contain these images as well as their surrounding texts. VIPS extracts the semantic structure of a web-page based on its visual presentation. Such semantic structure is represented as a tree; each node in the tree corresponds to a block. Each node will be assigned a value (Degree of Coherence) to indicate how coherent of the content in the block based on visual perception. The red rectangles in Figure <ref type="figure">3</ref> show some examples of the "blocks" obtained by <ref type="bibr" target="#b6">[6]</ref>.</p><p>Figure <ref type="figure">3</ref> shows three examples of image-block relationship. The left web-page shows a one-to-one projection between an image and a block. The right web-page shows a more-to-one and a one-to-more projections between images and blocks. That is, a block can contain multiple images (see the thick red rectangle) and an image can belong to more than one block (see the crocodile images. All of them are inside both the thick red Rectangle and a thin red rectangle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Examples of Image-Block Relationship</head><p>We refer the block with its image being removed as t-block and treat the images and their t-blocks as two types of objects. The containerships between blocks and their images are considered as links between the t-blocks and the images. The images are represented by a set of low-level color and texture features. The content features of t-blocks are extracted in such a way: first, the image surrounding texts, image captions and hyperlinks are parsed from the HTML documents. Then stopwords are filtered out and the rest of the terms construct term vectors which are weighted using TF*IDF <ref type="bibr" target="#b15">[15]</ref>. The resulted term vectors are used as the t-block features. When a block contains more than one image, the content feature of its t-block is obtained from the collection of the textual annotations of all images inside it. In this way, the textual annotations of images are transmitted to another type of object other than an additional feature vector.</p><p>Each of the three image-block relationships mentioned above has simultaneously its advantages and disadvantages. For example, although the content features of the large block in the right webpage are less precise than those of the left one, this block points out that two images are similar although they have different textual annotations (see the "alligator" and "caiman" in Figure <ref type="figure">3</ref>, both of them are crocodiles).</p><p>Based on these three kinds of relationships (i.e. one-to-one, one-to-more and more-to-one), a similar link graph as in Figure <ref type="figure">2</ref> between the images and the blocks can be established: the nodes in S and T are the images and blocks respectively. If an image is contained in a certain block, there will be a link (i.e. the dotted line) between them. Based on this graph, the intra-and interobject similarities can be propagated inside and across the images and blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Content Similarity Matrix Formulation</head><p>Let X be the visual feature matrix with rows as the images and columns as their visual features. Let i X denote the i th row of X . Let Y be the block feature matrix with rows the blocks and the columns the terms (weighted by TF*IDF). j Y represents the</p><formula xml:id="formula_21">j th row of Y .</formula><p>The initial image similarity matrix</p><formula xml:id="formula_22">ij M M K K × ⎡ ⎤ = ⎣ ⎦</formula><p>, which is totally based on the low-level visual features, is given by converting the Euclidean distances between images into similarities which monotonically increase as the distances decrease. K is given by , ,</p><formula xml:id="formula_23">( , ) 1 1 max ( , ) max ( )( ) T i j i j i j ij T i j i j i j i j i j X X X X Eud X X K Eud X X X X X X - - = - = - - -<label>( ) ( )</label></formula><p>The initial block similarity matrix</p><formula xml:id="formula_25">ij N N G G × ⎡ ⎤ = ⎣ ⎦ is calculated</formula><p>using the traditional cosine similarity measure in text retrieval which is given by:</p><formula xml:id="formula_26">i j ij i j Y Y G Y Y • = ⋅<label>(4)</label></formula><p>We set the initial intra-object similarities to be their content similarities, i.e. (0) K K = and . And we perform the iterative reinforcement using equation ( <ref type="formula" target="#formula_11">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>10,628 images associated with 16,720 blocks are crawled mainly from the websites listed in Table <ref type="table">1</ref>. These images cover from nature to artificial objects, human beings and Web logos. yahooligan.yahoo.com. For the other images that have no category information on web-pages, we let the volunteers select the most representative keywords as the labels.</p><p>The image visual features are 36-bin color correlogram <ref type="bibr" target="#b10">[10]</ref>, three-level color moment <ref type="bibr" target="#b18">[18]</ref> and three-level wavelet textures <ref type="bibr" target="#b1">[1]</ref>.</p><p>Two performance measures: precision-scope and recall-scope, are applied. Scope specifies the number of images returned to the user. Precision is defined as the number of retrieved relevant objects over the value of scope. Recall is defined as the number of retrieved relevant objects over the total number of relevant objects.</p><p>We use the alike linear combination method proposed in <ref type="bibr">[8]</ref> as our baseline method but tune an optimal weight set for it rather than fix each weight to 0.5 as proposed in <ref type="bibr">[8]</ref>. Although the lowlevel visual features we used are different from <ref type="bibr">[8]</ref>, these differences will not bias the final evaluation since it is the method itself rather than features used that determine the performances.</p><p>The reason that we choose the method proposed in <ref type="bibr">[8]</ref> as our baseline method is as follows. First, the approach in <ref type="bibr">[8]</ref> represents a traditional way of combining multi-modalities for image retrieval. Second, we do not involve a training phase to select representative query set and learn the optimal weight set for them, nor do we apply face detection and face recognition approaches in our method, hence it is impossible to compare our approach with <ref type="bibr" target="#b17">[17]</ref>. Third, our method is based on global images, while the approaches in <ref type="bibr" target="#b2">[2]</ref>[4] are based on segmented images. The works in <ref type="bibr" target="#b2">[2]</ref> <ref type="bibr" target="#b4">[4]</ref> are more like image auto-annotation and recognition.</p><p>We randomly selected 2,500 images to form the query set. And the final performances are the average precision and recall on these 2,500 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Evaluation</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the cooperation of retrieval performance. The red diamond lines represent the retrieval performance of our method. The blue square lines correspond to the baseline method, i.e. the linear combination method. The yellow and green lines show the performance of single-modality method. The yellow triangle lines are based on only textual features, and the green dot lines are corresponding to the method using only image low-level features.</p><p>The parameters selected are 0.3, 0.8, 0.8</p><formula xml:id="formula_27">α β λ = = =</formula><p>in our method, where α and β are the weights of image similarity matrix and t-block similarity matrix respectively. λ is the decay factor. In the baseline method, the weight of similarity matrix based on visual features is 0.2. The parameters are determined based on an extensive experiment which will be discussed in Section 5.3.</p><p>It can be seen from this figure that our method significantly outperforms the baseline method and the single-modality method. The average precision@10 for these four methods, i.e. the iterative similarity propagation method, the linear combination method, the textual feature based retrieval method and the visualfeature based retrieval, are 46.3%, 37.1%, 32% and 23.2% respectively.</p><p>From this figure, we can see that retrieval using only textual features surpasses greatly the method using only image content feature. It on the one hand proves the effectiveness of VIPS <ref type="bibr" target="#b6">[6]</ref> web-page segmentation algorithm, and on the other hand, confirms that the existence of semantic gap greatly affects the performance of content-based image retrieval.</p><p>However, although the textual features are better than image content features, still many web images will have noisy annotations. Also, textual annotations can be ambiguous, e.g. "apple" can both indicate "apple tree" and "apple computer". Hence, intuitively, combining the two kinds of features will do a better job, just as indicated in <ref type="bibr" target="#b3">[3]</ref> that "while text and images are separately ambiguous, jointly they tend not to be". And our experiment doubly confirmed this. However, separately combining different features can also be biased by the features themselves, as mentioned in Section 1. The iterative propagation approach better explores the mutual reinforcement among different data types which in some sense correct such biases. It can also be regarded as a non-linear combination method on different feature types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Convergence of Our Approach</head><p>It is proved in Section 3.3 that our approach will finally converge. In this section, we show the empirical result on the convergence test.</p><p>Note that the evaluation is independent with parameter selected. In our evaluation, we set 0.3, 0.8, 0.8</p><formula xml:id="formula_28">α β λ = = = .</formula><p>The precision vs. number of iteration is shown in Figure <ref type="figure" target="#fig_4">5</ref>. The method converges when the number of iteration is 3 (the precision@2 is a bit higher than precision@4). This figure proves the convergence of our approach as analyzed in Section 3.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration Effect on Retrieval Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Effect of Decay Factor</head><p>As mentioned in Section 3.2, the decay factor λ ensures that the propagated similarities are weaker than the original similarities. In this section, we evaluate its effect on the performance of image retrieval.</p><p>The dataset and the value of α and β are the same as that in Section 5.1. Figure <ref type="figure" target="#fig_5">6</ref> shows the retrieval precision for different λ .</p><p>It can be seen that the best performance is obtained when 0.8 λ = . This is reasonable because too much or too less propagation will both degrade the retrieval performance. When 0 λ = , this method is reduced to the single-modality image retrieval (i.e. CBIR), in which case the mutual reinforcement is not taken into consideration. When 1 λ = , it becomes the linear combination method, in which case the mutual reinforcement is not fully explored, and the result lean to be biased by the features of data as discussed in Section 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Decay Factor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Precision vs. Weighting Schema</head><p>Different weight , α β in equation (2) will affect the retrieval performance. Figure <ref type="figure" target="#fig_6">7</ref> shows the variation of retrieval precision vs. α with 0.8 β = . It can be seen that the best performance is achieved at 0.3 α = .</p><p>The best performance is obtained when α β &lt; shows that when calculating the similarities of images (i.e. K in equation ( <ref type="formula" target="#formula_11">2</ref>)), the similarities based on image visual features (i.e. K ) are less effective than the similarities propagated from the t-blocks (i.e. ). This coincides with the retrieval performance given in Figure <ref type="figure" target="#fig_3">4</ref>, where the retrieval performance based on text retrieval is far better than that based on image contents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>Multi-model Image Retrieval intends to deal with several data types which is heterogeneous and inter-active. How to seamlessly combine different retrieval models is still a research topic. In this paper, we proposed an Iterative Similarity Propagation model to solve this problem. It attempts to fully exploit the mutual reinforcement of relational data which result in a non-linear combination of different modalities. It uses the intra-object similarities of one data type to affect those of another data type which links to it, and perform this approach iteratively, by which the similarities of images in the semantic level are approached.</p><p>The assumption is that, if two objects of the same type are both related to an object of another type, these two objects are similar; and if two objects of the same type are related to two different, but similar objects in another type, then to some extent, these two objects can also be considered similar.</p><p>In this paper, this approach is used to learning the semantic similarities of images by leveraging the relationships between Web images and their textual annotations. The experimental results based on 10,628 images crawled from the Web showed the effectiveness of our proposed Iterative Similarity Propagation model for image retrieval.</p><p>In fact, the importance of each web page is different as well as their blocks. In the future, we will combine importance of the image and the text into our algorithm according to their blocks. Moreover, we did not discuss the integration of the proposed method to a relevance feedback system. A simple way could be weighting the content feature matrix K and in equation ( <ref type="formula" target="#formula_11">2</ref>) in each iteration according to users' feedbacks. We will research on this also in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance Evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Convergence of Our Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Effect of Decay Factor λ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ĜFigure 7 .</head><label>7</label><figDesc>Figure 7. Precision vs. Visual Feature Based Image SimilarityMatrix Weighting Schema</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,66.24,72.00,479.46,213.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>Special thanks should be given to Deng Cai, Xuemei Jiang and Shen Huang for their sincerely helps.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image Coding Using Wavelet Transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubchies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning the Semantic of Words and Pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Clustering</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><surname>Art</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="434" to="439" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<title level="m">Modeling Annotated Data. SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting Content Structure for Web Pages Based on Visual Representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APWeb</title>
		<imprint>
			<biblScope unit="page" from="406" to="417" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">VIPS: a Vision-Based Page Segmentation Algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>MSR-TR-2003-79</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Microsoft Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining Textual and Visual Cues for Content-based Image Retrieval on the World Wide Web</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Content-Based Access of Image and Video Libraries</title>
		<meeting>IEEE Workshop on Content-Based Access of Image and Video Libraries</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Web Mining for Web Image Retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="831" to="839" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Text-Image Interaction for Image Retrieval and Semi-Automatic Indexing. 20th Annual BCS-IRSG Colloquium on IR</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duffing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring Web Communities From Link Topology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9 th ACM Conference on Hypertext and Hypermedia</title>
		<meeting>9 th ACM Conference on Hypertext and Hypermedia</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image Indexing Using Color Correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on CVPR</title>
		<meeting>IEEE Conference on CVPR</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Semantic Similarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Authoritative Sources in a Hyperlinked Environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9 th ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>of the 9 th ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative Classification in Relational Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2000 Workshop</title>
		<meeting>the AAAI 2000 Workshop</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Term Weighting Approaches in Automatic Text Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining Statistical and Relational Methods in Hypertext Domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ILP</title>
		<meeting>ILP</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Munirathnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">A Model For Multimodal Information Retrieval. ICME</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Similarity of Color Images, In Storage and Retrieval for Image and Video Databases III</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-02">Feb. 1995</date>
			<biblScope unit="volume">2420</biblScope>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Recom</surname></persName>
		</author>
		<title level="m">Reinforcement Clustering of Multi-Type Interrelated Data Objects. SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<title level="m">Probabilistic Multimedia Retrieval. SIGIR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Similarity Spreading: A Unified Framework for Similarity Calculation of Interrelated Objects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unifying Keywords and Visual Contents in Image Retrieval</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
