<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEPA: SELF-SUPERVISED AUDIO EMBEDDING FOR DEPRESSION DETECTION</title>
				<funder ref="#_hyn4XGW">
					<orgName type="full">Major Program of National Social Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-29">29 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heinrich</forename><surname>Dinkel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pingyue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengyue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEPA: SELF-SUPERVISED AUDIO EMBEDDING FOR DEPRESSION DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-29">29 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.13028v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural networks</term>
					<term>automatic depression detection</term>
					<term>convolutional neural networks</term>
					<term>feature embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depression detection research has increased over the last few decades as this disease is becoming a socially-centered problem. One major bottleneck for developing automatic depression detection methods lies on the limited data availability. Recently, pretrained text-embeddings have seen success in sparse data scenarios, while pretrained audio embeddings are rarely investigated. This paper proposes DEPA, a self-supervised, Word2Vec like pretrained depression audio embedding method for depression detection. An encoderdecoder network is used to extract DEPA on sparse-data in-domain (DAIC) and large-data out-domain (switchboard, Alzheimer's) datasets. With DEPA as the audio embedding, performance significantly outperforms traditional audio features regarding both classification and regression metrics. Moreover, we show that large-data out-domain pretraining is beneficial to depression detection performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Depression, a disease of considerable attention, has been affecting more than 300 million people worldwide. With the severity of depression growing without an adequate cure, a person with such illness will suffer from multiple symptoms, including insomnia, loss of interest, and at the extreme end, committing suicide. An increasing amount of research has been conducted on automatic depression detection and severity prediction, in particular, from conversational speech, which has embedded crucial information about one's mental state. However, the models so far are heavily restricted by the limited amount of depression data. This data sparsity has caused difficulty in accuracy enhancement and reproduction.</p><p>Many sparse scenarios in natural language processing (NLP) tasks have benefited from pretrained text embed-dings like GloVe <ref type="bibr" target="#b1">[1]</ref>, BERT <ref type="bibr" target="#b2">[2]</ref> and ELMo <ref type="bibr" target="#b3">[3]</ref>. Regarding multi-modal research, pretrained audio embeddings such as SoundNet <ref type="bibr" target="#b4">[4]</ref> have been found to outperform traditional spectrogram based features regarding acoustic environment classification. All these pretrained neural networks take advantage of a self-supervised encoder-decoder model, which does not require manual labeling and, therefore, can be pretrained on large datasets.</p><p>However, little research has been done on pretraining audio features. Utilizing audio-based features for depression detection has its potential downsides compared to high-level text-based features: 1) Content-rich audio contains undesirable information, such as environmental sounds, interfering speech, and noise. 2) Features are typically low-level and extracted within a short time-scale (e.g., 40ms), each containing little information about high-level st entire sequence (e.g., spoken word).</p><p>In our point of view, a successful audio-embedding for depression detection needs to be extracted on sequence-level <ref type="bibr" target="#b5">[5]</ref> (e.g., sentence), in order to capture rich, long-term spoken context as well as emotional development within an interview. Thus, this work aims to explore whether depression detection via audio can benefit from a pretrained network.</p><p>Contribution This paper proposes DEPA, a self-supervised, Word2Vec like pretrained depression audio embedding method for automatic depression detection. Two sets of DEPA experiments are conducted. First, we investigate the use of DEPA by pretraining on depression (in-domain) data. Second, we further explore out-domain pretraining on other mental disorders interviewing conversation datasets and general-purpose speech datasets. To our knowledge, this is the first time a pretrained network is performed on a depression detection task. More importantly, this can be generated to other speech research with limited data resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, related work on depression detection and selfsupervised learning will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depression detection</head><p>Various methods have been proposed for automatic depression detection. Previous speech-based detection work has experimented on various acoustic features, like prosodic features (e.g., pitch, jitter, loudness, speaking rate, energy, pause time, intensity, etc.), spectral features (e.g., formants, energy spectrum density, spectral energy distribution, vocal tract spectrum, spectral noise, etc.) and cepstral features (e.g., Mel-Frequency Cepstral Coefficients <ref type="bibr" target="#b6">[6]</ref>), and more recently, feature combinations like COVAREP (CVP) <ref type="bibr">[7]</ref>, which consists of a high-dimensional feature vector covering common features such as fundamental frequency and peak slope. Also deep learning methods have been employed to extract highlevel feature representations <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8]</ref>. Despite the tryout on different features and models, the F1 accuracy generated from speech-based depression detection is average. Work in <ref type="bibr" target="#b9">[9]</ref> indicated that by pretraining text-embeddings on a large, task-independent corpus, can significantly enhance detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised learning</head><p>Self-supervised learning is a technique where training data is autonomously labeled, yet the training procedure is supervised. In NLP, pretrained word embeddings are trained with self-supervised learning, being applied to a variety of tasks, and achieving superior performance. The main philosophy is to predict the next words/sentences, given a contextual history/future, without requiring any manual labeling. Selfsupervised methods can also extract some useful information about the data itself. Our main inspiration for this work stems from <ref type="bibr" target="#b10">[10]</ref>, where a self-supervised approach was taken to extract general-purpose audio representations. This method can thus be applied to depression detection to capture implicit information underneath each speaker's speech and make predictions on their depressed state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>We propose DEPA, an auditory feature extracted via a neural network to capture non-trivial speech details. Our proposed method consists of a self-supervised encoder-decoder network, where the encoder is later used as DEPA embedding extractor from spectrograms. Given a spectrogram of a specific audio clip X ? R S?F , where S is the number of frames and F the data dimension (e.g., frequency bins).</p><p>We proceed to slice X into S (2k+1)?T non-overlapping sub-spectrograms X i ? R ((2k+1)?T )?F . Then, 2k + 1 subspectrograms are selected with k spectrograms before and after a center one M 0 :</p><formula xml:id="formula_0">X i = [M -k , M -k+1 , ? ? ? , M -1 , M 0 , M 1 , ? ? ? , M k-1 , M k ] ,</formula><p>where M i ? R T ?F . The self-supervised training process treats the center spectrogram M 0 as the target label, given its surrounding spectrograms M i , (i = 0) and computes the embedding loss (Equation ( <ref type="formula" target="#formula_3">1</ref>)). The detailed pretraining process can be seen in Algorithm 1 and depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Algorithm 1: Pseudo code of pretraining DEPA 1:Algorithm DEPA(X, ?, ?) 2: Xi = A sequence of (2k + 1), T sized spectrograms </p><formula xml:id="formula_1">3: M0 = center spectrogram of Xi 4: [M -k ? ? ? M-1, M1 ? ? ? M k ] = k spectrograms before and after M0 5: v = encode ([M -k ? ? ? M-1, M1 ? ? ? M k ], ?) 6: M 0 = decode(v, ?) 7: Lembed = MSE(M 0 , M0) 8: Update model parameters ?, ?.</formula><formula xml:id="formula_2">L embed = T t=1 D d=1 (M 0 t,d -M 0 t,d ) 2 .</formula><p>(</p><formula xml:id="formula_3">)<label>1</label></formula><p>Encoder architecture: The encoder architecture contains three downsampling blocks. Each block consists of a convolution, average pooling, batchnormalization, and ReLU activation layer.</p><p>Decoder architecture: The decoder upsamples v via three transposed convolutional upsampling blocks and predicts the center spectrogram M 0 ? R T ?F . The model is then updated via the embedding loss in Equation <ref type="bibr" target="#b1">(1)</ref>. The encoder-decoder architecture is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>After pretraining the encoder-decoder network, DEPA is extracted via feeding a variable-length audio segment R (here Data We aim to compare DEPA in regards to pretraining on related, e.g., in-domain (depression detection) and outdomain (e.g., speech recognition) datasets. Regarding in-domain data, we utilized the publicly available DAIC dataset for in-domain pretraining in order to compare DEPA to traditional audio feature approaches. In order to ascertain DEPAs' usability, we further used the mature switchboard (SWB) dataset, containing English telephone speech. The Alzheimer's disease (AD) dataset was privately collected from a Shanghai Mental Clinic, containing about 400 hours (questions and answers) of Mandarin interview material from senior patients. The three datasets can be seen Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS Depression Data</head><p>The most broadly used dataset within depression detection is the Distress Analysis Interview Corpus -Wizard of Oz (DAIC) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> DEPA Pretraining Process In this work, the encoderdecoder training utilizes MSP features, with the hyperparameters k = 3, T = 96, which extracts a 256 dimensional DEPA embedding. Moreover, the model is trained for 4000 epochs using Adam optimization with a starting learning rate of 0.004. The pretraining process differs for in-domain and out-domain datasets. For in-domain data, all responses of a patient are concatenated, meaning that silence or speech of the interviewer is neglected. For out-domain data, no preprocessing is done, meaning that the entire dataset is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Depression Detection</head><p>Model The final decision about the depression state and severity is carried out by a multi-task model, based on previous work in <ref type="bibr" target="#b9">[9]</ref>. This approach models a patients' depression sequentially, meaning that only the patients' responses are utilized. Due to the recent success of LSTM networks in this field <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, our depression prediction structure follows a bidirectional LSTM (BLSTM) approach with four layers of size 128. A dropout of 0.1 is applied after each BLSTM layer to prevent overfitting. The model outputs at each response r (timestep) a two dimensional vector (y c (r), y r (r)), representing the estimated binary patient state (y c (r)) as well as the PHQ8 score (y r (r)). Finally, first timestep pooling is applied to reduce all responses of a patient to a single vector (y c (0), y r (0)). The architecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Metric Similar to <ref type="bibr" target="#b9">[9]</ref>, binary cross entropy loss between y c , y c is used for binary classification (Equation ( <ref type="formula">2</ref>)), while </p><p>(y c , y c , y r , y r ) = bce (?(y c ), y c ) + hub (y r , y r )</p><p>Results are reported in terms of mean average error (MAE), and root mean square deviation (RMSE) for regression and macro-averaged F1 score for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection training process</head><p>Training the detection process differs among DEPA, HCVP, and MSP features slightly. Even though all of them are extracted on response-level, HCVP and DEPA are fixed-sized vector representations, while MSP is a variable-length feature sequence. Data standardization is applied by calculating a global mean and variance on the training set and applying those on the development set. Adam optimization with a starting learning rate of 0.004 is used. Out-domain DEPA pretraining has produced interesting results: pretraining on both out-domain datasets SWB and AD outperform the in-domain DAIC in terms of binary classification (F1). Further, pretraining on AD resulted in the lowest regression error rates in terms of MAE and RMSE. We think the superior performance of AD pretraining is because some cognitive impairment is highly related to depression; thus, more speech characteristics are shared between AD and DAIC (depression). More importantly, by jointly training on all available datasets (713h), performance reduces to MSP levels, implying that while pretraining can be done on virtually any dataset, one should pay attention to coherent dataset content. It is thus our future interest to explore how generalized a pretrained audio embedding is, given the fact that emotion can be language-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This work proposed DEPA, an audio embedding pretraining method for automatic depression detection. An encoderdecoder model is trained in self-supervised fashion to predict and reconstruct a center spectrogram given a spectrogram context. Then, DEPA is extracted from the trained encoder model and fed into a multi-task depression detection BLSTM. DEPA exhibits an excellent performance compared to traditional spectrogram and COVAREP features. In-domain results suggest a significantly better result (F1 0.72, MAE 4.72) on detection presence detection compared to traditional spectrogram features without DEPA (F1 0.61, MAE 6.07). Out-domain results imply that DEPA pretraining can be done on virtually any spoken-language dataset, while at the same time being beneficial to depression detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. DEPA pretraining framework.</figDesc><graphic url="image-1.png" coords="2,315.21,290.70,289.80,194.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. DEPA pretraining encoder-decoder architecture.</figDesc><graphic url="image-2.png" coords="3,63.07,71.99,226.50,176.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Depression detection with DEPA. The encoder from the proposed encoder-decoder model provides the BLSTM network with high-level auditory features.</figDesc><graphic url="image-3.png" coords="4,63.42,72.00,225.80,294.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Utilized datasets for DEPA pretraining.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Duration(h) Language</cell></row><row><cell>In</cell><cell>DAIC</cell><cell>13</cell><cell>English</cell></row><row><cell>Out</cell><cell>AD SWB</cell><cell cols="2">400 Mandarin 300 English</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison between detection with and without DEPA pretraining, regarding three utlilzed datasets. ? represents the use of all three datasets for DEPA extraction.Results inTable 2 are compared on two different levels: Feature Comparison The first two rows of Table 2, indicate that indeed, fixed-sized response-level features (HCVP) outperform variable-sized sequence features (MSP). Regarding in-domain training (3rd row), DEPA excels in comparison to both traditional features in terms of classification and regression performance.</figDesc><table><row><cell>Regression</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work has been supported by the <rs type="funder">Major Program of National Social Science Foundation of China</rs> (No.<rs type="grantNumber">18ZDA293</rs>). Experiments have been carried out on the PI supercomputer at <rs type="institution">Shanghai Jiao Tong University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hyn4XGW">
					<idno type="grant-number">18ZDA293</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Measuring depression symptom severity from spoken language and 3d facial expressions</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08592</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mfcc-based recurrent neural network for automatic clinical depression recognition and assessment from speech</title>
		<author>
			<persName><forename type="first">Emna</forename><surname>Rejaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Komaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Said</forename><surname>Agrebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Othmani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07208</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">COVAREP -A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-05">2014. May 2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting depression with audio/text sequence modeling of interviews</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Tuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1716" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Textbased depression detection: What triggers an alert</title>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05154</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-supervised audio representation learning for mobile devices</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Gfeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11796</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>F?lix de Chaumont Quitry, and Dominik Roblek</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Distress Analysis Interview Corpus of human and computer interviews</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gale</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC 2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="page" from="3123" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simsensei kiosk: A virtual human interviewer for healthcare decision support</title>
		<author>
			<persName><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Benn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alesia</forename><surname>Gainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arno</forename><surname>Hartholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaux</forename><surname>Lhommet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gale</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Morbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apar</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems</title>
		<meeting>the 2014 International Conference on Autonomous Agents and Multi-agent Systems<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The phq-8 as a measure of current depression in the general population</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">W</forename><surname>Strine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet B W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><forename type="middle">T</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">H</forename><surname>Mokdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Affective Disorders</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Avec 2017: Real-life depression, and affect recognition workshop and challenge</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Mozgai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 7th Annual Workshop on Audio/Visual Emotion Challenge<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
