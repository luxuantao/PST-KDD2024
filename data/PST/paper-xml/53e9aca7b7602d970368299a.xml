<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimedia data mining: state of the art and challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-11-16">16 November 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>B) â€¢ M. S</roleName><forename type="first">Chidansh</forename><forename type="middle">Amitkumar</forename><surname>Bhatt</surname></persName>
							<email>chidansh@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Kankanhalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimedia data mining: state of the art and challenges</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-11-16">16 November 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">6D872FDCA80C611FF7050EEB913F5913</idno>
					<idno type="DOI">10.1007/s11042-010-0645-5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advances in multimedia data acquisition and storage technology have led to the growth of very large multimedia databases. Analyzing this huge amount of multimedia data to discover useful knowledge is a challenging problem. This challenge has opened the opportunity for research in Multimedia Data Mining (MDM). Multimedia data mining can be defined as the process of finding interesting patterns from media data such as audio, video, image and text that are not ordinarily accessible by basic queries and associated results. The motivation for doing MDM is to use the discovered patterns to improve decision making. MDM has therefore attracted significant research efforts in developing methods and tools to organize, manage, search and perform domain specific tasks for data from domains such as surveillance, meetings, broadcast news, sports, archives, movies, medical data, as well as personal and online media collections. This paper presents a survey on the problems and solutions in Multimedia Data Mining, approached from the following angles: feature extraction, transformation and representation techniques, data mining techniques, and current multimedia data mining systems in various application domains. We discuss main aspects of feature extraction, transformation and representation techniques. These aspects are: level of feature extraction, feature fusion, features synchronization, feature correlation discovery and accurate representation of multimedia data. Comparison of MDM techniques with state of the art video processing, audio processing and image processing techniques is also provided. Similarly, we compare MDM techniques with the state of the art data mining techniques involving clustering, classification, sequence pattern mining, association rule mining and visualization. We review current multimedia data mining systems in detail, grouping them according to problem formulations and approaches. The review includes supervised and unsupervised discovery of events and actions from one</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>or more continuous sequences. We also do a detailed analysis to understand what has been achieved and what are the remaining gaps where future research efforts could be focussed. We then conclude this survey with a look at open research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, multimedia data like pictures, audio, videos, text, graphics, animations, and other multimodal sensory data have grown at a phenomenal rate and are almost ubiquitous. As a result, not only the methods and tools to organize, manage, and search such data have gained widespread attention but the methods and tools to discover hidden knowledge from such data have become extremely important. The task of developing such methods and tools is facing the big challenge of overcoming the semantic gap of multimedia data. But in certain sense datamining techniques are attempting to bridge this semantic gap in analytical tools. This is because such tools can facilitate decision making in many situations. Data mining refers to the process of finding interesting patterns in data that are not ordinarily accessible by basic queries and associated results with the objective of using discovered patterns to improve decision making <ref type="bibr" target="#b103">[104]</ref>. For example, it might not be possible to easily detect suspicious events using simple surveillance systems. But MDM tools that perform mining on captured trajectories from surveillance videos, can potentially help find suspicious behavior, suspects and other useful information.</p><p>MDM brings in strengths from both multimedia and data mining fields along with challenging problems in these fields. In terms of strength, we can say image, audio, video etc are more information rich than the simple text data alone in most of the domains. The knowledge available from such multimedia data can be universally understandable. Also, there can be certain situations where there is no other efficient way to represent the information other than the multimodal representation of the scenario.</p><p>Mining of multimedia data is more involved than that of traditional business data because multimedia data are unstructured by nature. There are no well-defined fields of data with precise and nonambiguous meaning, and the data must be processed to arrive at fields that can provide content information. Such processing often leads to non-unique results with several possible interpretations. In fact, multimedia data are often subject to varied interpretations even by human beings. For example, it is not uncommon to have different interpretation of an image by different people. Another difficulty in mining of multimedia data is its heterogeneous nature. The data are often the result of outputs from various kinds of sensor modalities with each modality needing sophisticated preprocessing, synchronization and transformation procedures. Yet another distinguishing aspect of multimedia data is its sheer volume. The high dimensionality of the feature spaces and the size of the multimedia datasets make feature extraction a difficult problem. MDM works focus their effort to handle these issues while following the typical data mining process.</p><p>The typical data mining process consists of several stages and the overall process is inherently interactive and iterative. The main stages of the data mining process are (1) Domain understanding; (2) Data selection; (3) Data preprocessing, cleaning and transformation; (4) Discovering patterns; <ref type="bibr" target="#b4">(5)</ref> Interpretation; and (6) Reporting and using discovered knowledge <ref type="bibr" target="#b103">[104]</ref>.</p><p>The domain understanding stage requires learning how the results of data-mining will be used so as to gather all relevant prior knowledge before mining. For example, while mining sports video for a particular sport like tennis, it is important to have a good knowledge and understanding of the game to detect interesting strokes used by players.</p><p>The data selection stage requires the user to target a database or select a subset of fields or data records to be used for data mining. A proper understanding of the domain at this stage helps in the identification of useful data. The quality and quantity of raw data determines the overall achievable performance.</p><p>The goal of preprocessing stage is to discover important features from raw data. The preprocessing step involves integrating data from different sources and/or making choices about representing or coding certain data fields that serve as inputs to the pattern discovery stage. Such representation choices are needed because certain fields may contain data at levels of details not considered suitable for the pattern discovery stage. This stage is of considerable importance in multimedia data mining, given the unstructured and heterogenous nature and sheer volume of multimedia data. The preprocessing stage includes data cleaning, normalization, transformation and feature selection. Cleaning removes the noise from data. Normalization is beneficial as there is often large difference between maximum and minimum values of data. Constructing a new feature may be of higher semantic value to enable semantically more meaningful knowledge. Selecting subset of features reduces the dimensionality and makes learning faster and more effective. Computation in this stage depends on modalities used and application's requirements.</p><p>The pattern discovery stage is the heart of the entire data mining process. It is the stage where the hidden patterns, relationships and trends in the data are actually uncovered. There are several approaches to the pattern discovery stage. These include association, classification, clustering, regression, time-series analysis, and visualization. Each of these approaches can be implemented through one of several competing methodologies, such as statistical data analysis, machine learning, neural networks, fuzzy logic and pattern recognition. It is because of the use of methodologies from several disciplines that data mining is often viewed as a multidisciplinary field.</p><p>The interpretation stage of the data mining process is used to evaluate the quality of discovery and its value to determine whether the previous stages should be revisited or not. Proper domain understanding is crucial at this stage to put a value to the discovered patterns. The final stage of the data mining process consists of reporting and putting to use the discovered knowledge to generate new actions or products and services or marketing strategies as the case may be. This stage is application dependent.</p><p>Among the above mentioned stages of data mining process Data preprocessing, cleaning and transformation; Discovering patterns; Interpretation; and Reporting and using discovered knowledge contains the highest importance and novelty from the MDM perspective. Thus, we organize Multimedia Data Mining State of the Art review as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The proposed scheme achieves the following goals, -Discussion of the existing preprocessing techniques for multimedia data in MDM literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDM problems and approaches</head><p>Preprocessing  In the following section we will detail the state of the art in MDM. We aim to provide insight into research issues, problems and motivations for doing MDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State of the art in multimedia data mining</head><p>Based on the description of the multimedia data mining process in the previous section, we will review the state of the art for each stage of MDM in the following sections. As the domain understanding and data selection are subjective topics, they are not covered. The interpretation stage, the stage of reporting and using discovered knowledge are combined under the heading "Knowledge interpretation, evaluation and representation".</p><p>In the following section, we organize the state of the art details depending upon the type of media. These mode of organization is preferable because (1) Basic datamining techniques do not change much when they are applied to different modalities but the major changes are seen in the way the each of these modalities are processed. <ref type="bibr" target="#b1">(2)</ref> If we organize by the data mining techniques (e.g., clustering) there are many many sub-techniques(e.g., hierarchical clustering, partition based etc.) whose repeated mention for these different modalities, would be a tedious task for reading as well as for writing. (3) Video mining, image mining, audio mining etc are also established as individual branch of studies and thus can be easy for readers with a specific interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data mining techniques</head><p>Data mining techniques on audio, video, text or image data are generally used to achieve two kinds of tasks (1) Descriptive Mining characterizes the general properties of the data in the database, and (2) Predictive Mining performs inference on the current data in order to make predictions. The following sections are organized by the modality type and mining stages for each of them. Each modality basically uses classification, clustering, association, time-series or visualization techniques. We provide an introduction to these basic data mining techniques to get a better understanding of the content in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Mining frequent patterns, associations and correlations</head><p>Frequent patterns are simply the patterns that appear in the dataset frequently. We use these data mining techniques to accomplish the task of (1) Finding frequent itemsets from large multimedia datasets, where an itemset is a set of items that occur together. (2) Mining association rules in multilevel and high dimensional multimedia data. (3) Finding the most interesting association rules etc. Following the original definition by Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> the problem of association rule mining is defined as: Let I = {i 1 , i 2 , ..., i n } be a set of n binary attributes called items. Let T be a database of transactions that contains a set of items such that T âŠ† I. Let D = {t 1 , t 2 , ..., t m } be a set of transactions called the transactional database. Each transaction in D has a unique transaction ID and contains a subset of the items in I. A rule is defined as an implication of the form X â‡’ Y where X, Y âŠ† I and X âˆ© Y = Ã˜. To select interesting rules from the set of all possible rules, constraints on various measures of significance and interest can be used. The best-known constraints are minimum thresholds on support and confidence. The support supp(X) of an itemset X is defined as the proportion of transactions in the data set which contain the itemset. Confidence can be interpreted as an estimate of the probability P(Y | X), the probability of finding the right hand side of the rule in transactions under the condition that these transactions also contain the left hand side <ref type="bibr" target="#b59">[60]</ref>.</p><p>In many cases, the association rule mining algorithms generate an extremely large number of association rules, often running into thousands or even millions. Further, the association rules are sometimes very large. It is nearly impossible for the endusers to comprehend or validate such large number of complex association rules, thereby limiting the usefulness of the data mining results. Several strategies have been proposed to reduce the number of association rules, such as generating only interesting rules, generating only nonredundant rules, or generating only those rules satisfying certain other criteria such as coverage, leverage, lift or strength <ref type="bibr" target="#b67">[68]</ref>.</p><p>The A-priori and FP-Tree are well known algorithms for association rule mining. A-priori has more efficient candidate generation process. However there are two bottlenecks of the A-priori algorithm. Firstly, the complex candidate generation process that uses a lot of the time, space and memory. Another disadvantage is the multiple scans of the database. Based on the A-priori algorithm, many new algorithms were designed with some modifications or improvements. FP-Tree <ref type="bibr" target="#b49">[50]</ref>, frequent pattern mining, is another milestone in the development of association rule mining, which removes the main bottlenecks of the A-priori algorithm. The frequent itemsets are generated with only two passes over the database and without any candidate generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Classif ication</head><p>Classification can be used to extract models describing important data classes or to predict categorical labels <ref type="bibr" target="#b21">[22]</ref>. Such analysis can help provide us with a better understanding of the data at large. Classification is a two step process. In the first step, a classifier is built describing a predetermined set of data classes called the learning step ( or training phase). Here we learn a mapping or a function, y = f(X), that can predict the associated class label y of a given data X. This mapping is represented in the form of classification rules, decision trees, or mathematical formulae. In the second step, the learned model is used for classification on test tuples. The accuracy of classifier is the percentage of test set tuples that are correctly classified by classifier. Most popular classification methods are decision tree, Bayesian classifier, support vector machines and k-nearest-neighbors. The other well known methods are Bayesian belief networks, rule based classifier, neural network technique, genetic algorithms, rough sets and fuzzy logic techniques etc. The basic issues that need to be taken care during classification are (1) Removing or reducing noisy data, irrelevant attributes and effect of missing values for learning classifier. (2) Selection of distance function and data transformation for suitable representation is also important.</p><p>A decision tree is a predictive model, that is a mapping from observations about an item to conclusions about its target value. Among ID3, C4.5 and CART decision tree algorithms, the C4.5 algorithm <ref type="bibr" target="#b109">[110]</ref> is the benchmark against which new classification algorithms are often compared. A naive Bayesian classifier based on Bayes theorem works well when applied to large databases. To overcome the weak assumption of class conditional independence of naive Bayes classifier, the Bayesian belief network is used when required. Probably one of the most widely used classifier is support vector machines. They can do both linear and nonlinear classification. Another easy to implement but slow classifier is the k-nearest neighbor classifier. These are the classifiers widely used in application though, in literature we can find many other classifiers too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Clustering</head><p>Clustering is the process of grouping the data into classes or clusters, so that objects within a cluster have high similarity in comparison to one another but very dissimilar to objects in other cluster <ref type="bibr" target="#b48">[49]</ref>. The clustering techniques can be organized as partitioning, hierarchical, density based, grid based and model based methods. Clustering is sometime biased as one can get only round-shaped clusters and also the scalability is an issue. Using Euclidean or Manhattan distance measures tends to find spherical clusters with similar size and density, but clusters could be of any shape. Some clustering methods are sensitive to order of input data and sometime can not incorporate newly inserted data. The clustering results interpretability and usability is an important issue. High dimensionality of data, noise and missing values are also problems for clustering. K-means <ref type="bibr" target="#b4">[5]</ref> clustering is one of the popular clustering technique based on the partitioning method. Chameleon and BIRCH <ref type="bibr" target="#b157">[159]</ref> are good hierarchical clustering methods. DBSCAN <ref type="bibr" target="#b24">[25]</ref> is a density based clustering method. Wavelet transform based clustering WaveCluster <ref type="bibr" target="#b122">[124]</ref> is a grid based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Time-series and sequence pattern mining</head><p>A time series database consists of sequences of values or events obtained over repeated measurements in time. Time-series database is also a sequence database. Multimedia data like video, audio are such time-series data. The main tasks to be performed on time-series data is to find correlation relationship within time-series, finding patterns, trends, bursts and outliers. Time series analysis has quite a long history. Techniques for statistical modelling and spectral analysis of real or complexvalued time series have been in use for more than fifty years <ref type="bibr" target="#b8">[9]</ref>. The sequence classification (finding patterns) applications have seen the use of both pattern based as well as model-based methods. In a typical pattern-based method, prototype feature sequences are available for each class (e.g., for each word, gesture etc.). The classifier then searches over the space of all prototypes, for the one that is closest (or most similar) to the feature sequence of the new pattern. Typically, the prototypes and the given features vector sequences are of different lengths. Thus, in order to score each prototype sequence against the given pattern, sequence aligning methods like Dynamic Time Warping are needed. Time warping methods have been used for sequence classification and matching <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69]</ref>. Another popular class of sequence recognition techniques is a model-based method that use Hidden Markov Models (HMMs) <ref type="bibr">[111]</ref>. Another class of approaches to discovering temporal patterns in sequential data is the frequent episode discovery framework <ref type="bibr" target="#b85">[86]</ref>. In the sequential patterns framework, we are given a collection of sequences and the task is to discover (ordered) sequences of items (i.e., sequential patterns) that occur in sufficiently many of those sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Visualization</head><p>Data visualization helps user understand what is going on <ref type="bibr" target="#b48">[49]</ref>. Data mining involves extracting hidden information from a database, thus understanding process can get somewhat complicated. Since the user does not know beforehand what the data mining process has discovered, it takes significant effort to take the output of the system and translate it into an actionable solution to a problem. There are usually many ways to graphically represent a model, the visualizations used should be chosen to maximize the value to the viewer. This requires that we understand the viewer's needs and design the visualization with that end-user in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image mining</head><p>Image mining deals with the extraction of implicit knowledge, image data relationship, or other patterns not explicitly stored in the images. Image mining is more than just an extension of data mining to image domain. The fundamental challenge in image mining is to determine how low-level pixel representation, contained in a raw image or image sequence, can be efficiently and effectively processed to identify high-level spatial objects and relationships. For example, many photographs of various painting have been captured and stored as digital images. These images, once mined, may reveal interesting patterns that could shed some light on the painters and artistic genres.</p><p>Clearly, image mining is different from low-level computer vision and image processing techniques. The focus of image mining is in the extraction of patterns from a large collection of images, whereas the focus of computer vision and image processing techniques is in understanding and/or extracting specific features from a single image. While there seems to be some overlap between image mining and content-based retrieval (since both deals with large collection of images), image mining goes beyond the problem of retrieving relevant images. In image mining, the goal is the discovery of image patterns that are significant in a given collection of images and the related alphanumeric data <ref type="bibr" target="#b145">[147]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Preprocessing</head><p>In image data, the spatial segmentation can be done at region and/or edge level based on the requirements of the application. It can be automatic or with manual intervention and should be approximate enough to yield features that can reasonably capture the image content. In many image mining applications, therefore, the segmentation step often involves simple blob extraction or image partitioning into fixed size rectangular blocks <ref type="bibr" target="#b103">[104]</ref>. In some of the image mining applications like medical image mining noise from the image is removed. For example, the cropping operation can be performed to remove the background, and image enhancement can be done to increase the dynamic range of chosen features so that they can be detected easily <ref type="bibr" target="#b110">[112]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Feature extraction and transformation</head><p>Color, edges, shape, and texture are the common image attributes that are used to extract features for mining. Feature extraction based on these attributes may be performed at the global or local level.</p><p>Color histogram of an image may be obtained at a global level or several localized histograms may be used as features to characterize the spatial distribution of color in an image. Here one can choose RGB or HSV any suitable color space for feature extraction. Apart from the choice of color space, histograms are sensitive to the number of bins and position of bin boundaries. They also do not include any spatial information of colors. Swain and Ballard <ref type="bibr" target="#b133">[135]</ref> proposed color histogram intersection for matching purposes. Color moments have been proposed in <ref type="bibr" target="#b132">[134]</ref> as a more compact representation. Color sets as an approximation of the color histogram proposed in <ref type="bibr" target="#b128">[130]</ref> are also an improvement over the global histogram, as it provides regional color information. The shape of a segmented region may be represented as a feature vector of Fourier descriptors to capture global shape property of the segmented region or a shape could be described in terms of salient points or segments to provide localized descriptions.</p><p>There are obvious trade-offs between global and local descriptors. Global descriptors are generally easy to compute, provide a compact representation, and are less prone to segmentation errors. However, such descriptors may fail to uncover subtle patterns or changes in shape because global descriptors tend to integrate the underlying information. Local descriptors, on the other hand, tend to generate more elaborate representation and can yield useful results even when part of the underlying attribute, for example, the shape of a region is occluded, is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image mining techniques</head><p>In image mining, the patterns types are very diverse. It could be classification patterns, description patterns, correlation patterns, temporal patterns, and spatial patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Classif ication</head><p>Intelligently classifying image by content is an important way to mine valuable information from large image collection. There are two major types of classifiers, the parametric classifier and non-parametric classifier. MM-Classifier, the classification module embedded in the MultiMedia Miner developed by <ref type="bibr" target="#b152">[154]</ref>, classifies multimedia data, including images, based on some provided class labels. Wang and Li <ref type="bibr" target="#b141">[143]</ref> propose IBCOW (Image-based Classification of Objectionable Websites) to classify whether a website is objectionable or benign based on image content. Vailaya et al. <ref type="bibr" target="#b138">[140]</ref> uses binary Bayesian classifier to attempt to perform hierarchical classification of vacation images into indoor and outdoor categories. An unsupervised retraining technique for a maximum likelihood (ML) classifier is presented to allow the existing statistical parameter to be updated whenever a new image lacking the corresponding training set has to be analyzed. Gaussian mixture model (GMM) approach uses GMMs to approximate the class distributions of image data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b138">140,</ref><ref type="bibr" target="#b155">157]</ref>. The major advantage of the GMM-based approach is that prior knowledge can be incorporated for learning more reliable concept models. Due to the diversity and richness of image contents, GMM models may contain hundreds of parameters in a high-dimensional feature space, and thus large amount of labeled images are needed to achieve reliable concept learning. The support vector machines (SVM) based approach uses SVMs to maximize the margins between the positive images and the negative images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b135">137,</ref><ref type="bibr" target="#b139">141]</ref>. The SVM-based approach is known by its smaller generalization error rate in high-dimensional feature space. However, searching the optimal model parameters (e.g., SVM parameters) is computationally expensive, and its performance is very sensitive to the adequate choices of kernel functions. Fan et al. <ref type="bibr" target="#b28">[29]</ref> has mined multilevel image semantics using salient objects and concept ontology for hierarchical classification for images. Previous works on hierarchical image classification are <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b60">61]</ref>, where MediaNet <ref type="bibr" target="#b7">[8]</ref> has been developed to represent the contextual relationships between image/video concepts and achieve hierarchical concept organization.</p><p>For web image mining, <ref type="bibr" target="#b158">[160]</ref> the problem of image classification is formulated as the calculation of the distance measure between training manifold (learned from training images) and test manifold (learned from test images). Classifying images with complex scenes is still a challenging task owing to their variability, ambiguity, and the wide range of illumination and scale conditions that may apply. Such image classification methods are extended for image annotation with the goal of obtaining greater semantic understanding of images. Automatic image annotation systems take advantage of annotated images to link the visual and textual modalities by using data mining techniques. We will discuss such techniques in the multimodal data mining section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Clustering</head><p>In unsupervised classification (or image clustering), the problem is to group a given collection of unlabeled images into meaningful clusters according to the image content without a priori knowledge <ref type="bibr" target="#b62">[63]</ref>. Chang et al. <ref type="bibr" target="#b11">[12]</ref> uses clustering technique in an attempt to detect unauthorized image copying on the World Wide Web. Jain et al. <ref type="bibr" target="#b62">[63]</ref> uses clustering in a preprocessing stage to identify pattern classes for subsequent supervised classification. They also described a partition based clustering algorithm and manual labeling technique to identify classes of a human head obtained at five different image channels (a five dimensional feature vector).</p><p>In <ref type="bibr" target="#b140">[142]</ref> image segmentation is treated as graph partitioning problem and solved using minimum spanning tree based clustering algorithm. Automated image mining is necessary in several applications. In <ref type="bibr" target="#b43">[44]</ref> the problem to apply appropriate clustering and classification to different types of images, and to decide on such processing automatically is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Association</head><p>Ordonez and Omiecinski <ref type="bibr" target="#b100">[101]</ref> present an image mining algorithm using blob needed to perform the mining of associations within the context of images. A prototype has been developed called Multimedia Miner <ref type="bibr" target="#b152">[154]</ref> where one of its major modules is called MM-Associator. In another application association rule mining used to discover associations between structures and functions of human brain <ref type="bibr" target="#b90">[91]</ref>. In <ref type="bibr" target="#b110">[112]</ref> association rule mining technique is used to classify the CT scan brain images into three categories namely normal, benign and malign. In <ref type="bibr" target="#b112">[114]</ref> association rules relating low-level image features to high-level knowledge about the images are discovered to select the most relevant features for application in medical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Visualization</head><p>Fan et al. <ref type="bibr" target="#b28">[29]</ref> has proposed new algorithm to achieve hyperbolic visualization of large scale concept ontology and images resulting from hierarchical image classification. Zaiane et al. <ref type="bibr" target="#b152">[154]</ref> uses 3-dimensional visualization to explicitly display the associations. Though we have seen the diverse applications of image mining in literature, the effectiveness of image data mining can be appreciated only when we associate semantic information with images like annotations etc. As can be seen later in the multimodal data mining section, images with text annotations can be very effective for learning semantic level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Discussion</head><p>Some of the common issues of image mining are summarized as shown in the Table <ref type="table" target="#tab_1">1</ref>. Image mining applications started with use of data mining methods on low-level image features. Later on, the limitation of the approach was realized. Firstly, this approach is normally used to classify or cluster only a small number of image Color histograms <ref type="bibr" target="#b133">[135]</ref>, color Sensitive to the parameters e.g., extraction and moments <ref type="bibr" target="#b132">[134]</ref>, color sets <ref type="bibr" target="#b128">[130]</ref>, number of bins, bin boundaries, transformation shape descriptors, texture selection of feature vectors descriptors, edges (Fourier etc.) Image classification GMM <ref type="bibr" target="#b29">[30]</ref>, SVM <ref type="bibr" target="#b139">[141]</ref>, Bayesian Large training data needed for classifier <ref type="bibr" target="#b138">[140]</ref> to classify image GMM, SVM kernel functions, etc. Image clustering K-means in preprocessing stage Unknown number of clusters to identify patterns <ref type="bibr" target="#b62">[63]</ref> Image association A-priori based association Scalability issue in terms of between structures and functions number of candidate patterns of human brain <ref type="bibr" target="#b90">[91]</ref> generated categories (indoor vs. outdoor, textured vs. non-textured, or city vs. landscape, etc). Secondly, it is often difficult to generalize these methods using low-level visual features to additional image data beyond the training set. Finally, this approach lacks of an intermediate semantic image description that can be extremely valuable in determining the image type. The modeling of images by a semantic intermediate representation was next proposed in order to reduce the gap between low-level and high-level image processing. One way is to detect objects in an image using object classifiers, and then the image is represented by the occurrence of these semantic objects. Another way is to represent semantics of an image by visual word distribution where intermediate properties are extracted from local descriptors.</p><p>The intermediate semantic modeling provides a potentially larger amount of information that must be exploited to achieve a higher performance for image mining. However, it needs to tackle the problem of uncertainty/inaccuracy involved in a local/ region processing and object recognition. Earlier image mining was limited to the specific set of images as personal photo album or CT scan image as dataset, whereas the recent approaches are considering web scale images as dataset. Thus, recent research is focuses on developing image mining frameworks that can handle different types of images at large scale automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Video mining</head><p>Video mining is a process which can not only automatically extract content and structure of video, features of moving objects, spatial or temporal correlations of those features, but also discover patterns of video structure, objects activities, video events, etc. from vast amounts of video data without little assumption about their contents. By using video mining techniques, video summarization, classification, retrieval, abnormal events alarm and other smart video applications can be implemented <ref type="bibr" target="#b16">[17]</ref>. Video mining is not only a content based process but also aims to obtain semantic patterns. While pattern recognition focusses on classifying special samples with an existing model, video mining tries to discover rules and patterns of samples with or without image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Preprocessing</head><p>To apply existing data mining techniques on video data, one of the most important steps is to transform video from non-relational data into a relational data set. Video as a whole is very large data to mine. Thus we need some preprocessing to get data in the suitable format for mining. Video data is composed of spatial, temporal and optionally audio features. All these features can be used to mine based on applications requirement. Commonly, video is hierarchically constructed of frames (key-frames), shots (segments), scenes, clips and full length video. Every hierarchical unit has its own features which are useful for pattern mining. For example, from frames we can get features like objects, their spatial positions etc whereas from shots we may be able to get the features like trajectories of object and their motion etc. The features among some hierarchical units also can be used for mining. Now based on the requirement of the application and structure of video, we can decide the preprocessing step to extract either frames or shots or scenes or clips from video. For example the spatiotemporal segmentation can involve breaking the video into coherent collections of frames that can be processed for feature extraction as a single unit. This is typically done via a shot detection algorithm wherein the successive video frames are compared to determine discontinuity along the time axis.</p><p>Structure of the video such as edited video sequences and raw video sequences influence feature extraction process. For surveillance video like raw video sequences first step is to group input frames to a set of basic units call segment <ref type="bibr" target="#b61">[62]</ref>. While for sports video like edited video sequences shot identification is the first step <ref type="bibr" target="#b159">[161]</ref>. Hwan et al. <ref type="bibr" target="#b61">[62]</ref> proposed multimedia data mining framework for raw video sequences, where segmentation is done using hierarchical clustering on motion features. The common preprocessing steps are extracting the background frame, quantizing color space to reduce noise, calculating the difference between the background frame and new frames, categorizing frames based on the difference values obtained using some threshold values to decide each category. These common steps can be configured based on requirements, like instead of color we want to use some other feature or we may decide to consider the difference between two consecutive frames instead of background frame etc. After categorizing of frames we can use these category labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Feature extraction and transformation</head><p>Color, edges, shape, and texture are the low level attributes that are used to extract higher level features like motion, objects etc for video mining from each frames or shot or segment. In addition to these features, attributes resulting from object and camera motion can also be used for video mining purpose. The qualitative camera motion extraction method proposed in <ref type="bibr" target="#b159">[161]</ref> uses motion vectors from p-frames to characterize camera motions. We can categorize the video in three different types (1) Raw video sequences e.g., surveillance video, they are neither scripted nor constrained by rules (2) Edited video e.g., drama, news etc, are well structured but with intra-genre variation in production styles that vary from country to country or content creator to content creator. (3) Sports video are not scripted but constrained by rules. We do not cover medical videos (ultra sound videos including echocardiogram) in our survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Video mining techniques</head><p>There are two levels in video mining. One is semantic information mining which is directly at the feature level: e.g., the occurrence of primitive events such as person X is talking. Another is patterns and knowledge mining which is at a higher level. Patterns may represent cross-event relations: e.g., person X talks often with person Y during office hours. Video mining techniques are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Classif ication</head><p>According to histogram of shot, motion features of moving objects, or other semantic descriptions, video classification mining approaches classify video objects into predefined categories. So, the semantic descriptions or the features of every category can be used to mine the implicit patterns among video objects in that category.</p><p>In gesture (or human body motion) recognition, video sequences containing hand or head gestures are classified according to the actions they represent or the messages they seek to convey. The gestures or body motions may represent, e.g., one of a fixed set of messages like waving hello, goodbye, and so on <ref type="bibr" target="#b17">[18]</ref>, or they could be the different strokes in a tennis video <ref type="bibr" target="#b147">[149]</ref>, or in other cases, they could belong to the dictionary of some sign language <ref type="bibr" target="#b147">[149]</ref> etc. Aradhye et al. <ref type="bibr" target="#b3">[4]</ref> developed a scheme to iteratively expand the vocabulary of learned concepts by using classifiers learned in prior iterations to learn composite, complex concepts. The semantic descriptive capacity of their approach is thus bounded not by the intuition of a single designer but by the collective, multilingual vocabulary of hundreds of millions of web users who tagged and uploaded the videos. Associative classification (AC) approach in <ref type="bibr" target="#b79">[80]</ref> generates the classification rules based on the correlation between different featurevalue pairs and the concept classes by using Multiple Correspondence Analysis (MCA). Rough set theory based approach is used in <ref type="bibr" target="#b126">[128]</ref> to extract multiple definitions of the event for learning. It overcomes the problem of wrongly selecting interesting event as negative examples for training and appropriately measure similarities in a high dimensional feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Clustering</head><p>The clustering approaches organize similar video objects by their features into clusters. So, the features of every cluster can be used to evaluate the video shots that contain those video objects. Those clusters can be labeled or visualized. Clustering is also very important in preprocessing stage for removing noise and doing transformation. As reported in <ref type="bibr" target="#b99">[100]</ref>, they use clustering to segment the raw video sequences. The pioneering application in video clustering is clustering of shots <ref type="bibr" target="#b150">[152,</ref><ref type="bibr" target="#b151">153]</ref>, and clustering of still images (keyframes) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b104">105]</ref> and then using it for efficient indexing, searching and viewing. Zhang and Zhong <ref type="bibr" target="#b154">[156]</ref> proposed a hierarchical clustering of keyframes based on multiple image features for improving the search efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.6">Association</head><p>The data of extracted features of video objects can be constructed into structural data stored in database. So, conventional association rule mining algorithms can be used to mine the associational patterns. For example, discovering two video objects that always occur simultaneously or getting the association information that important news are often very long in almost all over the world. In <ref type="bibr" target="#b125">[127]</ref> they extract meaningful patterns from extremely large search space of possible sequential patterns by imposing temporal constraints and using modified a-priori based association rule. Their strength was not introducing any rules based on domain knowledge. Their drawback was expecting user given semantic event boundary and temporal distance threshold parameter.</p><p>Association rule mining (ARM) has been used in <ref type="bibr" target="#b78">[79]</ref> to automatically detect the high-level features (concepts) from the video data, in an attempt to address the challenges such as bridging the semantic gap between the high-level concepts and low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.7">Time-series</head><p>Obviously, video consists of sequential frames with temporal features of video such as motion features. Techniques of time series analysis and sequence analysis each be used to mine the temporal patterns of temporal features, activities trends of moving objects and events, which are interesting and useful to some applications. E.g., <ref type="bibr" target="#b14">[15]</ref> found the trends of traffic crowd through trend mining by spatial temporal analysis of video objects. DTW and its variants are being used for motion time series matching <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b118">120]</ref> in video sequence mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.8">Visualization</head><p>Context-based whole video retrieval system MediaMatrix was proposed in <ref type="bibr" target="#b70">[71]</ref>. This system analyzes the color-emotions hidden within video content using methods developed for color psychology research. A visualization of the prominence of coloremotions and the extracted prominent scenes was done to find the similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.9">Discussion</head><p>The traditional video mining approach uses a vocabulary of labels to learn and search for features and classifiers that can best distinguish these labels. This approach is inherently limited by the hand-selected label set. The classifiers by definition can only learn concepts within this vocabulary of labels. It cannot scale well for diverse and enormous multimedia dataset of web scale. Thus, future research will be more focused on finding auto annotation techniques for such large scale multimedia data. Some of the common issues of video mining are summarized in the Table <ref type="table" target="#tab_2">2</ref>. Of course, feasible video mining algorithms are not limited within above mentioned types for a concrete application. On one hand, it is not possible to find a universal algorithm for a special type of video. What features are used to mine depend on the video content, and which type of mining algorithms are used to mine depend on the needs of the application. On the other hand, it is possible to have several approaches that satisfy the needs of the same application. Thus, a video mining framework that can handle these differences are essential.</p><p>There are many different applications of video mining covered in literature. But still we see the biggest limitation in term of size of data set on which such mining can be done. As video data itself is very voluminous, there are no works where many large videos are considered. Video data mining uses features from audio and caption Clustering of shots <ref type="bibr" target="#b150">[152,</ref><ref type="bibr" target="#b151">153]</ref>, segments Scalability for large video <ref type="bibr" target="#b99">[100]</ref> for video, for indexing etc.</p><p>clusters is an issue Video association A-priori based association finding Finding semantic event sequence of events in movies <ref type="bibr" target="#b125">[127]</ref> boundaries and temporal distance thresholds text also to discover higher semantic level knowledge. Most of the video mining literature uses all modalities and thus we have described them in the multimodal datamining section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Audio mining</head><p>In the past, companies had to create and manually analyze written transcripts of audio content because using computers to recognize, interpret, and analyze digitized speech was difficult. Audio mining can be use to analyze customer-service conversations, to analyze intercepted phone conversations, to analyze news broadcasts to find coverage of clients or to quickly retrieve information from old data. US prison is using ScanSoft's audio mining product to analyze recordings of prisoners phone calls to identify illegal activity <ref type="bibr" target="#b71">[72]</ref>. Audio data in general is mixture of music, speech, silence and noise. Some of the mining steps for music and speech may vary. In the following subsections we will try to understand the general steps for audio mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Preprocessing</head><p>Audio data can be processed either at the phoneme or word level or the data are broken into windows of fixed size. Dividing into windows of fixed size depends on application requirements and available data size. The text based approach also known as large-vocabulary continuous speech recognition (LVCSR), converts speech to text and then identifies words in a dictionary that can contain several hundred thousand entries. If a word or name is not in the dictionary, the LVCSR system will choose the most similar word it can find. The phoneme based approach does not convert speech to text but instead works only with sounds. Based on the application requirement one of the approach is chosen, for example phoneme based approach is more useful when dealing with foreign terms and names of people and places. There are also some applications where you may want to first segment out the silence, music, speech and noise from the source audio for further processing <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b116">118]</ref>. A method was presented to separate speech from music by tracking the change of the zero crossing rate <ref type="bibr" target="#b117">[119]</ref>. In Acoustic Speech Recognition systems, one of the commonly encountered problems is the mismatch between training and application conditions. Solutions to this problem are provided as pre-processing of the speech signal for enhancement, noise resistant feature extraction schemes and statistical adaptation of models to accommodate application conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Feature extraction and transformation</head><p>In the case of audio, both the temporal and the spectral domain features have been employed. Examples of some of the features used include short-time energy, pause rate, zero-crossing rate, normalized harmonicity, fundamental frequency, frequency spectrum, bandwidth, spectral centroid, spectral roll-off frequency, and band energy ratio. Many researchers have found the cepstral-based features, melfrequency cepstral coefficients (MFCC), and linear predictive coefficients (LPC), very useful, especially in mining tasks involving speech recognition. As far as feature extraction is concerned, the main research areas cannot be easily classified in completely distinct categories, since the cross-fertilization of ideas has triggered approaches that combine ideas from various fields. Filterbank analysis is an inherent component of many techniques for robust feature extraction. It is inspired by the physiological processing of speech sounds in separate frequency bands that is performed by the auditory system. Auditory processing has developed into a separate research field and has been the origin of important ideas, related to physiologically and perceptually inspired features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b120">122]</ref>. Mel Frequency Cepstral Coefficients (MFCC) are the most commonly used feature set for ASR applications. They were introduced by Davis and Mermelstein <ref type="bibr" target="#b18">[19]</ref>. The wide-spread use of the MFCC is due to the low complexity of the estimation algorithm and their efficiency in ASR tasks. Subband Spectral Centroids features have been introduced by Paliwal et al. <ref type="bibr" target="#b36">[37]</ref>. They can be considered as histograms of the spectrum energies distributed among nonlinearly-placed bins.</p><p>Equally important is the research field based on concepts relevant to speech resonance (short-term) modulations. Both physical observations and theoretical advances support the existence of modulations during speech production. The Frequency Modulation Percentages (FMP) are the ratio of the second over the first moment of these signals <ref type="bibr" target="#b20">[21]</ref>. These spectral moments have been tested as input feature sets for various ASR tasks yielding improved results. The Dynamic Cepstral Coefficients method <ref type="bibr" target="#b35">[36]</ref> attempts to incorporate long-term temporal information. In Relative Spectral Processing (RASTA) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> the modulation frequency components that do not belong to the range from 1 to 12 Hz are filtered out. Thus, this method suppresses the slowly varying convolutive distortions and attenuates the spectral components that vary more rapidly than the typical rate of change of speech. Temporal Patterns (TRAP) method was introduced in <ref type="bibr" target="#b58">[59]</ref>. The TRAP features describe likelihoods of sub-word classes at a given time instant, derived from temporal trajectories of band-limited spectral densities in the vicinity of the given time instant.</p><p>The human auditory system is a biological apparatus with excellent performance, especially in noisy environments. The adaption of physiologically based methods for spectral analysis <ref type="bibr" target="#b40">[41]</ref> is such an approach. The Ensemble Interval Histogram (EIH) model is constructed by a bank of cochlear filters followed by an array of level crossing detectors that model the motion to neural conversion. The Joint Synchrony/Mean-Rate model <ref type="bibr" target="#b119">[121,</ref><ref type="bibr" target="#b120">122]</ref> captures the essential features extracted by the cochlea in response to sound pressure waves. Perceptual linear prediction (PLP) is a variant of Linear Prediction Coding (LPC) which incorporates auditory peripheral knowledge <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>One of the latest approaches in speech analysis are the nonlinear/fractal methods. These diverge from the standard linear source-filter approach in order to explore nonlinear characteristics of the speech production system. Difference equation, oscillator and prediction nonlinear models were among the early works in the area <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b136">138]</ref>. Speech processing techniques that have been inspired by fractals have been introduced in <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b87">88]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Audio mining techniques</head><p>Tackling automatic content analysis of audio data and finding interesting knowledge is of major importance. Audio mining aims to detect important events or classes or clusters from large audio dataset to discover the hidden knowledge in it. In the following subsection we will see the algorithms and the applications of audio mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Classif ication</head><p>Audio cues, either alone or integrated with information extracted from other modalities, may contribute significantly to the overall semantic interpretation of data. Event detection in audio streams is an aspect of the aforementioned analysis. Event detection in audio streams aims at delineating audio as a sequence of homogeneous parts each one identified as a member of a predefined audio class. Determination of such audio classes (e.g., speech, music, silence, laughter, noisy speech etc.) is the first step in the design of an event detection algorithm. From the derived features decision is made about the content of the frame. An important statistical theoretic framework has been developed in <ref type="bibr" target="#b129">[131]</ref>.</p><p>The HMMs retain time information about the probability distribution of the frame features. The probability distribution functions of the features are modeled usually as mixtures Gaussian distributions. The decision is based on the simple Bayes rule <ref type="bibr" target="#b23">[24]</ref>. Spectrum transformation based on the properties of the human cochlea <ref type="bibr" target="#b92">[93]</ref>. Nonlinear transformation of the spectrum <ref type="bibr" target="#b39">[40]</ref>. Rule-based methods follow a hierarchical heuristic scheme to achieve classification. Based on the properties of the various audio classes in the feature space, simple rules are devised and form a decision tree aiming at the proper classification of the audio segments <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b156">158]</ref>. These methods usually lack robustness because they are threshold dependent, but no training phase is necessary and they can work in real-time.</p><p>In most of the model-based methods, segmentation and classification are performed at the same time. Models such as Gaussian Mixture Models and Hidden Markov Models are trained for each audio class and classification is achieved by Maximum Likelihood or Maximum a Posteriori selection over a sliding window <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b143">145]</ref>. These methods may yield quite good results but they cannot easily generalize, they do not work in real-time, since they usually involve a number of iterations, and data is needed for training. Classical pattern analysis techniques cope with the classification issue as a case of pattern recognition. So, various well known methods of this area are applied, such as neural networks and Nearest Neighbor (NN) methods. El-Maleh et al. <ref type="bibr" target="#b22">[23]</ref> apply either a quadratic Gaussian classifier or an NN classifier. In <ref type="bibr" target="#b121">[123]</ref> a multilayer perceptron combined with a genetic algorithm to achieve 16-class classification. A tree structure quantizer is used for speech/music classification <ref type="bibr" target="#b31">[32]</ref>. More modern approaches have also been tested like Nearest Feature Line Method <ref type="bibr" target="#b74">[75]</ref> which performs better than simple NN approaches, and Support Vector Machines <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b95">96]</ref>.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, a statistical manifold based classification approach is used to identify species of birds present in an audio recording. Rather than averaging frame-level features they considered codebook of clustered frame-level features for nearest neighbor based classification. A Multi-Layer Support Vector Machine approach for emotion recognition is proposed in judicial domain <ref type="bibr" target="#b30">[31]</ref>. They overcome the limitation that the inference model are influenced by language, gender and similar emotional states using hierarchical classification. A real world system WAPS (Web Audio Program Surveillance) was developed in <ref type="bibr" target="#b37">[38]</ref> to spot Chinese keywords from audio programs on the web and construct a surveillance system. WAPS has challenges to handle the high processing ability, live feature of audio data, heterogeneity of the audio data and high precision. Steganography is explored well for digital images but in <ref type="bibr" target="#b84">[85]</ref> SVM is used to discriminate unadulterated carrier signals and the steganograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">Clustering</head><p>The goal of speaker clustering is to identify and group together all speech segments that were uttered by the same speaker. In <ref type="bibr" target="#b91">[92]</ref>, speech segments with different gender classification are clustered separately. Deterministic methods cluster together similar audio segments. An on-line hierarchical speaker clustering algorithm was proposed in <ref type="bibr" target="#b76">[77]</ref>. Each segment is considered to belong exclusively to one speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.6">Discussion</head><p>In early developments analysis was based only on the current frame. Later, the decision algorithms evolved a memory which relates the prediction of the current frame with the previous frames, to become adaptive. Some of these techniques, update a simple threshold, or adapt continuously the parameters in the Hidden Markov Model (HMMs) <ref type="bibr" target="#b160">[162]</ref>. Also recent audio mining techniques consider codebooks of clustered frame-level features, where features are from more than one frames say bag of frames (BOF). As shown in the Table <ref type="table" target="#tab_3">3</ref> below there are several issues arising in the audio mining. Though the association rules were not explored much in audio data mining, there is a recent work <ref type="bibr" target="#b134">[136]</ref> using association rules for classification of multi-class audio data. To overcome combinatorial search problem of finding rules from large datasets, they considered closed itemset mining to extract non-redundant and condensed patterns. Thus, search space for classification rules is dramatically reduced. In future more work needs to explore association rules for audio data mining. Also visualization techniques have not been explored for audio data mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Text mining</head><p>Text mining refers generally to the process of extracting interesting information and knowledge from unstructured or semi structured text. The text data in multimedia data mining can be coming from many different kind of modalities. It can be thought of as text data produced from ASR (Automatic Speech Recognition) or from the Web pages or from some data record files. For mining large document collections it is necessary to pre-process the text documents and store the information in a data structure, which is more appropriate for further processing than a plain text file. The currently predominant approaches based on this idea are the vector space model <ref type="bibr" target="#b115">[117]</ref>, the probabilistic model <ref type="bibr" target="#b114">[116]</ref> and the logical model <ref type="bibr" target="#b113">[115]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Preprocessing</head><p>In order to obtain all words that are used in a given text, a tokenization process is required, i.e., a text document is split into a stream of words by removing all punctuation marks and by replacing tabs and other non-text characters by single white spaces. The set of different words obtained by merging all text documents of a collection is called the dictionary of a document collection. To reduce the size of the dictionary, filtering and lemmatization <ref type="bibr" target="#b33">[34]</ref> or stemming <ref type="bibr" target="#b107">[108]</ref> methods are used.</p><p>To further decrease the number of words that should be used, indexing or keyword selection algorithms can also be used. In this case, only the selected keywords are used to describe the documents. A simple method for keyword selection is to extract keywords based on their entropy. The entropy can be seen as a measure of the importance of a word in the given domain context. Sometimes additional linguistic preprocessing may be used to enhance the available information about terms. Partof-speech tagging (POS), text chunking, word sense disambiguation and parsing etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Feature extraction and transformation</head><p>In text mining, feature extraction usually refers to identifying the keywords that summarize the contents of the document. One way is to look for words that occur frequently in the document. These words tend to be what the document is about. A good heuristic is to look for words that occur frequently in documents of the same class, but rarely in documents of other classes. In order to cope with documents of different lengths, relative frequency is preferred over absolute frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Text mining techniques</head><p>In a text mining study, a document is generally used as the basic unit of analysis because a single writer commonly writes the entirety of a document and the document discusses a single topic. A document can be a paper, an essay, a paragraph, a web-page text, or a book, depending on the type of analysis being performed and depending upon the goals of the researcher. The goal is to classify, cluster or associate such documents or the structure within the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">Classif ication</head><p>Bayesian approaches, Decision trees and KNN based methods are standard tool in data mining. Decision trees are fast and scalable both in the number of variables and the size of the training set. For text mining, however, they have the drawback that the final decision depends only on relatively few terms. Some of the recent work also suggest that graph based text classification are much more expressive and gives improved classification accuracy than the standard bag of words/phrases approach <ref type="bibr" target="#b63">[64]</ref>. In <ref type="bibr" target="#b73">[74]</ref>, prediction of online forums hotspot was done using SVM based text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.5">Clustering</head><p>One fast text clustering algorithm, which is also able to deal with the large size of the textual data is the Bi-Section-k-means algorithm. In <ref type="bibr" target="#b130">[132]</ref> it was shown that Bi-Section-k-means is a fast and high-quality clustering algorithm for text documents which is frequently outperforming standard k-means as well as agglomerative clustering techniques. Co-clustering algorithm designate the simultaneous clustering of documents and terms <ref type="bibr" target="#b19">[20]</ref>. For knowledge unit pre-order relation mining <ref type="bibr" target="#b83">[84]</ref> in documents are clustered to find documents of similar topics. The pre-order relation exists mainly in knowledge units from the same text document or documents of similar topic. In <ref type="bibr" target="#b32">[33]</ref> it was shown that harmony search based clustering algorithm for web documents outperforms many other methods. In <ref type="bibr" target="#b73">[74]</ref> k-means clustering used to group the forums into various clusters, with the center of each cluster representing a hotspot forum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.6">Association</head><p>Mining of association rules in temporal document collection has been explored in <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98]</ref>, where inter-transaction rules are mined. In later work they considered semantics (concepts instead of words) in the pre-processing phase that could reduce the problems with synonyms, homonyms and dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.7">Visualization</head><p>Various text mining and data visualization tools have been described in the literature for application to patent information <ref type="bibr" target="#b131">[133,</ref><ref type="bibr" target="#b137">139]</ref>. Some of the commercial text mining tools ClearForest, Invention Machine GoldfireInnovator, OmniViz, and TEMIS works best with the unstructured data, such as, full-text patent documents, emails, internal reports, news, journal articles, and web content as given detail survey in <ref type="bibr" target="#b149">[151]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal data mining</head><p>Image mining, video mining and audio mining algorithms all suffer from the semantic gap problem. This bottleneck may be reduced by the multimodal data mining approaches as we will see in the following sections. The described approaches take advantage of the fact that in many applications, image data may co-exist with other modalities of information such as text, similarly video may co exist with audio or text etc. The synergy between different modalities may be exploited to capture the high level conceptual relationships. To exploit the synergy among the multimodal data, the relationships among these different modalities need to be learned. For example, we need to learn the relationship between images and text in annotated image database or video and audio in video database etc. The learned relationship between them can then be further used in multimodal data mining. This is the main motivation for focusing on multimodal data mining separately here. And considering multimodal data mining as a separate part of multimedia data mining where mining is done with cross modality like text and image together or video and audio together or any combination of these different modalities. Multimodal mining differs in many aspects from single modality mining in the ways to combine different features or in generating more semantically meaningful features. Distinguish processing stages of multimodal data mining are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The data analysis granularity level of video can be frame level, shot level, clip level etc., while for image it can be pixel level, grid level, region level etc., for audio it can be phoneme or word level or the data are broken into windows of fixed size. Each of them have semantic meaning within their granularity level of processing unit. Thus, we need to do careful pre-processing for multimodal data to avoid loss of actual semantic meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature extraction and transformation</head><p>The special features of multimodal data mining can be easily seen apart from traditional single modality features of image, audio or video modality. The image annotations can be considered as a very useful feature for cross modal mining for text and image. The subtitles or movie scripts, Optical character recognition (OCR) text label extracted from videos can be very useful feature for cross modal mining of video and text. Extracting speech from audio is semantically very rich. An important issue with features extracted from multimodal data is how the features should be integrated for mining. Most multimodal analysis is usually performed separately on each modality, and the results are brought together at a later stage to arrive at the final decision about the input data. This approach is called late fusion or decision-level fusion. Although this is a simpler approach, we lose valuable information about the multimedia events or objects present in the data because, by processing separately, we discard the inherent associations between different modalities. Another approach for combining features is to represent features from all modalities together as components of a high-dimensional vector for further processing. This approach is known as early fusion. The data mining through this approach is known as cross-modal analysis because such an approach allows the discovery of semantic associations between different modalities <ref type="bibr" target="#b72">[73]</ref>.</p><p>The problem involved in finding such cross-modal correlation discovery is defined as, "Given n multimedia objects, each consisting of m attributes (traditional numerical attributes, or multimedia ones such as text, video, audio, time-sequence, etc). Find correlations across the media (eg., correlated keywords with image blobs/regions; video motion with audio features)" <ref type="bibr" target="#b102">[103]</ref>. Their main motivation was to come up with generic graph based approach to find patterns and cross media correlation for multimedia database. It is one of the first approaches in the area, called Mixed Media Graph MMG. It constructs the graph for associating visual feature from image to their representative keyword and then find the steady state probability for future mapping. If provided with good similarity functions, the approach is very fast and scalable. Their approach has not been explored further. Correlations among different modalities vary based on the content and context thus it is difficult to get generalizable methodologies for generic cross modal correlation discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multimodal data mining techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Classif ication</head><p>Multimodal classifications are mainly used for event/concept detection purposes. Using multiple modalities, events like goal detection from soccer videos <ref type="bibr" target="#b15">[16]</ref> or commercial detection from TV program <ref type="bibr" target="#b41">[42]</ref> or news story segmentation etc. are more successfully classified than using the single modalities alone. There are generic classification issues which also need to be considered for successful multimodal classification.</p><p>-Class-imbalance (or rare event/concept detection): the events/concepts of interests are often infrequent <ref type="bibr" target="#b127">[129]</ref>. -Domain knowledge dependence: For bridging the semantic gap between lowlevel video features and high-level semantic concepts most current research works for event/concept extraction rely heavily on certain artifacts such as domain-knowledge and a priori models, which largely limit their extensibility in handling other application domains and/or video sources <ref type="bibr" target="#b127">[129]</ref>. -Scaling: Some good classifiers like SVM etc do not have capability to scale well as size of training data increases <ref type="bibr" target="#b13">[14]</ref>. It needs to be addressed.</p><p>Research works on multimodal classification problem for event detection is presented in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b127">129]</ref>. In <ref type="bibr" target="#b15">[16]</ref> they perform soccer goal detection with multimodal analysis and decision tree logic. As they found that traditional HMM cannot identify the goal event and has problem in dealing with long video sequences, they decided to go for a decision tree based approach. They follow a three step architecture video parsing, data pre-filtering and data mining. They discover important features from video parsing and based on domain knowledge they derive three rules for data prefiltering to remove the noisy data. This is novel in terms of thinking from class imbalance problem solution perspective. They were able to show that 81% of shots can be reduced by applying these rules. Then C4.5 based decision tree algorithm was used with information gain criteria to recursively determine the most appropriate attribute and to partition the dataset until labels are assigned. In <ref type="bibr" target="#b13">[14]</ref>, by using a pre filtering step with SVM light, they cleaned the dataset in terms of classifying grass and non-grass scenes. They apply the decision tree only on grass scenes as they have a higher rate (5% of data) of goal event than the raw data. They derived new midlevel temporal features apart from raw video and audio features. The distinguishing feature of their work was consideration of cause and effect as assumption that goal event might be caused by two past events and could affect two future events. This assumption was used to capture the interesting goal events within the temporal window of five shots. They proposed the approach in <ref type="bibr" target="#b127">[129]</ref> where they do intelligent integration of distance based and rule based data mining techniques to deal with the problem of semantic gap and class imbalance without using domain knowledge or relying on artifacts. Good experiments comparing the performance between different classification approaches show that the subspace based model is superior than the others. We highlight some very specific classification issues for multimodal data mining below:</p><p>-Multimodal Classifier Fusion Problem: For multimodal data the classifiers can run either on concatenated single long feature vector of multiple modalities or separately on single modality and then combine the result. While the curse of dimensionality does not allow for the first option, the second option needs to apply some well crafted multimodal classifier fusion technique to be successful <ref type="bibr" target="#b81">[82]</ref>. -Multimodal Classifier Synchronization Problem: For example, the speaker identification module needs a longer time frame to make reliable estimates than the face recognition module does, because the latter can make a judgment as soon as a single image is acquired. Consequently, the classification judgments from multimodal classifiers will be fed into the meta-classifier asynchronously, and a method of appropriately synchronizing them is needed <ref type="bibr" target="#b81">[82]</ref>.</p><p>Combining multiple classifiers can improve classification accuracy when the classifiers are not random guessers and complementary to each other. Concatenating the multidimensional features simply does not scale up. Instead of combining features, another approach is to build a classifier on each modality independently, and then to combine their results to make the final decision. Using an ensemble of multimedia classifiers has been explored in <ref type="bibr" target="#b81">[82]</ref> and <ref type="bibr" target="#b80">[81]</ref>, which demonstrated the effectiveness of combining three multimodal classifiers. In <ref type="bibr" target="#b81">[82]</ref> and <ref type="bibr" target="#b80">[81]</ref> they developed framework called "meta classification", which models the problem of combining classifiers as a classification problem itself. They showed the problem formulation of Meta Classification as reclassification of the judgments made by classifiers. It looks like promising approach for dealing with multimodal classifier fusion problem. The results show that it outperforms the traditional ad hoc approaches like majority voting or linear interpolation and probability based framework. Majority voting and linear interpolation methods ignore the relative quality and expertise among classifiers.</p><p>In meta classification, they synthesize asynchronous judgments from multimedia classifiers into a new feature vector, which is then fed into the meta-classifier. Though the authors mentioned that generating long single feature vector is not a good option to choose due to curse of dimensionality, it is not well justified for many applications. Synthesizing feature vectors for generating feature space for meta classifier is not very well explained in terms of how it can bring reliability of each modality for the task at hand. Though the synchronization problem is solved to some extent, negative effects of lack of synchronization have not been explained. Also <ref type="bibr" target="#b82">[83]</ref> is based on SVM meta classifier giving good results. Yan et al. <ref type="bibr" target="#b148">[150]</ref> found that the use of learning a set of query-independent weights to combine features sometimes performed worse than a system that uses text alone, thus highlighting the difficulty of multimodality combination. As different queries have different characteristics, they explore query dependent models for retrieval. They borrow the ideas from text-based question-answering research, a feasible idea is to classify queries into pre-defined classes and develop fusion models by taking advantage of the prior knowledge and characteristics of each query class. Ramachandran et al. <ref type="bibr" target="#b111">[113]</ref> proposed VideoMule, a consensus learning approach to solve the problem of multi label classification for user-generated videos. They train classification and clustering algorithms on textual metadata, audio and video, generated classes and clusters are used for building a multi label tree which in turn mapped to high dimensional belief graph with probability distribution. The probability value propagates from labeled nodes in tree to unlabeled nodes until the graph becomes stable, which then denotes multi label classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Clustering</head><p>Multimodal clustering can be used as an unsupervised approach to learn associations between continuous-valued attributes from different modalities. In <ref type="bibr" target="#b34">[35]</ref> the authors try to search for optimal clusters prototypes and the optimal relevance weight for each feature of each cluster. Instead of learning a weight for each feature, they divide the set of features into logical subsets, and learn a weight for each feature subset. Their approach claims to out-perform state of the art in captioning accuracy.</p><p>In domains where neither perceptual patterns nor semantic concepts have simple structures, unsupervised discovery processes may be more useful. In <ref type="bibr" target="#b146">[148]</ref> Hierarchical Hidden Markov Model (HHMM) is used. An audio-visual concept space a collection of elementary concepts such as people, building, and monologue each of which is learned from low-level features in a separate supervised training process. They believe that such mid-level concepts offer a promising direction to reveal the semantic meanings in patterns, since grouping and post-processing beyond the signal level is deemed a vital part for the understanding of sensory inputs. Multi-modal perception is no less complicated than perception in individual senses. They obtained the co-occurrence statistic C(q;w) for a HHMM label q and a token w by counting the number of times that the state label q and the word w both appear in the same temporal segment among all video clips. A few interesting issues not considered in their work are: (1) Using text processing techniques to exploit correlations inherent in raw word tokens; (2) Joint learning of the temporal model and the semantic association to obtain more meaningful labels.</p><p>In <ref type="bibr" target="#b6">[7]</ref>, the authors present multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model for learning the relationships between image regions and semantic correlates (words). Each cluster is associated with a path from a leaf to the root. Nodes close to the root are shared by many clusters, and nodes closer to leaves are shared by few clusters. Hierarchical clustering models do not model the relationships between specific image regions and words explicitly. However, they do encode this correspondence to some extent through cooccurrence because there is an advantage to having "topics" collect at the nodes. Another multimodal clustering approach in <ref type="bibr" target="#b41">[42]</ref> divided each video into Ws-minute chunks, and extracted audio and visual features from each of these chunks. Next, they apply k-means clustering to assign each chunk with a commercial/program label. They intend to use content-adaptive and computationally inexpensive unsupervised learning method. The idea of departure from stationarity is to measure the amount of usual characteristics in a sequence. They form a global window that consists of larger minutes of video chunks, and a local window with just one video chunk. Computation of a dissimilarity value from the histogram is based on the Kullback-Liebler distance metric. Dissimilarity based on comparison with global and local window is good idea but finding their sizes for effective computation will vary. For labeling as program or commercial they needed rule based heuristics which might not be generic and scalable.</p><p>The approach in <ref type="bibr" target="#b34">[35]</ref> is to extract representative visual profiles that correspond to frequent homogeneous regions, and to associate them with keywords. A novel algorithm that performs clustering and feature weighting simultaneously is used to learn the associations. Unsupervised clustering is used to identify representative profiles that correspond to frequent homogeneous regions. Representatives from each cluster and their relevant visual and textual features are used to build a thesaurus. Their assumption is that, if word w describes a given region R i , then a subset of its visual features would be present in many instances across the image database. Thus, an association rule among them could be mined. They claim that due to the uncertainties in the images/regions representation duplicated words, incorrect segmentation, irrelevant features etc. standard association rule extraction algorithms may not provide acceptable results. Wei et al. <ref type="bibr" target="#b142">[144]</ref> proposed the crossreference reranking (CR-Reranking) strategy for the refinement of the initial search results of video search engines. CR-Reranking method contains three main stages: clustering the initial search results separately for different modality, ranking the clusters by their relevance to the query, and hierarchically fusing all the ranked clusters using a cross-reference strategy. The fundamental idea of CR-Reranking is that, the semantic understanding of video content from different modalities can reach an agreement. The proposed re-ranking method is sensitive to the number of clusters due to the limitation of cluster ranking. While existing clustering methods typically output groups of items with no intrinsic structure the works in <ref type="bibr" target="#b93">[94]</ref> discovers deeper relations among grouped items like equivalence and entailment using crossmodal clustering. The work pointed out that, existing clustering methods mostly link information items symmetrically but two related items should be assigned different strength to link each other.</p><p>One of the multi-modal clustering application in <ref type="bibr" target="#b153">[155]</ref> reveals common sources of spam images by two-level clustering algorithm. The algorithm first calculates the image similarities in a pair-wised manner with respect to the visual features, and the images with sufficiently high similarities are grouped together. In the second level clustering, text clues are also considered. A string matching method is used to compare the closeness of texts in two images, which is used as a criterion to refine the clustering results from the first level clustering. Although they did not use synergy between two modalities, exploiting it through some association rule mining algorithm could enhance the effectiveness of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Association</head><p>As video and audio are continuous media, one of the potential forms of patterns can be sequential patterns (patterns that sequentially relates the adjacent shots). To extract meaningful patterns from extremely large search space of possible sequential patterns, we need to impose various constraints to eliminate the unlikely search area. It is mainly useful for multimodal event detection and thus in turn for indexing and retrieval. As the temporal information in video sequences is critical in conveying video content, temporal association mining has been proposed in literature. In <ref type="bibr" target="#b12">[13]</ref> the authors proposed hierarchical temporal association mining approach based on modification to traditional Association Rule Mining(ARM). The advantage of their work was to provide automatic selection of threshold values of temporal threshold, support and confidence. But due to pattern combinatorial explosion problem, researchers also suggested some non traditional ARM a type of neural network, named as Adaptive Resonance Theory in <ref type="bibr" target="#b64">[65]</ref>.</p><p>For event detection or classification, it is required to know the event model before hand but association mining can discover the patterns and then use it for classifying/labeling videos. It is therefore a better approach than hidden Markov models, classification rules or special pattern detections. The work in <ref type="bibr" target="#b159">[161]</ref> is novel in its attempt to do sequence association mining for assigning class labels to discovered associations for video indexing purpose. They first explore visual and audio cues that can help bridge the semantic gap between low-level features and video content. They do video association mining and discuss algorithms to classify video associations to construct video indexing. Once they generate the symbolic streams from different modalities, they either need to combine the streams or treat the streams separately. To find the co-occurrence of patterns that appear in multiple streams, symbol production synchronization is required for combining into single stream and for detecting periodic patterns. The proposed association mining nicely handles the problem but the question that arises here is, multimedia data streams do not generate equal amount of symbols in the same amount of time. Symbols from a video shot and words from an audio clip need to be synchronized in some way for creating the relational database for mining purposes. If we consider them as one stream, some information may be lost. Though they use visual, textual, audio and metadata features, they did not show any special usage of multimodal fusion to enhance the knowledge discovery process. Though algorithms have been proposed with nice set of low level and mid level features, no meaningful discovered patterns are shown in their results <ref type="bibr" target="#b159">[161]</ref>.</p><p>Similarly in the series of papers <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b123">125,</ref><ref type="bibr" target="#b124">126]</ref>, the aim is to show the importance of temporal constraint as temporal distance and temporal relationship. Applying Temporal Distance Threshold (TDT) and Semantic Event Boundary (SEB) constraints on extracted raw level metadata help to effectively extract Cinematic Rules and Frequent semantic events. For example, frequent patterns say SM1-SM1 represent "two continuous shots with human voice", SM1MV0 represent "a shot with human voice and no direction of movement" and MV0-MV0 represent "two continuous shots with no direction of movement". The considerably high recall values for these patterns indicate that characters talk to each other and hardly move in most of the talk event in this movie. They are not enforcing any rules based on domain knowledge (e.g., interview events of news videos has a shot where an interviewer followed by interviewee is repeated. ) thus it is kind of extraction of semantic patterns from rule independent videos.</p><p>They extracted raw level features from audio and video shot keyframes using MP-factory and OpenCV. Then they cluster the raw level data and assign the labels based on clusters to discriminate the frames more efficiently. The intuition behind using semantic level meta data is that, we can get some semantic level knowledge. For example, by considering the color histogram level data it is difficult to interpret the obtain knowledge for higher level semantics, but by clustering this histogram and labeling them as categories (water, snow, fire etc.) we can interpret the mined results at a semantic level. One nice idea that they use is "semantic content represented by two symbols occurring at same time point is completely different from that of two symbols occurring at different time points." Discriminating temporal relationship as parallel or sequential was a good idea. They did parallel processing for the mining, which is good for multimedia data mining algorithms as it is always computationally expensive.</p><p>In <ref type="bibr" target="#b123">[125]</ref> they pointed out that the Semantic Event Boundary (SEB) and video shot boundary relationship is not stable. It is rigid to consider that there are many shots within a semantic event, a shot may not convey a semantic event of interest though it is within given semantic event boundary. As there could be cases where a shot contains many semantic boundaries (e.g., surveillance video is just one shot with many semantic events whereas in movies battle scenes are made of many small shots.) Semantic Event Boundaries are very difficult to find automatically, and finding it manually is laborious. Temporal Distance Threshold (TDT) is also expected from the user and it is hard to judge without good domain knowledge. They did not attempt to reduce the dimensionality of generated multi-dimensional categorical streams. It could significantly make sequential pattern mining task faster.</p><p>In <ref type="bibr" target="#b77">[78]</ref> which is a continuation of the work done in <ref type="bibr" target="#b127">[129]</ref>, they considered that the class imbalance can be addressed by learning more positive instances. Here, the authors used association rule mining to learn more about positive and negative instances and then use that knowledge for classification. They apply some heuristics to give better classification based on the concept they wanted to learn. For example, the weather related concept has high number of negative instances so they included more negative rules in the weather classifier. Such rules are learned from association mining so no domain knowledge dependence is incurred for detecting the rules. Again such heuristics are not guaranteed to give good results. It is possible to learn such heuristics automatically from the available statistical information in dataset.</p><p>HMNews proposed in <ref type="bibr" target="#b94">[95]</ref> has a layered architecture. At the lowest level it has feature extraction, shot/speaker detection/clustering, and natural language tagging. At the aggregation layer association mining tools used for the generation of the multimodal aggregations. The final layer provides search and retrieval services. The work in <ref type="bibr" target="#b88">[89]</ref> proposed multi-modal motif mining method to model dialogue interaction of doctor and patient. They exploit a Jensen-Shannon Divergence measure to solve the problem of combinatorially very large pattern generation and extracted important patterns and motifs. In <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> Multi-Modal Semantic Association Rule (MMSAR) is proposed to fuse keywords and visual features automatically for Web image retrieval. These techniques associate a single keyword to several visual feature clusters in inverted file format. Based on the mined MMSARs in inverted files, the query keywords and the visual features are fused automatically in the retrieval process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Discussion</head><p>Multimodal data mining literature is summarized in Table <ref type="table" target="#tab_5">5</ref>. None of the current research works focuses on obtaining a realistic feature representation for mining  <ref type="bibr" target="#b98">[99]</ref> or Multimodal data stream consider multiple streams as one <ref type="bibr" target="#b159">[161]</ref> synchronization Feature transformation Metadata fusion <ref type="bibr" target="#b144">[146]</ref> External knowledge fusion Multimodal Pre filtering: heuristic rule based <ref type="bibr" target="#b15">[16]</ref>, Class imbalance, multimodal classification SVM classifier based <ref type="bibr" target="#b13">[14]</ref>, sub space classifier fusion based <ref type="bibr" target="#b127">[129]</ref>; fusion: late <ref type="bibr" target="#b98">[99]</ref>, early <ref type="bibr" target="#b159">[161]</ref> and meta <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref> Multimodal clustering Mixed media graph <ref type="bibr" target="#b102">[103]</ref>, Cross modal correlation clustering <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b146">148]</ref>, EEML <ref type="bibr" target="#b46">[47]</ref> discovery Multimodal association Generalized sequence pattern Automatic identification of mining <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b124">126]</ref> temporal structures purposes. For example, one of the technique is to extract the raw level feature then cluster or classify them and assign the labels to generate the categorical dataset.</p><p>The derived categorical labels are approximate. Thus, these labels should have some probability associated with them to represent the approximation factor in order for it to be a more realistic data representation. There have been efforts to discover the cross modal correlation and synergy between different modalities. But in multimodal data mining literature, there are not many examples showing the significant ways of exploiting such correlation knowledge for mining. Most works deal with low level raw data from each individual modality. There is not much research work showing that if we have significant prior knowledge about the relationships between context, content, and semantic labels, we can use them to substantially reduce the hypothesis space to search for the right model. For example, multimodal metadata fusion proposed in <ref type="bibr" target="#b144">[146]</ref> uses a semantic ontology for photo annotation. The ontology based approach looks promising but is not much explored for multimodal data mining.</p><p>In the next section we will detail the open research issues discovered in multimedia data mining so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multimodal data mining: open research issues</head><p>Due to the redundancy, ambiguity, heterogeneity of multimodal data, we have identified the issue of the use of realistic multimedia data for mining purposes. Multimedia systems utilize multiple types of media such as video, audio, text and even RFID for accomplishing various detection tasks. Different types of media possess different capabilities to accomplish various detection tasks under different contexts. Therefore, we usually have different confidence in the evidence obtained based on different media streams for accomplishing various detection tasks. None of the state of the art works in multimodal data mining utilize realistic features for mining purposes. They assume that the semantic labels can be obtained accurately. The reality is that the extracted features (labels and tags) from different modalities are not obtained with 100% accuracy. This is due to the well known semantic gap problem. There needs to be a way to represent such information with certain probabilistic weighting to mine more accurate datasets.</p><p>Problem def inition Let S be a multimedia system designed for accomplishing a set of detection tasks T r = {T 1 , T 2 , . . . , T r }, r being the total number of detection tasks. The multimedia system S utilizes n â‰¥ 1 correlated media streams. Let M = {M 1 , M 2 , ..., M n } be the set of n correlated media streams. Let L = {l 1 , l 2 , . . . , l r } be the semantic labels output by the various detectors T r .</p><p>For 1 â‰¤ i â‰¤ n, let 0 &lt; p Mit j &lt; 1 be the probability of label l Mi j output by the detector T j based on individual i th media stream at time t. The time is represented by starting time of label and ending time of label, representing the duration of an event label existence in the stream. p Mit j is determined by first extracting the low level content features from media stream i and then by employing a detector (e.g., a trained classifier) on it for the task T j . The dataset generated with such multimedia system is called as "probabilistic temporal multimodal dataset". Thus, we obtain a set of n correlated labeled streams correspond to the n media streams: L = {L 1 , L 2 , . . . , L n } where In the following subsections we will look at multimodal data mining problems arising on our probabilistic temporal multimodal dataset.</p><formula xml:id="formula_0">L 1 = l M1 1 , p M1t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mining probabilistic temporal multimodal dataset</head><p>Input: Assume that we have N correlated multimedia streams M N that generate L N set of symbols with p M jt i probability associated with each of the symbol during time t. The correlation among media streams influences how the probabilities with which symbols are generated can be utilized. The time stamps represents the temporal relationship between symbols. The time stamps for the similar symbol, generated from different streams, can be different due to different speeds of the detector in the corresponding stream. Here, we are trying to give a generic view of the probabilistic temporal multimodal dataset. The typical dataset looks as shown in Table <ref type="table" target="#tab_8">6</ref>. This dataset resembles a real world surveillance dataset or group meeting dataset or movie dataset used for mining application. In all these applications the intention is to discover interesting knowledge from involved objects and their interactions and behaviors. Output: Discovering interesting knowledge from these streams. It can be interesting correlations among symbols or streams, frequent patterns, associations, casual structures among set of items, clusters, outliers or classes hidden in given data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence pattern mining</head><p>We are given a probabilistic temporal multimodal dataset D as described in Section 3.1. We identify new problems for doing sequence pattern mining on dataset D by scrutinizing the original problem statement of sequence pattern mining given in <ref type="bibr" target="#b1">[2]</ref>.  is an item enclose in "[ ]". A sequence represent the items in their temporal order. An itemset is considered to be a sequence with a single element.</p><formula xml:id="formula_1">Definition Let I = {(l M1 1 ,</formula><p>(1) Probabilistic nature of data Given customer transaction database say D' in <ref type="bibr" target="#b1">[2]</ref>, it is clear that the rows in dataset D are not analogous to concept of transactions as in D'. Because each modality(audio, video, etc.) has generated probabilistic symbols in D while the symbols in D' are deterministic. So, the f irst problem identif ied is to consider the probabilistic nature of data for mining. As generalized sequence pattern methods are developed considering deterministic data they can not efficiently mine the patterns from probabilistic temporal multimodal dataset D. (2) Synchronization of correlated data streams The constraint that no customer has more than one transaction with the same transaction time may not be satisfied here, as the speed with which the probabilistic symbols are generated from different modalities are different. Even using a windowing technique for considering certain temporal interval as transactions we can not guarantee that the symbols timing do not overlap between two different transactions. Thus, the second problem is to synchronize the dif ferent streams of probabilistic symbols to f ind a valid transaction boundary. (3) Redundant symbol resolution in sequence patterns The problem encountered here is due to multimedia data's property of redundancy. The constraint that items can occur only once in an element of a sequence is violated for D. Different modalities might generate similar symbols with different or same probabilities. This problem may also lead to confusing sequence patterns like {(X, 0.3), (X, 0.9), (A, 0.5)} {(X, 0.3)} {(X, 0.5)} {(X, 0.7)} where X generated from different modalities at different times but we can not interpret them unless we incorporate the modality knowledge here. These symbols may differ in probability associated with them and definitely the modality which has generated them. Thus, to deal with this problem we need to come up with mechanisms to incorporate these probabilities and knowledge of modalities which has generated these symbols. This also leads in the direction of finding the correlation between these modalities to effectively handle the problem. (4) Finding subsequences to calculate support parameter We can see the problem in defining that sequence a 1 a 2 a 3 . . . a n is a subsequence of another sequence b 1 b 2 b 3 . . . b n for the dataset D. Once a mechanism for finding subsequence is discovered, the support for a sequence is defined as the fraction of total datasequences that contain this sequence.</p><p>Problem def inition: sequence pattern mining for probabilistic temporal multimodal dataset Given a probabilistic temporal multimodal dataset D of data sequences, the problem of mining sequential patterns is to find all sequences whose support is greater than the user-specified minimum support. Each such sequence represents a sequential pattern, also called frequent sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Association rule mining</head><p>The problems encountered while mining sequential patterns from probabilistic temporal multimodal dataset D do exist for association rule mining from D.</p><p>Definition Let I = {(l M1 1 , p M1t 1 ), . . . , (l M1 r , p M1t r ), (l M2 1 , p M2t 1 ), . . . , (l M2 r , p M2t r ), . . . , (l Mn 1 , p Mnt 1 ), . . . , (l Mn r , p Mnt r )} be called items. Let X be a set of some items in I. Transaction t satisfies X if for all items in X are present in t. Here, again the concept of transaction in dataset D is not properly defined. An association rule means an implication of the form X â‡’ (l Mk r , p Mkt r ) where X is a set of some items in I and (l Mk r , p Mkt r ) is single item in I that should not be present in X. The rule X â‡’ (l Mk r , p Mkt r ) is satisfied in the set of transactions T with the confidence factor 0 &lt; c &lt; 1 iff at least c% of transactions in T that satisfy X also satisfy (l Mk r , p Mkt r ).</p><p>(1) Symbol matching In X â‡’ (l Mk r , p Mkt r ) X might have repeated symbols and also (l Mk r , p Mkt r ) can be similar as the symbols repeated in X. If we use the term symbol for l r only, this violates the original definition of association rule mining given in <ref type="bibr" target="#b0">[1]</ref>, that l r can not be a symbol in X. But, if we consider (l Mk r , p Mkt r ) as a symbol, it may differ in probability associated with it and the modality which has generated it and then it is difficult to match the symbols. Thus, to deal with this problem we need to come up with a mechanism to incorporate this probability feature and knowledge of modalities which has generated these symbols while efficiently being able to compare them.</p><p>Problem def inition: association rule mining for probabilistic temporal multimodal dataset Given a probabilistic temporal multimodal dataset D of data sequences, the problem of association rule mining is to generate association rules like X â‡’ (i j , Pr j ) that satisfy user specified support and confidence parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multimodal fusion</head><p>Most multimedia analysis is usually performed separately on each modality, and the results are brought together at a later stage to arrive at the final decision about the input data. Although this is a simpler approach, we lose valuable information about the multimedia events or objects present in the data because, by processing each modality separately, we discard the inherent associations between different modalities. Combining all the multimodal features together is not feasible due to the curse of dimensionality. Thus, there is a need for an efficient way to apply mining techniques on multimodal data keeping the inherent correlation among different modalities intact.</p><p>Let us consider</p><formula xml:id="formula_2">M 1 = {a 1 1 , a 1 2 , . . . , a 1 i }, M 2 = {a 2 1 , a 2 2 , . . . , a 2 j } and M 3 = {a<label>3</label></formula><p>1 , a 3 2 , . . . , a 3 k }, where M 1 , M 2 and M 3 are three different modalities with i, j and k number of attributes respectively. Considering these streams together increases the dimensionality and thus mining becomes inefficient. While handling of M 1 , M 2 and M 3 separately for mining may lead to loss of information. For example, a 1  2 and a 3 1 together can best classify certain event, but now if we consider them separately and then combine the decisions from each of them, then we cannot utilize the inherent association between these modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Automatic attribute construction techniques</head><p>Attribute construction is one of the most appealing area for multimodal data mining as it can help reduce dimensionality by combining different features to represent as one feature. Generating new features by combining features from different modalities can also better capture the correlation property among the different media types. There is also the possibility that the new derived attribute is semantically more meaningful than its corresponding individual attributes. Usually the new attributes are constructed based on the domain knowledge. It can be a challenging problem to come up with automated attribute discovery from a given set of attributes.</p><p>Again consider M 1 , M 2 and M 3 being three different modalities with i, j and k number of attributes respectively. Let us assume that combining a 1 2 and a 3 1 together can give us a new attribute say a 1, 3  1,2 which can best classify a certain class of events. Thus we can reduce the dimensionality and can do more efficient mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Knowledge representation</head><p>For the discovered image or video patterns to be meaningful, they must be presented visually to the users <ref type="bibr" target="#b145">[147]</ref>. This translates to image/video pattern representation issue. How can we represent the image/video pattern such that the contextual information, spatial information, and important image characteristics are retained in the representation scheme? 4.7 Media stream synchronization Data analysis granularity level of a video can be frame level, shot level, clip level, while for an image it can be pixel level, grid level, region level, for audio it can be phoneme or word level or the data are broken into windows of fixed size. This granularity level is the processing unit of its corresponding modality. There is a semantic meaning associated for each modality within their processing unit. Each media type has different level of complexity and speed for analyzing their processing unit. Thus, before applying mining techniques on such multimodal data we need to align them temporally and semantically. It is a difficult problem to perform such synchronization or find an alignment among different modalities.</p><p>For the given media streams say M 1 , M 2 and M 3 each of them have different processing units say pr 1 , pr 2 and pr 3 and the corresponding time for computing on them is t 1 , t 2 and t 3 thus the time at which they may identify the symbol after computing over the processing unit may be different for each of them. It is not guaranteed to predict the temporal gap between them because pr i and t i are not static. For example, if M 1 is a video stream and processing unit pr 1 is shot level. Then the size of each shot may vary and thus their corresponding time for processing varies. These leads to the problem of deciding the boundaries for combined multimodal data, in its consideration as the data mining processing unit equivalent of a transaction or a tuple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Domain knowledge dependence</head><p>Much of the multimedia data mining approaches extract semantic patterns only using rule dependent methods, where there are apparent rules associated with semantic events. For example, in the interview events of news videos, a shot where an interviewer appears, followed by that of an interviewee, is repeated one after the other <ref type="bibr" target="#b101">[102]</ref>. Similarly, for goal events in ball game videos, the score in the telop changes after audience's cheers and applause <ref type="bibr" target="#b159">[161]</ref>. Also, in surveillance videos recorded with fixed cameras, if an object actively moves, the difference between two consecutive frames is clearly large <ref type="bibr" target="#b99">[100]</ref>. Like this, these apparent rules tell what kind of raw level data should be used to extract semantic patterns in rule-dependent methods. Thus, the extracted semantic patterns are previously known. These rule dependent algorithms are not robust and extendible. There is a need for discovering robust and generic rule independent method for multimodal data mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Class imbalance</head><p>For classification purposes the multimedia data sometimes suffer from the classimbalance (skewed data distribution) problem <ref type="bibr" target="#b12">[13]</ref>. For example, if we are trying to mine interesting events from sports video or suspicious events from surveillance video, there is very little training data for the interesting events as compared to the remaining large set of normal or uninteresting data. In other words, as the text mining has the steps like stop word removal, word stemming etc for noise removal and maintaining the relevant bag of words for mining, we usually do not properly know the noise characteristics for multimedia data like image, audio or video before hand. Thus we might end up with noisy data as part of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">High-dimensionality and scalability</head><p>Multimedia data is voluminous and it can have a very large set of features. Considering example of sequential pattern mining, we need to extract sequential patterns from long categorical streams generated from the m different modalities. The task is challenging because search space of possible sequential patterns is extremely large. For m-dimensional multimodal data stream with each component stream containing n kinds of symbols has O(n mk ) possible sequential patterns of time length k. Similarly for association rule mining, classification or clustering, the high-dimensionality and scalability is an issue. Thus, there is a need for efficient ways of reducing dimensions and handling longer categorical data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Knowledge integration for iterative mining</head><p>One of the major issues that we observe in current state of the art is of knowledge representation and utilization of that knowledge to enhance the next iteration of mining algorithms. It can be assumed that on each iteration if we utilize the obtained knowledge and if we feed that knowledge back into system it will be able to generate more semantically meaningful knowledge than the previous mining iteration. Here, there are two major directions in which research is needed (1) adaptive knowledge representation mechanism which can be expanded on each iteration of the mining algorithm to incorporate the acquired new knowledge and (2) Feedback and integration mechanisms that can efficiently utilize the acquired knowledge of current iteration into future iterations. Even the knowledge can be from external resources like ontology which can help guide the mining process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.12">Synchronous cross modal mining</head><p>The new problem from multimodal data can be seen as synchronous cross modal mining. For example, face recognition systems can identify from video frames and output the decision with a certain probability while speaker recognition systems are still doing the recognition tasks. Is there a possible way to transfer the information discovered by face recognition system to speaker recognition system and vice versa? The problem is not similar to late fusion or early fusion. Here, we are trying for synchronous fusion dynamically. Such mining can aid in faster and more robust knowledge discovery. But it will be challenging to come up with algorithms which can be adaptive to such synchronous knowledge while the mining is being done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.13">General tool for multimodal data mining</head><p>There is a need for a generic tool like a spreadsheet where one can deposit the multimodal dataset. The tool can then automatically extract the features independent of the domain knowledge or application requirement. It should provide the basic functionality in frequent pattern mining, association rule mining, clustering, outlier detection, change detection etc. for generic purposes. As the popular data mining tool like Weka <ref type="bibr" target="#b47">[48]</ref> exists for the structured data, there should be a tool for multimodal data mining. The tool should consider the cross modal correlation and should allow for visualization of the extracted knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>There are lot of areas like surveillance, medicine, entertainment, etc. where multimedia data mining techniques are explored. Audio mining, video mining and image mining have each established its own place as separate research field. Most of the research focus has been on detection of concepts/events in multimedia content. In the literature, novel techniques for feature extraction and new attribute discovery perspective have been proposed but very few works consider multimodal mining algorithms. The main bottleneck found is the semantic gap due to which existing mining algorithms suffer from scalability issues. The existing techniques do not utilize cross-modal correlations and fusion practices from multimedia systems research. There are not many generic frameworks for multimedia data mining, many existing ones are more domain specific.</p><p>We did a comprehensive literature survey for the state of the art in multimedia data mining. After finishing the survey for image mining, video mining and audio mining we realize that multimodal data mining has a tremendous amount of untapped potential to deal with semantic gap problem. We have identified specific research issues in the area to suggest avenues for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Multimedia data mining state of the art review scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Image data mining literature summary</figDesc><table><row><cell>Task</cell><cell>Approach/Application</cell><cell>Issue</cell></row><row><cell>Preprocessing</cell><cell>Pixel, region and grid level</cell><cell>Global vs. local level features</cell></row><row><cell>Feature</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Video data mining literature summary</figDesc><table><row><cell>Task</cell><cell>Approach/Application</cell><cell>Issue</cell></row><row><cell>Preprocessing</cell><cell>Shot level, frame level,</cell><cell>Depends on video structure</cell></row><row><cell></cell><cell>scene level, clip level</cell><cell>and application type</cell></row><row><cell>Feature extraction</cell><cell>Image features at frame level as well</cell><cell>Motion calculation is</cell></row><row><cell>and transformation</cell><cell>as motion descriptors, camera</cell><cell>computationally</cell></row><row><cell></cell><cell>metadata like motion [161],</cell><cell>expensive</cell></row><row><cell></cell><cell>date, place, time etc.</cell><cell></cell></row><row><cell>Video classification</cell><cell>Human body motion recognition [18],</cell><cell>Not enough training data</cell></row><row><cell></cell><cell>goal detection [161], gestures etc.</cell><cell>to learn events of interest,</cell></row><row><cell></cell><cell></cell><cell>domain knowledge</cell></row><row><cell></cell><cell></cell><cell>dependence</cell></row><row><cell>Video clustering</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Audio data mining literature summary</figDesc><table><row><cell>Task</cell><cell>Approach/Application</cell><cell>Issue</cell></row><row><cell>Preprocessing</cell><cell>Phoneme level, word level,</cell><cell>Domain dependence e.g., phoneme</cell></row><row><cell></cell><cell>window of fixed sizes</cell><cell>level if foreign terms, segmenting</cell></row><row><cell></cell><cell></cell><cell>out silence, noise etc.</cell></row><row><cell>Feature extraction</cell><cell>Pause rate, zero crossing rate, Mel</cell><cell>Sensitive to the parameters e.g.,</cell></row><row><cell>and transformation</cell><cell>frequency cepstral coefficients [19],</cell><cell>number of bins, frequency</cell></row><row><cell></cell><cell>bandwidth, spectral centroid,</cell><cell>resolution, smoothing</cell></row><row><cell></cell><cell>frequency spectrum</cell><cell>factors etc.</cell></row><row><cell>Audio classification</cell><cell>HMM [24], GMM [67], SVM [46],</cell><cell>Model based approaches have</cell></row><row><cell></cell><cell>Bayesian classifier to do</cell><cell>problem to work well in</cell></row><row><cell></cell><cell>segmentation and classification</cell><cell>real time as more number</cell></row><row><cell></cell><cell>of speech, music etc.</cell><cell>of iterations and lot of data</cell></row><row><cell></cell><cell></cell><cell>are needed for training.</cell></row><row><cell>Audio clustering</cell><cell>Clustering speaker gender/speech</cell><cell>Large clusters are sometimes</cell></row><row><cell></cell><cell>segment of same speaker [92]</cell><cell>b i a s e d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Text data mining literature summary</figDesc><table><row><cell>Task</cell><cell>Approach/Application</cell><cell>Issue</cell></row><row><cell>Preprocessing</cell><cell>Part of speech tagging, stemming,</cell><cell>Resolving ambiguity is main</cell></row><row><cell></cell><cell>stop word removal,</cell><cell>problem, single word different</cell></row><row><cell></cell><cell>text chunking etc.</cell><cell>meaning in different context</cell></row><row><cell>Feature extraction</cell><cell>Identification of important</cell><cell>Different sizes and contexts</cell></row><row><cell>and transformation</cell><cell>keywords using TF,</cell><cell>of the documents</cell></row><row><cell></cell><cell>IDF [117] etc.</cell><cell></cell></row><row><cell>Text classification</cell><cell>K-nearest neighbor classification,</cell><cell>Final decisions depends</cell></row><row><cell></cell><cell>decision tress, naÃ¯ve bayes</cell><cell>on relatively few terms</cell></row><row><cell></cell><cell>classifier [34]</cell><cell></cell></row><row><cell>Text clustering</cell><cell>Bi section k means clustering,</cell><cell>Finding good distance</cell></row><row><cell></cell><cell>co clustering [34]</cell><cell>measures</cell></row><row><cell>2.6.8 Discussion</cell><cell></cell><cell></cell></row></table><note><p><p><p>For text mining the choice of document representation should be taken seriously. It should be decided based on the amount of the noise in the text collection and the amount of time and resources available. The simpler models require less processing, while the more complicated models such as RDR require POS tagging and other NLP text processing techniques. The NLP methods and tagging are time-consuming and heavy users of resources. As shown in the Table</p>4</p>below there are some of common issues arising in the text mining. A new trend of finding a association rules, ontology, taxonomies from temporal documents has started. Also the more number of text mining applications are utilizing the advantages offered by visualization techniques.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Multimodal data mining literature summary</figDesc><table><row><cell>Task</cell><cell>Approach/Application</cell><cell>Issue</cell></row><row><cell>Preprocessing</cell><cell>Treat multiple streams separately</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>is an ordered list of itemsets. Denoting a sequence s by s 1 s 2 s 3 . . . s n where s j is an itemset. We also call s j an element of the sequence. We denote an element of a sequence by([l M1  </figDesc><table><row><cell></cell><cell cols="6">A sample probabilistic temporal multimodal (PTM) dataset</cell><cell></cell><cell></cell></row><row><cell>Video</cell><cell></cell><cell></cell><cell>Audio</cell><cell></cell><cell></cell><cell>Sensor X</cell><cell></cell><cell></cell></row><row><cell cols="9">Person (Start, End) Event Person (Start, End) Event Person (Start, End) Event</cell></row><row><cell>id</cell><cell>time (s)</cell><cell>name</cell><cell>id</cell><cell>time (s)</cell><cell>name</cell><cell>id</cell><cell>time (s)</cell><cell>name</cell></row><row><cell cols="2">P1, 0.7 (0, 0.20)</cell><cell>X, 0.6</cell><cell cols="2">P1, 0.9 (0.10, 0.20)</cell><cell cols="3">B, 0.6 P1, 0.3 (1.12, 6.50)</cell><cell>X, 0.2</cell></row><row><cell cols="2">P4, 0.6 (0.50, 2.50)</cell><cell cols="3">W, 0.7 P3, 0.3 (0.20, 1.56)</cell><cell cols="3">A, 0.3 P2, 0.5 (3.20, 4.21)</cell><cell>A, 0.5</cell></row><row><cell cols="2">P3, 0.4 (1.50, 3.10)</cell><cell cols="3">A, 0.4 P1, 0.6 (0.60, 2.60)</cell><cell cols="3">B, 0.7 P1, 0.8 (5.70, 7.00)</cell><cell>A, 0.9</cell></row><row><cell cols="2">P1, 0.9 (3.0, 5.40)</cell><cell>X, 0.8</cell><cell cols="2">P2, 0.9 (1.00, 5.40)</cell><cell cols="3">X, 0.4 P2, 0.6 (6.00, 7.60)</cell><cell>X, 0.3</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row></table><note><p>sequence</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD international conference on management of data</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining sequential patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on data engineering</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust hmm-based speech/music segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ajmera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1746" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video2text: learning to annotate video content</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on data mining workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Clustering algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Artigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An audio-based sports video segmentation and event detection algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on event mining, detection and recognition of events in video</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multimedia information network for knowledge representation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Bellingham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinsel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Pearson Education</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio classification of bird species: a statistical manifold approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fern</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Content-based annotation for multimodal image retrieval using bayes point machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sychay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst Video Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching near replicas of image via clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE multimedia storage and archiving systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical temporal association mining for video event detection in video databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia databases and data management</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic event detection via multimodal data mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wickramaratna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimedia data mininig for traffic video sequenices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strickrott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decision tree-based multimodal data mining framework for soccer goal detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference multimedia and expo</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video mining: concepts, approaches and applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia modelling</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Space-time gestures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computing Society conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Acoust Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Co-clustering documents and words using bipartite spectral graph partitioning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust energy demodulation based on continuous models with application to speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on speech communication and technology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech/music discrimination for multimedia application</title>
		<author>
			<persName><forename type="first">K</forename><surname>El-Maleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kabal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="2445" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic segmentation of speech recorded in uknown noisy channel characteristics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Ellom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A fully automated content based video search engine supporting spatio-temporal queries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">W</forename><surname>Jianhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="615" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient and effective querying by image content</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="231" to="262" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical classification for automatic image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining multilevel image semantics via hierarchical classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="187" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical modeling and conceptualization of natural scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="865" to="885" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Audio-based emotion recognition in judicial domain: a multilayer support vector machines approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Archetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning and data mining in pattern recognition (MLDM)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Content-based retrieval of music and audio</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">3229</biblScope>
			<biblScope unit="page" from="138" to="147" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Web text mining using harmony search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="51" to="64" />
		</imprint>
	</monogr>
	<note>Recent advances in harmony search algorithm</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Information retrieval: data structures and algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining visual and textual data for constructing a multi-modal thesaurus</title>
		<author>
			<persName><forename type="first">H</forename><surname>Frigui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caudill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM international conference on data mining</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cepstral analysis technique for automatic speaker verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Acoust Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="272" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust feature extraction using subband spectral centroid histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Waps: an audio program surveillance system for large scale web data stream</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on web information systems and mining (WISM)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="116" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Incorporate concept ontology to enable probabilistic concept reasoning for multi-level image annotation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM MIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A differential spectral voice activity detector</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Komori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Auditory nerve representation as a front-end in a noisy environment</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ghitza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Speech Lang</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="130" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Audio-visual event detection based on mining of semantic audio-visual labels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miyahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radhakrishan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE conference on storage and retrieval of multimedia databases</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5307</biblScope>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Speech and audio signal processing: processing and perception of speech and music</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining from large image sets</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on image and video retrieval(CIVR)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Texture orientation for sorting photos at a glance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Con</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on pattern recognition</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Content-based audio classification and retrieval by support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="215" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced max margin learning on multimodal data mining in a multimedia database</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Weka data mining software: an update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD explorations</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Data mining concepts and techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mining frequent patterns by pattern-growth: methodology and implications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Speech/music/silence and gender detection algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Auloge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on distributed multimedia systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Using multi-modal semantic association rules to fuse keywords and visual features automatically for web image retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information fusion</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-modal mining in web image retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific conference on computational intelligence and industrial applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An efficient speaker independent automatic speech recognition by simulation of some properties of human auditory perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="1156" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An efficient speaker independent automatic speech recognition by simulation of some properties of human auditory perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="1156" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (plp) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Acoust Soc Am</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rasta processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Acoust Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Compensation for the effect of the communication channel in auditory-like analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bayya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="578" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Traps-classifiers of temporal patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on speech and language processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Algorithms for association rule mining a general survey and comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>GÃ¼ntzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nakhaeizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">An automatic hierarchical image classification scheme</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Real time video data mining for surveillance video streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Hwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia conference on knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Text classification using graph mining-based feature extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coenena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandersona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="302" to="308" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning image text associations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="177" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Strategies for automatic segmentation of audio data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Association rules mining: a recent overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanellopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Trans Comput Sci Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An overview of sequence comparison: timewarps, string edits and macromolecules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="201" to="237" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Time-scale modification of speech based on a nonlinear oscillator model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mediamatrix: A video stream retrieval system with mechanisms for mining contexts of query examples</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kurabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kiyoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database systems for advanced applications (DASFAA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Let&apos;s hear it for audio mining</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leavitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23" to="25" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multimedia content processing through cross-modal association</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM multimedia</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Using text mining and sentiment analysis for online forums hotspot detection and forecast</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis Support Syst</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="354" to="368" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Content-based audio classification and retrieval using the nearest feature line method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="619" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A generative/discriminative learning algorithm for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference of computer vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Online speaker clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kubala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Video semantic concept discovery using multimodalbased association classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="859" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mining high-level features from video using associations and correlations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on semantic computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Video semantic concept detection via associative classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="418" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Triggering memories of conversations using multimodal classifiers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">R</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on intelligent situation aware media and presentation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Meta-classification: combining multimodal classifiers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect Notes Comput Sci</title>
		<imprint>
			<biblScope unit="volume">2797</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">News video classification using svm-based multimodal classifiers and combination strategies</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">R</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ACM multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Mining preorder relation between knowledge elements from text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM symposium on applied computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Spectrum steganalysis of wav audio streams</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning and data mining in pattern recognition (MLDM)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="582" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Discovery of frequent episodes in event sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Fractal aspects of speech signals: dimension and interpolation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fractal dimensios of speech sounds: computation and application to automatic speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Acoust Soc Am</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1925" to="1932" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Interaction pattern and motif mining method for doctor-patient multi-modal dialog analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sawamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Katsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal sensor-based systems and mobile phones for social computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Video data mining: extracting cinematic rules from movies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multimedia data mining</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Mining lesion-deficit associations in a brain image database</title>
		<author>
			<persName><forename type="first">V</forename><surname>Megalooikonomou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davataikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Herskovits</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">A stream-based audio segmentation, classification and clustering pre-processing system for broadcast news using ann models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meinedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Interspeech-Eurospeech</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Speech discrimination based on multiscale spectrotemporal modulations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A generalised cross-modal clustering method applied to multimedia news semantic indexing and retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montagnuolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on world wide web (WWW)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Hmnews: a multimodal news data association framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montagnuolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on applied computing (SAC)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1823" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Using the fisher kernel method for web audio classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rifkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Mining association rules in temporal document collections</title>
		<author>
			<persName><forename type="first">K</forename><surname>NÃ¸rvÃ¥g</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã˜ivind</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skogstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on methodologies for intelligent systems (ISMIS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Semantic-based temporal text-rule mining</title>
		<author>
			<persName><forename type="first">K</forename><surname>NÃ¸rvÃ¥g</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Fivelstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computational linguistics and intelligent text processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="442" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Searching for structure in multiplestreams of data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference of machine learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="346" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Multimedia data mining framework for raw video sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multimedia data mining (MDM/KDD)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Discovering association rules based on image content</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Omiecinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Videocube: a novel tool for video mining and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Asian digital libraries (ICADL)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Automatic multimedia cross-modal correlation discovery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD conference on knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Multimedia data mining: an overview</title>
		<author>
			<persName><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia data mining and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Photobook: content-based manipulation of image databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Automatic audio content analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM multimedia</title>
		<imprint>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Robust speech/music classification in audio documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pinquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rouas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">-</forename><surname>Obrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on speech and language processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2005" to="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Short-time signal representation by nonlinear difference equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Hofstetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rabiner LR</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989">1993. 1989</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>Proc IEEE</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">An improved image mining technique for brain tumour classification using efficient classifier</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madheswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Information Security (IJCSIS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Videomule: a consensus learning approach to multi-label classification from noisy user-generated videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nahrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Mining statistical association rules to select the most relevant medical image features</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agr</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Felipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajm</forename><surname>Traina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Traina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining complex data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="113" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A non-classical logic for information retrieval</title>
		<author>
			<persName><forename type="first">Cjv</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="481" to="485" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">The probability ranking principle</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Doc</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Audio as a support to scene change detection and characterization of video sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saraceno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech, and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2597" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Real-time discrimination of broadcast speech/music</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="993" to="996" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Motion mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multimedia databases and image communication</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Pitch and spectral estimation of speech based on an auditory synchrony model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="3621" to="3624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A joint synchrony/mean-rate model of auditory speech processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Phon</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="76" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Applying neural network on content based audio classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific-Rim conference on multimedia</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Wavecluster: a multi-resolution clustering approach for very large spatial databases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sheikholeslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on very large data bases (VLDB)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Video data mining: mining semantic patterns with temporal constraints from movies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ideno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international symposium on multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A time constrained sequential pattern mining for extracting semantic events in videoss</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ideno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia data mining</title>
		<imprint>
			<publisher>Springer Link</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Video data mining: rhythms in a movie</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Mining event definitions from queries for video retrieval on the internet</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sugihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on data mining workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Video semantic event concept detection using a subspace based multimedia data mining framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="259" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Local color and texture extraction and spatial query</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int Conf Image Proc</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1011" to="1014" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A statistical model-based voice activity detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A comparison of document clustering techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD world text mining conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Patent data mining and effective portfolio management</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stembridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Intellect Asset Manage</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Similarity of color images. Storage retr image video databases (SPIE)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2420</biblScope>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Ballard DH Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Rule-based classification for audio data based on closed itemset mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International multiconference of engineers and computer scientists (IMECS)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Support vector machine active learning for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Nonlinear prediction of speech signals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Townshend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Patinformatics: tasks to tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trippe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Pat Inf</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="221" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A bayesian framework for semantic classification of outdoor vacation images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="issue">3656</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A novel minimum spanning tree based clustering algorithm for image mining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Scientific Research (EJSR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="540" to="546" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Classifying objectionable websites based on image content</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firschein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture notes in computer science</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Multimodal fusion for video search reranking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Speech/music discrimination based on posterior probability features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Eurospeech</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Multimodal metadata fusion using causal strength</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Image mining: trends and developments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wynne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Intell Inf Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="23" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Discover meaningful multimedia patterns with audio-visual concepts and associated text</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Recognizing human action in time-sequential images using hiddenmarkov model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computing Society conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="379" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Learning query class dependent weights in automatic video retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM multimedia</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Text mining and visualization tools-impressions of emerging capabilities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Pat Inf</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="280" to="293" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Extracting story units from long programs for video browsing and navigation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in multimedia computing and networking</title>
		<meeting><address><addrLine>San Mateo</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Time-constrained clustering for segmentation of video into story unites</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Yeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int Conf Pattern Recognit</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Multimediaminer: a system prototype for multimedia data mining</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM SIGMOD</publisher>
			<biblScope unit="page" from="581" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A multimodal data mining framework for revealing common sources of spam images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Warner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Multimedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="321" to="330" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A scheme for visual feature based image indexing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE conference on storage and retrieval for image and video databases</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">A probabilistic semantic model for image annotation and multi-modal image retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Audio content analysis for online audiovisual data segmentation and classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ccj</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="457" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Birch: an efficient data clustering method for very large databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD conference</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Image classification approach based on manifold learning in web image mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on advanced data mining and applications (ADMA)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Video data mining: semantic indexing and event detection from the association perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="665" to="677" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Phone based voice activity detection using online bayesian adaptation with conjugate normal distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pellom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
