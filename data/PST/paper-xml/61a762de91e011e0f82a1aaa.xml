<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dataset shift assessment measures in monitoring predictive models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Aneta</forename><surname>Becker</surname></persName>
							<email>abecker@zut.edu.pl</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Economics</orgName>
								<orgName type="institution">West Pomeranian University of Technology</orgName>
								<address>
									<addrLine>Janickiego 31</addrLine>
									<postCode>71-270</postCode>
									<settlement>Szczecin</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Economics</orgName>
								<orgName type="institution">West Pomeranian University of Technology</orgName>
								<address>
									<addrLine>Janickiego 31</addrLine>
									<postCode>71-270</postCode>
									<settlement>Szczecin</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jarosław</forename><surname>Becker</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Technology</orgName>
								<orgName type="institution">The Jacob of Paradies University</orgName>
								<address>
									<addrLine>F. Chopina 52</addrLine>
									<postCode>66-400</postCode>
									<settlement>Gorzów Wielkopolski</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Technology</orgName>
								<orgName type="institution">The Jacob of Paradies University</orgName>
								<address>
									<addrLine>F. Chopina 52</addrLine>
									<postCode>66-400</postCode>
									<settlement>Gorzów Wielkopolski</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Economics</orgName>
								<orgName type="institution">West Pomeranian University of Technology</orgName>
								<address>
									<addrLine>Janickiego 31</addrLine>
									<postCode>71-270</postCode>
									<settlement>Szczecin</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Technology</orgName>
								<orgName type="institution">The Jacob of Paradies University</orgName>
								<address>
									<addrLine>F. Chopina 52</addrLine>
									<postCode>66-400</postCode>
									<settlement>Gorzów Wielkopolski</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Economics</orgName>
								<orgName type="institution">West Pomeranian University of Technology</orgName>
								<address>
									<addrLine>Janickiego 31</addrLine>
									<postCode>71-270</postCode>
									<settlement>Szczecin</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Technology</orgName>
								<orgName type="institution">The Jacob of Paradies University</orgName>
								<address>
									<addrLine>F. Chopina 52</addrLine>
									<postCode>66-400</postCode>
									<settlement>Gorzów Wielkopolski</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">International 25th International Conference on Knowledge-Based and Intelligent Information &amp; Engineering Systems</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">International 25th International Conference on Knowledge-Based and Intelligent Information &amp; Engineering Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dataset shift assessment measures in monitoring predictive models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.procs.2021.09.112</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dataset shift</term>
					<term>Population Stability Index (PSI)</term>
					<term>Population Accuracy Index (PAI)</term>
					<term>monitoring of predictive model; Dataset shift</term>
					<term>Population Stability Index (PSI)</term>
					<term>Population Accuracy Index (PAI)</term>
					<term>monitoring of predictive model;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The article presents the results of the study, which is a fragment of the work carried out under the project entitled "Hybrid system for intelligent diagnostics of prognostic models", co-financed through the National Centre for Research and Development from the European Regional Development Fund. They concerned the analysis of the phenomenon of dataset shift, also known as the shift of variable distributions. It is important to answer the question: has the distribution of current data for the implemented forecasting model changed significantly compared to the distribution of data used to develop it? If so, it could lead to incorrect operation. In the context of assessing and monitoring the stability of variable distributions of predictive models, the aim of the study was to compare the properties of two indicators, the Population Stability Index (PSI) and Population Accuracy Index (PAI). These measures were calculated for 78 controlled shifts of the distribution of the 3 explanatory variables of the hypothetical prognostic model. The research procedure was carried out in 2 scenarios. The first involved a comparison of PSI and PAI for the distributions of categorical variables. In scenario 2, an answer was sought to the question whether discretization of variables significantly influenced the assessment of the stability of their distributions using PSI compared to PAI, which does not require such a procedure? The results of the research proved that both indicators complement each other well, and when used together to assess the stability of the model's variable distributions, they compensate each other's shortcomings. PSI and PAI measure subtly different concepts of stability -PSI measures any change in the distribution of explanatory variables, and PAI only measures how this change affects the prognostic accuracy of the modeltherefore they should be treated as complementary measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A dataset shift takes place in a situation, where the joint distribution of inputs and outputs differs between the stages of training, testing, and using predictive models. Until recently, this phenomenon was relatively unnoticeable by researchers. Scientists dealing with, for example, machine learning have paid more attention to this subject. They focused on seemingly similar problems found in partially supervised or active learning. Today, in most practical applications, data set shifts are observed. Most often this is due to the subjectivism introduced by the experimental design or the inability to repeat the test conditions when training the models. Moreno-Torres et al. <ref type="bibr" target="#b0">[1]</ref> list two, in their opinion, main reasons for the shift in the distribution of variables in classification studies: the error of sample selection and the occurrence of a non-stationary environment. In the first case, the shift is due to the fact that the training examples obtained do not reliably represent the operating environment in which the classifier would be implemented (which in terms of machine learning would constitute a test set). The second reason arises when the training environment differs from the test environment, whether it is due to a temporal or spatial change. According to Quionero-Candela et al. <ref type="bibr" target="#b1">[2]</ref>, dataset shift is one of the common problems in predictive modelling when the common distribution of input and output data differs between training and testing phases. According to Moreno-Torres et al. <ref type="bibr" target="#b0">[1]</ref>, a dataset shift occurs when a phenomenon occurs during data testing that leads to a change in the distribution of a single feature, combination of features, or class boundaries.</p><p>Most of the applications in use today must incorporate some form of change in their solutions. Therefore, research in this area is important and useful. However, their carrying out is difficult, for example due to the use of different concepts describing the same phenomenon in the literature. The issues related to the dataset shift are widely discussed in the literature related to the prediction of phenomena. In forecasting, the aim is to estimate the desired properties of future events, using generalizations obtained on the basis of previous experiences. Future events are expected to be almost identical to past events. Therefore, predictive models assume that the distribution of test data does not differ from the distribution of training data, which in fact is not always feasible. After implementing the predictive model, it is also important to monitor the distribution of new data. Too large deviations of this distribution in relation to the data distribution from the model construction phase may result in lower accuracy of forecasts and indicate the need to update or build a new model.</p><p>The aim of the article is to present the results of scientific research involving the analysis of the dataset shift, also known as the shift of the distribution of variables. In the practical part, the properties of two indicators, the Population Stability Index (PSI) and Population Accuracy Index (PAI) were examined in the context of assessing and monitoring the stability of the variable distributions of the predictive models. The issue of a dataset shift is so important that in a critical situation it leads to incorrect operation of the implemented model. These studies are part of the work carried out under the second stage of the project entitled "Hybrid system for intelligent diagnostics of prognostic models" (Measure 1.1. R&amp;D projects of enterprises of the Intelligent Development Operational Program 2014-2020) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The hybrid system under development is used to model, evaluate and monitor prognostic models (validation of the existing models, creation of new "better" models) and managing their life cycle.</p><p>The article consists of 6 sections. The introduction defines the problem and research goal. Section 2 provides a brief overview of the dataset shift literature. Section 3 contains a formal description of both examined indicators (PSI and PAI). Section 4 describes the testing procedure and section 5 the results of the tests and comparative analyses obtained under the two scenarios. Section 5 provides a summary of the research and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature review on dataset shift</head><p>In the literature on the dataset shift, there is a disordered nomenclature, which makes it difficult to navigate the topic in question. Quiñonero-Candela et al. <ref type="bibr" target="#b1">[2]</ref> used the term dataset shift for the first time and defined it as "cases where the overall distribution of inputs and outputs differs between training phase and test" <ref type="bibr" target="#b4">[5]</ref>. Among other concepts that appear in the literature, and are intended to characterize the dataset shift in classification issues, the following can be distinguished: the concept of shift or the concept of drift <ref type="bibr" target="#b5">[6]</ref>, change of classification <ref type="bibr" target="#b6">[7]</ref>, changing surroundings (environment) <ref type="bibr" target="#b7">[8]</ref>, exploration of contrasts in learning classification <ref type="bibr" target="#b8">[9]</ref>, the concept of break points <ref type="bibr" target="#b9">[10]</ref> and data brakes <ref type="bibr" target="#b0">[1]</ref>. Moreno-Torres et al. <ref type="bibr" target="#b0">[1]</ref> presented an extensive analysis of the different types of shifts identified in the actual classification tasks in their work. They discussed the problem of the covariate shift, defined for the first time by Shimodair <ref type="bibr" target="#b10">[11]</ref>. The mentioned type of shift is known as population drift <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Another suggestion is an earlier probability shift and concerns changes in the class distribution <ref type="bibr" target="#b13">[14]</ref>. On the other hand, the next approach, defined as a concept shift, is usually referred to in the literature as a "concept change" that occurs as a result of a changing context and may cause changes in the concepts of the target research <ref type="bibr" target="#b5">[6]</ref>.</p><p>The dataset shift problem, especially in the area of machine learning, has been fully described, among others, in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>. On the other hand, Storkey <ref type="bibr" target="#b4">[5]</ref> classified various forms of dataset shift into six groups: covariate shift <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, prior probability shift <ref type="bibr" target="#b4">[5]</ref>, sample selection bias, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> [21], <ref type="bibr" target="#b21">[22]</ref>, imbalanced data <ref type="bibr" target="#b4">[5]</ref>, domain shift <ref type="bibr" target="#b22">[23]</ref>, source component shift <ref type="bibr" target="#b4">[5]</ref>.</p><p>The occurrence of a dataset shift can to some extent be eliminated by using certain techniques. The literature proposes two main groups of methods based on: instances and distribution. Among the instance-based techniques, the methods for detecting outliers <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> deserve attention. Other proposals on this subject were presented by Kitchenham et al. <ref type="bibr" target="#b26">[27]</ref> and Keung et al. <ref type="bibr" target="#b27">[28]</ref>, Turhan et al. <ref type="bibr" target="#b18">[19]</ref> and Menzies et al. <ref type="bibr" target="#b19">[20]</ref>, Lin et al. <ref type="bibr" target="#b28">[29]</ref>. The methods by which the relevance is filtered and the introduction of a controlled error of sample selection adapt to the samples of the test kit are presented by Turhan et al. <ref type="bibr" target="#b18">[19]</ref> and Kocaguneli et al. <ref type="bibr" target="#b17">[18]</ref> and Kocaguneli and Menzies <ref type="bibr" target="#b29">[30]</ref>, about the algorithms known as soft filtering can be found in <ref type="bibr" target="#b30">[31]</ref>. Among the distribution-based methods, stratification used in post hoc analyses deserves attention, as well as the cost curve analysis in empirical studies of predictive models, which Drummond and Holte <ref type="bibr" target="#b31">[32]</ref> as well as Jiang et al. <ref type="bibr" target="#b32">[33]</ref> wrote about and taking into account the displacement of the source component of different origin <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>A comprehensive empirical study discussing how to combine dimension reduction and testing two samples to create a practical tool for detecting a distribution change in real machine learning systems is presented in Rabanser et al. <ref type="bibr" target="#b34">[35]</ref>. In the literature related to machine learning, many probabilistic deep learning methods are proposed to quantify the predictive uncertainty. Noteworthy is the proposal for the practical application of deep neutral networks (DNN). Predictive distributions of these models, integrated with conventional approaches, for example, in medical diagnosis assisted by machine learning, in imaging <ref type="bibr" target="#b35">[36]</ref> and in autonomous cars <ref type="bibr" target="#b36">[37]</ref>. The software using DNN supports: vision systems in social networks <ref type="bibr" target="#b37">[38]</ref>, radiologists <ref type="bibr" target="#b38">[39]</ref>, Internet platforms <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, speech recognition <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b40">[41]</ref>and translators <ref type="bibr" target="#b42">[43]</ref>.</p><p>Applications using DNN require point forecasts and qualification of predictive uncertainty. Safe implementation of machine learning requires model reliability in situations related to out-of-distribution (OOD) <ref type="bibr" target="#b43">[44]</ref>. Probabilistic neural networks such as mixture density networks capture the intrinsic ambiguity of outputs for a given input <ref type="bibr" target="#b44">[45]</ref>. Bayesian neural networks learn posterior decomposition on parameters that quantify parameter uncertainty, a kind of epistemic uncertainty that can be reduced by collecting additional data. Popular similar Bayesian approaches include the Laplace approximation <ref type="bibr" target="#b45">[46]</ref>, variational reasoning <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, resignation-based variation inference <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, expectation propagation <ref type="bibr" target="#b50">[51]</ref> and MCMC stochastic gradient <ref type="bibr" target="#b51">[52]</ref>. Non-Bayesian methods apply to training many probabilistic neural networks, for example with a bootstrap <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p><p>The literature on the subject points out that machine learning-based software systems are sensitive and difficult to test <ref type="bibr" target="#b54">[55]</ref>. Seemingly insignificant changes in data distributions may disrupt the operation of even the most modern classifiers <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>. When decisions are made under conditions of uncertainty, even label shifts have an impact on their accuracy <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Among the publications, the work by Lipton et al. <ref type="bibr" target="#b57">[58]</ref> who discussed the Black Box Offset Detection (BBSD) method and proposed the use of an appropriate label classifier. Other literature proposals focus on anomaly detection, for example the article by Chandol et al. <ref type="bibr" target="#b23">[24]</ref> and Markou and Singh <ref type="bibr" target="#b59">[60]</ref>. Whereas Truong et al. <ref type="bibr" target="#b60">[61]</ref> presented the classic problem of time series and detection of the change point in a data stream. A significant area of literature interest relates to the dataset shift in the context of domain adaptation. Note that shifts cannot be corrected without assumptions <ref type="bibr" target="#b61">[62]</ref> and a covariate shift <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref> or a label shift <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b58">[59]</ref> is often assumed. In their work, Scholkopf et al. <ref type="bibr" target="#b65">[66]</ref> presented a unified picture of these changes and combined the assumed invariants with the corresponding causal assumptions. Popular topics include outlier detection mechanisms, which are described in the literature as out of distribution (OOD) sample detection. More information on this was published by Hendrycks and Gimpel <ref type="bibr" target="#b66">[67]</ref>, Liang et al. <ref type="bibr" target="#b67">[68]</ref> and Lee et al. <ref type="bibr" target="#b68">[69]</ref>. Whereas Shafaei et al. <ref type="bibr" target="#b69">[70]</ref> examined numerous techniques for the detection of OOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Measures of compliance of variable distributions -PSI and PAI</head><p>A popular measure to verify that the distribution of the current data has changed significantly compared to the distribution of the data used to develop the model is the Population Stability Index (PSI). The Population Accuracy Index (PAI) proposed by Taplin and Hunt <ref type="bibr" target="#b70">[71]</ref> is an interesting alternative to the PSI. According to the authors, PAI can more accurately summarize the level of population stability and thus help risk analysts and managers determine if the model is working as intended.</p><p>The PSI population stability index is closely related to well-established entropy measures and is essentially a symmetric measure of the difference between two statistical distributions. Significant studies on PSI are the publications of Karakoulas <ref type="bibr" target="#b71">[72]</ref> and Siddiqi <ref type="bibr" target="#b72">[73]</ref>. For the purpose of determining the PSI, it is assumed that there are K mutually exclusive categories, numbered from 1 to K, and the mathematical formula can be written as:</p><formula xml:id="formula_0">𝑃𝑃𝑃𝑃𝐼𝐼 = ∑ (𝑂𝑂 𝐾𝐾 𝑖𝑖=1 𝑖𝑖 − 𝐸𝐸 𝑖𝑖 ) × 𝑙𝑙𝑙𝑙 ( 𝑂𝑂 𝑖𝑖 𝐸𝐸 𝑖𝑖 ),<label>(1)</label></formula><p>where: Oi − the observed relative frequency of occurrences in category and during the review, Ei − the relative frequency of occurrences in the category and during the construction phase (the relative frequency during the survey is expected to be similar to the relative frequency during the model construction), i − the category taking values from 1 to K, ln() − natural logarithm.</p><p>The PSI value equal to 0 means that the observed and expected distributions are identical, but as the two distributions are divergent, the value of the PSI index increases. According to Siddiqi <ref type="bibr" target="#b72">[73]</ref>, the PSI values can be classified as follows: below 10% there are no significant changes in stability, in the range of [10%, 25%] show little change requiring testing, above 25% show a significant change.</p><p>The Prediction Accuracy Index (PAI) is defined as the mean variable of the estimated mean response in a model review divided by the mean variance of the estimated mean response at the time of its development. The values of the explanatory variables (design space) are important, while the response values are irrelevant and are not required. A high PAI value occurs when, during the review, the explanatory variables assume values that cause the variance of the predicted response to be higher than the corresponding variance in the model development phase.</p><p>PAI measures the increase in the variance of the estimated mean response since the model was built. Taplin and Hunt <ref type="bibr" target="#b70">[71]</ref> recommend using the following classification of PAI values: below 1.1 indicates no significant deterioration of the prognosis accuracy, in the range [1,1; 1,5] − deterioration requiring further investigation, above 1.5 − the prognostic accuracy of the model deteriorated significantly.</p><p>It is worth considering several situations when the PAI prognosis accuracy index can be determined. 1) For simple linear regression of the form (for k = 1):</p><formula xml:id="formula_1">𝛽𝛽 0 𝑥𝑥 𝑖𝑖0 + 𝛽𝛽 1 𝑥𝑥 𝑖𝑖1 + ⋯ + 𝛽𝛽 𝑘𝑘 𝑥𝑥 𝑖𝑖𝑘𝑘 ,<label>(2)</label></formula><p>where: β0, …, βk − estimated regression coefficients, xi0, …, xik − values of the explanatory (numerical) variables for the i-th observation, the variance of the estimated mean response when the explanatory variable xi is equal to z can be according to Ramsey and Schafer ( <ref type="bibr" target="#b73">[74]</ref>, p. 187) written as: ).</p><formula xml:id="formula_2">𝑀𝑀𝑃𝑃𝐸𝐸 × ( 1 𝑛𝑛 + (𝑧𝑧−𝑥𝑥̅ ) 2 ∑(𝑥𝑥 𝑖𝑖 −𝑥𝑥̅ ) 2 ),<label>(3)</label></formula><p>(4) Author name / Procedia Computer Science 00 (2021) 000-000</p><p>2) In the case of a multiple regression model:</p><formula xml:id="formula_3">𝛽𝛽 0 𝑥𝑥 𝑖𝑖0 + 𝛽𝛽 1 𝑥𝑥 𝑖𝑖1 + ⋯ + 𝛽𝛽 𝑘𝑘 𝑥𝑥 𝑖𝑖𝑘𝑘 (5)</formula><p>estimated variance of the mean response when the explanatory variables 𝑥𝑥 𝑖𝑖1 , 𝑥𝑥 𝑖𝑖2 , … , 𝑥𝑥 𝑖𝑖𝑖𝑖 take the values 𝑧𝑧 𝑖𝑖1 , 𝑧𝑧 𝑖𝑖2 , … , 𝑧𝑧 𝑖𝑖𝑖𝑖 Johnson and Wichern ( <ref type="bibr" target="#b74">[75]</ref>, p. 378) propose to be written as:</p><formula xml:id="formula_4">𝑀𝑀𝑀𝑀𝑀𝑀 × 𝑧𝑧 𝑗𝑗 𝑇𝑇 (𝑋𝑋 𝑇𝑇 𝑋𝑋) −1 𝑧𝑧 𝑗𝑗 ,<label>(6)</label></formula><p>where: 𝑧𝑧 𝑗𝑗 𝑇𝑇 = 𝑧𝑧 𝑖𝑖1 , 𝑧𝑧 𝑖𝑖2 , … , 𝑧𝑧 𝑖𝑖𝑖𝑖 − vector of a row of explanatory variables, X − matrix of explanatory variables in the development stage, MSE − mean square error (residuals) from model development, T − transposition, () -1 − matrix reverse.</p><p>The X columns are equal to the values of the explanatory variables of the development data (the rows are similar to zj T for each observation in the model development data). Equation ( <ref type="formula" target="#formula_4">6</ref>) can be calculated by the formula:</p><formula xml:id="formula_5">𝑧𝑧 𝑗𝑗 𝑇𝑇 𝑉𝑉𝑧𝑧 𝑗𝑗 , (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>where 𝑉𝑉 = 𝑀𝑀𝑀𝑀𝑀𝑀 × (𝑋𝑋 𝑇𝑇 𝑋𝑋) −1 − the variance-covariance matrix of the estimated regression coefficients (1, 2, …, p). PAI for multiple regression is defined as the mean of equation ( <ref type="formula" target="#formula_5">7</ref>) calculated with the values of the explanatory variables zj during the model review divided by the mean of equation ( <ref type="formula" target="#formula_5">7</ref>) calculated with the values of explanatory variables zj from the model development phase:</p><formula xml:id="formula_7">𝑃𝑃𝑃𝑃𝑃𝑃 = ∑ 𝑟𝑟 𝑗𝑗 𝑇𝑇 𝑉𝑉𝑟𝑟 𝑗𝑗 𝑁𝑁 𝑗𝑗=1 /𝑁𝑁 ∑ 𝑥𝑥 𝑖𝑖 𝑇𝑇 𝑉𝑉𝑥𝑥 𝑖𝑖 𝑛𝑛 𝑗𝑗=1 /𝑛𝑛<label>(8)</label></formula><p>where: rj − vector of explanatory variables for the j-th observation of the review data (j = 1, …, N), xi − vector of explanatory variables for the i-th observation of the development data (i = 1, …, n) <ref type="bibr" target="#b70">[71]</ref>. Equation ( <ref type="formula" target="#formula_7">8</ref>) can be used for a multiple regression model and when one categorical variable has more than two feature variants, which requires constructing proxy variables to model the differences between these categories. PAI has the property of invariance, which means that any changes in the data related to its movementfrom one category during the model development phase to another category during its reviewdo not affect the precision of the model, if both categories were estimated with the same precision in development phase. For each breakdown of the survey data, if they are evenly distributed among the categories during the construction phase of the model, the PAI will always be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The course of the research procedure</head><p>The research procedure of the comparative analysis of population stability measures in the monitoring of prognostic models was carried out under 2 scenarios (1covered the dataset shift study for categorical variables, 2carried out for the corresponding continuous variables). Five main stages were distinguished in the research procedure. Stages 1 and 2 included the preparation of a special set of research data that allowed for the simulation of the phenomenon of shift in the distribution of the model variables in a fully controlled manner. The idea behind this procedure was to establish data distributions for a categorical variable, transform them into the form of distributions of a continuous variable in order to conduct comparative experiments. In order to compare the results of both research scenarios, the values of continuous variables were generated according to the order of observations in the set of categorical variables.</p><p>In stage 1 of the research procedure, a hypothetical explanatory variable 𝑥𝑥 𝐴𝐴1 o 𝑛𝑛 = 7 with (𝑘𝑘 1 , … , 𝑘𝑘 𝑛𝑛 ) categories was determined, and its individual distributions were described on the set containing 𝑧𝑧 = 100 observations. The choice of the number of categories was deliberate due to the manual control of the shift in the distribution of variables and the proper illustration of the influence of persons migrating between the categories of observations on the shaping of the values of the compared indicators (PSI, PAI). The number of one hundred observations was indicated due to the ease of calculating the vector 𝐶𝐶 𝑟𝑟 = [𝑐𝑐 𝑟𝑟,𝑖𝑖 , … , 𝑐𝑐 𝑟𝑟,𝑛𝑛 ] of absolute frequencies of the observations (in terms of 𝑘𝑘 𝑖𝑖 , 𝑖𝑖 = 1, … , 𝑛𝑛 and for individual distributions 𝑟𝑟 = 1, … , 𝑚𝑚 ) on the relative values of the vector 𝑀𝑀 𝑟𝑟 = [𝑒𝑒 𝑟𝑟,𝑖𝑖 , … , 𝑒𝑒 𝑟𝑟,𝑛𝑛 ] during the development of the model and the vector 𝑂𝑂 𝑟𝑟 = [𝑜𝑜 𝑟𝑟,𝑖𝑖 , … , 𝑜𝑜 𝑟𝑟,𝑛𝑛 ] during the model review. In the next step of this stage, the vector of absolute frequency of occurrences was determined for the hypothetical categorical variable 𝑥𝑥 𝐴𝐴1 and for each 𝑘𝑘 𝑖𝑖 (𝑖𝑖 = 1, … , 7) 𝐶𝐶 24 = <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref>. Then, 78 different data movements were determined against the 𝐶𝐶 24 vector. These activities were performed using three different strategies. The first one concerned one-way, unbalanced in relation to the base 𝐶𝐶 24 distribution, migration of observations between categories, from more frequent to less frequent (distributions 𝐶𝐶 25 , …, 𝐶𝐶 54 ) and vice versa, from less frequent to more frequent categories (𝐶𝐶 23 , , 𝐶𝐶 1 ). The second strategy consisted in a bidirectional migration of observations in terms of 𝐶𝐶 24 in extreme categories: 𝑘𝑘 1 , 𝑘𝑘 2 𝑖𝑖𝑘𝑘 6 , 𝑘𝑘 7 from more to less frequent and in central categories: 𝑘𝑘 3 , 𝑘𝑘 4 , 𝑘𝑘 5 from less to more frequent (distributions 𝐶𝐶 60 , …, 𝐶𝐶 69 ). These activities were repeated, changing the roles of the extreme categories with the central ones, and the distributions 𝐶𝐶 59 , …, 𝐶𝐶 55 were obtained. The third strategy included a one-way, 𝐶𝐶 24 balanced migration of observations between categories. The samples were moved separately in extreme and central categories, always in one direction, from more to less frequent (𝐶𝐶 74 , …, 𝐶𝐶 70 ) and from less to more frequent (𝐶𝐶 75 , …, 𝐶𝐶 79 ). In total, 79 distributions of the categorical variable were established (𝐶𝐶 1 , …, 𝐶𝐶 79 ; tab. 1).</p><p>Stage 2 consisted in transforming m = 79 declared frequency vectors of the categorical variable 𝐶𝐶 𝑟𝑟 (𝑟𝑟 = 1, … , 𝑚𝑚) to the form of vectors of values 𝑉𝑉 𝑟𝑟 = [𝑣𝑣 𝑟𝑟,1 , …, 𝑣𝑣 𝑟𝑟,𝑧𝑧 ], where 𝑧𝑧 = 100 observations. First, exemplary ranges of continuous variable values were determined for each category 𝑘𝑘 𝑖𝑖 (𝑖𝑖 = 1, … , 7): 𝑝𝑝 1 = 30; 150), 𝑝𝑝 2 = 150; 160), 𝑝𝑝 3 = 160; 164), 𝑝𝑝 4 = 164; 167), 𝑝𝑝 5 = 167; 170), 𝑝𝑝 6 = 170; 180), 𝑝𝑝 7 = 180; 240). Then, for each interval 𝑝𝑝 𝑖𝑖 real numbers were drawn 𝑐𝑐 𝑟𝑟,𝑖𝑖 , which together gave the one-element vectors 𝑉𝑉 𝑟𝑟 . The Mersenne-Twister generator available in the "R" programming package was used to determine the pseudo-random numbers.</p><p>In stage 3 of the research procedure, from among all the distributions 𝐶𝐶 1 , …, 𝐶𝐶 79 , three were selected that meet the condition of no full collinearity: 𝐶𝐶 16 , 𝐶𝐶 24 , 𝐶𝐶 43 and were established with the distributions of 3 explanatory variables: 𝑥𝑥 𝐴𝐴1 , 𝑥𝑥 𝐴𝐴2 , 𝑥𝑥 𝐴𝐴3 of a hypothetical model in the phase of its development. The remaining distributions of categorical variables were treated in the PAIA and PSIA calculations as if they came from the model review phases. Scenario 2 was a follow-up study. It verified whether the discretization of variables can significantly affect the assessment of population stability using PSIA compared to PAIB, which does not require such a procedure. Therefore, the generated values of 𝑉𝑉 𝑟𝑟 vectors were established in PAIB calculations with distributions of continuous variables: 𝑥𝑥 𝐵𝐵1 , 𝑥𝑥 𝐵𝐵2 , 𝑥𝑥 𝐵𝐵3 .</p><p>Stage 4 of the research procedure was required in the PAI 𝐴𝐴 calculations (scenario 1) and involved the quantification of qualitative factors (categorical variables) with binary variables. This was done by transforming 𝑛𝑛 categories into 𝑛𝑛 − 1 artificial variables (zero-one).</p><p>Stage 5 consisted in the development of algorithms in the "R" program and the performance of PSI 𝐴𝐴 , PAI 𝐴𝐴 calculations for categorical variables and PAI 𝐵𝐵 for continuous variables in accordance with (1) -(8) contained in section 5. In the final part of this stage, the results obtained in under both research scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion of the obtained results</head><p>The results of the study, including a comparative analysis of univariate population stability assessment indicators, are summarized in Table <ref type="table" target="#tab_0">1</ref> -PSI and PAI values for categorical variables (scenario 1) and PAI values for continuous variables (scenario 2). Blue marks the fields for situations where it was impossible to calculate the PSI value, i.e. for the distributions of categorical variables from the model review phase, for which the absolute frequency of observations in the 𝑘𝑘 𝑖𝑖 category was zero (𝑐𝑐 𝑟𝑟;𝑖𝑖 = 0; 𝑟𝑟 = 1, … , 79), which meant the disappearance of the 𝑘𝑘 𝑖𝑖 category in the model review phase. The remaining fields are marked with colours that reflect the three interpretable ranges of values for PSI and PAI recommended in the literature <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b75">[76]</ref>: 1) safe area (green, normal conditions)population stability (PSI &lt; 0.1; PAI &lt; 1.1), 2) hazard warning area (yellow, warning conditions)slight deterioration of population stability (0.1  PSI  0.25 and 1.1  PAI  1.5), 3) critical area (red, risky conditions)significant instability of the population (PSI &gt; 0.25; PAI &gt; 1.5).</p><p>Calculations in scenarios 1 and 2 were performed for 𝑚𝑚 − 1 = 78 shifts of the base distribution (from the model development phase) for each variable. In Table <ref type="table" target="#tab_0">1</ref>, the lines with the base distributions are marked in grey. The values PSI 𝐴𝐴 = 0 and PAI 𝐴𝐴 = PAI 𝐵𝐵 = 1 included in them mean that the distributions from the model construction and review phase are identical.</p><p>Omitting 20 cases for which it was impossible to calculate the PSI (Table <ref type="table" target="#tab_0">1</ref>, blue), out of 78 different shifts of the distribution, i.e. for 58 cases and each explanatory variable 𝑥𝑥 𝐴𝐴1 , 𝑥𝑥 𝐴𝐴2 , 𝑥𝑥 𝐴𝐴3 , a comparative analysis was made to assess the degree of stability of the population of both indicators. The number of cases for which the population stability assessments using PSI 𝐴𝐴1 , PSI 𝐴𝐴2 , PSI 𝐴𝐴3 and PAI 𝐴𝐴1 , PAI 𝐴𝐴2 , PAI 𝐴𝐴3 were different, was as follows: for the explanatory variable 𝑥𝑥 𝐴𝐴1 it was 19 cases (32.8%), for 𝑥𝑥 𝐴𝐴2 16 (27.6%) and for 𝑥𝑥 𝐴𝐴3 as many as 44 cases (75.9%). Comparing the colours of the fields representing the population stability classes defined by PSI 𝐴𝐴 and PAI 𝐴𝐴 for individual explanatory variables, it should be stated that signalling about the threat or instability of the population (yellow and orange) in many cases lies both with PSI 𝐴𝐴 and PAI 𝐴𝐴 .</p><p>In scenario 2, it was verified whether PAI 𝐵𝐵 calculated for the distributions of continuous variables, signals significantly differently about the degree of risk of population instability compared to PSI 𝐴𝐴 and PAI 𝐴𝐴 calculated for categorical variables (especially in relation to PSI 𝐴𝐴 , which requires data discretization).</p><p>For 58 cases of categorical variables and the corresponding continuous variables, a comparative analysis was performed to assess the degree of stability of the population of indicators: PSI 𝐴𝐴1 , PSI 𝐴𝐴2 , PSI 𝐴𝐴3 with: PAI 𝐵𝐵1 , PAI 𝐵𝐵2 , PAI 𝐵𝐵3 . The number of cases for which the population stability assessments obtained using both indicators were different, was as follows: for variables 𝑥𝑥 𝐴𝐴1 and 𝑥𝑥 𝐵𝐵1 there were 18 cases (31%), for 𝑥𝑥 𝐴𝐴2 and 𝑥𝑥 𝐵𝐵2 13 (22.4%) and for 𝑥𝑥 𝐴𝐴3 and 𝑥𝑥 𝐵𝐵3 as many as 46 cases (79.3%). The results of comparing PSI 𝐴𝐴 values with PAI 𝐵𝐵 are very close to the results of scenario 1 for PSI 𝐴𝐴 and PAI 𝐴𝐴 . In the next step, within 78 different shifts of the distribution of 3 variables, the following were compared: PAI 𝐴𝐴1 , PAI 𝐴𝐴2 , PAI 𝐴𝐴3 with: PAI 𝐵𝐵1 , PAI 𝐵𝐵2 , PAI 𝐵𝐵3 . The number of cases for which the population stability assessments, calculated using PAI 𝐴𝐴 for categorical variables and PAI 𝐵𝐵 for continuous variables, differed and got summarized in Table <ref type="table" target="#tab_0">1</ref>. Fora 𝑥𝑥 𝐴𝐴1 and 𝑥𝑥 𝐵𝐵1 it was 16 cases (20.5%), for 𝑥𝑥 𝐴𝐴2 and 𝑥𝑥 𝐵𝐵2 6 (7.7%), and for 𝑥𝑥𝑥𝑥 𝐴𝐴3 and 𝑥𝑥𝑥𝑥 𝐵𝐵3 61 cases (78.2%). The question is, do differences in population stability assessments favour of continuous or categorical variables? The results indicate that a hypothetical model based on continuous variables 𝑥𝑥 𝐵𝐵1 , 𝑥𝑥 𝐵𝐵2 , 𝑥𝑥 𝐵𝐵3 could demonstrate greater prognostic accuracy, as these variables turned out to be less sensitive to the change of distribution compared to their categorical counterparts. It is clearly noticeable in the values of PAI 𝐴𝐴3 and PAI 𝐵𝐵3 (Table <ref type="table" target="#tab_0">1</ref>). The PAI 𝐴𝐴3 index for the categorical variable 𝑥𝑥 𝐴𝐴3 in 57 cases out of 78 examined indicated the deterioration of the prognostic accuracy of the model, while PAI 𝐵𝐵3 for the continuous variable 𝑥𝑥 𝐵𝐵3 only in four: 𝑉𝑉 47 , 𝑉𝑉 48 , 𝑉𝑉 52 𝑎𝑎𝑎𝑎𝑎𝑎𝑉𝑉 54 .</p><p>The PSI assessment of population stability in models based on continuous variable distributions always requires discretization, which can significantly distort this assessment, which was confirmed by the results obtained in scenario 2. In this context, the PSI is a less reliable measure, as its values depend on the adopted method of discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Final conclusions</head><p>PSI and PAI measure subtly different aspects of stability. The PSI reflects any change in the distribution of the explanatory variables, and the PAI only indicates how these changes affect the prognostic accuracy of the model. PAI in relation to the commonly used PSI index is a complementary solution. The results of the comparative studies showed that these indicators complement each other, and their simultaneous use to assess the stability of the model variable distributions compensates for their shortcomings (Table <ref type="table" target="#tab_1">2</ref>). PAI is directly applicable to explanatory variables that are numeric or categorical (ordered or unordered). It can be determined even when the categories have frequencies close to zero in the development or review data. It can also be derived from the review data and without making any assumptions of linearity considered appropriate in designing the model. PAI's drawbacks include the use of variable segmentation to remove the effect of outliers.</p><p>If during the model construction phase a transformation of statistical data consisting in limiting the impact of outliers or minor observation errors was used, then when calculating PAI, these actions should be taken into account, because this indicator for the data from the review may be strongly influenced by several extreme outliers, which was confirmed in work <ref type="bibr" target="#b70">[71]</ref>. However, if it is desired to monitor modelling decisions with such transformations in mind, it is recommended to calculate PAI with and without these activities. In cases of one-dimensional distribution for each variable, PAI or PSI values may expose the variables responsible for instability. The monitoring reports obtained in this way will allow to assess whether the model in the review phase meets the assumed goal or not.</p><p>The conducted research does not exhaust the issues related to detecting dataset shift in order to monitor statistical models. This phenomenon is so new and complex that it requires further research and experimental research. An important research problem and an important direction for further analyses is the assessment of the dataset shift in a multidimensional approach, e.g. testing the properties of the Multivariate Predictive Accuracy Index (MPAI), especially due to the fact that this issue is rarely present in the literature on the subject.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where: MSE − mean square error (of residuals) from the model development, 𝑥𝑥̅ − mean value of the explanatory variable xi at the development stage, n − sample size in the development stage. PAI for a simple linear regression equals equation (3) averaged over all z values equal to the review data (denoted rj; j = 1, …, N) divided by equation (3) ave3raged over all z values equal to the development data (denoted xi; i = 1, …, n): 𝑖𝑖 −𝑥𝑥̅ ) 2 /𝑛𝑛 𝑛𝑛 𝑗𝑗=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the values of the compliance assessments of the distributions of explanatory variables</figDesc><table><row><cell>Cr</cell><cell></cell><cell cols="3">The absolute frequency of the observations in categories ki (i = 1, …, 7)</cell><cell></cell><cell cols="6">Indicator values for categorical variables PSIA PAIA</cell><cell>Vr</cell><cell>Indicator values for continuous variables PAIB</cell></row><row><cell></cell><cell cols="5">cr;1 cr;2 cr;3 cr;4 cr;5 cr;6 cr;7</cell><cell>xA1</cell><cell>xA2</cell><cell>xA3</cell><cell>xA1</cell><cell>xA2</cell><cell>xA3</cell><cell></cell><cell>xB1</cell><cell>xB2</cell><cell>xB3</cell></row><row><cell>C1</cell><cell>0</cell><cell>0</cell><cell>20 60 20</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5714 0.4637 1.3766</cell><cell cols="2">V1 0.5095 0.5218 0.5171</cell></row><row><cell>C2</cell><cell>0</cell><cell>0</cell><cell>21 58 21</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5762 0.4680 1.3740</cell><cell cols="2">V2 0.5092 0.5205 0.5171</cell></row><row><cell>C3</cell><cell>0</cell><cell>0</cell><cell>22 56 22</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5810 0.4724 1.3714</cell><cell cols="2">V3 0.5105 0.5238 0.5181</cell></row><row><cell>C4</cell><cell>0</cell><cell>0</cell><cell>23 54 23</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5857 0.4768 1.3688</cell><cell cols="2">V4 0.5106 0.5247 0.5178</cell></row><row><cell>C5</cell><cell>0</cell><cell>0</cell><cell>24 52 24</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5905 0.4812 1.3662</cell><cell cols="2">V5 0.5113 0.5264 0.5183</cell></row><row><cell>C6</cell><cell>0</cell><cell>0</cell><cell>25 50 25</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.5952 0.4856 1.3636</cell><cell cols="2">V6 0.5108 0.5245 0.5182</cell></row><row><cell>C7</cell><cell>0</cell><cell>0</cell><cell>26 48 26</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6000 0.4900 1.3610</cell><cell cols="2">V7 0.5099 0.5227 0.5175</cell></row><row><cell>C8</cell><cell>0</cell><cell>0</cell><cell>27 46 27</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6048 0.4944 1.3584</cell><cell cols="2">V8 0.5116 0.5277 0.5184</cell></row><row><cell>C9</cell><cell>0</cell><cell>0</cell><cell>28 44 28</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6095 0.4987 1.3558</cell><cell cols="2">V9 0.5112 0.5277 0.5177</cell></row><row><cell>C10</cell><cell>0</cell><cell>1</cell><cell>27 44 27</cell><cell>1</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6238 0.5345 1.3442</cell><cell cols="2">V10 0.5129 0.5336 0.5182</cell></row><row><cell>C11</cell><cell>0</cell><cell>2</cell><cell>27 42 27</cell><cell>2</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6429 0.5746 1.3299</cell><cell cols="2">V11 0.5148 0.5399 0.5189</cell></row><row><cell>C12</cell><cell>0</cell><cell>3</cell><cell>26 42 26</cell><cell>3</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6571 0.6103 1.3182</cell><cell cols="2">V12 0.5158 0.5441 0.5190</cell></row><row><cell>C13</cell><cell>0</cell><cell>4</cell><cell>26 40 26</cell><cell>4</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6762 0.6504 1.3039</cell><cell cols="2">V13 0.5175 0.5488 0.5200</cell></row><row><cell>C14</cell><cell>0</cell><cell>5</cell><cell>25 40 25</cell><cell>5</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6905 0.6861 1.2922</cell><cell cols="2">V14 0.5239 0.5723 0.5218</cell></row><row><cell>C15</cell><cell>0</cell><cell>6</cell><cell>25 38 25</cell><cell>6</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.7095 0.7262 1.2779</cell><cell cols="2">V15 0.5282 0.5877 0.5231</cell></row><row><cell>C16</cell><cell>1</cell><cell>6</cell><cell>24 38 24</cell><cell>6</cell><cell cols="2">1 0.2031</cell><cell>0</cell><cell cols="2">1.5999 0.7524</cell><cell>1</cell><cell>1.2724</cell><cell cols="2">V16 0.6938</cell><cell>1</cell><cell>0.5754</cell></row><row><cell>C17</cell><cell>1</cell><cell>7</cell><cell>24 36 24</cell><cell>7</cell><cell cols="7">1 0.1757 0.0042 1.4950 0.7714 1.0401 1.2581</cell><cell cols="2">V17 0.6591 1.0508 0.5644</cell></row><row><cell>C18</cell><cell>2</cell><cell>7</cell><cell>23 36 23</cell><cell>7</cell><cell cols="7">2 0.0957 0.0189 1.2500 0.8143 1.3139 1.2525</cell><cell cols="2">V18 0.6364 0.9728 0.5562</cell></row><row><cell>C19</cell><cell>2</cell><cell>8</cell><cell>23 34 23</cell><cell>8</cell><cell cols="7">2 0.0773 0.0307 1.1577 0.8333 1.3540 1.2382</cell><cell cols="2">V19 0.7223 1.2721 0.5852</cell></row><row><cell>C20</cell><cell>3</cell><cell>8</cell><cell>22 34 22</cell><cell>8</cell><cell cols="7">3 0.0382 0.0634 1.0050 0.8762 1.6278 1.2327</cell><cell cols="2">V20 0.9486 2.0785 0.6542</cell></row><row><cell>C21</cell><cell>3</cell><cell>9</cell><cell>22 32 22</cell><cell>9</cell><cell cols="7">3 0.0276 0.0821 0.9230 0.8952 1.6679 1.2184</cell><cell cols="2">V21 1.0089 2.2888 0.6743</cell></row><row><cell>C22</cell><cell>4</cell><cell>9</cell><cell>21 32 21</cell><cell>9</cell><cell cols="7">4 0.0088 0.1258 0.8114 0.9381 1.9417 1.2128</cell><cell cols="2">V22 0.7413 1.3281 0.5954</cell></row><row><cell>C23</cell><cell>4</cell><cell cols="3">10 21 30 21 10</cell><cell cols="7">4 0.0054 0.1510 0.7382 0.9571 1.9818 1.1985</cell><cell cols="2">V23 0.7273 1.2929 0.5854</cell></row><row><cell>C24</cell><cell>5</cell><cell cols="3">10 20 30 20 10</cell><cell>5</cell><cell>0</cell><cell cols="2">0.2031 0.6513</cell><cell>1</cell><cell cols="2">2.2556 1.1929</cell><cell>V24</cell><cell>1</cell><cell>1.8745 0.7273</cell></row><row><cell>C25</cell><cell>5</cell><cell cols="3">11 20 28 20 11</cell><cell cols="7">5 0.0033 0.2345 0.5859 1.0190 2.2957 1.1787</cell><cell cols="2">V25 0.9219 1.9824 0.6464</cell></row><row><cell>C26</cell><cell>6</cell><cell cols="3">11 19 28 19 11</cell><cell cols="7">6 0.0080 0.2937 0.5160 1.0619 2.5695 1.1731</cell><cell cols="2">V26 0.9256 1.9899 0.6497</cell></row><row><cell>C27</cell><cell>6</cell><cell cols="3">12 19 26 19 12</cell><cell cols="7">6 0.0177 0.3313 0.4576 1.0810 2.6096 1.1588</cell><cell cols="2">V27 0.9682 2.1468 0.6607</cell></row><row><cell>C28</cell><cell>7</cell><cell cols="3">12 18 26 18 12</cell><cell cols="7">7 0.0307 0.3967 0.4006 1.1238 2.8835 1.1532</cell><cell cols="2">V28 1.3112 3.3561 0.7704</cell></row><row><cell>C29</cell><cell>7</cell><cell cols="3">13 18 24 18 13</cell><cell cols="7">7 0.0468 0.4406 0.3489 1.1429 2.9236 1.1390</cell><cell cols="2">V29 1.1943 2.9444 0.7328</cell></row><row><cell>C30</cell><cell>8</cell><cell cols="3">13 17 24 17 13</cell><cell cols="7">8 0.0671 0.5120 0.3023 1.1857 3.1974 1.1334</cell><cell cols="2">V30 1.2516 3.1396 0.7538</cell></row><row><cell>C31</cell><cell>8</cell><cell cols="3">14 17 22 17 14</cell><cell cols="7">8 0.0897 0.5624 0.2568 1.2048 3.2375 1.1191</cell><cell cols="2">V31 1.1739 2.8689 0.7276</cell></row><row><cell>C32</cell><cell>9</cell><cell cols="3">14 16 22 16 14</cell><cell cols="7">9 0.1166 0.6394 0.2191 1.2476 3.5113 1.1135</cell><cell cols="2">V32 1.5526 4.2142 0.8448</cell></row><row><cell>C33</cell><cell>9</cell><cell cols="3">15 16 20 16 15</cell><cell cols="7">9 0.1460 0.6969 0.1797 1.2667 3.5514 1.0993</cell><cell cols="2">V33 1.1409 2.7523 0.7172</cell></row><row><cell cols="12">C34 10 15 15 20 15 15 10 0.1792 0.7795 0.1498 1.3095 3.8252 1.0937</cell><cell cols="2">V34 1.6856 4.6806 0.8882</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the properties of the PSI and PAI indicators Measures the overall difference in the distribution of data from review and model development + -Measures the predictability of the model on the basis of changes in the distribution of explanatory variables. -+ It is used to evaluate a shift in the distribution of a categorical variable. + + It is applicable to distributions of equal frequency of observations in categories. + -It can be calculated when a category is no longer present in the review data (the frequency of observations in the category is zero). -+ It is used to evaluate the shift of the continuous variable distribution (it does not require discretization of the continuous variable value) -+ It is used as a multivariate measure that is used to evaluate a shift in a multivariate distribution. -+</figDesc><table><row><cell>Properties</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The project was co-financed through the National Centre for Research and Development with headquarters in Warsaw (contract no. POIR.01.01.01-00-0322/18-00) from the European Regional Development Fund. Contractor: BD Polska Sp. z o.o. with its registered office in Warsaw at 9/11 Wierzbowa street, Poland. Subcontractor: Jacob of Paradyz University with headquarters in Gorzów Wielkopolski, 25 Teatralna street, Poland. Aneta Becker, Jarosław Becker / Procedia Computer Science 00 (2021) 000-000</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author name / Procedia Computer Science 00 (2021) 000-000 Author name / Procedia Computer Science 00 (2021) 000-000</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unifying view on dataset shift in classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alaiz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2011.06.019</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2011.06.019" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
	<note>First edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Client evaluation decision models in the credit scoring tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ziemba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radomska-Zalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2020.09.068</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2020.09.068" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="3301" to="3309" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rough set theory in the classification of loan applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radomska-Zalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ziemba</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2020.09.125</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2020.09.125" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="3235" to="3244" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When Training and Test Sets Are Different: Characterizing Learning Transfer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dataset Shift in Machine Learning</title>
				<editor>
			<persName><forename type="first">Joaquin</forename><surname>Quiñonero-Candela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning in the Presence of Concept Drift and Hidden Contexts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="69" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining Changes of Classification by Correspondence Tracing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611972733.9</idno>
		<ptr target="https://doi.org/10.1137/1.9781611972733.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 SIAM International Conference on Data Mining</title>
				<meeting>the 2003 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Assessing the Impact of Changing Environments on Classifier Performance</title>
		<author>
			<persName><forename type="first">Alaiz-Rodríguez R</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-68825-9_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-68825-9_2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bergler</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conceptual equivalence for contrast mining in classification learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2008.07.001</idno>
		<ptr target="https://doi.org/10.1016/j.datak.2008.07.001" />
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="413" to="429" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for monitoring classifiers&apos; performance: when and why failure occurs?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-008-0139-1</idno>
		<ptr target="https://doi.org/10.1007/s10115-008-0139-1" />
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="83" to="108" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
		<idno>S0378-3758(00)00115-4</idno>
		<ptr target="https://doi.org/10.1016/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of changing populations on classifier performance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<idno type="DOI">10.1145/312129.312285</idno>
		<ptr target="https://doi.org/10.1145/312129.312285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the fifth ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="367" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rejoinder: Classifier Technology and the Illusion of Progress</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the Application of ROC Analysis to Predict Classification Performance Under Varying Class Distributions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-005-4257-7</idno>
		<ptr target="https://doi.org/10.1007/s10994-005-4257-7" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative Learning Under Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correcting Sample Selection Bias by Unlabeled Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0080</idno>
		<ptr target="https://doi.org/10.7551/mitpress/7503.003.0080" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19, Proceedings of the 2006 Conference</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Direct importance estimation for covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Bünau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10463-008-0197-x</idno>
		<ptr target="https://doi.org/10.1007/s10463-008-0197-x" />
	</analytic>
	<monogr>
		<title level="j">Ann Inst Stat Math</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="699" to="746" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">When to use data from other projects for effort estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kocaguneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Keung</surname></persName>
		</author>
		<idno type="DOI">10.1145/1858996.1859061</idno>
		<ptr target="https://doi.org/10.1145/1858996.1859061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM international conference on Automated software engineering</title>
				<meeting>the IEEE/ACM international conference on Automated software engineering<address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the relative value of cross-company and within-company data for defect prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Turhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Bener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10664-008-9103-7</idno>
		<ptr target="https://doi.org/10.1007/s10664-008-9103-7" />
	</analytic>
	<monogr>
		<title level="j">Empir Software Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="540" to="578" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Implications of ceiling effects in defect predictors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cukic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1370788.1370801</idno>
		<ptr target="https://doi.org/10.1145/1370788.1370801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international workshop on Predictor models in software engineering</title>
				<meeting>the 4th international workshop on Predictor models in software engineering<address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new perspective on data homogeneity in software cost estimation: A study in the embedded systems domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bener</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11219-009-9081-z</idno>
		<ptr target="https://doi.org/10.1007/s11219-009-9081-z" />
	</analytic>
	<monogr>
		<title level="j">Software Quality Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="57" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building Software Cost Estimation Models using Homogenous Data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/esem.2007.34</idno>
		<ptr target="https://doi.org/10.1109/esem.2007.34" />
	</analytic>
	<monogr>
		<title level="m">First International Symposium on Empirical Software Engineering and Measurement</title>
				<imprint>
			<publisher>ESEM</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conceptual Association of Functional Size Measurement Methods</title>
		<author>
			<persName><forename type="first">O</forename><surname>Demirors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gencel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Assessing the applicability of fault-proneness models across object-oriented software projects</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wust</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2002.1019484</idno>
		<ptr target="https://doi.org/10.1109/TSE.2002.1019484" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="706" to="720" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empirical Studies of Quality Models in Object-Oriented Systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wüst</surname></persName>
		</author>
		<idno>S0065-2458(02)80005-5</idno>
		<ptr target="https://doi.org/10.1016/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Computers</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Zelkowitz</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="97" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross versus Within-Company Cost Estimation Studies: A Systematic Review</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Travassos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2007.1001</idno>
		<ptr target="https://doi.org/10.1109/TSE.2007.1001" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="316" to="329" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analogy-X: Providing Statistical Inference to Analogy-Based Software Cost Estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeffery</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2008.34</idno>
		<ptr target="https://doi.org/10.1109/TSE.2008.34" />
	</analytic>
	<monogr>
		<title level="m">Software Engineering</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="471" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visually mining and monitoring massive time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lankford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Nystrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining -KDD&apos;04</title>
				<meeting>the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining -KDD&apos;04<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page">460</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to Find Relevant Data for Effort Estimation?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kocaguneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<idno type="DOI">10.1109/ESEM.2011.34</idno>
		<ptr target="https://doi.org/10.1109/ESEM.2011.34" />
	</analytic>
	<monogr>
		<title level="m">in: 2011 International Symposium on Empirical Software Engineering and Measurement</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning weighted naive Bayes with accurate ranking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2004.10030</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2004.10030" />
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE International Conference on Data Mining (ICDM&apos;04)</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="567" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cost curves: An improved method for visualizing classifier performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="95" to="130" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Techniques for evaluating fault prediction models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cukic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="561" to="595" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introduction to Machine Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd ed. The</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rabanser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_20.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1396" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature21056</idno>
		<ptr target="https://doi.org/10.1038/nature21056" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">End to End Learning for Self-Driving Cars</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno>ArXiv:160407316</idno>
		<ptr target="https://arxiv.org/pdf/1704.07911.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autotagging Facebook: Social network context improves photo annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">T</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2008.4562956</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2008.4562956" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sundaram</surname></persName>
		</author>
		<idno type="DOI">10.1148/radiol.2017162326</idno>
		<ptr target="https://doi.org/10.1148/radiol.2017162326" />
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">H-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<idno>ArXiv:160607792</idno>
		<ptr target="https://arxiv.org/pdf/1606.07792.pdf%29/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Cs, Stat</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959190</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speech Recognition with Deep Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2013.6638947</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2013.6638947" />
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Concrete Problems in AI Safety</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<idno>ArXiv:160606565 [Cs</idno>
		<ptr target="https://arxiv.org/pdf/1606.06565.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian methods for adaptive models</title>
		<author>
			<persName><forename type="first">Djc</forename><surname>Mackay</surname></persName>
		</author>
		<idno type="DOI">10.7907/H3A1-WM07</idno>
	</analytic>
	<monogr>
		<title level="m">California Institute of Technology</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Practical Variational Inference for Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1506.02142.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational Dropout and the Local Reparameterization Trick</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bayesian Learning via Stochastic Gradient Langevin Dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Omnipress</publisher>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep Exploration via Bootstrapped DQN</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17: Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6405" to="6416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Machine Learning: The High Interest Credit Card of Technical Debt</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SE4ML: Software Engineering for Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>Workshop</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial Attacks on Neural Networks for Graph Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220078</idno>
		<ptr target="https://doi.org/10.1145/3219819.3220078" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Detecting and Correcting for Label Shift with Black Box Predictors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03916</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Domain Adaptation under Target and Conditional Shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Novelty detection: a review-part 1: statistical approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="2481" to="2497" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A review of change point detection methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vayatis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00718v2</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs.CE</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Impossibility Theorems for Domain Adaptation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Covariate Shift by Kernel Mean Matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dataset Shift in Machine Learning</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation with Distribution Estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1010" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602753284446</idno>
		<ptr target="https://doi.org/10.1162/089976602753284446" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning</title>
				<meeting>the 29th International Coference on International Conference on Machine Learning<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136[cs.NE]</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690[cs.LG]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09325</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation of &quot;Outlier&quot; Detectors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04729v2[cs.LG]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The Population Accuracy Index: A New Measure of Population Stability for Model Monitoring</title>
		<author>
			<persName><forename type="first">R</forename><surname>Taplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Empirical Validation of Retail Credit-Scoring Models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karakoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The RMA Journal</title>
		<imprint>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siddiqi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">The Statistical Sleuth: A Course in Methods of Data Analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Duxbury Press Hardcover</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Applied multivariate statistical analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Wichern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Pearson Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, N.J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siddiqi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Hoboken, N.J, Wiley</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
