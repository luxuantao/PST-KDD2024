<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
							<email>schickt@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sulzer GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hinrich</forename><surname>Sch Ütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., <ref type="bibr" target="#b28">Radford et al., 2019)</ref>. While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text:</p><p>• T 1 : This was the best pizza I've ever had.</p><p>• T 2 : You can get better sushi for half the price.</p><p>• T 3 : Pizza was average. Not worth the price. 1 Our implementation is publicly available at https:// github.com/timoschick/pet.</p><p>Best pizza ever! +1 ) ∈ T ( Best pizza ever! It was . Furthermore, imagine we are told that the labels of T 1 and T 2 are l and l , respectively, and we are asked to infer the correct label for T 3 . Based only on these examples, this is impossible because plausible justifications can be found for both l and l . However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l to T 3 . This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about.</p><p>With the rise of pretrained language models (PLMs) such as GPT <ref type="bibr" target="#b27">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task <ref type="bibr" target="#b28">(Radford et al., 2019;</ref><ref type="bibr" target="#b26">Puri and Catanzaro, 2019)</ref>. So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all.</p><p>In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.</p><p>On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.</p><p>2 Related Work <ref type="bibr" target="#b28">Radford et al. (2019)</ref> provide hints in the form of natural language patterns for zero-shot learning of challenging tasks such as reading comprehension and question answering (QA). This idea has been applied to unsupervised text classification <ref type="bibr" target="#b26">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type="bibr" target="#b7">(Davison et al., 2019)</ref> and argumentative relation classification <ref type="bibr" target="#b23">(Opitz, 2019)</ref>. <ref type="bibr" target="#b35">Srivastava et al. (2018)</ref> use task descriptions for zero-shot classification but require a semantic parser. For relation extraction, <ref type="bibr" target="#b2">Bouraoui et al. (2020)</ref> automatically identify patterns that express given relations. <ref type="bibr">Mc-Cann et al. (2018)</ref> rephrase several tasks as QA problems. <ref type="bibr" target="#b29">Raffel et al. (2020)</ref> frame various problems as language modeling tasks, but their patterns only loosely resemble natural language and are unsuitable for few-shot learning. 2  Another recent line of work uses cloze-style phrases to probe the knowledge that PLMs acquire during pretraining; this includes probing for factual 2 For example, they convert inputs (a, b) for recognizing textual entailment (RTE) to "rte sentence1: a sentence2: b", and the PLM is asked to predict strings like "not entailment". and commonsense knowledge <ref type="bibr" target="#b38">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b25">Petroni et al., 2019;</ref><ref type="bibr" target="#b42">Wang et al., 2019;</ref><ref type="bibr" target="#b32">Sakaguchi et al., 2020)</ref>, linguistic capabilities <ref type="bibr" target="#b10">(Ettinger, 2020;</ref><ref type="bibr" target="#b19">Kassner and Schütze, 2020)</ref>, understanding of rare words <ref type="bibr" target="#b33">(Schick and Schütze, 2020)</ref>, and ability to perform symbolic reasoning <ref type="bibr" target="#b37">(Talmor et al., 2019)</ref>. <ref type="bibr" target="#b18">Jiang et al. (2020)</ref> consider the problem of finding the best pattern to express a given task.</p><p>Other approaches for few-shot learning in NLP include exploiting examples from related tasks <ref type="bibr" target="#b49">(Yu et al., 2018;</ref><ref type="bibr" target="#b11">Gu et al., 2018;</ref><ref type="bibr" target="#b9">Dou et al., 2019;</ref><ref type="bibr">Qian and Yu, 2019;</ref><ref type="bibr" target="#b48">Yin et al., 2019)</ref> and using data augmentation <ref type="bibr" target="#b45">(Xie et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>; the latter commonly relies on back-translation <ref type="bibr" target="#b34">(Sennrich et al., 2016)</ref>, requiring large amounts of parallel data. Approaches using textual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., <ref type="bibr" target="#b31">Romera-Paredes and Torr, 2015;</ref><ref type="bibr" target="#b41">Veeranna et al., 2016;</ref><ref type="bibr" target="#b47">Ye et al., 2020)</ref>. In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge.</p><p>The idea behind iPET -training multiple generations of models on data labeled by previous generations -bears resemblance to self-training and bootstrapping approaches for word sense disambiguation <ref type="bibr" target="#b46">(Yarowsky, 1995)</ref>, relation extraction <ref type="bibr" target="#b3">(Brin, 1999;</ref><ref type="bibr" target="#b0">Agichtein and Gravano, 2000;</ref><ref type="bibr" target="#b1">Batista et al., 2015)</ref>, parsing <ref type="bibr" target="#b22">(McClosky et al., 2006;</ref><ref type="bibr" target="#b30">Reichart and Rappoport, 2007;</ref><ref type="bibr" target="#b17">Huang and Harper, 2009)</ref>, machine translation <ref type="bibr" target="#b15">(Hoang et al., 2018)</ref>, and sequence generation <ref type="bibr" target="#b13">(He et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pattern-Exploiting Training</head><p>Let M be a masked language model with vocabulary V and mask token ∈ V , and let L be a set of labels for our target classification task A. We write an input for task A as a sequence of phrases x = (s 1 , . . . , s k ) with s i ∈ V * ; for example, k = 2 if A is textual inference (two input sentences). We define a pattern to be a function P that takes x as input and outputs a phrase or sentence P (x) ∈ V * that contains exactly one mask token, i.e., its output can be viewed as a cloze question. Furthermore, we define a verbalizer as an injective function v : L → V that maps each label to a word from M 's vocabulary. We refer to (P, v) as a pattern-verbalizer pair (PVP).</p><p>Using a PVP (P, v) enables us to solve task A as follows: Given an input x, we apply P to obtain an input representation P (x), which is then processed by M to determine the label y ∈ L for which v(y) is the most likely substitute for the mask. For example, consider the task of identifying whether two sentences a and b contradict each other (label y 0 ) or agree with each other (y 1 ). For this task, we may choose the pattern P (a, b) = a?</p><p>, b. combined with a verbalizer v that maps y 0 to "Yes" and y 1 to "No". Given an example input pair x = (Mia likes pie, Mia hates pie), the task now changes from having to assign a label without inherent meaning to answering whether the most likely choice for the masked position in P (x) = Mia likes pie?</p><p>, Mia hates pie.</p><p>is "Yes" or "No".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PVP Training and Inference</head><p>Let p = (P, v) be a PVP. We assume access to a small training set T and a (typically much larger) set of unlabeled examples D. For each sequence z ∈ V * that contains exactly one mask token and w ∈ V , we denote with M (w | z) the unnormalized score that the language model assigns to w at the masked position. Given some input x, we define the score for label l ∈ L as</p><formula xml:id="formula_0">s p (l | x) = M (v(l) | P (x))</formula><p>and obtain a probability distribution over labels using softmax: l|x)   l ∈L e sp(l |x)</p><formula xml:id="formula_1">q p (l | x) = e sp(</formula><p>We use the cross-entropy between q p (l | x) and the true (one-hot) distribution of training example (x, l) -summed over all (x, l) ∈ T -as loss for finetuning M for p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auxiliary Language Modeling</head><p>In our application scenario, only a few training examples are available and catastrophic forgetting can occur. As a PLM finetuned for some PVP is still a language model at its core, we address this by using language modeling as auxiliary task. With L CE denoting cross-entropy loss and L MLM language modeling loss, we compute the final loss as</p><formula xml:id="formula_2">L = (1 − α) • L CE + α • L MLM</formula><p>This idea was recently applied by <ref type="bibr" target="#b5">Chronopoulou et al. (2019)</ref> in a data-rich scenario. As L MLM is typically much larger than L CE , in preliminary experiments, we found a small value of α = 10 −4 to consistently give good results, so we use it in all our experiments. To obtain sentences for language modeling, we use the unlabeled set D. However, we do not train directly on each x ∈ D, but rather on P (x), where we never ask the language model to predict anything for the masked slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining PVPs</head><p>A key challenge for our approach is that in the absence of a large development set, it is hard to identify which PVPs perform well. To address this, we use a strategy similar to knowledge distillation <ref type="bibr" target="#b14">(Hinton et al., 2015)</ref>. First, we define a set P of PVPs that intuitively make sense for a given task A. We then use these PVPs as follows:</p><p>(1) We finetune a separate language model M p for each p ∈ P as described in Section 3.1.</p><p>As T is small, this finetuning is cheap even for a large number of PVPs.</p><p>(2) We use the ensemble M = {M p | p ∈ P} of finetuned models to annotate examples from D. We first combine the unnormalized class scores for each example x ∈ D as</p><formula xml:id="formula_3">s M (l | x) = 1 Z p∈P w(p) • s p (l | x)</formula><p>where Z = p∈P w(p) and the w(p) are weighting terms for the PVPs. We experiment with two different realizations of this weighing term: either we simply set w(p) = 1 for all p or we set w(p) to be the accuracy obtained using p on the training set before training. We refer to these two variants as uniform and weighted. Jiang et al. ( <ref type="formula">2020</ref>) use a similar idea in a zero-shot setting.</p><p>We transform the above scores into a probability distribution q using softmax. Following <ref type="bibr" target="#b14">Hinton et al. (2015)</ref>, we use a temperature of T = 2 to obtain a suitably soft distribution. All pairs (x, q) are collected in a (soft-labeled) training set T C .</p><p>(3) We finetune a PLM C with a standard sequence classification head on T C .</p><p>The finetuned model C then serves as our classifier for A. All steps described above are depicted in Figure <ref type="figure" target="#fig_1">2</ref>; an example is shown in Figure <ref type="figure" target="#fig_0">1</ref>. (2) The final set of models is used to create a soft-labeled dataset T C . (3) A classifier C is trained on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Iterative PET (iPET)</head><p>Distilling the knowledge of all individual models into a single classifier C means they cannot learn from each other. As some patterns perform (possibly much) worse than others, the training set T C for our final model may therefore contain many mislabeled examples.</p><p>To compensate for this shortcoming, we devise iPET, an iterative variant of PET. The core idea of iPET is to train several generations of models on datasets of increasing size. To this end, we first enlarge the original dataset T by labeling selected examples from D using a random subset of trained PET models (Figure <ref type="figure" target="#fig_1">2a</ref>). We then train a new generation of PET models on the enlarged dataset (b); this process is repeated several times (c).</p><p>More formally, let M 0 = {M 0 1 , . . . , M 0 n } be the initial set of PET models finetuned on T , where each M 0 i is trained for some PVP p i . We train k generations of models M 1 , . . . , M k where M j = {M j 1 , . . . , M j n } and each M j i is trained for p i on its own training set T j i . In each iteration, we multiply the training set size by a fixed constant d ∈ N while maintaining the label ratio of the original dataset. That is, with c 0 (l) denoting the number of examples with label l in T , each</p><formula xml:id="formula_4">T j i contains c j (l) = d • c j−1 (l)</formula><p>examples with label l. This is achieved by generating each T j i as follows:</p><p>1. We obtain N ⊂ M j−1 \ {M j−1 i } by randomly choosing λ • (n − 1) models from the previous generation with λ ∈ (0, 1] being a hyperparameter.</p><p>2. Using this subset, we create a labeled dataset</p><formula xml:id="formula_5">T N = {(x, arg max l∈L s N (l | x)) | x ∈ D} .</formula><p>For each l ∈ L, we obtain T N (l) ⊂ T N by randomly choosing c j (l) − c 0 (l) examples with label l from T N . To avoid training future generations on mislabeled data, we prefer examples for which the ensemble of models is confident in its prediction. The underlying intuition is that even without calibration, examples for which labels are predicted with high confidence are typically more likely to be classified correctly <ref type="bibr" target="#b12">(Guo et al., 2017)</ref>. Therefore, when drawing from T N , we set the probability of each (x, y) proportional to s N (l | x).</p><p>3. We define T j i = T ∪ l∈L T N (l). As can easily be verified, this dataset contains c j (l) examples for each l ∈ L.</p><p>After training k generations of PET models, we use M k to create T C and train C as in basic PET.</p><p>With minor adjustments, iPET can even be used in a zero-shot setting. To this end, we define M 0 to be the set of untrained models and c 1 (l) = 10/|L| for all l ∈ L so that M 1 is trained on 10 examples evenly distributed across all labels. As T N may not contain enough examples for some label l, we create all T N (l) by sampling from the 100 examples x ∈ D for which s N (l | x) is the highest, even if l = arg max l∈L s N (l | x). For each subsequent generation, we proceed exactly as in basic iPET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate PET on four English datasets: Yelp Reviews, AG's News, Yahoo Questions <ref type="bibr" target="#b50">(Zhang et al., 2015)</ref> and MNLI <ref type="bibr" target="#b43">(Williams et al., 2018)</ref>. Additionally, we use x-stance <ref type="bibr" target="#b39">(Vamvas and Sennrich, 2020)</ref> to investigate how well PET works for other languages. For all experiments on English, we use RoBERTa large <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> as language model; for x-stance, we use XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020)</ref>. We investigate the performance of PET and all baselines for different training set sizes; each model is trained three times using different seeds and average results are reported.</p><p>As we consider a few-shot setting, we assume no access to a large development set on which hyperparameters could be optimized. Our choice of hyperparameters is thus based on choices made in previous work and practical considerations. We use a learning rate of 1 • 10 −5 , a batch size of 16 and a maximum sequence length of 256. Unless otherwise specified, we always use the weighted variant of PET with auxiliary language modeling. For iPET, we set λ = 0.25 and d = 5; that is, we select 25% of all models to label examples for the next generation and quintuple the number of training examples in each iteration. We train new generations until each model was trained on at least 1000 examples, i.e., we set k = log d (1000/|T |) . As we always repeat training three times, the ensemble M (or M 0 ) for n PVPs contains 3n models. Further hyperparameters and detailed explanations for all our choices are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Patterns</head><p>We now describe the patterns and verbalizers used for all tasks. We use two vertical bars ( ) to mark boundaries between text segments. <ref type="foot" target="#foot_0">3</ref>Yelp For the Yelp Reviews Full Star dataset <ref type="bibr" target="#b50">(Zhang et al., 2015)</ref>, the task is to estimate the rating that a customer gave to a restaurant on a 1to 5-star scale based on their review's text. We define the following patterns for an input text a:</p><formula xml:id="formula_6">P 1 (a) = It was . a P 2 (a) = Just ! a P 3 (a) = a.</formula><p>All in all, it was .</p><p>P 4 (a) = a In summary, the restaurant is .</p><p>We define a single verbalizer v for all patterns as</p><formula xml:id="formula_7">v(1) = terrible v(2) = bad v(3) = okay v(4) = good v(5) = great</formula><p>AG's News AG's News is a news classification dataset, where given a headline a and text body b, news have to be classified as belonging to one of the categories World (1), Sports (2), Business (3) or Science/Tech (4). For x = (a, b), we define the following patterns:</p><formula xml:id="formula_8">P 1 (x) = : a b P 2 (x) = a ( ) b P 3 (x) = -a b P 4 (x) = a b ( ) P 5 (x) = News: a b P 6 (x) = [ Category: ] a b</formula><p>We use a verbalizer that maps 1-4 to "World", "Sports", "Business" and "Tech", respectively.</p><p>Yahoo Yahoo Questions <ref type="bibr" target="#b50">(Zhang et al., 2015)</ref> is a text classification dataset. Given a question a and an answer b, one of ten possible categories has to be assigned. We use the same patterns as for AG's News, but we replace the word "News" in P 5 with the word "Question". We define a verbalizer that maps categories 1-10 to "Society", "Science", "Health", "Education", "Computer", "Sports", "Business", "Entertainment", "Relationship" and "Politics".</p><p>MNLI The MNLI dataset <ref type="bibr" target="#b43">(Williams et al., 2018)</ref> consists of text pairs x = (a, b). The task is to find out whether a implies b (0), a and b contradict each other (1) or neither (2). We define</p><formula xml:id="formula_9">P 1 (x) = "a"? , "b" P 2 (x) = a? , b</formula><p>and consider two different verbalizers v 1 and v 2 :</p><formula xml:id="formula_10">v 1 (0) = Wrong v 1 (1) = Right v 1 (2) = Maybe v 2 (0) = No v 2 (1) = Yes v 2 (2) = Maybe</formula><p>Combining the two patterns with the two verbalizers results in a total of 4 PVPs.  supports the subject of the question (0) or not (1). We use two simple patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-Stance</head><formula xml:id="formula_11">P 1 (x) = "a" . "b" P 2 (x) = a . b</formula><p>and define an English verbalizer v En mapping 0 to "Yes" and 1 to "No" as well as a French (German) verbalizer v Fr (v De ), replacing "Yes" and "No" with "Oui" and "Non" ("Ja" and "Nein"). We do not define an Italian verbalizer because x-stance does not contain any Italian training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>English Datasets Table <ref type="table" target="#tab_0">1</ref> shows results for English text classification and language understanding tasks; we report mean accuracy and standard deviation for three training runs. a 12-layer Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>. Both <ref type="bibr" target="#b45">Xie et al. (2020)</ref> and Chen et al. ( <ref type="formula">2020</ref>) use large development sets to optimize the number of training steps. We instead try several values for both approaches directly on the test set and only report the best results obtained. Despite this, Table <ref type="table">2</ref> shows that PET and iPET substantially outperform both methods across all tasks, clearly demonstrating the benefit of incorporating human knowledge in the form of PVPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-Stance</head><p>We evaluate PET on x-stance to investigate (i) whether it works for languages other than English and (ii) whether it also brings improvements when training sets have medium size. In contrast to <ref type="bibr" target="#b39">Vamvas and Sennrich (2020)</ref>, we do not perform any hyperparameter optimization on dev and use a shorter maximum sequence length (256 vs 512) to speed up training and evaluation.</p><p>To investigate whether PET brings benefits even when numerous examples are available, we consider training set sizes of 1000, 2000, and 4000; for each of these configurations, we separately finetune Results are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Combining PVPs We first investigate whether PET is able to cope with situations were some PVPs perform much worse than others. For |T | = 10, Table <ref type="table">4</ref> compares the performance of PET to that of the best and worst performing patterns after finetuning; we also include results obtained using the ensemble of PET models corresponding to individual PVPs without knowledge distillation. Even after finetuning, the gap between the best and worst pattern is large, especially for Yelp. However, PET is not only able to compensate for this, but even improves accuracies over using only the bestperforming pattern across all tasks. Distillation brings consistent improvements over the ensemble; additionally, it significantly reduces the size of the final classifier. We find no clear difference between the uniform and weighted variants of PET.</p><formula xml:id="formula_12">M 0 M 1 M 2 M 3 M 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Language Modeling</head><p>We analyze the influence of the auxiliary language modeling task on PET's performance. Figure <ref type="figure" target="#fig_4">3</ref> shows performance improvements from adding the language modeling task for four training set sizes. We see that the auxiliary task is extremely valuable when training on just 10 examples. With more data, it becomes less important, sometimes even leading to worse performance. Only for MNLI, we find language modeling to consistently help.</p><p>Iterative PET To check whether iPET is able to improve models over multiple generations, Figure <ref type="figure" target="#fig_5">4</ref> shows the average performance of all generations of models in a zero-shot setting. Each additional iteration does indeed further improve the ensemble's performance. We did not investigate whether continuing this process for even more iterations gives further improvements. Another natural question is whether similar results can be obtained with fewer iterations by increasing the training set size more aggressively. To answer this question, we skip generations 2 and 3 for AG's News and Yahoo and for both tasks directly let ensemble M 1 annotate 10 • 5 4 examples for M 4 . As indicated in Figure <ref type="figure" target="#fig_5">4</ref> through dashed lines, this clearly leads to worse performance, highlighting the importance of only gradually increasing the training set size. We surmise that this is the case because annotating too many examples too early leads to a large percentage of mislabeled training examples. In-Domain Pretraining Unlike our supervised baseline, PET makes use of the additional unlabeled dataset D. Thus, at least some of PET's performance gains over the supervised baseline may arise from this additional in-domain data.</p><p>To test this hypothesis, we simply further pretrain RoBERTa on in-domain data, a common technique for improving text classification accuracy (e.g., <ref type="bibr" target="#b16">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b36">Sun et al., 2019)</ref>. As language model pretraining is expensive in terms of GPU usage, we do so only for the Yelp dataset. Figure <ref type="figure" target="#fig_6">5</ref> shows results of supervised learning and PET both with and without this indomain pretraining. While pretraining does indeed improve accuracy for supervised training, the supervised model still clearly performs worse than PET, showing that the success of our method is not simply due to the usage of additional unlabeled data. Interestingly, in-domain pretraining is also helpful for PET, indicating that PET leverages unlabeled data in a way that is clearly different from standard masked language model pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation</head><p>Our implementation of PET and iPET is based on the Transformers library <ref type="bibr" target="#b44">(Wolf et al., 2020)</ref> and PyTorch <ref type="bibr" target="#b24">(Paszke et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>Except for the in-domain pretraining experiment described in Section 5, all of our experiments were conducted using a single GPU with 11GB RAM (NVIDIA GeForce GTX 1080 Ti).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperparameter Choices</head><p>Relevant training hyperparameters for both individual PET models and the final classifier C as well as our supervised baseline are listed in Table <ref type="table">5</ref>. All hyperparameters were selected based on the following considerations and experiments: Batch size / maximum length Both batch size and maximum sequence length (or block size) are chosen so that one batch fits into 11GB of GPU memory. As <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> and <ref type="bibr" target="#b20">Liu et al. (2019)</ref> use larger batch sizes of 16-32, we accumulate gradients for 4 steps to obtain an effective batch size of 16.</p><p>Learning rate We found a learning rate of 5e−5 (as used by <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>) to often result in unstable training for regular supervised learning with no accuracy improvements on the training set. We therefore use a lower learning rate of 1e−5, similar to <ref type="bibr" target="#b20">Liu et al. (2019)</ref>. Experiments with various learning rates can be found in Appendix D.</p><p>Training steps As the number of training epochs recommended by <ref type="bibr" target="#b20">Liu et al. (2019)</ref>  Temperature We choose a temperature of 2 when training the final classifier following <ref type="bibr" target="#b14">Hinton et al. (2015)</ref>.</p><p>Auxiliary language modeling To find a suitable value of α for combining language modeling loss and cross-entropy loss, we first observed that in the early stages of training, the former is a few orders of magnitude higher than the latter for all tasks considered. We thus selected a range {1e−3, 1e−4, 1e−5} of reasonable choices for α and performed preliminary experiments on Yelp with 100 training examples to find the best value among these candidates. To this end, we split the training examples into a training set and a dev set using both a 90/10 split and a 50/50 split and took the value of α that maximizes average dev set accuracy. We adopt this value for all other tasks and training set sizes without further optimization.</p><p>Models per ensemble As we always train three models per pattern, for both iPET and training the final classifier C, the ensemble M (or M 0 ) for n PVPs contains 3n models. This ensures consistency as randomly choosing any of the three models for each PVP would result in high variance. In preliminary experiments, we found this to have only little impact on the final model's performance.</p><p>iPET dataset size For iPET, we quintuple the number of training examples after each iteration (d = 5) so that only a small number of generations is required to reach a sufficient amount of labeled data. We did not choose a higher value because we presume that this may cause training sets for early generations to contain a prohibitively large amount of mislabeled data.</p><p>iPET dataset creation We create training sets for the next generation in iPET using 25% of the models in the current generation (λ = 0.25) because we want the training sets for all models to be diverse while at the same time, a single model should not have too much influence.</p><p>Others For all other hyperparameters listed in Table <ref type="table">5</ref>, we took the default settings of the Transformers library <ref type="bibr" target="#b44">(Wolf et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Number of parameters</head><p>As PET does not require any additional learnable parameters, the number of parameters for both PET and iPET is identical to the number of parameters in the underlying language model: 355M for RoBERTa (large) and 270M for XLM-R (base). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Average runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Comparison with SotA</head><p>For comparing PET to UDA <ref type="bibr" target="#b45">(Xie et al., 2020)</ref> and MixText <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>, we reduce the number of unlabeled examples by half to speed up the required backtranslation step. We use the backtranslation script provided by <ref type="bibr" target="#b4">Chen et al. (2020)</ref> with their recommended hyperparameter values and use both Russian and German as intermediate languages.</p><p>For MixText, we use the original implementation 5 and the default set of hyperparameters. Specifically, each batch consists of 4 labeled and 8 unlabeled examples, we use layers 7, 9 and 12 for mixing, we set T = 5, α = 16, and use a learning rate of 5 • 10 −6 for RoBERTa and 5 • 10 −4 for the final classification layer. We optimize the number of training steps for each task and dataset size in the range {1000, 2000, 3000, 4000, 5000}.</p><p>For UDA, we use a PyTorch-based reimplementation 6 . We use the same batch size as for MixText and the hyperparameter values recommended by <ref type="bibr" target="#b45">Xie et al. (2020)</ref>; we use an exponential schedule for training signal annealing and a learning rate of 2 • 10 −5 . We optimize the number of training steps for each task and dataset size in the range {500, 1000, 1500, . . . , 10000}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 In-Domain Pretraining</head><p>For in-domain pretraining experiments described in Section 5, we use the language model finetuning script of the Transformers library <ref type="bibr" target="#b44">(Wolf et al., 2020)</ref>; all hyperparameters are listed in the last column of Table <ref type="table">5</ref>. Pretraining was performed on a total of 3 NVIDIA GeForce GTX 1080 Ti GPUs. For evaluation, we use the official test set for all tasks except MNLI, for which we report results on the dev set; this is due to the limit of 2 submissions per 14 hours for the official MNLI test set. An overview of the number of test examples and links to downloadable versions of all used datasets can be found in Table <ref type="table">6</ref>.</p><p>Preprocessing In some of the datasets used, newlines are indicated through the character sequence "\n". As the vocabularies of RoBERTa and XLM-R do not feature a newline, we replace this sequence with a single space. We do not perform any other preprocessing, except shortening all examples to the maximum sequence length of 256 tokens. This is done using the longest first strategy implemented in the Transformers library. For PET, all input sequences are truncated before applying patterns.</p><p>Evaluation metrics For Yelp, AG's News, Yahoo and MNLI, we use accuracy. For x-stance, we report macro-average of F1 scores using the evaluation script of <ref type="bibr" target="#b39">Vamvas and Sennrich (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameter Importance</head><p>To analyze the importance of hyperparameter choices for PET's performance gains over supervised learning, we look at the influence of both the learning rate (LR) and the number of training steps on their test set accuracies.</p><p>We try values of {1e−5, 2e−5, 5e−5} for the learning rate and {50, 100, 250, 500, 1000} for the number of training steps. As this results in 30 different configurations for just one task and training set size, we only perform this analysis on Yelp with 100 examples, for which results can be seen in Figure 6. For supervised learning, the configuration used throughout the paper (LR = 1e−5, 250 steps) turns out to perform best whereas for PET, training for fewer steps consistently performs even better. Importantly, PET clearly outperforms regular supervised training regardless of the chosen learning rate and number of training steps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PET for sentiment classification. (1) A number of patterns encoding some form of task description are created to convert training examples to cloze questions; for each pattern, a pretrained language model is finetuned. (2) The ensemble of trained models annotates unlabeled data. (3) A classifier is trained on the resulting soft-labeled dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic representation of PET (1-3) and iPET (a-c). (1) The initial training set is used to finetune an ensemble of PLMs. (a) For each model, a random subset of other models generates a new training set by labeling examples from D. (b) A new set of PET models is trained using the larger, model-specific datasets. (c) The previous two steps are repeated k times, each time increasing the size of the generated training sets by a factor of d. (2) The final set of models is used to create a soft-labeled dataset T C . (3) A classifier C is trained on this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The x-stance dataset<ref type="bibr" target="#b39">(Vamvas and Sennrich, 2020</ref>) is a multilingual stance detection dataset with German, French and Italian examples. Each example x = (a, b) consists of a question a concerning some political issue and a comment b; the task is to identify whether the writer of b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>French and German models to allow for a more straightforward downsampling of the training data. Additionally, we train models on the entire French (|T Fr | = 11 790) and German (|T De | = 33 850) training sets. In this case we do not have any additional unlabeled data, so we simply set D = T . For the French models, we use v En and v Fr as verbalizers and for German v En and v De (Section 4.1). Finally, we also investigate the performance of a model trained jointly on French and German data (|T Fr + T De | = 45 640) using v En , v Fr and v De .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy improvements for PET due to adding L MLM during training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average accuracy for each generation of models with iPET in a zero-shot setting. Accuracy on AG's News and Yahoo when skipping generation 2 and 3 is indicated through dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy of supervised learning (sup.) and PET both with and without pretraining (PT) on Yelp</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>in a data-rich scenario is in the range 2-10, we perform supervised training for 250 training steps, corresponding to 4 epochs when training on 1000 examples. For individual PET models, we subdivide each batch into one labeled example from T to compute L CE and three unlabeled examples from D to compute L MLM . Accordingly, we multiply the number of total training steps by 4 (i.e., 1000), so that the number of times each labeled example is seen remains constant (16 • 250 = 4 • 1000). For the final PET classifier, we train for 5000 steps due to the increased training set size (depending on the task, the unlabeled set D contains at least 20 000 examples). Deviating from the above, we always perform training for 3 epochs on x-stance to match the setup of Vamvas and Sennrich (2020) more closely. The effect of varying the number of training steps is further investigated in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Training a single PET classifier for 250 steps on one GPU took approximately 30 minutes; training for 1000 steps with auxiliary language modeling took 60 minutes. Depending on the task, labeling examples from D took 15-30 minutes per model. Training the final classifier C for 5000 steps on the soft-labeled dataset T C took 2 hours on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5</head><label></label><figDesc>https://github.com/GT-SALT/MixText 6 https://github.com/SanghunYun/UDA_ pytorch C Dataset Details For each task and number of examples t, we create the training set T by collecting the first t/|L| examples per label from the original training set, where |L| is the number of labels for the task. Similarly, we construct the set D of unlabeled examples by selecting 10 000 examples per label and removing all labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average accuracy and standard deviation for RoBERTa (large) on Yelp, AG's News, Yahoo and MNLI (m:matched/mm:mismatched) for five training set sizes |T |.</figDesc><table><row><cell cols="2">Line Examples</cell><cell>Method</cell><cell>Yelp</cell><cell>AG's</cell><cell>Yahoo</cell><cell>MNLI (m/mm)</cell></row><row><cell>1</cell><cell></cell><cell>unsupervised (avg)</cell><cell>33.8 ±9.6</cell><cell>69.5 ±7.2</cell><cell cols="2">44.0 ±9.1 39.1 ±4.3 / 39.8 ±5.1</cell></row><row><cell>2</cell><cell>|T | = 0</cell><cell cols="2">unsupervised (max) 40.8 ±0.0</cell><cell>79.4 ±0.0</cell><cell cols="2">56.4 ±0.0 43.8 ±0.0 / 45.0 ±0.0</cell></row><row><cell>3</cell><cell></cell><cell>iPET</cell><cell>56.7 ±0.2</cell><cell>87.5 ±0.1</cell><cell cols="2">70.7 ±0.1 53.6 ±0.1 / 54.2 ±0.1</cell></row><row><cell>4</cell><cell></cell><cell>supervised</cell><cell>21.1 ±1.6</cell><cell>25.0 ±0.1</cell><cell cols="2">10.1 ±0.1 34.2 ±2.1 / 34.1 ±2.0</cell></row><row><cell>5</cell><cell>|T | = 10</cell><cell>PET</cell><cell>52.9 ±0.1</cell><cell>87.5 ±0.0</cell><cell cols="2">63.8 ±0.2 41.8 ±0.1 / 41.5 ±0.2</cell></row><row><cell>6</cell><cell></cell><cell>iPET</cell><cell>57.6 ±0.0</cell><cell>89.3 ±0.1</cell><cell cols="2">70.7 ±0.1 43.2 ±0.0 / 45.7 ±0.1</cell></row><row><cell>7</cell><cell></cell><cell>supervised</cell><cell>44.8 ±2.7</cell><cell>82.1 ±2.5</cell><cell cols="2">52.5 ±3.1 45.6 ±1.8 / 47.6 ±2.4</cell></row><row><cell>8</cell><cell>|T | = 50</cell><cell>PET</cell><cell>60.0 ±0.1</cell><cell>86.3 ±0.0</cell><cell cols="2">66.2 ±0.1 63.9 ±0.0 / 64.2 ±0.0</cell></row><row><cell>9</cell><cell></cell><cell>iPET</cell><cell>60.7 ±0.1</cell><cell>88.4 ±0.1</cell><cell cols="2">69.7 ±0.0 67.4 ±0.3 / 68.3 ±0.3</cell></row><row><cell>10</cell><cell></cell><cell>supervised</cell><cell>53.0 ±3.1</cell><cell>86.0 ±0.7</cell><cell cols="2">62.9 ±0.9 47.9 ±2.8 / 51.2 ±2.6</cell></row><row><cell>11</cell><cell>|T | = 100</cell><cell>PET</cell><cell>61.9 ±0.0</cell><cell>88.3 ±0.1</cell><cell cols="2">69.2 ±0.0 74.7 ±0.3 / 75.9 ±0.4</cell></row><row><cell>12</cell><cell></cell><cell>iPET</cell><cell>62.9 ±0.0</cell><cell>89.6 ±0.1</cell><cell cols="2">71.2 ±0.1 78.4 ±0.7 / 78.6 ±0.5</cell></row><row><cell>13 14</cell><cell>|T | = 1000</cell><cell>supervised PET</cell><cell>63.0 ±0.5 64.8 ±0.1</cell><cell>86.9 ±0.4 86.9 ±0.2</cell><cell cols="2">70.5 ±0.3 73.1 ±0.2 / 74.8 ±0.3 72.7 ±0.0 85.3 ±0.2 / 85.5 ±0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>= 1000 underfitted the training data and performed extremely poorly. This run is excluded in the reported score (73.1/74.8). Results on x-stance intra-target for XLM-R (base) trained on subsets of T De and T Fr and for joint training on all data (T De + T Fr ). (*): Best results for mBERT reported in<ref type="bibr" target="#b39">Vamvas and Sennrich (2020)</ref>.</figDesc><table><row><cell></cell><cell cols="2">Ex. Method</cell><cell>Yelp</cell><cell>AG's</cell><cell cols="2">Yahoo MNLI</cell></row><row><cell></cell><cell>|T | = 10</cell><cell>UDA MixText PET iPET</cell><cell>27.3 20.4 48.8 52.9</cell><cell>72.6 81.1 84.1 87.5</cell><cell>36.7 20.6 59.0 67.0</cell><cell>34.7 32.9 39.5 42.1</cell></row><row><cell></cell><cell>|T | = 50</cell><cell>UDA MixText PET iPET</cell><cell>46.6 31.3 55.3 56.7</cell><cell>83.0 84.8 86.4 87.3</cell><cell>60.2 61.5 63.3 66.4</cell><cell>40.8 34.8 55.1 56.3</cell></row><row><cell></cell><cell cols="6">Table 2: Comparison of PET with two state-of-the-art</cell></row><row><cell></cell><cell cols="6">semi-supervised methods using RoBERTa (base)</cell></row><row><cell></cell><cell cols="6">we increase the training set size, the performance</cell></row><row><cell></cell><cell cols="6">gains of PET and iPET become smaller, but for</cell></row><row><cell></cell><cell cols="6">both 50 and 100 examples, PET continues to con-</cell></row><row><cell>Lines 1-2 (L1-L2) show unsupervised performance, i.e., individual PVPs without any training (similar to Radford et al., 2018; Puri and Catanzaro, 2019); we give both av-erage results across all PVPs (avg) and results for</cell><cell cols="6">siderably outperform standard supervised training (L8 vs L7, L11 vs L10) with iPET (L9, L12) still giving consistent improvements. For |T | = 1000, PET has no advantage on AG's but still improves accuracy for all other tasks (L14 vs L13). 4</cell></row><row><cell>the PVP that works best on the test set (max). The</cell><cell cols="6">Comparison with SotA We compare PET to</cell></row><row><cell>large difference between both rows highlights the</cell><cell cols="6">UDA (Xie et al., 2020) and MixText (Chen et al.,</cell></row><row><cell>importance of coping with the fact that without</cell><cell cols="6">2020), two state-of-the-art methods for semi-</cell></row><row><cell>looking at the test set, we have no means of eval-</cell><cell cols="6">supervised learning in NLP that rely on data aug-</cell></row><row><cell>uating which PVPs perform well. Zero-shot iPET</cell><cell cols="6">mentation. Whereas PET requires that a task can be</cell></row><row><cell>clearly outperforms the unsupervised baselines for</cell><cell cols="6">expressed using patterns and that such patterns be</cell></row><row><cell>all datasets (L3 vs L1); on AG's News, it even per-</cell><cell cols="6">found, UDA and MixText both use backtranslation</cell></row><row><cell>forms better than standard supervised training with</cell><cell cols="6">(Sennrich et al., 2016) and thus require thousands</cell></row><row><cell>1000 examples (L3 vs L13). With just 10 training</cell><cell cols="6">of labeled examples for training a machine transla-</cell></row><row><cell>examples, standard supervised learning does not</cell><cell cols="6">tion model. We use RoBERTa (base) for our com-</cell></row><row><cell>perform above chance (L4). In contrast, PET (L5)</cell><cell cols="6">parison as MixText is specifically tailored towards</cell></row><row><cell>performs much better than the fully unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baselines (L1-L2); training multiple generations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>using iPET (L6) gives consistent improvements. As</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4 One of the three supervised MNLI runs for |T |</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 ;</head><label>3</label><figDesc>following Vamvas    </figDesc><table><row><cell cols="2">Method</cell><cell>Yelp</cell><cell cols="3">AG's Yahoo MNLI</cell></row><row><cell>min</cell><cell></cell><cell>39.6</cell><cell>82.1</cell><cell>50.2</cell><cell>36.4</cell></row><row><cell>max</cell><cell></cell><cell>52.4</cell><cell>85.0</cell><cell>63.6</cell><cell>40.2</cell></row><row><cell cols="2">PET (no distillation)</cell><cell>51.7</cell><cell>87.0</cell><cell>62.8</cell><cell>40.6</cell></row><row><cell cols="2">PET uniform</cell><cell>52.7</cell><cell>87.3</cell><cell>63.8</cell><cell>42.0</cell></row><row><cell cols="2">PET weighted</cell><cell>52.9</cell><cell>87.5</cell><cell>63.8</cell><cell>41.8</cell></row><row><cell cols="6">Table 4: Minimum (min) and maximum (max) accu-</cell></row><row><cell cols="6">racy of models based on individual PVPs as well as PET</cell></row><row><cell cols="6">with and without knowledge distillation (|T | = 10).</cell></row><row><cell>Accuracy Improvements</cell><cell>0 5 10 15</cell><cell></cell><cell>Yelp MNLI</cell><cell></cell><cell>AG's Yahoo</cell></row><row><cell></cell><cell>10</cell><cell>50</cell><cell cols="2">100</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell cols="3">Training set size</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">The way different segments are handled depends on the model being used; they may e.g. be assigned different embeddings<ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> or separated by special tokens<ref type="bibr" target="#b20">(Liu et al., 2019;</ref><ref type="bibr" target="#b45">Yang et al., 2019)</ref>. For example, "a b" is given to BERT as the input "[CLS] a [SEP] b [SEP]".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1">We tried values of k and imax in {250, 500, 1000} and {5, 10, 20}, respectively, but found the resulting verbalizers to be almost identical.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by the European Research Council (ERC #740516). We would like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Automatic Verbalizer Search</head><p>Given a set of patterns P 1 , . . . , P n , manually finding a verbalization v(l) for each l ∈ L that represents the meaning of l well and corresponds to a single token in V can be difficult. We therefore devise automatic verbalizer search (AVS), a procedure that automatically finds suitable verbalizers given a training set T and a language model M .</p><p>Assuming we already have a PVP p = (P, v), we can easily check whether some token t ∈ V is a good verbalization of l ∈ L. To this end, we define p[l ← t] = (P, v ), where v is identical to v, except that v (l) = t. Intuitively, if t represents l well, then q p[l←t] (l | x) (i.e., the probability M assigns to t given P (x)) should be high only for those examples (x, y) ∈ T where y = l. We thus define the score of t for l given p as</p><p>where AVS solves this problem as follows: We first assign random verbalizations to all labels and then repeatedly recompute the best verbalization for each label. As we do not want the resulting verbalizer to depend strongly on the initial random assignment, we simply consider multiple such assignments. Specifically, we define an initial probability distribution ρ 0 where for all t ∈ V, l ∈ L, ρ 0 (t | l) = 1/|V | is the probability of choosing t as verbalization for l. For each l ∈ L, we then sample k verbalizers v 1 , . . . , v k using ρ 0 to compute</p><p>for all t ∈ V . 7 These scores enable us to define a probability distribution ρ 1 that more closely reflects 7 Note that the score s k l (t) jointly considers all patterns; in preliminary experiments, we found this to result in more robust verbalizers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp</head><p>where Z = t ∈V max(s k l (t ), ) and ≥ 0 ensures that ρ 1 is a proper probability distribution. We repeat this process to obtain a sequence of probability distributions ρ 1 , . . . , ρ imax . Finally, we choose the m ∈ N most likely tokens according to ρ imax (t | l) as verbalizers for each l. During training and inference, we compute the unnormalized score s p (y | x) for each label by averaging over its m verbalizers.</p><p>We analyze the performance of AVS for all tasks with |T | = 50 training examples and set k = 250, = 10 −3 , i max = 5 and m = 10. 8 To speed up the search, we additionally restrict our search space to tokens t ∈ V that contain at least two alphabetic characters. Of these tokens, we only keep the 10 000 most frequent ones in D.</p><p>Results are shown in Table <ref type="table">7</ref>. As can be seen, carefully handcrafted verbalizers perform much better than AVS; however, PET with AVS still considerably outperforms regular supervised training while eliminating the challenge of manually finding suitable verbalizers. Table <ref type="table">8</ref> shows the most probable verbalizers found using AVS for the Yelp dataset. While most verbalizers for this dataset intuitively make sense, we found AVS to struggle with finding good verbalizers for three out of ten labels in the Yahoo dataset and for all MNLI labels.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<idno type="DOI">10.1145/336597.336644</idno>
		<idno>DL &apos;00</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM Conference on Digital Libraries</title>
				<meeting>the Fifth ACM Conference on Digital Libraries<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised bootstrapping of relationship extractors with distributional semantics</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mário</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1056</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inducing relational knowledge from BERT</title>
		<author>
			<persName><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting patterns and relations from the world wide web</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web and Databases</title>
				<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="172" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach for transfer learning from pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Investigating meta-learning algorithms for low-resource natural language understanding tasks</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1192" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning for lowresource neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;17, page 1321-1330. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative backtranslation for neural machine translation</title>
		<author>
			<persName><forename type="first">Duy</forename><surname>Vu Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
				<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selftraining PCFG grammars with latent annotations across languages</title>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7811" to="7818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
				<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Argumentative relation classification as plausibility ranking</title>
		<author>
			<persName><forename type="first">Juri</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preliminary proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019): Long Papers</title>
				<meeting><address><addrLine>Erlangen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>German Society for Computational Linguistics &amp; Language Technology</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1253</idno>
		<idno type="arXiv">arXiv:1912.10165</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2639" to="2649" />
		</imprint>
	</monogr>
	<note>Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WinoGrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot learning of classifiers from natural language quantification</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How to fine-tune BERT for text classification?</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13283</idno>
		<title level="m">oLMpics -on what language model pre-training captures. Computing Research Repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">X-stance: A multilingual multi-target dataset for stance detection</title>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Vamvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08385</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using semantic similarity for multi-label zero-shot classification of text documents</title>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Sappadla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinseok</forename><surname>Veeranna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneldo</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Loza Mencıa</surname></persName>
		</author>
		<author>
			<persName><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
				<meeting>eeding of European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Does it make sense? And why? A pilot study for sense making and explanation</title>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1393</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4020" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<idno type="DOI">10.3115/981658.981684</idno>
	</analytic>
	<monogr>
		<title level="m">33rd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot text classification via reinforced self-training</title>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.272</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3014" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1404</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3914" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Diverse few-shot text classification with multiple metrics</title>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
