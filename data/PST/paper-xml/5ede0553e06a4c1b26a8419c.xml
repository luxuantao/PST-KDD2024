<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
							<email>&lt;kaveh.hassani@autodesk.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Autodesk AI Lab</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><forename type="middle">Hosein</forename><surname>Khasahmadi</surname></persName>
							<email>&lt;amir.khasahmadi@autodesk.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Autodesk AI Lab</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-ofthe-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph neural networks (GNN) <ref type="bibr" target="#b31">(Li et al., 2015;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b54">Veličković et al., 2018;</ref><ref type="bibr" target="#b64">Xu et al., 2019b)</ref> reconcile the expressive power of graphs in modeling interactions with unparalleled capacity of deep models in learning representations. They process variable-size permutation-invariant graphs and learn lowdimensional representations through an iterative process of transferring, transforming, and aggregating the representations from topological neighbors. Each iteration expands the receptive field by one-hop and after k iterations the nodes within k-hops influence one another <ref type="bibr" target="#b21">(Khasahmadi et al., 2020)</ref>. GNNs are applied to data with arbitrary topology such as point clouds <ref type="bibr" target="#b16">(Hassani &amp; Haley, 2019)</ref>, meshes <ref type="bibr" target="#b58">(Wang et al., 2018)</ref>, robot designs <ref type="bibr" target="#b59">(Wang et al., 2019)</ref>, physical processes <ref type="bibr" target="#b44">(Sanchez-Gonzalez et al., 2018)</ref>, social net-Proceedings of the 37 th International Conference on Machine <ref type="bibr">Learning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>works <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>, molecules <ref type="bibr" target="#b8">(Duvenaud et al., 2015)</ref>, and knowledge graphs <ref type="bibr" target="#b56">(Vivona &amp; Hassani, 2019)</ref>. For an overview see <ref type="bibr" target="#b69">(Zhang et al., 2020;</ref><ref type="bibr" target="#b61">Wu et al., 2020)</ref>.</p><p>GNNs mostly require task-dependent labels to learn rich representations. Nevertheless, annotating graphs is challenging compared to more common modalities such as video, image, text, and audio. This is partly because graphs are usually used to represent concepts in specialized domains, e.g., biology. Also, labeling graphs procedurally using domain knowledge is costly <ref type="bibr" target="#b49">(Sun et al., 2020)</ref>. To address this, unsupervised approaches such as reconstruction based methods <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref> and contrastive methods <ref type="bibr" target="#b32">(Li et al., 2019)</ref> are coupled with GNNs to allow them learn representations without relying on supervisory data. The learned representations are then transferred to a priori unknown down-stream tasks. Recent works on contrastive learning by maximizing mutual information (MI) between node and graph representations have achieved state-of-the-art results on both node classification <ref type="bibr" target="#b55">(Veličković et al., 2019)</ref> and graph classification <ref type="bibr" target="#b49">(Sun et al., 2020)</ref> tasks. Nonetheless, these methods require specialized encoders to learn graph or node level representations.</p><p>Recent advances in multi-view visual representation learning <ref type="bibr" target="#b51">(Tian et al., 2019;</ref><ref type="bibr" target="#b2">Bachman et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2020)</ref>, in which composition of data augmentations is used to generate multiple views of a same image for contrastive learning, has achieved state-of-the-art results on image classification benchmarks surpassing supervised baselines. However, it is not clear how to apply these techniques to data represented as graphs. To address this, we introduce a self-supervised approach to train graph encoders by maximizing MI between representations encoded from different structural views of graphs. We show that our approach outperforms previous self-supervised models with significant margin on both node and graph classification tasks without requiring specialized architectures. We also show that when compared to supervised baselines, it performs on par with or better than strong baselines on some benchmarks.</p><p>To further improve contrastive representation learning on node and graph classification tasks, we systematically study the major components of our framework and surprisingly show that unlike visual contrastive learning: (1) increasing the number of views, i.e., augmentations, to more than two views does not improve the performance and the best performance is achieved by contrasting encodings from first-order neighbors and a general graph diffusion, (2) contrasting node and graph encodings across views achieves better results on both tasks compared to contrasting graphgraph or multi-scale encodings, (3) a simple graph readout layer achieves better performance on both tasks compared to hierarchical graph pooling methods such as differentiable pooling (DiffPool) <ref type="bibr" target="#b67">(Ying et al., 2018)</ref>, and (4) applying regularization (except early-stopping) or normalization layers has a negative effect on the performance.</p><p>Using these findings, we achieve new state-of-the-art in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora node classification benchmark, our approach achieves 86.8% accuracy, which is a 5.5% relative improvement over previous state-of-the-art <ref type="bibr" target="#b55">(Veličković et al., 2019)</ref>, and on Reddit-Binary graph classification benchmark, it achieves 84.5% accuracy, i.e., a 2.4% relative improvement over previous state-of-the-art <ref type="bibr" target="#b49">(Sun et al., 2020)</ref>. When compared to supervised baselines, our approach performs on par with or better than strong supervised baselines, e.g., graph isomorphism network (GIN) <ref type="bibr" target="#b64">(Xu et al., 2019b)</ref> and graph attention network (GAT) <ref type="bibr" target="#b54">(Veličković et al., 2018)</ref>, on 4 out of 8 benchmarks. As an instance, on Cora (node) and IMDB-Binary (graph) classification benchmarks, we observe 4.5% and 5.3% relative improvements over GAT, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Representation Learning on Graphs</head><p>Random walks <ref type="bibr" target="#b42">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b50">Tang et al., 2015;</ref><ref type="bibr" target="#b13">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017)</ref> flatten graphs into sequences by taking random walks across nodes and use language models to learn node representations. They are shown to over-emphasize proximity information at the expense of structural information <ref type="bibr" target="#b55">(Veličković et al., 2019;</ref><ref type="bibr" target="#b43">Ribeiro et al., 2017)</ref>. Also, they are limited to transductive settings and cannot use node features <ref type="bibr" target="#b68">(You et al., 2019)</ref>. Graph kernels <ref type="bibr" target="#b4">(Borgwardt &amp; Kriegel, 2005;</ref><ref type="bibr" target="#b46">Shervashidze et al., 2009;</ref><ref type="bibr" target="#b47">2011;</ref><ref type="bibr" target="#b65">Yanardag &amp; Vishwana, 2015;</ref><ref type="bibr" target="#b27">Kondor &amp; Pan, 2016;</ref><ref type="bibr" target="#b30">Kriege et al., 2016)</ref> decompose graphs into substructures and use kernel functions to measure graph similarity between them. Nevertheless, they require non-trivial task of devising similarity measures between substructures. Graph autoencoders (GAE) <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b9">Garcia Duran &amp; Niepert, 2017;</ref><ref type="bibr" target="#b57">Wang et al., 2017;</ref><ref type="bibr" target="#b40">Pan et al., 2018;</ref><ref type="bibr" target="#b41">Park et al., 2019)</ref> train encoders that impose the topological closeness of nodes in the graph structure on the latent space by predicting the first-order neighbors. GAEs overemphasize proximity information <ref type="bibr" target="#b55">(Veličković et al., 2019)</ref> and suffer from unstructured predictions <ref type="bibr" target="#b51">(Tian et al., 2019)</ref>.</p><p>Contrastive methods <ref type="bibr" target="#b32">(Li et al., 2019;</ref><ref type="bibr" target="#b55">Veličković et al., 2019;</ref><ref type="bibr" target="#b49">Sun et al., 2020)</ref> measure the loss in latent space by contrasting samples from a distribution that contains dependencies of interest and the distribution that does not. These methods are the current state-of-the-art in unsupervised node and graph classification tasks. Deep graph Infomax (DGI) <ref type="bibr" target="#b55">(Veličković et al., 2019)</ref> extends deep InfoMax <ref type="bibr" target="#b18">(Hjelm et al., 2019)</ref> to graphs and achieves state-of-the-art results in node classification benchmarks by learning node representations through contrasting node and graph encodings. InfoGraph <ref type="bibr" target="#b49">(Sun et al., 2020)</ref>, on the other hand, extends deep Info-Max to learn graph-level representations and outperforms previous models on unsupervised graph classification tasks. Although these two methods use the same contrastive learning approach, they utilize specialized encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Diffusion Networks</head><p>Graph diffusion networks (GDN) reconcile spatial message passing and generalized graph diffusion <ref type="bibr" target="#b26">(Klicpera et al., 2019b)</ref> where diffusion as a denoising filter allows messages to pass through higher-order neighborhoods. GDNs can be categorized to early-and late-fusion models based on the stage the diffusion is used. Early-fusion models <ref type="bibr" target="#b62">(Xu et al., 2019a;</ref><ref type="bibr" target="#b20">Jiang et al., 2019)</ref> use graph diffusion to decide the neighbors, e.g., graph diffusion convolution (GDC) replaces adjacency matrix in graph convolution with a sparsified diffusion matrix <ref type="bibr" target="#b26">(Klicpera et al., 2019b)</ref>, whereas, late-fusion models <ref type="bibr" target="#b53">(Tsitsulin et al., 2018;</ref><ref type="bibr" target="#b25">Klicpera et al., 2019a)</ref> project the node features into a latent space and then propagate the learned representation based on a diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Learning by Mutual Information Maximization</head><p>InfoMax principle <ref type="bibr" target="#b33">(Linsker, 1988)</ref> encourages an encoder to learn representations that maximizes the MI between the input and the learned representation. Recently, a few self-supervised models inspired by this principle are proposed which estimate the lower bound of the InfoMax objective, e.g., using noise contrastive estimation <ref type="bibr" target="#b14">(Gutmann &amp; Hyvärinen, 2010)</ref>, across representations. Contrastive predictive coding (CPC) <ref type="bibr" target="#b38">(Oord et al., 2018)</ref> contrasts a summary of ordered local features to predict a local feature in the future whereas deep InfoMax (DIM) <ref type="bibr" target="#b18">(Hjelm et al., 2019)</ref> simultaneously contrasts a single summary feature, i.e., global feature, with all local features. Contrastive multiview coding (CMC) <ref type="bibr" target="#b51">(Tian et al., 2019)</ref>, augmented multi-scale DIM (AMDIM) <ref type="bibr" target="#b2">(Bachman et al., 2019)</ref>, and SimCLR <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> extend the InfoMax principle to multiple views and maximize the MI across views generated by composition of data augmentations. Nevertheless, it is shown that success of these models cannot only be attributed to the properties of MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type="bibr" target="#b52">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type="figure">1</ref>. The proposed model for contrastive multi-view representation learning on both node and graph levels. A graph diffusion is used to generate an additional structural view of a sample graph which along with a regular view are sub-sampled and fed to two dedicated GNNs followed by a shared MLP to learn node representations. The learned features are then fed to a graph pooling layer followed by a shared MLP to learn graph representations. A discriminator contrasts node representations from one view with graph representation of another view and vice versa, and scores the agreement between representations which is used as the training signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Inspired by recent advances in multi-view contrastive learning for visual representation learning, our approach learns node and graph representations by maximizing MI between node representations of one view and graph representation of another view and vice versa which achieves better results compared to contrasting global or multi-scale encodings on both node and graph classification tasks (see section 4.4). As shown in Figure <ref type="figure">1</ref>, our method consists of the following components:</p><p>• An augmentation mechanism that transforms a sample graph into a correlated view of the same graph. We only apply the augmentation to the structure of the graphs and not the initial node features. This is followed by a sampler that sub-samples identical nodes from both views, i.e., similar to cropping in visual domain.</p><p>• Two dedicated GNNs, i.e., graph encoders, one for each view, followed by a shared MLP, i.e., projection head, to learn node representations for both views.</p><p>• A graph pooling layer, i.e., readout function, followed by a shared MLP, i.e., projection head, to learn graph representations for both views.</p><p>• A discriminator that contrasts node representations from one view with graph representation from another view and vice versa, and scores the agreement between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Augmentations</head><p>Recent works in self-supervised visual representation learning suggest that contrasting congruent and incongruent views of images allows encoders to learn rich representations <ref type="bibr" target="#b51">(Tian et al., 2019;</ref><ref type="bibr" target="#b2">Bachman et al., 2019)</ref>. Unlike images in which views are generated by standard augmentations, e.g., cropping, rotating, distorting colors, etc., defining views on graphs is not a trivial task. We can consider two types of augmentations on graphs: (1) feature-space augmentations operating on initial node features, e.g., masking or adding Gaussian noise, and (2) structure-space augmentations and corruptions operating on graph structure by adding or removing connectivities, sub-sampling, or generating global views using shortest distances or diffusion matrices. The former augmentation can be problematic as many benchmarks do not carry initial node features. Moreover, we observed that masking or adding noise on either spaces degrades the performance. Hence, we opted for generating a global view followed by sub-sampling.</p><p>We empirically show that in most cases the best results are achieved by transforming an adjacency matrix to a diffusion matrix and treating the two matrices as two congruent views of the same graph's structure (see section 4.4). We speculate that because adjacency and diffusion matrices provide local and global views of a graph structure, respectively, maximizing agreement between representation learned from these two views allows the model to simultaneously encode rich local and global information.</p><p>Diffusion is formulated as Eq. ( <ref type="formula" target="#formula_1">1</ref>) where T ∈ R n×n is the generalized transition matrix and Θ is the weighting coefficient which determines the ratio of global-local information. Imposing</p><formula xml:id="formula_0">∞ k=0 θ k = 1, θ k ∈ [0, 1],</formula><p>and λ i ∈ [0, 1] where λ i are eigenvalues of T, guarantees convergence. Diffusion is computed once using fast approximation and sparsification methods <ref type="bibr" target="#b26">(Klicpera et al., 2019b)</ref>.</p><formula xml:id="formula_1">S = ∞ k=0 Θ k T k ∈ R n×n<label>(1)</label></formula><p>Given an adjacency matrix A ∈ R n×n and a diagonal degree matrix D ∈ R n×n , Personalized PageRank (PPR) <ref type="bibr" target="#b39">(Page et al., 1999)</ref> and heat kernel <ref type="bibr" target="#b28">(Kondor &amp; Lafferty, 2002)</ref>, i.e., two instantiations of the generalized graph diffusion, are defined by setting T = AD −1 , and</p><formula xml:id="formula_2">θ k = α(1 − α) k and θ k = e −t t k /k!, respectively</formula><p>, where α denotes teleport probability in a random walk and t is diffusion time <ref type="bibr" target="#b26">(Klicpera et al., 2019b)</ref>. Closed-form solutions to heat and PPR diffusion are formulated in Eq. ( <ref type="formula">2</ref>) and (3), respectively.</p><formula xml:id="formula_3">S heat = exp tAD −1 − t (2) S PPR = α I n − (1 − α)D −1/2 AD −1/2 −1<label>(3)</label></formula><p>For sub-sampling, we randomly sample nodes and their edges from one view and select the exact nodes and edges from the other view. This procedure allows our approach to be applied to inductive tasks with graphs that do not fit into the GPU memory and also to transductive tasks by considering sub-samples as independent graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoders</head><p>Our framework allows various choices of the network architecture without any constraints. We opt for simplicity and adopt the commonly used graph convolution network (GCN) <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> as our base graph encoders. As shown in Figure <ref type="figure">1</ref>, we use a dedicated graph encoder for each view, i.e., g θ (.), g ω (.) : R n×dx × R n×n −→ R n×d h . We consider adjacency and diffusion matrices as two congruent structural views and define GCN layers as σ( ÃXΘ) and σ SXΘ to learn two sets of node representations each corresponding to one of the views, respectively.</p><formula xml:id="formula_4">Ã = D−1/2 Â D−1/2 ∈ R n×n is symmetrically normal- ized adjacency matrix, D ∈ R n×n is the degree matrix of Â = A + I N where I N is the identity matrix, S ∈ R n×n is diffusion matrix, X ∈ R n×dx is the initial node features, Θ ∈ R dx×d h is</formula><p>network parameters, and σ is a parametric ReLU (PReLU) non-linearity <ref type="bibr" target="#b17">(He et al., 2015)</ref>. The learned representations are then fed into a shared projection head f ψ (.) : R n×d h −→ R n×d h which is an MLP with two hidden layers and PReLU non-linearity. This results in two sets of node representations H α , H β ∈ R n×d h corresponding to two congruent views of a same graph.</p><p>For each view, we aggregate the node representations learned by GNNs, i.e., before the projection head, into a graph representation using a graph pooling (readout) function P(.) : R n×d h −→ R d h . We use a readout function similar to jumping knowledge network (JK-Net) <ref type="bibr" target="#b63">(Xu et al., 2018)</ref> where we concatenate the summation of the node representations in each GCN layer and then feed them to a single layer feed-forward network to have a consistent dimension size between node and graph representations:</p><formula xml:id="formula_5">h g = σ L l=1 n i=1 h (l) i W ∈ R h d (4)</formula><p>where h</p><p>(l)</p><p>i is the latent representation of node i in layer l, || is the concatenation operator, L is the number of GCN layers, W ∈ R (L×d h )×d h is the network parameters, and σ is a PReLU non-linearity. In section 4.4, we show that this pooling function achieves better results compared to more complicated graph pooling methods such as DiffPool <ref type="bibr" target="#b67">(Ying et al., 2018)</ref>. Applying the readout function to node representations results in two graph representations, each associated with one of the views. The representations are then fed into a shared projection head f φ (.) : R d h −→ R d h , which is an MLP with two hidden layers and PReLU non-linearity, resulting in the final graph representations:</p><formula xml:id="formula_6">h α g , h β g ∈ R d h .</formula><p>At inference time, we aggregate the representations from both views in both node and graph levels by summing them up: h = h α g + h β g ∈ R n and H = H α + H β ∈ R n×d h and return them as graph and node representations, respectively, for the down-stream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>In order to train the encoders end-to-end and learn rich node and graph level representations that are agnostic to downstream tasks, we utilize the deep InfoMax <ref type="bibr" target="#b18">(Hjelm et al., 2019)</ref> approach and maximize the MI between two views by contrasting node representations of one view with graph representation of the other view and vice versa. We empirically show that this approach consistently outperforms contrasting graph-graph or multi-scale encodings on both node and graph classifications benchmarks (see section 4.4). We define the objective as follows:</p><formula xml:id="formula_7">max θ,ω,φ,ψ 1 |G| g∈G   1 |g| |g| i=1 MI h α i , h β g + MI h β i , h α g  </formula><p>(5) where θ, ω, φ, ψ are graph encoder and projection head parameters, |G| is the number of graphs in train set or number of sub-sampled graphs in transductive setting, |g| is the number of nodes in graph g, and h α i , h β g are representations of node i and graph g encoded from views α, β, respectively. MI is modeled as discriminator D(., .) : R d h × R d h −→ R that takes in a node representation from one view and a graph representation from another view, and scores the agreement between them. We simply implement the discriminator as the dot product between two representations: D( h n , h g ) = h n . h T g . We observed a slight improvement in node classification benchmarks when discriminator and projection heads are integrated into a bilinear layer. In order to decide the MI estimator, we investigate four estimators and chose the best one for each benchmark (see section 4.4).</p><p>We provide the positive samples from joint distribution</p><formula xml:id="formula_8">x p ∼ p ([X, τ α (A)] , [X, τ β (A)]</formula><p>) and the negative samples from the product of marginals</p><formula xml:id="formula_9">x n ∼ p ([X, τ α (A)])p([X, τ β (A)]</formula><p>). To generate negative samples in transductive tasks, we randomly shuffle the features <ref type="bibr" target="#b55">(Veličković et al., 2019)</ref>. Finally, we optimize the model parameters with respect to the objective using mini-batch stochastic gradient descent. Assuming a set of training graphs G where a sample graph g = (A, X) ∈ G consists of an adjacency matrix A ∈ {0, 1} n×n and initial node features X ∈ R n×dx , our proposed multi-view representation learning algorithm is summarized as follows:</p><p>Algorithm 1 Contrastive multi-view graph representation learning algorithm. Input: Augmentations τ α and τ β , sampler Γ, pooling P, discriminator D, loss L, encoders g θ , g ω , f ψ , f φ , and training graphs {G|g = (X, A) ∈ G} for sampled batch {g k } N k=1 ∈ G do // compute encodings:</p><formula xml:id="formula_10">for k = 1 to N do X k , A k = Γ(g k ) // sub-sample graph V α k = τ α (A k ) // first view Z α k = g θ (X k , V α k ) // node rep. H α k = f ψ (Z α k ) // projected node rep. h α k = f φ (P (Z α k ))</formula><p>// projected graph rep.</p><formula xml:id="formula_11">V β k = τ β (A k ) // second view Z β k = g ω X k , V β k // node rep. H β k = f ψ Z β k // projected node rep. h β k = f φ P Z β k // projected graph rep.</formula><p>end // compute pairwise similarity:</p><p>for i = 1 to N and j = 1 to N do</p><formula xml:id="formula_12">s α ij = D h α i , H β j , s β ij = D h β i , H α j end // compute gradients: θ,ω,φ,ψ 1 N 2 N i=1 N j=1 L s α ij + L s β ij end return H α g + H β g , h α g + h β g , ∀g ∈ G</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks</head><p>We use three node classification and five graph classification benchmarks widely used in the literature <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b54">Veličković et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b49">Sun et al., 2020)</ref>. For node classification, we use Citeseer, Cora, and Pubmed citation networks <ref type="bibr" target="#b45">(Sen et al., 2008)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Protocol</head><p>For both node and graph classification benchmarks, we evaluate the proposed approach under the linear evaluation protocol and for each task, we closely follow the experimental protocol of the previous state-of-the-art approaches.</p><p>For node classification, we follow DGI and report the mean classification accuracy with standard deviation on the test nodes after 50 runs of training followed by a linear model. For graph classification, we follow InfoGraph and report the mean 10-fold cross validation accuracy with standard deviation after 5 runs followed by a linear SVM. The linear classifier is trained using cross validation on training folds of data and the best mean classification accuracy is reported. Moreover, for node classification benchmarks, we evaluate the proposed method under clustering evaluation protocol and cluster the learned representations using K-Means algorithm. Similar to <ref type="bibr" target="#b41">(Park et al., 2019)</ref>, we set the number of clusters to the number of ground-truth classes and report the average normalized MI (NMI) and adjusted rand index (ARI) for 50 runs.</p><p>We initialize the parameters using Xavier initialization <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010</ref>) and train the model using Adam optimizer <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2014)</ref> with an initial learning rate of 0.001. To have fair comparisons, we follow InfoGraph for graph classification and choose the number of GCN layers, number of epochs, batch size, and the C parameter of the SVM from <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">12]</ref>, <ref type="bibr">[10,</ref><ref type="bibr">20,</ref><ref type="bibr">40,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>, and [10 −3 , 10 −2 , ..., 10 2 , 10 3 ], respectively. For node classification, we follow DGI and set the number of GCN layers and the number of epochs and to 1 and 2,000, respectively, and choose the batch size from <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">8]</ref>. We also use early stopping with a patience of 20. Finally, we set the size of hidden dimension of both node and graph representations to 512. For chosen hyper-parameters see Appendix.  <ref type="bibr">2017)</ref> 83.2 ± 9.6 60.2 ± 6.9 71.1 ± 0.5 50.4 ± 0.9 75.8 ± 1.0 INFOGRAPH (SUN ET AL., <ref type="bibr">2020)</ref> 89.0 ± 1.1 61.7 ± 1.4 73.0 ± 0.9 49.7 ± 0.5 82.5 ± 1.4 OURS 89.7 ± 1.1 62.5 ± 1.7 74.2 ± 0.7 51.2 ± 0.5 84.5 ± 0.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>To evaluate node classification under the linear evaluation protocol, we compare our results with unsupervised models including DeepWalk <ref type="bibr" target="#b42">(Perozzi et al., 2014)</ref> and DGI. We also train a GAE <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref>, a variant of DGI with a GDC encoder, and a variant of VERSE <ref type="bibr" target="#b53">(Tsitsulin et al., 2018)</ref> by minimizing KL-divergence between node representations and diffusion matrix. Furthermore, we compare our results with supervised models including an MLP, iterative classification algorithm (ICA) <ref type="bibr" target="#b34">(Lu &amp; Getoor, 2003)</ref>, label propagation (LP) <ref type="bibr" target="#b70">(Zhu et al., 2003)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref>, semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b60">(Weston et al., 2012)</ref>, Planetoid <ref type="bibr" target="#b66">(Yang et al., 2016)</ref>, Chebyshev <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref>, mixture model networks (MoNet) <ref type="bibr" target="#b35">(Monti et al., 2017)</ref>, JK-Net, GCN, and GAT. The results reported in Table <ref type="table" target="#tab_2">2</ref> show that we achieve state-of-the-art results with respect to previous unsupervised models. For example, on Cora, we achieve 86.8% accuracy, which is a 5.5% relative improvement over previous state-of-the-art. When compared to supervised baselines, we outperform strong supervised baselines: on Cora and PubMed benchmarks we observe 4.5% and 1.4% relative improvement over GAT, respectively.</p><p>To evaluate node classification under the clustering protocol, we compare our model with models reported in <ref type="bibr" target="#b41">(Park et al., 2019)</ref> including: variational GAE (VGAE) <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref>, marginalized GAE (MGAE) <ref type="bibr" target="#b57">(Wang et al., 2017)</ref>, adversarially regularized GAE (ARGA) and VGAE (ARVGA) <ref type="bibr" target="#b40">(Pan et al., 2018)</ref>, and GALA <ref type="bibr" target="#b41">(Park et al., 2019)</ref>.</p><p>The results shown in Table <ref type="table" target="#tab_3">3</ref> suggest that our model achieves state-of-the-art NMI and ARI scores across all benchmarks.</p><p>To evaluate graph classification under the linear evaluation protocol, we compare our results with five graph kernel methods including shortest path kernel (SP) <ref type="bibr" target="#b4">(Borgwardt &amp; Kriegel, 2005)</ref>, Graphlet kernel (GK) <ref type="bibr" target="#b46">(Shervashidze et al., 2009)</ref>, Weisfeiler-Lehman sub-tree kernel (WL) <ref type="bibr" target="#b47">(Shervashidze et al., 2011)</ref>, deep graph kernels (DGK) <ref type="bibr" target="#b65">(Yanardag &amp; Vishwana, 2015)</ref>, and multi-scale Laplacian kernel (MLG) <ref type="bibr" target="#b27">(Kondor &amp; Pan, 2016)</ref> reported in <ref type="bibr" target="#b49">(Sun et al., 2020)</ref>. We also compare with five supervised GNNs reported in <ref type="bibr" target="#b64">(Xu et al., 2019b)</ref> including GraphSAGE <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>, GCN, GAT, and two variants of GIN: GIN-0 and GIN-. Finally, We compare the results with five unsupervised methods including random walk <ref type="bibr" target="#b10">(Gärtner et al., 2003)</ref>, node2vec <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016)</ref>, sub2vec <ref type="bibr" target="#b0">(Adhikari et al., 2018)</ref>, graph2vec <ref type="bibr" target="#b36">(Narayanan et al., 2017)</ref>, and In-foGraph. The results shown in Table <ref type="table" target="#tab_4">4</ref> suggest that our approach achieves state-of-the-art results with respect to unsupervised models. For example, on Reddit-Binary <ref type="bibr" target="#b65">(Yanardag &amp; Vishwana, 2015)</ref>, it achieves 84.5% accuracy, i.e., a 2.4% relative improvement over previous state-of-the-art.</p><p>Our model also outperforms kernel methods in 4 out of 5 datasets and also outperforms best supervised model in one of the datasets. When compared to supervised baselines individually, our model outperforms GCN and GAT models in 3 out of 5 datasets, e.g., a 5.3% relative improvement over GAT on IMDB-Binary dataset.</p><p>It is noteworthy that we achieve the state-of-the-art results on both node and graph classification benchmarks using a unified approach and unlike previous unsupervised models <ref type="bibr" target="#b55">(Veličković et al., 2019;</ref><ref type="bibr" target="#b49">Sun et al., 2020)</ref>, we do not devise a specialized encoder for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">EFFECT OF MUTUAL INFORMATION ESTIMATOR</head><p>We investigated four MI estimators including: noisecontrastive estimation (NCE) <ref type="bibr" target="#b14">(Gutmann &amp; Hyvärinen, 2010;</ref><ref type="bibr" target="#b38">Oord et al., 2018)</ref>, Jensen-Shannon (JSD) estimator following formulation in <ref type="bibr" target="#b37">(Nowozin et al., 2016)</ref>, normalized temperature-scaled cross-entropy (NT-Xent) <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>, and Donsker-Varadhan (DV) representation of the KL-divergence <ref type="bibr" target="#b7">(Donsker &amp; Varadhan, 1975)</ref>. The results shown in Table <ref type="table" target="#tab_6">5</ref> suggests that Jensen-Shannon estimator achieves better results across all graph classification benchmarks, whereas in node classification benchmarks, NCE achieves better results in 2 out of 3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">EFFECT OF CONTRASTIVE MODE</head><p>We considered five contrastive modes including: localglobal, global-global, multi-scale, hybrid, and ensemble modes. In local-global mode we extend deep InfoMax <ref type="bibr" target="#b18">(Hjelm et al., 2019)</ref> and contrast node encodings from one view with graph encodings from another view and vice versa. Global-global mode is similar to <ref type="bibr" target="#b32">(Li et al., 2019;</ref><ref type="bibr" target="#b51">Tian et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2020)</ref>   best results <ref type="bibr" target="#b51">(Tian et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2020)</ref>, whereas for graphs, contrasting node and graph encodings achieves better performance for both node and graph classification tasks, and (2) contrasting multi-scale encodings helps visual representation learning <ref type="bibr" target="#b2">(Bachman et al., 2019)</ref> but has a negative effect on graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">EFFECT OF VIEWS</head><p>We investigated four structural views including adjacency matrix, PPR and heat diffusion matrices, and pair-wise distance matrix where adjacency conveys local information and the latter three capture global information. Adjacency matrix is processed into a symmetrically normalized adjacency matrix (see section 3.2), and PPR and heat diffusion matrices are computed using Eq. ( <ref type="formula" target="#formula_3">3</ref>) and (2), respectively. The shortest pair-wise distance matrix is computed by Floyd-Warshall algorithm, inversed in an element-wise fashion, and row-normalized using softmax. All views are computed once in preprocessing. The results shown in Table <ref type="table" target="#tab_6">5</ref> suggest that contrasting encodings from adjacency and PPR views performs better across the benchmarks.</p><p>Furthermore, we investigated whether increasing the number of views increases the performance on down-stream tasks, monotonically. Following <ref type="bibr" target="#b51">(Tian et al., 2019)</ref>, we extended number of views to three by anchoring the main view on adjacency matrix and considering two diffusion matrices as other views. The results (see Appendix) suggest that unlike visual representation learning, extending the views does not help. We speculate this is because different diffusion matrices carry similar information about the graph structure. We also followed <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> and randomly sampled 2 out of 4 views for each training sample in each mini-batch and contrasted the representation. We observed that unlike visual domain, it degraded the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">NEGATIVE SAMPLING &amp; REGULARIZATION</head><p>We investigated the performance with respect to batch size where a batch of size N consists of N − 1 negative and 1 positive examples. We observed that in graph classification, increasing the batch size slightly improves the performance, whereas, in node classification, it does not have a significant effect. Thus we opted for efficient smaller batch sizes.</p><p>To generate negative samples in node classification, we considered two corruption functions: (1) random feature permutation, and (2) adjacency matrix corruption. We observed that applying the former achieves significantly better results compared to the latter or a combination of both.</p><p>Furthermore, we observed that applying normalization layers such as BatchNorm <ref type="bibr" target="#b19">(Ioffe &amp; Szegedy, 2015)</ref> or Layer-Norm <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, or regularization methods such as adding Gaussian noise, L2 regularization, or dropout <ref type="bibr" target="#b48">(Srivastava et al., 2014)</ref> during the pre-training degrades the performance on down-stream tasks (except early stopping).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a self-supervised approach for learning node and graph level representations by contrasting encodings from two structural views of graphs including first-order neighbors and a graph diffusion. We showed that unlike visual representation learning, increasing the number of views or contrasting multi-scale encodings do not improve the performance. Using these findings, we achieved new state-of-the-art in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol and outperformed strong supervised baselines in 4 out of 8 benchmarks. In future work, we are planning to investigate large pre-training and transfer learning capabilities of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of classification benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell>Node</cell><cell></cell><cell></cell><cell></cell><cell>Graph</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">CORA CITESEER PUBMED MUTAG PTC-MR IMDB-BIN IMDB-MULTI REDDIT-BIN</cell></row><row><cell>| GRAPHS|</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>188</cell><cell>344</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell></row><row><cell>| NODES|</cell><cell>3,327</cell><cell>2,708</cell><cell>19,717</cell><cell>17.93</cell><cell>14.29</cell><cell>19.77</cell><cell>13.0</cell><cell>508.52</cell></row><row><cell>| EDGES|</cell><cell>4,732</cell><cell>5,429</cell><cell>44,338</cell><cell>19.79</cell><cell>14.69</cell><cell>193.06</cell><cell>65.93</cell><cell>497.75</cell></row><row><cell>| CLASSES|</cell><cell>6</cell><cell>7</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Mean node classification accuracy for supervised and unsupervised models. The input column highlights the data available to each model during training (X: features, A: adjacency matrix, S: diffusion matrix, Y: labels).</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>INPUT</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell></row><row><cell></cell><cell>MLP (VELI ČKOVI Ć ET AL., 2018)</cell><cell>X, Y</cell><cell>55.1</cell><cell>46.5</cell><cell>71.4</cell></row><row><cell></cell><cell>ICA (LU &amp; GETOOR, 2003)</cell><cell>A, Y</cell><cell>75.1</cell><cell>69.1</cell><cell>73.9</cell></row><row><cell></cell><cell>LP (ZHU ET AL., 2003)</cell><cell>A, Y</cell><cell>68.0</cell><cell>45.3</cell><cell>63.0</cell></row><row><cell>SUPERVISED</cell><cell cols="3">MANIREG (BELKIN ET AL., 2006) SEMIEMB (WESTON ET AL., 2012) PLANETOID (YANG ET AL., 2016) CHEBYSHEV (DEFFERRARD ET AL., 2016) X, A, Y 81.2 X, A, Y 59.5 X, Y 59.0 X, Y 75.7 GCN (KIPF &amp; WELLING, 2017) X, A, Y 81.5</cell><cell>60.1 59.6 64.7 69.8 70.3</cell><cell>70.7 71.7 77.2 74.4 79.0</cell></row><row><cell></cell><cell>MONET (MONTI ET AL., 2017)</cell><cell cols="2">X, A, Y 81.7 ± 0.5</cell><cell>−</cell><cell>78.8 ± 0.3</cell></row><row><cell></cell><cell>JKNET (XU ET AL., 2018)</cell><cell cols="4">X, A, Y 82.7 ± 0.4 73.0 ± 0.5 77.9 ± 0.4</cell></row><row><cell></cell><cell>GAT (VELI ČKOVI Ć ET AL., 2018)</cell><cell cols="4">X, A, Y 83.0 ± 0.7 72.5 ± 0.7 79.0 ± 0.3</cell></row><row><cell>UNSUPERVISED</cell><cell>LINEAR (VELI ČKOVI Ć ET AL., 2019) DEEPWALK (PEROZZI ET AL., 2014) GAE (KIPF &amp; WELLING, 2016) VERSE (TSITSULIN ET AL., 2018) DGI (VELI ČKOVI Ć ET AL., 2019) DGI (VELI ČKOVI Ć ET AL., 2019) OURS</cell><cell>X X, A X, A X, S, A X, A X, S X, S, A</cell><cell cols="3">47.9 ± 0.4 49.3 ± 0.2 69.1 ± 0.3 70.7 ± 0.6 51.4 ± 0.5 74.3 ± 0.9 71.5 ± 0.4 65.8 ± 0.4 72.1 ± 0.5 72.5 ± 0.3 55.5 ± 0.4 − 82.3 ± 0.6 71.8 ± 0.7 76.8 ± 0.6 83.8 ± 0.5 72.0 ± 0.6 77.9 ± 0.3 86.8 ± 0.5 73.3 ± 0.5 80.1 ± 0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance on node clustering task reported in normalized MI (NMI) and adjusted rand index (ARI) measures.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>CORA</cell><cell></cell><cell cols="2">CITESEER</cell><cell>PUBMED</cell></row><row><cell></cell><cell></cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>UNSUPERVISED</cell><cell cols="6">VGAE (KIPF &amp; WELLING, 2016) 0.3292 0.2547 0.2605 0.2056 0.3108 0.3018 MGAE (WANG ET AL., 2017) 0.5111 0.4447 0.4122 0.4137 0.2822 0.2483 ARGA (PAN ET AL., 2018) 0.4490 0.3520 0.3500 0.3410 0.2757 0.2910 ARVGA (PAN ET AL., 2018) 0.4500 0.3740 0.2610 0.2450 0.1169 0.0777 GALA (PARK ET AL., 2019) 0.5767 0.5315 0.4411 0.4460 0.3273 0.3214 OURS 0.6291 0.5696 0.4696 0.4497 0.3609 0.3386</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Mean 10-fold cross validation accuracy on graphs for kernel, supervised, and unsupervised methods.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>MUTAG</cell><cell>PTC-MR</cell><cell>IMDB-BIN</cell><cell>IMDB-MULTI</cell><cell>REDDIT-BIN</cell></row><row><cell>KERNEL</cell><cell>SP (BORGWARDT &amp; KRIEGEL, 2005) GK (SHERVASHIDZE ET AL., 2009) WL (SHERVASHIDZE ET AL., 2011) DGK (YANARDAG &amp; VISHWANA, 2015)</cell><cell>85.2 ± 2.4 81.7 ± 2.1 80.7 ± 3.0 87.4 ± 2.7</cell><cell cols="2">58.2 ± 2.4 55.6 ± 0.2 57.3 ± 1.4 65.9 ± 1.0 58.0 ± 0.5 72.3 ± 3.4 60.1 ± 2.6 67.0 ± 0.6</cell><cell>38.0 ± 0.3 43.9 ± 0.4 47.0 ± 0.5 44.6 ± 0.5</cell><cell>64.1 ± 0.1 77.3 ± 0.2 68.8 ± 0.4 78.0 ± 0.4</cell></row><row><cell></cell><cell>MLG (KONDOR &amp; PAN, 2016)</cell><cell>87.9 ± 1.6</cell><cell cols="2">63.3 ± 1.5 66.6 ± 0.3</cell><cell>41.2 ± 0.0</cell><cell>−</cell></row><row><cell>SUPERVISED</cell><cell>GRAPHSAGE (HAMILTON ET AL., 2017) GCN (KIPF &amp; WELLING, 2017) GIN-0 (XU ET AL., 2019B) GIN-(XU ET AL., 2019B) GAT (VELI ČKOVI Ć ET AL., 2018)</cell><cell>85.1 ± 7.6 85.6 ± 5.8 89.4 ± 5.6 89.0 ± 6.0 89.4 ± 6.1</cell><cell cols="2">63.9 ± 7.7 72.3 ± 5.3 64.2 ± 4.3 74.0 ± 3.4 64.6 ± 7.0 75.1 ± 5.1 63.7 ± 8.2 74.3 ± 5.1 66.7 ± 5.1 70.5 ± 2.3</cell><cell>50.9 ± 2.2 51.9 ± 3.8 52.3 ± 2.8 52.1 ± 3.6 47.8 ± 3.1</cell><cell>-50.0 ± 0.0 92.4 ± 2.5 92.2 ± 2.3 85.2 ± 3.3</cell></row><row><cell>UNSUPERVISED</cell><cell cols="4">RANDOM WALK (G ÄRTNER ET AL., 2003) NODE2VEC (GROVER &amp; LESKOVEC, 2016) 72.6 ± 10.2 58.6 ± 8.0 83.7 ± 1.5 57.9 ± 1.3 50.7 ± 0.3 − SUB2VEC (ADHIKARI ET AL., 2018) 61.1 ± 15.8 60.0 ± 6.4 55.3 ± 1.5 GRAPH2VEC (NARAYANAN ET AL.,</cell><cell>34.7 ± 0.2 − 36.7 ± 0.8</cell><cell>− − 71.5 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Effect of MI estimator, contrastive mode, and views on the accuracy on both node and graph classification tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Node</cell><cell></cell><cell></cell><cell></cell><cell>Graph</cell></row><row><cell></cell><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell>MUTAG</cell><cell>PTC-MR</cell><cell>IMDB-BIN</cell><cell>IMDB-MULTI REDDIT-BIN</cell></row><row><cell>MI EST.</cell><cell>NCE JSD NT-XENT DV</cell><cell cols="6">85.8 ± 0.7 73.3 ± 0.5 80.1 ± 0.7 82.2 ± 3.2 54.6 ± 2.5 73.7 ± 0.5 50.8 ± 0.8 86.7 ± 0.6 72.9 ± 0.6 79.4 ± 1.0 89.7 ± 1.1 62.5 ± 1.7 74.2 ± 0.7 51.1 ± 0.5 86.8 ± 0.5 72.9 ± 0.6 79.3 ± 0.8 75.4 ± 7.8 51.2 ± 3.3 63.6 ± 4.2 50.4 ± 0.6 85.4 ± 0.6 73.3 ± 0.5 78.9 ± 0.8 83.4 ± 1.9 56.7 ± 2.5 72.5 ± 0.8 51.1 ± 0.5</cell><cell>79.7 ± 2.2 84.5 ± 0.6 82.0 ± 1.1 76.3 ± 5.6</cell></row><row><cell></cell><cell cols="7">LOCAL-GLOBAL 86.8 ± 0.5 73.3 ± 0.5 80.1 ± 0.7 89.7 ± 1.1 62.5 ± 1.7 74.2 ± 0.7 51.1 ± 0.5</cell><cell>84.5 ± 0.6</cell></row><row><cell>MODE</cell><cell>GLOBAL MULTI-SCALE HYBRID</cell><cell cols="6">− 83.2 ± 0.9 63.5 ± 1.5 75.7 ± 1.1 88.0 ± 0.8 56.6 ± 1.8 72.7 ± 0.4 50.6 ± 0.5 − − 85.4 ± 2.8 56.0 ± 2.1 72.4 ± 0.4 49.7 ± 0.8 − − − 86.1 ± 1.7 56.1 ± 1.4 73.3 ± 1.2 49.6 ± 0.6</cell><cell>80.8 ± 1.8 82.8 ± 0.6 78.2 ± 4.2</cell></row><row><cell></cell><cell>ENSEMBLE</cell><cell cols="6">86.2 ± 0.6 73.3 ± 0.5 79.7 ± 0.9 82.5 ± 1.9 54.0 ± 3.0 73.0 ± 0.4 49.9 ± 0.9</cell><cell>81.4 ± 1.8</cell></row><row><cell></cell><cell>ADJ-PPR</cell><cell cols="6">86.8 ± 0.5 73.3 ± 0.5 80.1 ± 0.7 89.7 ± 1.1 62.5 ± 1.7 74.2 ± 0.7 51.1 ± 0.5</cell><cell>84.5 ± 0.6</cell></row><row><cell>VIEWS</cell><cell>ADJ-HEAT ADJ-DIST PPR-HEAT PPR-DIST</cell><cell cols="6">86.4 ± 0.5 71.8 ± 0.5 77.2 ± 1.2 85.0 ± 1.9 55.8 ± 1.1 72.8 ± 0.5 50.0 ± 0.6 84.5 ± 0.6 72.7 ± 0.7 74.6 ± 1.4 87.1 ± 1.0 58.7 ± 2.2 72.0 ± 0.7 50.7 ± 0.6 85.8 ± 0.5 72.9 ± 0.5 78.1 ± 0.9 87.7 ± 1.2 57.6 ± 1.6 72.2 ± 0.6 51.2 ± 0.8 85.9 ± 0.5 73.2 ± 0.4 74.7 ± 1.2 87.1 ± 1.0 60.0 ± 2.5 72.4 ± 1.4 51.1 ± 0.8</cell><cell>81.6 ± 0.9 81.8 ± 0.7 82.3 ± 1.0 82.5 ± 1.1</cell></row><row><cell></cell><cell>HEAT-DIST</cell><cell cols="6">85.2 ± 0.4 70.4 ± 0.7 72.8 ± 0.7 87.4 ± 1.2 58.6 ± 1.7 72.2 ± 0.6 50.5 ± 0.5</cell><cell>80.3 ± 0.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Adamo Young, Adrian Butscher, and Tonya Custis for their feedback. We are also grateful for general support from Autodesk AI lab and Vector institute teams in Toronto.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature learning for subgraphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><surname>Sub2vec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8160" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data representation and learning with graph diffusion-embedding networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10414" to="10423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory-based graph networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khasahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Amethod for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weiß Enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">Learning distributed representations of graphs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><surname>F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><surname>Struc2vec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Verse: Versatile graph embeddings from similarity measures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relational graph representation learning for open-domain question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vivona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Graph Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Mgae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generating 3d mesh models from single rgb images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Pixel2mesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural graph evolution: Automatic robot design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph convolutional networks using heat kernel for semisupervised learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1928" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Semisupervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
