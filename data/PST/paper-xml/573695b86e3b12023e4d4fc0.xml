<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt&apos;s Dream and John von Neumann&apos;s Puzzle</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
							<email>egbhuang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cogn</forename><surname>Comput</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt&apos;s Dream and John von Neumann&apos;s Puzzle</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CD4A826C9D13323026420DFE56BC5EE9</idno>
					<idno type="DOI">10.1007/s12559-015-9333-0</idno>
					<note type="submission">Received: 9 April 2015 / Accepted: 27 April 2015 Ó Springer Science+Business Media New York 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Extreme learning machine</term>
					<term>Random vector functional link</term>
					<term>QuickNet</term>
					<term>Radial basis function network</term>
					<term>Feedforward neural network</term>
					<term>Randomness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The emergent machine learning techniqueextreme learning machines (ELMs)-has become a hot area of research over the past years, which is attributed to the growing research activities and significant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that (1) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes/neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes/ neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc.). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; (2) ELM is proposed for both generalized single-hidden-layer feedforward network and multi-hidden-layer feedforward networks (including biological neural networks); (3) homogeneous architecture-based ELM is proposed for feature learning, clustering, regression and (binary/multi-class) classification. (4) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi-hidden-layer feedforward networks either.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Despite that the relationships and differences between extreme learning machines (ELMs) and those earlier works (e.g., Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and RVFL <ref type="bibr" target="#b1">[2]</ref>) have been clarified in <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, recently several researchers insist that those earlier works are the ''origins'' of ELM and ELM essentially the same as those earlier works, and thus, further claimed that it is not necessary to have a new term extreme learning machines (ELMs). This paper wishes to clarify the essential elements of ELMs which may have been overlooked in the past years.</p><p>We prefer to avoid the word ''origin'' in this paper as (1) it may be really difficult to show which is the ''true'' ''origin'' in a research area as most works are related to each other and (2) it may cause unnecessary inconvenience or potential ''controversy'' among those listed as ''origins'' and other pioneering works which may have been missing in discussions. The ultimate goal of research is to find the truth of natural phenomena and to move research forward instead of arguing for being listed as ''origins.'' Otherwise, many earlier works should not have had their own terms, and instead, almost all should simply have been called ''feedforward neural networks'' or should even simply go back to Frank Rosenblatt's ''perceptrons'' <ref type="bibr" target="#b8">[9]</ref>. Such misunderstandings on ignoring the needs of having new terms would actually discourage researchers' creativeness and their spirit of telling the truth and differences in research. Similarly, there is nothing wrong to have new terms for the variants of ELM (with Fourier series nodes) with significant extensions (such as random kitchen sink (RKS) <ref type="bibr" target="#b9">[10]</ref>, RKS' further extension-FastFood <ref type="bibr" target="#b10">[11]</ref> and Convex Network <ref type="bibr" target="#b11">[12]</ref>) as well as ELM with LMS referred to as No-Prop algorithm <ref type="bibr" target="#b12">[13]</ref>.</p><p>Generally speaking, as analyzed in Huang et al. <ref type="bibr" target="#b5">[6]</ref>: '' 'Extreme' here means to move beyond conventional artificial learning techniques and to move toward brain alike learning. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism. 'Extreme learning machine (ELM)' represents a suite of machine learning techniques in which hidden neurons need not be tuned with the consideration of neural network generalization theory, control theory, matrix theory and linear system theory. <ref type="bibr">''</ref> In order to have clearer understanding of ELM, it is better to analyze ELM in the aspects of its philosophy, theories, network architecture, network neuron types and its learning objectives and algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELM's Beliefs, Philosophy and Objectives</head><p>ELM works start from our intuitive belief on biological learning and neural networks generalization performance theories <ref type="bibr" target="#b13">[14]</ref>. Further development of ELM works is also built on top of Frank Rosenblatt's multilayer 'perceptrons'' <ref type="bibr" target="#b8">[9]</ref>, SVM <ref type="bibr" target="#b14">[15]</ref>, LS-SVM <ref type="bibr" target="#b15">[16]</ref>, Fourier series, linear systems, numeral methods, matrix theories, etc., but with essential extensions.</p><p>Frank Rosenblatt <ref type="bibr" target="#b8">[9]</ref> believes that multilayer feedforward networks (perceptrons) can enable computers to ''walk, talk, see, write, reproduce itself and be conscious of its existence.'' <ref type="foot" target="#foot_0">1</ref> . Minsky and Papert <ref type="bibr" target="#b16">[17]</ref> do not believe that perceptrons have such learning capabilities by giving a counter example showing that a perceptron without having hidden layers even could not handle the simple XOR problem. Such a counter example made many researchers run away from artificial neural networks and finally resulted in the ''Artificial Intelligence (AI) winter'' in 1970s. To our understanding, there is an interesting issue between Rosenblatt's dream and Minsky's counter example. Rosenblatt may not be able to give efficient learning algorithms in the very beginning of neural networks research. Rosenblatt's perceptron is a multilayer feedforward network. In many cases, a feedforward network with input and output layers but without hidden layers is considered as a two-layer perceptron, which were actually used in Minsky and Papert <ref type="bibr" target="#b16">[17]</ref>. However, a feedforward network with input and output layers but without hidden layers seems like a ''brain'' which has input layers (eyes, noses, etc.) and output layers (motor sensors, etc.) but without ''central neurons.'' Obviously, such a ''brain'' is an empty shell and has no ''learning and cognition'' capabilities at all. However, Rosenblatt and Minsky's controversy also tells the truth that one small step of development in artificial intelligence and machine learning may request one or several generations' great efforts. Their professional controversy<ref type="foot" target="#foot_1">2</ref> may turn out to indirectly inspire the reviving of artificial neural network research in the end.</p><p>Thus, there is no doubt that neural networks research revives after hidden layers are emphasized in learning since 1980s. However, an immediate dilemma in neural network research is that since hidden layers are important and necessary conditions of learning, by default expectation and understanding of neural network research community, hidden neurons of all networks need to be tuned. Thus, since 1980s tens of thousands of researchers from almost every corner of the world have been working hard on looking for learning algorithms to train various types of neural networks mainly by tuning hidden layers. Such a kind of ''confusing'' research situation turned out to force us to seriously ask several questions as early as 1995 <ref type="bibr" target="#b17">[18]</ref>:</p><p>1. Do we really need to spend so much manpower and great effort on finding learning algorithms and manually tuning parameters for different neural networks and applications? However, obviously, in contrast there is no ''pygmy'' sitting in biological brains and tuning parameters there. 2. Do we really need to have different learning algorithms for different types of neural networks in order to achieve good learning capabilities of feature learning, clustering, regression and classification? 3. Why are biological brains more ''efficient'' and ''intelligent'' than those machines/computers embedded with artificially designed learning algorithms? 4. Are we able to address John von Neumann's puzzle<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> why ''an imperfect neural network, containing many random connections, can be made to perform reliably those functions which might be represented by idealized wiring diagrams?''</p><p>No solutions to the above-mentioned problems were found until 2003 after many years of efforts spent. Finally, we found that the key ''knot'' in the above-mentioned open problems is that 1. The counter example given by Minsky and Papert <ref type="bibr" target="#b16">[17]</ref> shows that hidden layers are necessary.</p><p>2. Earlier neural networks theories on universal approximation capabilities (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>) are also built on the assumption that hidden neurons need to be tuned during learning. 3. Thus, naturally and reasonably speaking, hidden neurons need to be tuned in artificial neural networks. In order to address the above-mentioned open problems, one has to untie the key ''knot,'' that is, for wide types of networks (artificial neural networks or biological neural networks whose network architectures and neuron modeling are even unknown to human being), hidden neurons are important but do not need to be tuned.</p><p>Our such beliefs and philosophy in both machine learning and biological learning finally result in the new techniques referred to extreme learning machines (ELMs) and related ELM theories. As emphasized in Huang et al. <ref type="bibr" target="#b5">[6]</ref>, 'Extreme' means to move beyond conventional artificial learning techniques and to move toward brain alike learning. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism. 'Extreme learning machine (ELM)' represents a suite of machine learning techniques (including single-hidden-layer feedforward networks and multi-hidden-layer feedforward networks) in which hidden neurons do not need to be tuned with the consideration of neural network generalization theory, control theory, matrix theory and linear system theory. To randomly generate hidden nodes is one of the typical implementations which ensures that ''hidden neurons do not need to be tuned'' in ELM; however, there also exist many other implementations such as kernels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>, SVD and local receptive fields <ref type="bibr" target="#b7">[8]</ref>. We believe that ELM reflects the truth of some biological learning mechanisms. Its machine-based learning efficiency was confirmed in 2004 <ref type="bibr" target="#b23">[24]</ref>, and its universal approximation capability (for ''generalized SLFNs'' in which a hidden node may be a subnetwork of several nodes and/or with almost any nonlinear piecewise continuous neurons (although their exact mathematical modeling/formula/shapes may be unknown to human beings)) was rigorously proved in theory in 2006-2008 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Its concrete biological evidence subsequently appears in 2011-2013 <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>.</p><p>ELM targets at not only ''generalized'' single-hiddenlayer feedforward networks but also ''generalized'' multihidden-layer feedforward networks in which a node may be a subnetwork consisting of other hidden nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>. Single hidden layer of ELM also covers wide types of neural networks including but not limited to sigmoid networks and RBF networks (refer to '''Generalized' Single-Hidden-Layer Feedforward Networks (SLFNs)'' section for details).</p><p>Compression, feature learning, clustering, regression and classification are fundamental to machine learning and machine intelligence. ELM aims to implement these five fundamental operations/roles of learning in homogeneous ELM architectures (cf. Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELM Theories</head><p>Although there are few attempts on random sigmoid hidden neurons and/or RBF neurons in 1950s-1990s <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>, this kind of implementations did not really ''take off'' except for RVFL <ref type="bibr" target="#b33">[34]</ref> due to several reasons:</p><p>1. The common understanding and tenet is that hidden neurons of various types of neural networks need to be tuned. 2. There lacks of theoretical analysis except for RVFL. 3. There lacks of strong motivation from biological learning except for Rosenblatt's perceptron.</p><p>ELM theories managed to address the challenging issue: ''Whether wide types of neural networks (including biological neural networks) with wide types of hidden nodes/ neurons (almost any nonlinear piecewise continuous nodes) can be randomly generated.'' Although ELM aims to deal with both single-hidden-layer feedforward networks (SLFNs) and multi-hidden-layer feedforward networks, its theories have mainly focused on SLFN cases in the past 10 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universal Approximation Capability</head><p>Strictly speaking, none of those earlier works (e.g., Baum <ref type="bibr" target="#b30">[31]</ref> and Schmidt et al. <ref type="bibr" target="#b0">[1]</ref>, RVFL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>) has addressed in theory whether random hidden nodes can be used in their specific sigmoid or RBF networks, let alone the wide type of networks covered by ELM theories. Lowe's <ref type="bibr" target="#b34">[35]</ref> RBF network does not use the random impact factor although the centers of their RBF nodes are randomly generated. One has to adjust impact factors based on applications. In other words, semi-random RBF nodes are used in RBF network <ref type="bibr" target="#b34">[35]</ref>. Detail analysis has been given in Huang <ref type="bibr" target="#b2">[3]</ref>.</p><p>Both Baum <ref type="bibr" target="#b30">[31]</ref> and Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> focus on empirical simulations on specific network architectures (a specific case of ELM models). <ref type="foot" target="#foot_3">4</ref> To the best of our knowledge, both earlier works do not have theoretical analysis, let alone the rigorous theoretical proof. Although intuitively speaking, Igelnik and Pao <ref type="bibr" target="#b31">[32]</ref> try to prove the universal approximation capability of RVFL, as analyzed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, actually Igelnik and Pao <ref type="bibr" target="#b31">[32]</ref> only prove RVFL's universal approximation capability when semi-random sigmoid and RBF hidden nodes are used, that is, the input weights a i are randomly generated, while the hidden node biases b i are calculated based on the training samples x i and the input weights a i (refer to Huang et al. <ref type="bibr" target="#b3">[4]</ref> for detail analysis).</p><p>In contrast, ELM theories have shown that almost any nonlinear piecewise continuous random hidden nodes (including sigmoid and RBF nodes mentioned in those earlier works, but also including wavelet, Fourier series and biological neurons) can be used in ELM, and the resultant networks have universal approximation capabilities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Unlike the semi-random sigmoid and RBF hidden nodes used in the proof of RVFL <ref type="bibr" target="#b31">[32]</ref> in which some parameters are not randomly generated, the physical meaning of random hidden nodes in ELM theories is that all the parameters of the hidden nodes are randomly generated independently from the training samples, e.g., both random input weights a i and biases b i for additive hidden nodes, or both centers a i and impact factor b i for RBF networks, parameters for Fourier series and wavelets, etc. It is ELM theories first time to show that all the hidden nodes/neurons can be not only independent from training samples but also independent from each other in wide types of neural networks and mathematical series/expansions as well as in biological learning mechanism <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Definition 3.1 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> A hidden layer output mapping hðxÞ ¼ ½h 1 ðxÞ; . . .; h L ðxÞ is said to be an ELM random feature mapping if all its hidden node parameters are randomly generated according to any continuous sampling distribution probability, where h i ðxÞ ¼ G i ða i ; b i ; xÞ, i ¼ 1; . . .; L (the number of neurons in the hidden layer).. Different hidden nodes may have different output functions G i . In most applications, for the sake of simplicity, same output functions can be chosen for all hidden nodes, that is, G i ¼ G j for all i; j ¼ 1; . . .; L. Theorem 3.1 (Universal approximation capability <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>) Given any non-constant piecewise continuous function as the activation function, if tuning the parameters of hidden neurons could make SLFNs approximate any target continuous function f ðxÞ, then the sequence fh i ðxÞg L i¼1 can be randomly generated according to any continuous distribution probability, and lim L!1 k P L i¼1 b i h i ðxÞ À f ðxÞk ¼ 0 holds with probability one with appropriate output weights b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Capability</head><p>In addition, ELM theories also prove the classification capability of wide types of networks with random hidden neurons, and such theories have not been studied by those earlier works. Theorem 3.2 (Classification capability <ref type="bibr" target="#b22">[23]</ref>) Given any non-constant piecewise continuous function as the activation function, if tuning the parameters of hidden neurons could make SLFNs approximate any target continuous function f ðxÞ, then SLFNs with random hidden layer mapping hðxÞ can separate arbitrary disjoint regions of any shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Hidden-Layer Feedforward Networks Versus Multi-Hidden-Layer Feedforward Networks</head><p>It is difficult to deal with multi-hidden layers of ELM directly without having complete solutions of single hidden layer of ELM. Thus, in the past 10 years, most of ELM works have been focusing on ''generalized'' single-hiddenlayer feedforward networks (SLFNs).</p><p>''Generalized'' Single-Hidden-Layer Feedforward Networks (SLFNs)</p><p>The study by Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> focuses on sigmoid networks, and the study by Pao et al. <ref type="bibr" target="#b31">[32]</ref> focuses on RVFL (with sigmoid or RBF nodes). Both have strict standard single hidden layers, which are not ''generalized'' singlehidden-layer feedforward networks (SLFNs) studied in ELM. Similar to SVM <ref type="bibr" target="#b14">[15]</ref>, the feedforward neural network with random weights proposed in Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> requires a bias in the output node in order to absorb the system error as its universal approximation capability with random sigmoid nodes was not proved when proposed:</p><formula xml:id="formula_0">f L ðxÞ ¼ X L i¼1 b i g sig ða i Á x þ b i Þ þ b<label>ð1Þ</label></formula><p>where g sig ðxÞ ¼ 1 1þexpðÀxÞ . Both QuickNet and RVFL have the direct link between the input node and the output node:</p><formula xml:id="formula_1">f L ðxÞ ¼ X L i¼1 b i g sig or RBF ða i ; b i ; xÞ þ a Á x<label>ð2Þ</label></formula><p>ELM is proposed for ''generalized'' single-hidden-layer feedforward networks and mathematical series/expansions (which may not be a conventional neural network even, such as wavelet and Fourier series):</p><formula xml:id="formula_2">f L ðxÞ ¼ X L i¼1 b i Gða i ; b i ; xÞ ð<label>3Þ</label></formula><p>The basic ELM is for generalized SLFN, unlike the fully connected networks in those earlier works, there are three levels of randomness in ELM (Fig. <ref type="figure" target="#fig_1">3</ref> for details):</p><p>1. Fully connected, hidden node parameters are randomly generated. 2. Connection can be randomly generated, not all input nodes are connected to a particular hidden node.</p><p>Possibly only some input nodes in some local field are connected to one hidden node. 3. A hidden node itself can be a subnetwork formed by several nodes which naturally forms the local receptive fields and pooling functions, and thus results in learning local features. In this sense, some local parts of a single ELM can contain multi-hidden layers.</p><p>Note Unlike Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and Pao et al. <ref type="bibr" target="#b31">[32]</ref> in which each node is a sigmoid or RBF node only, each hidden node in ELM can be a subnetwork of other nodes in which feature learning can be implemented efficiently (refer to Huang et al. <ref type="bibr" target="#b7">[8]</ref>, Figs. 2 and 3 for details).</p><p>According to ELM theories <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, ELM SLFNs include but are not limited to:</p><p>1. Sigmoid networks 2. RBF networks 3. Threshold networks <ref type="bibr" target="#b35">[36]</ref> 4. Trigonometric networks 5. Fuzzy inference systems 6. Fully complex neural networks <ref type="bibr" target="#b36">[37]</ref> 7. High-order networks 8. Ridge polynomial networks 9. Wavelet networks 10. Fourier series <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> 11. Biological neurons whose modeling/shapes may be unknown, etc.</p><p>Multi-Hidden-Layer Feedforward Networks However, unlike Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and RVFL <ref type="bibr" target="#b31">[32]</ref> which only works for single-hidden-layer feedforward networks, the ultimate tenet of ELM is: Hidden nodes of wide types of multi-hidden-layer networks do not need to be tuned (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>) (cf. Fig. <ref type="figure">4</ref>). Although multilayers of ELM concepts have been given in ELM theories in 2007 <ref type="bibr" target="#b25">[26]</ref>, it has not been used until recently (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>). In essence:</p><p>1. Rosenblatt tried to transfer learned behavior from trained rats to naive rats by the injection of brain extracts, <ref type="foot" target="#foot_4">5</ref> which may not consider the fact that different layers of neurons may play different roles. Unlike Rosenblatt's perceptron concept, we think that it is impossible to have all the layers randomly generated. If all layers in a multilayer network are randomly generated, the useful information may not pass through two or more purely random hidden layers. However, each basic ELM can be used in each hidden layer, and hidden neurons do not need to be tuned layer wise, and different layer may have different targets (in terms of ELM's five fundamental operations: compression, feature learning, clustering, regression and classification). 2. The meanings that hidden nodes do not need to be tuned are twofold:</p><p>(a) Hidden nodes may be randomly generated. (b) Although hidden nodes do not need to be randomly generated, they need not be tuned either. For example, a hidden node in the next layer can simply be a linear sum or nonlinear transform of some randomly generated nodes in the earlier layer. In this case, some nodes are randomly generated and some are not, but none of them are tuned. <ref type="bibr" target="#b7">[8]</ref> 3. Each single ELM can deal with compression, feature learning, clustering, regression or classification. Thus, a homogeneous hierarchical blocks of ELM can be built. For example, one ELM as feature learning, the next ELM works as a classifier. In this case, we have two hidden layers of ELM, overall speaking it is not randomly generated and it is ordered, but hidden nodes in each layer do not need to be tuned (e.g., randomly generated or explicitly given/calculated (Fig. <ref type="figure">4a</ref>). 4. ELM slices which play feature learning or clustering roles can also be used to link different learning models. Or as an entire networks, some layers are trained by ELM, and some are trained by other models (cf. Fig. <ref type="figure" target="#fig_3">6</ref>).</p><p>Relationship and Differences Among ELM, Deep Learning and SVM/LS-SVM ELM is different from deep Learning in the sense that hidden neurons of the entire ELM do not need to be tuned. Due to ELM's different roles of feature learning and clustering, ELM can be used as the earlier layers in multilayer networks in which the late layers are trained by other methods such as deep learning (cf. Figure <ref type="figure">5</ref>). SVM was originally proposed to handle multilayer feedforward networks by Cortes and Vapnik <ref type="bibr" target="#b14">[15]</ref> which assumes that when there is no algorithm to train a multilayer network, one can consider the output function of the last hidden layer as /ðxÞ.</p><formula xml:id="formula_3">E 1 E i E L Problem based optimization constraints 1 i L (a i ,b i ) 1 d x j</formula><p>Feature learning Clustering Regression Classification L Random Hidden Neurons (which need not be algebraic sum based) or other ELM feature mappings. Different type of output functions could be used in different neurons:</p><p>( ) ( , , )</p><formula xml:id="formula_4">i i i i h x G b a x d Input Nodes (a) E 1 E i E L Problem based optimization constraints 1 L i 1 d x j</formula><p>Feature learning Clustering Regression Classification</p><p>Hidden nodes need not be tuned. A hidden node can be a subnetwork of several nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>= Fig. <ref type="figure">2</ref> ELM theories <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> show that wide types of hidden nodes can be used in each ELM slice (ELM feature mapping) in which a hidden node can be a subnetwork of several nodes.  <ref type="table">1</ref> for detail comparisons between ELM and SVM/LS-SVM, and Huang et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> for detail analysis on the reasons why SVM and LS-SVM provide suboptimal solutions in general).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden Neuron Types</head><p>Unlike Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and Pao et al. <ref type="bibr" target="#b31">[32]</ref> in which each node is a sigmoid or RBF function, ELM is valid for wide types of neural nodes and non-neural nodes. ELM is efficient for kernel learning as well <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Domain</head><p>As ELM has universal approximation capability for a wide type of nonlinear piecewise continuous functions Gða; b; xÞ, it does not need any bias in the output layer. Some commonly used activation functions covered in ELM theories are:</p><p>1. Sigmoid function:</p><formula xml:id="formula_5">Gða; b; xÞ ¼ 1 1 þ expðÀða Á x þ bÞÞ<label>ð4Þ</label></formula><p>2. Fourier function <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref>:  </p><formula xml:id="formula_6">Gða; b; xÞ ¼ sinða Á x þ</formula><formula xml:id="formula_7">Gða; b; xÞ ¼ 1; ifa Á x À b ! 0 0; otherwise &amp;<label>ð6Þ</label></formula><p>4. Gaussian function <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_8">Gða; b; xÞ ¼ expðÀbkx À ak 2 Þ ð<label>7Þ</label></formula><p>5. Multi-quadrics function <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_9">Gða; b; xÞ ¼ ðkx À ak 2 þ b 2 Þ 1=2<label>ð8Þ</label></formula><p>6. Wavelet <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>:</p><formula xml:id="formula_10">Gða; b; xÞ ¼ kak À1=2 W x À a b<label>ð9Þ</label></formula><p>where W is a single mother wavelet function.</p><p>Remark Due to the validity of universal approximation and classification capability on general nonlinear piecewise continuous activation functions, combinations of different type of hidden neurons can be used in ELM <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Domain</head><p>According to Li et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, random hidden nodes used in ELM can be fully complex hidden nodes proposed by Kim and Adali <ref type="bibr" target="#b49">[50]</ref>, and the resultant ELM in complex domain has the universal approximation capability too. The complex hidden nodes of ELM include but are not limited to:</p><p>1. Circular functions:</p><formula xml:id="formula_11">tanðzÞ ¼ e iz À e Àiz iðe iz þ e Àiz Þ<label>ð10Þ</label></formula><formula xml:id="formula_12">sinðzÞ ¼ e iz À e Àiz 2i<label>ð11Þ</label></formula><p>2. Inverse circular functions:</p><formula xml:id="formula_13">arctanðzÞ ¼ Z z 0 dt 1 þ t 2<label>ð12Þ</label></formula><formula xml:id="formula_14">arccosðzÞ ¼ Z z 0 dt ð1 À t 2 Þ 1=2<label>ð13Þ</label></formula><p>3. Hyperbolic functions:</p><formula xml:id="formula_15">tanhðzÞ ¼ e z À e Àz e z þ e Àz<label>ð14Þ</label></formula><formula xml:id="formula_16">sinhðzÞ ¼ e z À e Àz 2<label>ð15Þ</label></formula><p>4. Inverse hyperbolic functions: However, inspired by neural networks generalization performance theories proposed in 1998 <ref type="bibr" target="#b13">[14]</ref>, which are published after Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and Pao et al. <ref type="bibr" target="#b31">[32]</ref>, ELM theory aims to reach the smallest training error but also the smallest norm of output weights <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">53]</ref> (in this sense, generally speaking, ELM is a kind of regularization neural networks but with non-tuned hidden layer mappings (formed by either random hidden nodes, kernels or other implementations)):</p><formula xml:id="formula_17">arctanhðzÞ ¼ Z z 0 dt 1 À t 2<label>ð16Þ</label></formula><formula xml:id="formula_18">arcsinhðzÞ ¼ Z z 0 dt ð1 þ t 2 Þ 1=2<label>ð17Þ</label></formula><formula xml:id="formula_19">Minimize: kbk r 1 p þ CkHb À Tk r 2 q<label>ð18Þ</label></formula><p>where r 1 [ 0; r 2 [ 0; p; q ¼ 0; 1 2 ; 1; 2; . . .; þ1. Different combinations of kbk r 1 p and kHb À Tk r 2 q can be used and result in different learning algorithms for feature learning and clustering <ref type="bibr" target="#b6">[7]</ref>. H is the ELM hidden layer output matrix (randomized matrix):</p><formula xml:id="formula_20">H ¼ hðx 1 Þ . . . hðx N Þ 2 6 4 3 7 5 ¼ Gða 1 ; b 1 ; x 1 Þ Á Á Á Gða L ; b L ; x 1 Þ . . . . . . . . . Gða 1 ; b 1 ; x N Þ Á Á Á Gða L ; b L ; x N Þ 2 6 4 3 7 5<label>ð19Þ</label></formula><p>and T is the training data target matrix:</p><formula xml:id="formula_21">T ¼ t T 1 . . . t T N 2 6 4 3 7 5 ¼ t 11 Á Á Á t 1m . . . . . . . . . t N1 Á Á Á t Nm 2 6 4 3 7 5<label>ð20Þ</label></formula><p>One can linearly apply many ELM solutions (but not all) to the specific sigmoid network with b (Schmidt et al. <ref type="bibr" target="#b0">[1]</ref>) and a network with direct link from the input layer to the output network (including but not limited to QuickNet <ref type="bibr" target="#b53">[54]</ref> and RVFL <ref type="bibr" target="#b1">[2]</ref>); suboptimal solutions will be reached compared to the original ELM. The resultant learning algorithms can be referred to ELM?b and ELM?ax, respectively (refer to Huang et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> for details).</p><p>For RVFL, the hidden layer output matrix is:</p><formula xml:id="formula_22">H RVFL ¼ g sig; RBF ða 1 ; b 1 ; x 1 Þ Á Á Á g sig; RBF ða L ; b L ; x 1 Þ x 1 . . . . . . . . . . . . g sig;RBF ða 1 ; b 1 ; x N Þ Á Á Á g sig; RBF ða L ; b L ; x N Þ x N 2 6 6 4 3 7 7 5 ¼ H ELM for sigmoid or RBF basis X NÂd ½<label>ð21Þ</label></formula><p>where H ELM for sigmoid or RBF basis are two specific ELM hidden layer output matrices <ref type="bibr" target="#b18">(19)</ref> with sigmoid or RBF basis, and X NÂd is a N Â d matrix with i-th input x i as the i-th row.</p><p>If the output neuron bias is considered as a bias neuron in the hidden layer as done in most conventional neural networks, the hidden layer output matrix for Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> will be  matrix with constant element 1. Although bias b in Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> seems like a simple parameter, however, it is known that from both mathematical machine learning point of view, a parameter may result in some significant differences. Its role has drawn researchers' attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. In fact, one of the main reasons why it is difficult to apply SVM and LS-SVM in multi-class applications in the past two decades is mainly due to the output node bias b.</p><formula xml:id="formula_23">H Schmidt; et al: ð1992Þ ¼ g sig ða 1 Á x 1 þ b 1 Þ ÁÁÁ g sig ða L Á x 1 þ b L Þ 1 Á Á Á 1 . . . . . . . . . . . . g sig ða 1 Á x N þ b 1 Þ ÁÁÁ g sig ða L Á x N þ b L Þ 1 Á Á Á 1<label>2</label></formula><formula xml:id="formula_24">Á /ðx s Þ þ b f ðxÞ ¼ P N i¼1 a i t i /ðxÞ Á /ðx i Þ þ b Multi-</formula><p>Without the output node bias b , SVM and LS-SVM solutions would become much easier <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closed-Form Solutions Versus Non-closed-Form Solutions</head><p>In many cases, closed-form solutions of ELM can be given when ( <ref type="formula" target="#formula_19">18</ref>)</p><formula xml:id="formula_25">r 1 ¼ r 2 ¼ p ¼ q ¼ 2.</formula><p>However, non-closedform solutions can also be given if <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57]</ref> or if other values are given to r 1 ,r 2 , p, and q , especially when ELM is used in the applications of compression, feature learning and clustering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. Actually the original proof on the universal approximation capability of ELM is based on non-closedform solutions of ELM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id="formula_26">r 1 ¼ r 2 ¼ p ¼ q ¼ 2 [5,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>It has been around 60 years since Frank Rosenblatt <ref type="bibr" target="#b8">[9]</ref> dreamed that his perceptron could enable computers to ''walk, talk, see, write, reproduce itself and be conscious to its existence.'' It was difficult for many researchers to believe his great dream due to lack of efficient learning algorithms and strong theoretical support in the very beginning of artificial neural network era. On the other hand, John von Neumann was puzzled <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> why ''an imperfect neural network, containing many random connections, can be made to perform reliably those functions which might be represented by idealized wiring diagrams'' <ref type="bibr" target="#b8">[9]</ref>. This paper shows that ELM theories and framework may fill such a gap between Frank Rosenblatt's dream and John von Neumann's puzzle:</p><p>1. ELM can be used to train wide type of multi-hidden layer of feedforward networks: Each hidden layer can be trained by one single ELM based on its role as feature learning, clustering, regression or classification. Entire network as a whole can be considered as a single ELM in which hidden neurons need not be tuned (refer to Fig. <ref type="figure">8</ref> for the detail summary of ELM). 2. ELM slice can be ''inserted'' into many local parts of a multi-hidden-layer feedforward network, or work together with other learning architectures/models. It is not rare to meet some cases in which intuitively speaking some techniques superficially seem similar to each other, but actually they are significantly different. ELM theories provide a unifying platform for wide types of neural networks, Fourier series <ref type="bibr" target="#b24">[25]</ref>, wavelets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, mathematical series <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, etc. Although the relationship and differences between ELM and those earlier works have clearly been discussed in the main context of this paper, in response to the anonymous malign letter, some more background and discussions need to be highlighted in this appendix further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misunderstanding on References</head><p>Several researchers thought that ELM community has not referred to those earlier related work, e.g., Schmidt et al. <ref type="bibr" target="#b0">[1]</ref>, RVFL <ref type="bibr" target="#b1">[2]</ref> and Broomhead and Lowe <ref type="bibr" target="#b60">[61]</ref>. We wish to draw their serious attention that our earlier work (2008) <ref type="bibr" target="#b3">[4]</ref> has explicitly stated: ''Several researchers, e.g., Baum <ref type="bibr" target="#b30">[31]</ref>, Igelnik and Pao <ref type="bibr" target="#b31">[32]</ref>, Lowe <ref type="bibr" target="#b34">[35]</ref>, Broomhead and Lowe <ref type="bibr" target="#b60">[61]</ref>, Ferrari and Stengel <ref type="bibr" target="#b61">[62]</ref>, have independently found that the input weights or centers a i do not need to be tuned'' (these works were published in different years, one did not refer to the others. The study by Ferrari and Stengel <ref type="bibr" target="#b61">[62]</ref> has been kindly referred in our work although it was even published later than ELM <ref type="bibr" target="#b23">[24]</ref>). In addition, in contrast to the misunderstanding that those earlier works were not referred, we have even referred to Baum <ref type="bibr" target="#b30">[31]</ref>'s work and White's QuickNet <ref type="bibr" target="#b53">[54]</ref> in our 2008 works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, which we consider much earlier than Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and RVFL <ref type="bibr" target="#b1">[2]</ref> in the related research areas. There is also a misunderstanding that Park and Sandberg's RBF theory <ref type="bibr" target="#b20">[21]</ref> has not been referred in ELM work. In fact, Park and Sandberg's RBF theory has been referred in the proof of ELM's theories on RBF cases as early as 2006 <ref type="bibr" target="#b24">[25]</ref>.</p><p>Although we did not know Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> until 2012, we have referred to it in our work <ref type="bibr" target="#b5">[6]</ref> immediately. We spent almost 10 years (back to 1996) on proving ELM theories and may have missed some related works. However, from literature survey point of view, Baum <ref type="bibr" target="#b30">[31]</ref> may be the earliest related work we could find so far and has been referred at the first time. Although the study by Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> is interesting, the citations of Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> were almost zero before 2013 (Google Scholar), and it is not easy for his work to turn up in search engine unless one intentionally flips hundreds of search pages. Such information may not be available in earlier generation of search engine when ELM was proposed. The old search engines available in the beginning of this century were not as powerful as most search engines available nowadays and many publications were not online 10-15 years ago. As stated in our earlier work <ref type="bibr" target="#b3">[4]</ref>, Baum <ref type="bibr" target="#b30">[31]</ref> claimed that (seen from simulations) one may fix the weights of the connections on one level and simply adjust the connections on the other level, and no (significant) gain is possible by using an algorithm able to adjust the weights on both levels simultaneously. Surely, almost every researcher knows that the easiest way is to calculate the output weights by leastsquare method (closed-form) as done in Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> and ELM if the input weights are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss of Feature Learning Capability</head><p>The earlier works (Schmidt et al. <ref type="bibr" target="#b0">[1]</ref>, RVFL <ref type="bibr" target="#b1">[2]</ref>, Broomhead and Lowe <ref type="bibr" target="#b60">[61]</ref>) may lose learning capability in some cases.</p><p>As analyzed in our earlier work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, although Lowe <ref type="bibr" target="#b34">[35]</ref>'s RBF network chooses RBF network centers randomly, it uses one value b for all the impact factors in all RBF hidden nodes, and such a network will lose learning capability if the impact factor b is randomly generated. Thus, in RBF network implementation, the single value of impact factors is usually adjusted manually or based on cross-validation. In this sense, Lower's RBF network does not use random RBF hidden neurons, let alone wide types of ELM networks. 7 Furthermore, Chen et al. <ref type="bibr" target="#b63">[64]</ref> point out 7 Differences between Lowe's RBF networks and ELM have been clearly given in our earlier reply <ref type="bibr" target="#b2">[3]</ref> in response to another earlier comment letter on ELM <ref type="bibr" target="#b62">[63]</ref>. It is not clear why several researchers in their anonymous letter refer to the comment letter on ELM <ref type="bibr" target="#b62">[63]</ref> for Lowe <ref type="bibr" target="#b34">[35]</ref>'s RBF network and RVFL but do not give readers right and clear information by referring to the response <ref type="bibr" target="#b2">[3]</ref>.  that such Lowe <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b60">61]</ref>'s RBF learning may not be satisfactory and thus they proposed an alternative learning procedure to choose RBF node centers one by one in a rational way which is also different from random hidden nodes used by ELM. Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> at its original form may face difficulty in sparse data applications; however, one can linearly extend sparse ELM solutions to Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> (the resultant solution referred to ELM?b).</p><formula xml:id="formula_27">ða i Á x þ b i Þ þ b f L<label>ðxÞ</label></formula><p>ELM is efficient for auto-encoder as well <ref type="bibr" target="#b38">[39]</ref>. However, when RVFL is used for auto-encoder, the weights of the direct link between its input layer to its output layer will become a constant value one and the weights of the links between its hidden layer to its output layer will become a constant value zero; thus, RVFL will lose learning capability in auto-encoder cases. Schmidt et al. <ref type="bibr" target="#b0">[1]</ref> which has the biases in output nodes may face difficulty in autoencoder cases too.</p><p>It may be difficult to implement those earlier works (Schmidt et al. <ref type="bibr" target="#b0">[1]</ref>, RVFL <ref type="bibr" target="#b1">[2]</ref>, Broomhead and Lowe <ref type="bibr" target="#b60">[61]</ref>) in multilayer networks, while hierarchical ELM with multi-ELM each working in one hidden layer can be considered as a single ELM itself. Table <ref type="table" target="#tab_4">2</ref> summarizes the relationship and main differences between ELM and those earlier works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Fundamental operations/roles of ELM. Courtesy to the anonymous designer who provides the robot icon in Internet</figDesc><graphic coords="3,340.12,59.24,170.20,221.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig.2ELM theories<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> show that wide types of hidden nodes can be used in each ELM slice (ELM feature mapping) in which a hidden node can be a subnetwork of several nodes. a ELM slice/feature mapping with fully connected random hidden nodes. b ELM slice/ feature mapping with subnetworks</figDesc><graphic coords="6,203.68,59.24,165.40,314.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 3 .</head><label>53</label><figDesc>Fig. 5 ELM slice works as the input of other learning models</figDesc><graphic coords="7,53.95,363.11,212.68,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig.6ELM slices work with different learning models: However, each ELM slice as a fundamental learning element can be incorporated into other learning models (e.g.,<ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>). a Other learning models work between different ELM slices. b ELM slices work between different learning models</figDesc><graphic coords="8,178.51,59.24,323.32,110.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,53.92,59.24,487.68,272.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>problems of ELM and M. Brandon Westover, Harvard Medical School, USA, for the constructive comments and suggestions on the potential links between ELM and biological learning in local receptive fields.</figDesc><table><row><cell>Cogn Comput</cell></row><row><cell>LS-SVM Appendix: Further Clarification No (Maximal margin concept is a specific case of ridge regression theory used in ELM forbinary classification.) of Misunderstandings on the Relationship Between ELM and Some Earlier Works Recently, it has drawn our attention that several researchers have been making some very negative and unhelpful com-Difficult in handling auto-encoders ments on ELM in neither academic nor professional manner due to various reasons and intentions, which mainly state that ELM does not refer to some earlier works (e.g., Schmidt et al. Closed-form [1] and RVFL), and ELM is the same as those earlier works.</cell></row><row><cell>SVM It should be pointed out that these earlier works have actually No (Maximal margin concept is a specific case of been referred in our different publications on ELM as early as in 2007. It is worth mentioning that ELM actually provides a unifying learning platform for wide types of neural networks ridge regression theory used in ELM for binary classification.) and machine learning techniques by absorbing the common advantages of many seemingly isolated different research efforts made in the past 60 years, and thus, it may not be surprising to see apparent relationships between ELM and different techniques. As analyzed in this paper, the essential Difficult in handling auto-encoders 3. A hidden node in an ELM slice (a ''generalized'' SLFN) can be a network of several nodes; thus, local receptive fields can be formed. differences between ELM and those mentioned related Non-closed-form works are subtle but crucial.</cell></row><row><cell>4. In each hidden layer, input layers to hidden nodes can</cell></row><row><cell>ELMs be fully or partially randomly connected according to Yes (Consistent for feature learning, clustering, regression and binary/multi-class classification.) Efficient in feature learning (auto-encoders) and clustering incremental different continuous probability distribution function. The performance of the network stable even if a finite number of hidden neurons and their related connections change. Thus, overall speaking, from ELM theories point of view, the entire multilayers of networks are structured and ordered, but they may be seemingly ''messy'' and ''un-structured'' in a particular layer or neuron slice. ''Hard wiring'' can be randomly built locally with full connection or partial connections. Coexistence of globally structured architectures and locally random hidden neurons happen to have fundamental learning capabilities of compression, feature learning, clustering, regression and classification. This may have addressed John von Neumann's puzzle. Biological learning mechanisms are sophisticated, and we Closed-form and non-closed-form, online, sequential and believe that ''learning without tuning hidden neurons'' is</cell></row><row><cell>Table 1 continued one of the fundamental biological learning mechanisms in Properties many modules of learning systems. Furthermore, random Ridge regression theory Learning capability hidden neurons and ''random wiring'' are only two specific implementations of such ''learning without tuning hidden Solutions neurons'' learning mechanisms.</cell></row></table><note><p>mathematical</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Relationship and difference comparison among different methods: ELMs, Schmidt et al.<ref type="bibr" target="#b0">[1]</ref>, QuickNet/RVFL theories and common understanding, ELM belief: Learning can be made without tuning hidden neurons in wide type of biological learning mechanisms and wide types of neural networks</figDesc><table><row><cell>ELMs</cell></row><row><cell>Properties</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>ELM variants can be linearly extended to Schmidt et al.<ref type="bibr" target="#b0">[1]</ref> and RVFL/QuickNet instead of vice versa, the resultant algorithms are referred to as ''ELMs?b'' for Schmidt et al.<ref type="bibr" target="#b0">[1]</ref> and ''ELM?ax'' for QuickNet/RVFL</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cogn Comput</cell></row><row><cell>¼ P L i¼1 b i g sig or RBF þ a Á x</cell><cell>Standard SLFN only Standard SLFN plus direct links</cell><cell>between input layer to the</cell><cell>output layer</cell><cell>No No</cell><cell>Fully connected Fully connected</cell><cell>Sigmoid Sigmoid and RBF</cell><cell></cell><cell>No No</cell><cell></cell><cell cols="2">Real domain Real domain Â H ELM for sigmoid basis ; 1 NÂm Ã H ELM for sigmoid or RBF basis ; X NÂd Â Ã</cell><cell></cell><cell>No theoretical proof Theoretical proof for semi-</cell><cell>random sigmoid or RBF nodes</cell><cell></cell><cell>Not considered Not considered</cell><cell></cell><cell>Difficult in handling auto-encoders Lose learning capability in auto-</cell><cell>encoders</cell><cell>Closed-form Closed-form and non-closed-form</cell><cell>for QuickNet, Closed-form for</cell><cell>RVFL</cell></row><row><cell></cell><cell>''Generalized'' SLFN in which a hidden node can be a subnetwork</cell><cell></cell><cell></cell><cell>Yes</cell><cell>For both fully connected and randomly (partially) connected network</cell><cell>Wide types (sigmoid, kernel, Fourier series, etc.)</cell><cell></cell><cell>Yes</cell><cell></cell><cell>Both real and complex domains</cell><cell>H ELM</cell><cell></cell><cell>Proved for wide types of random neurons</cell><cell></cell><cell></cell><cell>Minimize :kbk r1 p þ CkHb À Tk r2 q</cell><cell></cell><cell>Efficient in feature learning (auto-encoders) and clustering</cell><cell>Closed-form and non-closed-form, sequential and incremental</cell><cell>Many (but not all)</cell></row><row><cell></cell><cell>SLFNs</cell><cell></cell><cell></cell><cell>Multilayer networks</cell><cell>Connectivity</cell><cell>Hidden node types</cell><cell>(mathematical model)</cell><cell>Hidden node types</cell><cell>(biological neurons)</cell><cell>Domain</cell><cell>Hidden layer output</cell><cell>matrix</cell><cell>Universal approximation</cell><cell>and classification</cell><cell>capability</cell><cell>Structural risk</cell><cell>minimization</cell><cell>Learning capability</cell><cell>Solutions</cell><cell>Portability</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://en.wikipedia.org/wiki/Perceptron.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Professional controversies should be advocated in academic and research environments; however, irresponsible anonymous attack which intends to destroy harmony research environment and does not help maintain healthy controversies should be refused.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>John von Neumann was also acknowledged as a ''Father of Computers.'' Cogn Comput</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Schmidt et al.<ref type="bibr" target="#b0">[1]</ref> only reported some experimental results on three synthetic toy data as usually done by many researchers in 1980s-1990s; however, it may be difficult for machine learning community to make concrete conclusions based on experimental results on toy data in most cases.Cogn Comput</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://en.wikipedia.org/wiki/Frank-Rosenblatt.Cogn Comput</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Chen et  al.<ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> provide some interesting learning algorithms for RVFL networks and suggested that regularization could be used to avoid overfitting. Their works are different from structural risk minimization and maximum margin concept adopted in SVM. ELM's regularization objective moves beyond maximum margin concept, and ELM is able to unify neural network generalization theory, structural risk minimization, control theory, matrix theory and linear system theory in ELM learning models (refer to Huang<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> for detail analysis).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Minsky and Rosenblatt's controversy may have indirectly inspired the reviving of artificial neural network research in 1980s. Their controversy turned out to show that hidden neurons are critical. Although the main stream of research has focused learning algorithms on tuning hidden neurons since 1980s, few pioneers independently studied alternative solutions (e.g., Halbert White for QuickNet, Yao-Han Pao, Boris Igelnik, and C. L. Philip Chen for RVFL, D. S. Broomhead and David Lowe for RBF networks, Corinna Cortes and Vladimir Vapnik for SVM, J. A. K. Suykens and J.</p><p>Vandewalle for LS-SVM, E. Baum, Wouter F. Schmidt, Martin A. Kraaijveld and Robert P. W. Duin for Random Weights Sigmoid Network) in 1980s-1990s. Although few of those attempts did not take off finally due to various reasons and constrains, they have been playing significant and irreplaceable roles in the relevant research history. We would appreciate their historical contributions. As discussed in Huang <ref type="bibr" target="#b5">[6]</ref>, without BP and SVM/LS-SVM, the research and applications on neural networks would never have been so intensive and extensive. We would like to thank Bernard Widrow, Stanford University, USA, for sharing his vision in neural networks and precious historical experiences with us in the past years. It is always pleasant to discuss with him on biological learning, neuroscience and Rosenblatt's work. We both feel confident that we have never been so close to natural biological learning. We would like to thank C. L. Philip Chen, University of Macau, China and Boris Igelnik, BMI Research, Inc., USA, for invaluable discussions on RVFL, Johan Suykens, Katholieke Universiteit Leuven, Belgium, for invaluable discussions on LS-SVM and Stefano Fusi, Columbia University, USA, for the discussion on the links between biological learning and ELM, and Wouter F. Schmidt and Robert P. W. Duin for the kind constructive feedback on the discussions between ELM and their 1992 work. We would also like to thank Jose Principe, University of Florida, USA, for nice discussions on neuroscience (especially on neuron layers/slices) and his invaluable suggestions on potential Fig. 8 Essential elements of ELM Cogn Comput</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feed forward neural networks with random weights</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rpw</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th IAPR international conference on pattern recognition methodology and systems</title>
		<meeting>11th IAPR international conference on pattern recognition methodology and systems<address><addrLine>Hague, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName><forename type="first">Y-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reply to comments on &apos;the extreme learning machine</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1495" to="1496" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental extreme learning machine with fully complex hidden nodes</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="576" to="583" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced random search based incremental extreme learning machine</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="3460" to="3468" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An insight into extreme learning machines: random neurons, random features and kernels</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised and unsupervised extreme learning machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jnd</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2405" to="2417" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local receptive fields based extreme learning machine</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llc</forename><surname>Kasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Vong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput Intell Mag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="386" to="408" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 neural information processing systems (NIPS2007)</title>
		<meeting>the 2007 neural information processing systems (NIPS2007)</meeting>
		<imprint>
			<date type="published" when="2007-12-06">3-6 Dec 2007</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fastfood approximating kernel expansions in loglinear time</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlo ´s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on machine learning</title>
		<meeting>the 30th international conference on machine learning<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random features for kernel deep convex network</title>
		<author>
			<persName><forename type="first">P-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international conference on acoustics, speech, and signal processing (ICASSP 2013)</title>
		<meeting>the 38th international conference on acoustics, speech, and signal processing (ICASSP 2013)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The no-prop algorithm: a new learning algorithm for multilayer neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="182" to="188" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inform Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">Jak</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Perceptrons: an introduction to computational geometry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning capability of neural networks</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic logics and the synthesis of reliable organisms from unreliable components</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automata studies</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</editor>
		<meeting><address><addrLine>Princeton</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1956">1956</date>
			<biblScope unit="page" from="43" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The general and logical theory of automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cerebral mechanisms in behavior</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeffress</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basisfunction networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extreme learning machine for regression and multiclass classification</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="529" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extreme learning machine: a new learning scheme of feedforward neural networks</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international joint conference on neural networks (IJCNN2004)</title>
		<meeting>international joint conference on neural networks (IJCNN2004)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convex incremental extreme learning machine</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3056" to="3062" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinct representations of olfactory information in different cortical centres</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sosulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cutforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Axel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">472</biblScope>
			<biblScope unit="page" from="213" to="216" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large-scale model of the functioning brain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="1202" to="1205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3844" to="3856" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The importance of mixed selectivity in complex cognitive tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page" from="585" to="590" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the capabilities of multilayer perceptrons</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Complex</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="193" to="215" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic choice of basis functions in adaptive function approximation and the functional-link net</title>
		<author>
			<persName><forename type="first">B</forename><surname>Igelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-H</forename><surname>Pao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Capabilities of four-layered feedforward neural network: four layers versus three</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tateishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="255" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal approximation with convex optimization: gimmick or reality?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Principle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput Intell Mag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive radial basis function nonlinearities and the problem of generalisation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of first IEE international conference on artificial neural networks</title>
		<meeting>first IEE international conference on artificial neural networks</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Can threshold networks be trained directly?</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst II</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="191" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully complex extreme learning machine</title>
		<author>
			<persName><forename type="first">M-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="306" to="314" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extreme learning machine for multilayer perceptron</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2424995</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representational learning with extreme learning machine for big data</title>
		<author>
			<persName><forename type="first">Llc</forename><surname>Kasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Vong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="31" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE 12th international conference on computer vision</title>
		<meeting>the 2009 IEEE 12th international conference on computer vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">29 Sept-2 Oct 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning<address><addrLine>Bellevue, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-02">28 June-2 July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond simple features: a large-scale feature search approach to unconstrained face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on automatic face and gesture recognition and workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhanced image classification with a fast-learning shallow convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vladusich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international joint conference on neural networks (IJCNN&apos;2015)</title>
		<meeting>international joint conference on neural networks (IJCNN&apos;2015)<address><addrLine>Killarney, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Traffic sign recognition using extreme learning classifier with deep convolutional features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 international conference on intelligence science and big data engineering</title>
		<meeting><address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-14">IScIDE 2015. June 14-16, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Least squares support vector machines</title>
		<author>
			<persName><forename type="first">Jak</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>World Scientific</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uniform approximation of functions with random bases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 46th annual allerton conference on communication, control, and computing</title>
		<meeting>the 2008 46th annual allerton conference on communication, control, and computing</meeting>
		<imprint>
			<date type="published" when="2008-09">Sept 2008</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Orthonormal bases of compactly supported wavelets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun Pure Appl Math</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="909" to="996" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The wavelet transform, time-frequency localization and signal analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inform Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="961" to="1005" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">OP-ELM: optimally pruned extreme learning machine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="162" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Approximation by fully complex multilayer perceptrons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1641" to="1666" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A rapid supervised learning neural network for function interpolation and approximation</title>
		<author>
			<persName><forename type="first">Clp</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1220" to="1230" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A rapid learning and dynamic stepwise updating algorithm for flat neural networks and the applications to time-series prediction</title>
		<author>
			<persName><forename type="first">Clp</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern B Cybern</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An additional hidden unit test for neglected nonlinearity in multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on neural networks</title>
		<meeting>the international conference on neural networks</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="451" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A.I. Memo</title>
		<imprint>
			<biblScope unit="volume">2001</biblScope>
			<biblScope unit="issue">011</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Artificial Intelligence Laboratory, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>CBCL Memo</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training SVMs without offset</title>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scovel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="202" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sparse bayesian extreme learning machine for multi-classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="836" to="843" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient digital implementation of extreme learning machines for classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Decherchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leoncini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst II</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sparse extreme learning machine for classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1870" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Feature selection for nonlinear models with extreme learning machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fre ´nay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Heeswijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multivariable functional interpolation and adaptive networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="355" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Smooth function approximation using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Stengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Comments on &apos;the extreme learning machine</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1494" to="1495" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Orthogonal least squares learning algorithm for radial basis function networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cfn</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="309" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
