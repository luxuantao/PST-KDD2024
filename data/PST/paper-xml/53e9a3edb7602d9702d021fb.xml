<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KinectAvatar: Fully Automatic Body Capture Using a Single Kinect</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DFKI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Will</forename><surname>Chang</surname></persName>
							<email>william.y.chang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Nöll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Didier</forename><surname>Stricker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DFKI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KinectAvatar: Fully Automatic Body Capture Using a Single Kinect</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10B695950C07C059593BD66EE37FC80D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel scanning system for capturing a full 3D human body model using just a single depth camera and no auxiliary equipment. We claim that data captured from a single Kinect is sufficient to produce a good quality full 3D human model. In this setting, the challenges we face are the sensor's low resolution with random noise and the subject's non-rigid movement when capturing the data. To overcome these challenges, we develop an improved superresolution algorithm that takes color constraints into account. We then align the super-resolved scans using a combination of automatic rigid and non-rigid registration. As the system is of low price and obtains impressive results in several minutes, full 3D human body scanning technology can now become more accessible to everyday users at home.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Three-dimensional geometric models of real world objects, especially human models, are essential for many applications such as design, virtual prototyping, quality assurance, games, and special effects. However, highly trained artists are mainly responsible for performing this modeling task, often using specialized modeling software. While 3D scanning technology has been available as an alternative for some time, it is still not used very widely for capturing models. This is because scanning devices are still expensive and often require expert knowledge for operation. And many scanning systems are limited for only rigid objects.</p><p>In this work, we present a novel approach to full 3D human body scanning which employs a single Microsoft Kinect <ref type="bibr">[1]</ref> sensor. The Kinect has a variety of advantages over existing 3D scanning devices. It can capture depth and image data at video rates without a significant dependence on specific lighting and texture conditions. Also the Kinect is compact, low-price, and as easy to use as a normal video camera.</p><p>The challenge in using a single Kinect for scanning is that it provides relatively lowresolution data (320 × 240) with a high noise level. With these data characteristics, it is difficult to produce high-quality 3D models. Devices such as the Kinect were designed for use in object detection and natural user interfaces, not for high-quality 3D scanning. In addition, since we observe our subject only from a single, fixed location, movement of the subject is unavoidable during scanning. Therefore, we must estimate and compensate for this motion in order to combine scans from different viewpoints and produce a complete model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent developments in low cost real-time depth cameras have opened up a new field for 3D content acquisition. In particular, several publications about 3D shape scanning with the Kinect have appeared. Henry et al. <ref type="bibr" target="#b2">[4]</ref> build dense 3D maps of indoor environments, and Newcombe et al. <ref type="bibr" target="#b3">[5]</ref> present a real-time 3D scanning system for arbitrary indoor scenes. These projects focus mainly on scanning static scenes of indoor environments.</p><p>Specifically concerning the problem of human body scanning, Cui et al. <ref type="bibr" target="#b4">[6]</ref> try to scan a human body with one Kinect. However, they do not use the color information, and fundamental proplem is that they can not handle non-rigid movement, so the reconstructed results in the arm and leg parts are not of high quality. The work by Weiss et al. <ref type="bibr" target="#b5">[7]</ref> estimates the body shape by fitting the parameters of a SCAPE model <ref type="bibr" target="#b6">[8]</ref> to depth data and image silhouettes from a single Kinect. In contrast, our work does not require a prior shape model and relies mainly on registration. Thus, our method reproduces personalized details such as faces or dresses from the scans. Most recently, Tong et al. <ref type="bibr" target="#b7">[9]</ref> present a system to scan a body using three Kinects and a turntable. They also utilize a global non-rigid registration algorithm which uses a rough template constructed from the first depth frame. While we share some similarities, our system setup is much simpler (using only a single Kinect), and our global registration works directly with the data without requiring a rough template.</p><p>As the raw Kinect depth data is of low resolution with high noise levels, a smoothing algorithm should be applied as a pre-processing step. Newcombe et al. <ref type="bibr" target="#b3">[5]</ref> apply a bilateral filter <ref type="bibr" target="#b8">[10]</ref> to the raw Kinect depth map to obtain a discontinuity preserved depth map with reduced noise. Schuon et al. <ref type="bibr" target="#b9">[11]</ref> develop a super-resolution algorithm (LidarBoost) to improve the depth resolution and data quality of a ToF range scan, and Cui et al. <ref type="bibr" target="#b0">[2]</ref> further develop this method. In this paper, we compare these existing methods and provide a new super-resolution algorithm for the Kinect depth and color data. This improves the resolution, reduces the noise, and preserves shape detail. The global rigid registration problem is an important topic in object scanning. Iterative Closest Points (ICP) and its variants <ref type="bibr" target="#b10">[12]</ref> can solve the local rigid alignment problem. Global rigid alignment techniques <ref type="bibr" target="#b11">[13]</ref>[14] <ref type="bibr" target="#b13">[15]</ref> can be used to register the scans against each other and solve the well-known loop closure problem. However, the input data used in these algorithms are acquired using structured light or laser scanning, which has high resolution and little noise. Cui et al. <ref type="bibr" target="#b0">[2]</ref> present a probabilistic scan alignment approach that explicitly takes the sensor's noise characteristics into account. However, this method can only solve a local alignment task. We improve upon their approach and develop a probabilistic global alignment algorithm.</p><p>Even after the global rigid alignment, the scans do not align well because of the human bodies' non-rigid deformation during scanning (especially in the arms and legs). Aligning these scans is a challenging task that often requires high quality scan data and small changes of the pose in each scan <ref type="bibr">[3][16]</ref>[17] <ref type="bibr" target="#b16">[18]</ref>[19] <ref type="bibr">[20][21]</ref>. While most of these approaches mainly deal with high-resolution scan data, we propose a robust nonrigid global alignment that can work well even with the Kinect's resolution and noise levels. Since the human body is largely articulated, we build on the global articulated registration work by Chang and Zwicker <ref type="bibr" target="#b1">[3]</ref> and significantly improve the algorithm using a probabilistic scan alignment approach robust to the noise of the scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>First, we give an overview of our scanning system, easily built as shown in Fig. <ref type="figure" target="#fig_0">1a</ref>. The user stands before a Kinect in a range of about 2 meters, such that the full body falls in the Kinect's range of view. Then, the user simply turns around 360 degrees for about 20 to 30 seconds while maintaining an approximate "T" pose.</p><p>While scanning, we capture s = 10 frames as one view "chunk" with depth maps D = (D 1 , . . . , D s ) and color maps C = (C 1 , . . . , C s ). We then stop 0.5 seconds, capture another chunk, stop, capture, and so on until the human body has turned completely. The capture process yields about 10 raw data frames for each view, with the body turning approximately 30 degrees between the views. This ensures that there are enough overlapping areas for registration while providing full 360 degree coverage. The depth maps and color maps are calibrated and aligned (calibration off-line). In total, we get K chunks, where K = 30 ∼ 40. Since the Kinect captures at about 30 FPS, the displacement is relatively small within a single chunk.</p><p>The resulting range data is then processed by our reconstruction algorithm, comprised of the following steps (Fig. <ref type="figure" target="#fig_0">1b</ref>). First, a super-resolution step takes each chunk as input, and produces a single super-resolved depth scan with both greater depth accuracy and resolution (Sec. 4). Second, global rigid and non-rigid alignment steps combine the super-resolved scans into a final model (Sec. 5). Here, by "global" we mean that all of the frames in the sequence are aligned together. After the alignment is complete, we reconstruct a single mesh using Poisson mesh reconstruction <ref type="bibr" target="#b20">[22]</ref> as a post processing step. Finally, textures are created with the Kinect's raw color data (Sec. 6). Our results show that this system can capture impressive 3D human shapes with personalized geometric details. In subequent sections, we describe each step of the system in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Super-Resolution</head><p>We apply our super-resolution algorithm to each chunk of depth images, yielding K new super-resolved depth maps H with much higher X/Y resolution and less noise. Our algorithm is largely based on Cui et al <ref type="bibr" target="#b21">[23]</ref>.</p><p>First, we align all depth and color maps of a chunk to its middle frame using optical flow. This is sufficiently accurate since the maximum viewpoint displacements throughout the entire chunk are typically one to three pixels. This corresponds to a turning speed of aobut 30 seconds per revolution by the user. Second, we extract a highresolution denoised depth map H from the aligned low resolution depth and color maps by optimizing the following objective function:</p><formula xml:id="formula_0">min H E data (D 1 , . . . , D s , C 1 , . . . , C s , H ) + γE reg (H ) ,</formula><p>(1)</p><formula xml:id="formula_1">E data = s k=1 W k • (H -f (D k )) 2 W k = 1 C k -1 s s k=1 C k . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Here, we upsample the depth data by a factor β = 2 in both X and Y dimensions. After the optimization, the depth map</p><formula xml:id="formula_3">H is converted into a 3D point cloud Y = {y j | j = 1 . . . β 2 N X N Y } using the Kinect's intrinsic parameters after optimization (N X , N Y</formula><p>are the resolution of the raw depth data).</p><p>To explain the optimization further, E data measures the agreement of H with the low resolution depth maps. Here, f is a function, which upsamples the low resolution D to higher resolution of β 2 N X N Y and align to the center depth frame. We performed experiments to determine the best resampling strategy. It turned out that a nearest neighbor sampling from the low resolution images is preferable over any type of interpolated sampling. Interpolation implicitly introduces unwanted blurring that leads to a less accurate reconstruction of high-frequency shape details in the superresolved result. W k is a per-pixel weight that measures the quality of the optical flow alignment, • denotes element-wise multiplication, and 2 denotes sum of the square normal for each pixel.</p><p>The 1 • notation for W k means that we invert the value per pixel. Since the human is moving in 3D space but optical flow just considers the movement in 2D image space, some points are not aligned properly in each chunk. The weight term W k quantifies this by producing a larger value if the color in each raw frame is similar to the average color; otherwise, the point is not correctly aligned and the weight is smaller. Finally, the E reg term is a feature-preserving smoothing regularizer tailored to the Kinect depth data, with γ as the weighting coefficient. We use the same definition as <ref type="bibr" target="#b21">[23]</ref>, with their four different versions of regularizers that are inspired by image processing counterparts <ref type="bibr" target="#b22">[24]</ref>: linear, square nonlinear, isotropic nonlinear and anisotropic nonlinear. Compared to previous work <ref type="bibr" target="#b21">[23]</ref>, the new contribution is the weight term W k which allows the optimization to the color into account as an additional constraint.</p><p>Our implementation uses the Euler-Lagrange equation to transform the optimization problem into a linear system of equations, which we then solve using Gauss-Seidel <ref type="bibr" target="#b21">[23]</ref>. The runtime is shown in Tab. 1 with a C++ implementation.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> compares the difference between super-resolution without the color constraint (Fig. <ref type="figure" target="#fig_1">2b</ref>) and with the color constraint (Fig. <ref type="figure" target="#fig_1">2d</ref>). Incorporating the color constraint gives a smoother surface while preserving important shape detail. It also produces a result that compares more favorably to ground truth Fig. <ref type="figure">5c</ref>. Here, the errors are shown in the color-coded error plots ( Fig. <ref type="figure" target="#fig_1">2c</ref> and Fig. <ref type="figure" target="#fig_1">2e</ref>, error range in Fig. <ref type="figure">5e</ref>). The average Euclidean error (AEE) with the color constraint is smaller as a result.</p><p>We also demonstrate the effect of the color constraint for a human scan. Fig. <ref type="figure" target="#fig_2">3b</ref> shows results using the LidarBoost <ref type="bibr" target="#b9">[11]</ref> filter (square regularization), and Fig. <ref type="figure" target="#fig_2">3c</ref> shows results using the anisotropic nonlinear filter <ref type="bibr" target="#b21">[23]</ref>. In general, anisotropic performs best because it employes a diffusion tensor instead of a simple square regularization. Notice that for both filters, the result that uses the color constraint gives a smoother appearance while preserving important shape detail. We thus use the anisotropic nonlinear operator with the color constraint to super-resolve the point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Global Registration</head><p>We consider the human body as articulated, with rigid structures connected by joints. Aligning such point clouds is challenging, especially considering that the Kinect depth data has much noise even after super-resolution. We solve this special global registration problem inspired by two approaches. First, we incorporate the maximum-likelihood formulation described by Myronenko et al. <ref type="bibr" target="#b23">[25]</ref> and Cui et al. <ref type="bibr" target="#b0">[2]</ref> which explicitly takes into account the sensor's noise characteristics. Second, we employ the articulated model described in Chang and Zwicker <ref type="bibr" target="#b1">[3]</ref> to describe the non-rigid motion. Here, the surface motion is expressed in terms of an articulated model. We extend their ideas to our setting and develop an approach for global rigid and non-rigid registration for noisy Kinect data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Problem Setup</head><p>The input to registration are 3D point clouds Y f = {y f,n | n = 1, . . . , N f } from each super-resolved frame f ∈ H . Here, N f is number of points in frame f . We need to solve for the motion of each frame so that all frames align to each other. To parameterize the motion, we define a set of rigid transformations M 0 f , M 1 f , M 2 f , . . . for each frame f . This set will describe the motion of the frame from its original location. Since there are multiple transformations per frame, we will need to define which transformation associates with each point. We define this association as a label i(n): the index of the transformation assigned to y f,n . Therefore, M i(n) f will be the transformation for the point y f,n . This parameterization of the motion is essentially an articulated model, since the points divide into rigid parts according to their label <ref type="bibr" target="#b1">[3]</ref>.</p><p>When we set up the problem in this way, the alignment task then reduces to solving for the transformations and labels that give the best alignment possible. We model a likelihood function describing the quality of the alignment, and we maximize this likelihood for all frames simultaneously to produce the result.</p><p>Conformal Geometric Algebra. We use an exponential map based on conformal geometric algebra to express the transformations. The Euclidean transformations of a point X into X in the conformal space caused by a motor M and a reverse motor M is approximated as:</p><formula xml:id="formula_4">X = M X M = E + e ∞ (x -l • x -m), (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where E is the identity matrix, e ∞ is the point at infinity, and l, m ∈ R 3 corresponds to the rotation and traslation in the conformal geometric algebra, respectively. This formulation has the advantage that we only have 6 degrees of freedom (DoF). A formulation with a 3 × 3 ∈ R matrix and a 3 × 1 ∈ R translation vector would have 12 variables instead. A further advantage is that we can gain linear equations with respect to minimizing an energy function using the motor. For notational convenience, we use the expression</p><formula xml:id="formula_6">T i(n) f (X) to denote the transformation of point X using M i(n) f (T i(n) f (X) = M i(n) f X M i(n) f</formula><p>). For more background information for Eqn. 3 and its optimization, please refer to the references <ref type="bibr" target="#b24">[26]</ref>[27] <ref type="bibr" target="#b26">[28]</ref>.</p><p>Deformation Model. To aid the optimization, we find neighbor relations between the rigid parts and constrain such neighbors to be joined together at a common location. These locations we call "joints" and infer them during the optimization. Specifically, we use ball joint relations (3 DoF) between rigid parts, expressed by a single point denoted y f,ab ∈ R 3 relating two rigid transformations M a f and M b f . The joints constrain neighboring transformations to agree on this common ball joint location. This ensures that the rigid parts stay connected and do not drift away. We estimate the joint locations automatically using the same technique as in previous work <ref type="bibr" target="#b1">[3]</ref>.</p><p>Difference to Previous Work. Our problem setup is similar to that of Chang and Zwicker <ref type="bibr" target="#b1">[3]</ref>. However, there are notable differences in our work. First, we do not subsample the frames but instead define a label for all points in all frames. Therefore, we use the mesh connectivity in each frame to define neighborhood relationships between the points.</p><p>Second, we do not define a reference frame or employ a dynamic sample graph. The transformations operate directly on the scanned points so that every scan aligns to all others. In addition, since we associate the labels directly to the scan points, we obtain a separate, independent segmentation per frame (based on the motion occuring in the frame). This means that, unlike prior work, all frames are moving independently. Also, we have a separate set of joint locations per frame, whereas the joint locations were defined on the reference frame previously. Changing the problem setup in this manner allows us to optimize the alignment of all frames in a single optimization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Model.</head><p>The key ingredient for making the registration robust to noise and outliers is the probabilistic modeling of the point clouds. We consider each Y f to be generated from a Gaussian Mixture Model (GMM), with density as follows <ref type="bibr" target="#b0">[2]</ref>[25]:</p><formula xml:id="formula_7">p(x) = N f n=1 1 N f p (x | n) with p (x | n) ∝ N (y f,n , σ 2 I) . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>Simply speaking, p(x) gives the probability that x ∈ R 3 is generated by Y f . As the equation shows, we model the GMM using the original point set Y f and center each multi-variate Gaussian N (y f,n , σ 2 I) at the scanned points y f,n . All Gaussians share the same isotropic covariance matrix σ 2 I, with I as a 3 × 3 identity matrix and σ 2 as the variance in all directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Energy Function</head><p>We define a measure of how well the point sets align using the following function:</p><formula xml:id="formula_9">arg min M,L E data (M, L) + λE reg (M, L), (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where M is the entire transformation set and L are the labels for all the points of the K frames. E data measures the alignment distance between points in each frame, E reg constrains the labels for a smooth segmentation and also neighboring transformations to agree on a common joint location. λ is weighting coefficient.</p><p>Data Term E data . To achieve a closed model, all frames should be aligned globally with minimal distance. For each pair of frames f and g, the alignment task is performed by minimizing the negative log-likelihood based on the probabilistic model:</p><formula xml:id="formula_11">E data (M, L) = - f,g N f n=1 log Ng m=1 exp T i(n) f (y f,n )-T j(m) g (yg,m) 2 -2σ 2 f,g , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where i(n) is the index of the transformation assigned to the point y f,n , and j(m) the same for the point y g,m but based on the segmentation of frame g. The variance σ 2 f,g of mixture components is estimated separately for each point using</p><formula xml:id="formula_13">σ 2 f,g = 1 N f N near N f n=1 Nnear m∈Near(y f,n ) T i(n) f (y f,n ) -T j(m) g (y g,m ) 2 , (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>where m ∈ Near(y f,n ) denotes the indices of the nearest N near points in frame g for point y f,n . In our experiments, we use a value of N near = 20.</p><p>Regularization term E reg . There are two parts for the regularization term E reg <ref type="bibr" target="#b1">[3]</ref>. The first part is a smoothness term for the labels, which constrains neighboring points n, m to have a similar label to ensure a smooth segmentation result. If the label is not the same (i(n) = i(m)), we apply a penalty I(•) = 1 which is added to E reg . The second part is the joint constraint which ensures that the rigid parts do not drift away from each other. Each ball joint specifies that its point y f,ab should move to the same location when applying M a f and M b f . With α providing a relative weighting of the two constraints, the resulting energy function is</p><formula xml:id="formula_15">Ereg(M, L) = f ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ (n,m)∈f I (i(n) = i(m)) Label Constraint + α Joints (a,b) T a f (y f,ab ) -T b f (y f,ab ) 2 Joint Constraint ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ . (<label>8</label></formula><formula xml:id="formula_16">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expectation-Maximization</head><p>Therefore, for each of the K frames, we minimize the above energy to get a the set of transformations per frame. We use an iterative Expectation Maximization (EM) like procedure to find a maximum likelihood solution of Eqn. 5.</p><p>During the E-step, the best alignment parameters from the previous iteration are used to compute an estimate of the posterior P old f,g (m | y f,n ) of mixture components using Bayes theorem <ref type="bibr" target="#b0">[2]</ref>. This posterior is a matrix of dimension N g × N f , where each matrix entry p mn gives a conditional probability for a pair of points (y f,n , y g,m ) from frame f and g. Computing this matrix is intensive and would spend about 10 hours for two frames. It turns out that most entries are zero, and a relatively small number of pairs are actually close enough to yield a non-zero probability. Therefore, we only consider the N near closest points in frame g for each point in frame f . As mentioned earlier, we use N near = 20 which we empirically determined to be sufficient for our experiments. In addition, we compute the posterior matrix values only for four frames before and four frames after Y f . These eight neighbor frames were enough for the registration, since the subject turns continuously in our experiments. Using these approximations, we can simultaneously optimize the alignment of all frames in a reasonable computation time.</p><p>During the M-step, the new alignment parameter values are found by minimizing the negative log-likelihood function, or more specifically, its upper bound Q which evaluates to:</p><formula xml:id="formula_17">Q (M, L) = f,g N f n=1 Ng m=1 P old f,g (m | y f,n ) T i(n) f (y f,n )-T j(m) g (yg,m) 2 2σ f,g 2 . (9)</formula><p>Here, the variances σ 2 f,g in Eqn. 7 are continuously recomputed which is similar to an annealing procedure, in which the support of the Gaussians is reduced as the point sets get closer. Since the deformation parameters change after each M-step, the variances are recomputed after the M-step update.</p><p>The EM procedure converges to a local minimum of the negative log-likelihood function. To improve convergence, we optimize in two phases. In the first phase, we perform rigid registration by setting all labels i(n), n = 1, . . . , N f to be the same within each frame Y f . Thus, each frame is exactly one rigid part. We iterate the E and M steps until the transformations converge, yielding a rigid registration of all frames.</p><p>After completion of the first phase, we move to the second phase. This time, we run the EM procedure once again, but relaxing the restriction on the labels. Thus, while the E-step remains the same, we solve for both labels and transformations in the M-step.</p><p>Iterating the E and M steps in this fashion completes the non-rigid registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Non-Rigid M-Step</head><p>Since the rigid registration is essentially the same as previous work, we refer to the references for further information <ref type="bibr" target="#b0">[2]</ref>[3] <ref type="bibr" target="#b26">[28]</ref>. Instead, we describe the non-rigid M-step optimization in a little more detail.</p><p>For the non-rigid registration with joint constraints, we need to minimize the whole term given in Eqn. 9, including both the labels and the transformations. Using the rigid transformation results from the first phase as the initial input of the non-rigid energy, we perform the M-step in two sub-steps iteratively until convergence. Sub-Step 1. Fix labels L and solve for transformations M. For E data , the labels i(n) and j(m) are fixed, and the variables are the transformations M . For the regularization  E reg , only the joint constraint remains, since the labels are fixed. We use the same optimization method as the rigid registration, except that the joint constraints are added as additional terms, and we solve for more transformations simultaneously. Sub-Step 2. Fix transformations M and solve for labels L. The labels i(n) are the variables that we are solving for, and this affects the location of the points because it changes which transformation is being applied. Therefore, the goal is to re-segment the points to yield a better registration. In E reg , the joint constraint can be ingnored, and only the label constraint is left to ensure that the number of segmented parts in each frame is not too high. We solve the resulting discrete optimization problem using the α-expansion algorithm <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Texture Mapping</head><p>After the registration is complete, we reconstruct a closed mesh and apply textures to reproduce the person's appearance. In order to assemble a texture, we first compute a 2D parameterization for the reconstructed mesh. Since 3D surfaces cannot be mapped to 2D without any form of distortion, we segment the mesh in regions homeomorphic to discs that can be unfolded to the 2D domain without exceeding a certain threshold of distortion. These regions are called charts (Fig. <ref type="figure" target="#fig_4">4b</ref>). We use the method of Sander et al. <ref type="bibr" target="#b28">[30]</ref> that automatically segments the mesh into charts. Then we compute the 2D parameterization of each chart independently. To obtain a global non-overlapping parameterization of the whole mesh we pack the 2D parameterizations of each chart into a common texture atlas <ref type="bibr" target="#b29">[31]</ref> (Fig. <ref type="figure" target="#fig_4">4b</ref>).</p><p>Based on the non-rigid global alignment, we build a textured depth map for each view. This map provides the combined depth and texture information (after non-rigid registration) for each pixel of the depth camera. For each pixel p in the texture map we compute a corresponding 3D point P on the reconstructed surface using the parameterization. We project P into each textured depth map and use the difference in depth to decide whether P is visible in the respective view. The final color of p is assembled as the weighted average of the texture information from each view where P is visible. We use the scalar product of the surface normal and the viewing direction as weight. Fig. <ref type="figure" target="#fig_4">4b</ref> depicts our texture mapping result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>First, we demonstrate the accuracy of our rigid global registration with a static Lion model (Fig. <ref type="figure">5</ref>). A local ICP algorithm optimizes the transformations frame by frame, so the registration drifts and loops are not closed properly (shown in Fig. <ref type="figure">5a</ref> from a top view). However, our result (Fig. <ref type="figure">5b</ref>) computes the transformation based on a global energy function and properly solves the loop closure problem. We also quantitatively measure the reconstruction quality with a laser-scanned ground truth model. Fig. <ref type="figure">5d</ref> shows the final reconstruction result and the accuracy of the reconstruction compares favorably to the ground truth Fig. <ref type="figure">5c</ref>, as seen in the color-coded error plots (Fig. <ref type="figure">5d,</ref><ref type="figure">5e</ref>). While the ICP registration yields an error above 1cm for 50% of the points, our result yields an error below 3mm for 90% of the points. Next, we compare the non-rigid alignment algorithm to previous work. We perform two types of experiments: one testing the alignment of super-resolved scans, and one testing in extreme noise conditions using the raw Kinect scans (Fig. <ref type="figure">6</ref>). By raw data, we mean the raw output from the Kinect depth sensor without the super-resolution step. In both cases, our method gives more accurate alignment results, especially in the arm and hand regions that have much movement. Therefore, our algorithm is more robust to noise in a high-noise data scenario.</p><p>The most important reason our method is successful is the probabilistic distance model. This works very well in the presence of noise. In addition, unlike previous work <ref type="bibr" target="#b1">[3]</ref> which uses a sliding window to solve for the transformations, our method always takes all frames into account and can perform loop closure more effectively.</p><p>In most of our experiments, the real non-rigid displacement for each frame is not large. However, we demonstrate the results of a more challenging task in Fig. <ref type="figure" target="#fig_4">4a</ref>. Here, the Kinect has captured five frames of a upper body human model with waving arms. The displacement between each frame is about 20 pixels, and the width of the arm is only about 10 pixels. Even in this case, the non-rigid algorithm can still find the correct registration and joint positions. Finally, Fig. <ref type="figure">7</ref> shows our scanning results on five users. Our result reproduces the whole human structure well (especially the arms and legs), and can reconstruct detailed geometry such as the face structure and the wrinkles of the clothes. To evaluate the accuracy of the reconstruction, we compare the biometric measurements of the reconstructed human models with data from the actual people in Tab. 1. The values are the average absolute distance among eight people.</p><p>We also show average runtime statistics in Tab. 1. The whole processing time for each model is about 14 minutes on average, using an Intel(R) Xeon 2.67GHz CPU with 12GB of memory. Note that 90% of the time in our method is used for computing closest points. Previous work on human body reconstruction <ref type="bibr" target="#b5">[7]</ref> can only capture nearly naked human bodies and spends nearly one hour of computation time, and prior work on articulated registration <ref type="bibr" target="#b1">[3]</ref> computes the registration frame by frame in K minimization steps, taking nearly two hours to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we demonstrate that a full 3D human body model can be scanned with a single Kinect, which at first glance seems completely inappropriate for the task. Currently, the system requires the user to maintain a relatively awkward "T" pose while turning. Too much motion in the arms and legs can throw the registration off. And there are three main cases caused possilbe failure: 1) Not enough overlapping area for corresponding views. 2) The non-rigid movement segmentations are similar shape. 3) The segmentations change a lot in different views. To make this process more comfortable, we plan to investigate more sophisticated noise and deformation models to be able to handle larger user movements. In addition, we would like improve our algorithm to run in real-time. This would provide real-time feedback to the user about inaccurate or unfilled areas during the scanning process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Outline of our working setup and processing pipeline</figDesc><graphic coords="3,47.10,50.82,59.88,56.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Super-resolved mesh and error distribution results, produced without the color constraint (b,c) and with the color constraint (d,e). AEE denotes average Euclidean error, computed by averaging the error values over all points in the mesh.</figDesc><graphic coords="5,49.98,60.90,64.82,60.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Super-resolved meshes using the LidarBoost and Anisotropic filters. The images compare results with and without the color constraint.</figDesc><graphic coords="6,51.24,60.93,53.47,98.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Registration of upper body (b) Charts and textures on mesh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Challenging task for the non-rigid registration. Here, the joints are shown as blue balls. (b) Texture mapping.</figDesc><graphic coords="10,56.10,68.58,86.41,65.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Fig. 5. Global rigid registration for a Lion model (height 18cm). The rightmost two figures show a comparison with ground truth.</figDesc><graphic coords="11,189.81,362.31,63.85,84.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Runtime for each processing part and biometric error measurements. Timings and measurements are averaged among eight people.</figDesc><table><row><cell>Frames</cell><cell>S. R.</cell><cell cols="3">Rigid Non-Rigid Poisson</cell><cell>All</cell></row><row><cell>36</cell><cell>28 sec</cell><cell cols="2">110 sec 620 sec</cell><cell>68 sec</cell><cell>826 sec (13.8 min)</cell></row><row><cell cols="5">Neck-Hip Shoulder W. Arm L. Leg L. Waist Girth</cell><cell>Hip Girth</cell></row><row><cell>2.1cm</cell><cell>1.0cm</cell><cell>2.3cm</cell><cell>3.1cm</cell><cell>3.2cm</cell><cell>2.6cm</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D shape scanning with a time-offlight camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proc. CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global registration of dynamic range scans for articulated model reconstruction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RGB-D: Advanced Reasoning with Depth Cameras Workshop in Conjunction with RSS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISMAR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3D shape scanning with a kinect</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACM SIGGRAPH Posters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Home 3D body scans from noisy image and range data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl, Conf. Computer Vision, CVPR</title>
		<meeting>IEEE Intl, Conf. Computer Vision, CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scanning 3D full human bodies using kinects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE Virtual Reality</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Computer Vision</title>
		<meeting>the Sixth International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lidarboost: Depth superresolution for ToF 3D shape scanning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Solution for the Registration of Multiple 3D Point Sets Using Unit Quaternions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benjemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 1998</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Neumann</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1407</biblScope>
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards a general multi-view registration technique</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gagnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laurendeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="540" to="547" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online loop closure for real-time interactive 3D scanning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wismer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic geometry registration</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-rigid registration under isometric deformations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient reconstruction of non-rigid shape and motion from real-time 3D scanner data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globally consistent space-time reconstruction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>South-Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proceedings of SGP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic shapes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Animation cartography -intrinsic reconstruction of shape and motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tevs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Eurographics Symposium on Geometry Processing</title>
		<meeting>the Fourth Eurographics Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for 3D shape scanning with a depth camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Hagen</surname></persName>
		</author>
		<title level="m">Visualization and Processing of Tensor Fields</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-rigid point set registration: Coherent Point Drift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1009</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Mathematical Introduction to Robotic Manipulation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRC</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pose estimation revisited</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Universität Kiel</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pose estimation based on geometric algebra</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hildenbrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>GraVisMa</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-chart geometry images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics/ACM SIGGRAPH Symposium on Geometry Processing</title>
		<meeting>the Eurographics/ACM SIGGRAPH Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
		<respStmt>
			<orgName>SGP</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient packing of arbitrary shaped charts for automatic texture atlas generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nöll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1309" to="1317" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
